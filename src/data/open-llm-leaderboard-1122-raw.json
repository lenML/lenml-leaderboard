[
  {
    "eval_name": "0-hero_Matter-0.2-7B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/0-hero/Matter-0.2-7B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">0-hero/Matter-0.2-7B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/0-hero__Matter-0.2-7B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "0-hero/Matter-0.2-7B-DPO",
    "Model sha": "26a66f0d862e2024ce4ad0a09c37052ac36e8af6",
    "Average ‚¨ÜÔ∏è": 8.805656367208497,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6095870820618575,
    "IFEval Raw": 0.3302792147058693,
    "IFEval": 33.02792147058693,
    "BBH Raw": 0.3596254301656297,
    "BBH": 10.055525080241035,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.381375,
    "MUSR": 5.871874999999999,
    "MMLU-PRO Raw": 0.1163563829787234,
    "MMLU-PRO": 1.8173758865248217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-13",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "0-hero/Matter-0.2-7B-DPO"
  },
  {
    "eval_name": "01-ai_Yi-1.5-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-34B",
    "Model sha": "4b486f81c935a2dadde84c6baa1e1370d40a098f",
    "Average ‚¨ÜÔ∏è": 25.62131796065766,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 46,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.35169893737245,
    "IFEval Raw": 0.2841172533322695,
    "IFEval": 28.411725333226947,
    "BBH Raw": 0.5976391706360018,
    "BBH": 42.74936268839652,
    "MATH Lvl 5 Raw": 0.15181268882175228,
    "MATH Lvl 5": 15.181268882175228,
    "GPQA Raw": 0.36577181208053694,
    "GPQA": 15.436241610738257,
    "MUSR Raw": 0.4236041666666667,
    "MUSR": 11.217187500000003,
    "MMLU-PRO Raw": 0.4665890957446808,
    "MMLU-PRO": 40.732121749408975,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-34B"
  },
  {
    "eval_name": "01-ai_Yi-1.5-34B-32K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-34B-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-34B-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-34B-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-34B-32K",
    "Model sha": "2c03a29761e4174f20347a60fbe229be4383d48b",
    "Average ‚¨ÜÔ∏è": 26.677560441237244,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.577314287549456,
    "IFEval Raw": 0.3118691737922047,
    "IFEval": 31.186917379220468,
    "BBH Raw": 0.6015685776542417,
    "BBH": 43.38184666762572,
    "MATH Lvl 5 Raw": 0.15105740181268884,
    "MATH Lvl 5": 15.105740181268883,
    "GPQA Raw": 0.36325503355704697,
    "GPQA": 15.100671140939594,
    "MUSR Raw": 0.4398229166666667,
    "MUSR": 14.07786458333333,
    "MMLU-PRO Raw": 0.4709109042553192,
    "MMLU-PRO": 41.21232269503546,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-34B-32K"
  },
  {
    "eval_name": "01-ai_Yi-1.5-34B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-34B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-34B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-34B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-34B-Chat",
    "Model sha": "f3128b2d02d82989daae566c0a7eadc621ca3254",
    "Average ‚¨ÜÔ∏è": 32.89223334850039,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 250,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.211921933663872,
    "IFEval Raw": 0.6066758423205982,
    "IFEval": 60.66758423205982,
    "BBH Raw": 0.6083748310271819,
    "BBH": 44.262825981005655,
    "MATH Lvl 5 Raw": 0.24924471299093656,
    "MATH Lvl 5": 24.924471299093657,
    "GPQA Raw": 0.3649328859060403,
    "GPQA": 15.324384787472036,
    "MUSR Raw": 0.4281979166666667,
    "MUSR": 13.058072916666665,
    "MMLU-PRO Raw": 0.45204454787234044,
    "MMLU-PRO": 39.11606087470449,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-10",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-34B-Chat"
  },
  {
    "eval_name": "01-ai_Yi-1.5-34B-Chat-16K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-34B-Chat-16K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-34B-Chat-16K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-34B-Chat-16K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-34B-Chat-16K",
    "Model sha": "ff74452e11f0f749ab872dc19b1dd3813c25c4d8",
    "Average ‚¨ÜÔ∏è": 29.239909324079818,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.3870112290744174,
    "IFEval Raw": 0.456449997118756,
    "IFEval": 45.6449997118756,
    "BBH Raw": 0.6100218256499571,
    "BBH": 44.53615654671034,
    "MATH Lvl 5 Raw": 0.20392749244712993,
    "MATH Lvl 5": 20.392749244712995,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.43976041666666665,
    "MUSR": 13.736718750000001,
    "MMLU-PRO Raw": 0.45445478723404253,
    "MMLU-PRO": 39.383865248226954,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-07-15",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-34B-Chat-16K"
  },
  {
    "eval_name": "01-ai_Yi-1.5-6B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-6B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-6B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-6B",
    "Model sha": "cab51fce425b4c1fb19fccfdd96bd5d0908c1657",
    "Average ‚¨ÜÔ∏è": 16.695345587701233,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 30,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1847571621382451,
    "IFEval Raw": 0.26166017278598563,
    "IFEval": 26.166017278598567,
    "BBH Raw": 0.44925820198929056,
    "BBH": 22.027904536694773,
    "MATH Lvl 5 Raw": 0.06344410876132932,
    "MATH Lvl 5": 6.344410876132931,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.43740625,
    "MUSR": 13.309114583333335,
    "MMLU-PRO Raw": 0.31441156914893614,
    "MMLU-PRO": 23.823507683215126,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-6B"
  },
  {
    "eval_name": "01-ai_Yi-1.5-6B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-6B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-6B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-6B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-6B-Chat",
    "Model sha": "3f64d3f159c6ad8494227bb77e2a7baef8cd808b",
    "Average ‚¨ÜÔ∏è": 20.983905584895307,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9688401161038919,
    "IFEval Raw": 0.5145270105542183,
    "IFEval": 51.452701055421834,
    "BBH Raw": 0.4571311331954389,
    "BBH": 23.67872313235784,
    "MATH Lvl 5 Raw": 0.054380664652567974,
    "MATH Lvl 5": 5.438066465256798,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.43917708333333333,
    "MUSR": 14.030468750000002,
    "MMLU-PRO Raw": 0.3193151595744681,
    "MMLU-PRO": 24.368351063829788,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-10-22",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-6B-Chat"
  },
  {
    "eval_name": "01-ai_Yi-1.5-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-9B",
    "Model sha": "8cfde9604384c50137bee480b8cef8a08e5ae81d",
    "Average ‚¨ÜÔ∏è": 22.14131339736707,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7344460160408538,
    "IFEval Raw": 0.29358435617494916,
    "IFEval": 29.358435617494916,
    "BBH Raw": 0.514294179104191,
    "BBH": 30.50071699492122,
    "MATH Lvl 5 Raw": 0.11329305135951663,
    "MATH Lvl 5": 11.329305135951664,
    "GPQA Raw": 0.37919463087248323,
    "GPQA": 17.225950782997764,
    "MUSR Raw": 0.43278124999999995,
    "MUSR": 12.030989583333332,
    "MMLU-PRO Raw": 0.3916223404255319,
    "MMLU-PRO": 32.402482269503544,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-9B"
  },
  {
    "eval_name": "01-ai_Yi-1.5-9B-32K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-9B-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-9B-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-9B-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-9B-32K",
    "Model sha": "116561dfae63af90f9d163b43077629e0e916bb1",
    "Average ‚¨ÜÔ∏è": 19.784610052239916,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7840367066348469,
    "IFEval Raw": 0.23031113002389217,
    "IFEval": 23.031113002389215,
    "BBH Raw": 0.496332115988265,
    "BBH": 28.937011582169664,
    "MATH Lvl 5 Raw": 0.10649546827794563,
    "MATH Lvl 5": 10.649546827794563,
    "GPQA Raw": 0.35906040268456374,
    "GPQA": 14.541387024608499,
    "MUSR Raw": 0.4186145833333333,
    "MUSR": 10.82682291666667,
    "MMLU-PRO Raw": 0.37649601063829785,
    "MMLU-PRO": 30.721778959810877,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-9B-32K"
  },
  {
    "eval_name": "01-ai_Yi-1.5-9B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-9B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-9B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-9B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-9B-Chat",
    "Model sha": "bc87d8557c98dc1e5fdef6ec23ed31088c4d3f35",
    "Average ‚¨ÜÔ∏è": 27.89441703395684,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 134,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7267717454553025,
    "IFEval Raw": 0.6045525871354672,
    "IFEval": 60.455258713546726,
    "BBH Raw": 0.555906430281685,
    "BBH": 36.95293138417893,
    "MATH Lvl 5 Raw": 0.12764350453172205,
    "MATH Lvl 5": 12.764350453172204,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.42590625,
    "MUSR": 12.838281249999996,
    "MMLU-PRO Raw": 0.39752327127659576,
    "MMLU-PRO": 33.05814125295508,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-10",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-9B-Chat"
  },
  {
    "eval_name": "01-ai_Yi-1.5-9B-Chat-16K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-1.5-9B-Chat-16K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-1.5-9B-Chat-16K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-1.5-9B-Chat-16K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-1.5-9B-Chat-16K",
    "Model sha": "2b397e5f0fab87984efa66856c5c4ed4bbe68b50",
    "Average ‚¨ÜÔ∏è": 23.03528157450677,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 34,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7923725067008767,
    "IFEval Raw": 0.4214040966856829,
    "IFEval": 42.14040966856828,
    "BBH Raw": 0.5153383364651778,
    "BBH": 31.497608947018318,
    "MATH Lvl 5 Raw": 0.13444108761329307,
    "MATH Lvl 5": 13.444108761329307,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.40990624999999997,
    "MUSR": 10.03828125,
    "MMLU-PRO Raw": 0.39935172872340424,
    "MMLU-PRO": 33.261303191489354,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-1.5-9B-Chat-16K"
  },
  {
    "eval_name": "01-ai_Yi-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-34B",
    "Model sha": "e1e7da8c75cfd5c44522228599fd4d2990cedd1c",
    "Average ‚¨ÜÔ∏è": 22.385715135754378,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1285,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 12.828741644389773,
    "IFEval Raw": 0.3045751938190667,
    "IFEval": 30.45751938190668,
    "BBH Raw": 0.5457099951794562,
    "BBH": 35.542431259008794,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.36661073825503354,
    "GPQA": 15.548098434004473,
    "MUSR Raw": 0.4118541666666667,
    "MUSR": 9.648437500000004,
    "MMLU-PRO Raw": 0.441156914893617,
    "MMLU-PRO": 37.90632387706855,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-01",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-34B"
  },
  {
    "eval_name": "01-ai_Yi-34B-200K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-200K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-34B-200K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-34B-200K",
    "Model sha": "8ac1a1ebe011df28b78ccd08012aeb2222443c77",
    "Average ‚¨ÜÔ∏è": 19.887594167797086,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 316,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 12.75192792356476,
    "IFEval Raw": 0.15424850507763843,
    "IFEval": 15.424850507763843,
    "BBH Raw": 0.5441817925289527,
    "BBH": 36.02211028900003,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.38171874999999994,
    "MUSR": 9.414843749999998,
    "MMLU-PRO Raw": 0.45345744680851063,
    "MMLU-PRO": 39.273049645390074,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-06",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-34B-200K"
  },
  {
    "eval_name": "01-ai_Yi-34B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-34B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-34B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-34B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-34B-Chat",
    "Model sha": "2e528b6a80fb064a0a746c5ca43114b135e30464",
    "Average ‚¨ÜÔ∏è": 23.96231219963117,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 344,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.562847844990783,
    "IFEval Raw": 0.4698887839820565,
    "IFEval": 46.98887839820566,
    "BBH Raw": 0.5560872910766164,
    "BBH": 37.623987597243485,
    "MATH Lvl 5 Raw": 0.04682779456193354,
    "MATH Lvl 5": 4.682779456193354,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.39784375,
    "MUSR": 8.363802083333338,
    "MMLU-PRO Raw": 0.4093251329787234,
    "MMLU-PRO": 34.369459219858165,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-34B-Chat"
  },
  {
    "eval_name": "01-ai_Yi-6B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-6B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-6B",
    "Model sha": "7f7fb7662fd8ec09029364f408053c954986c8e5",
    "Average ‚¨ÜÔ∏è": 13.611617485376058,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 371,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5492746268500204,
    "IFEval Raw": 0.28933784580468713,
    "IFEval": 28.93378458046871,
    "BBH Raw": 0.4309230591000865,
    "BBH": 19.408504737915056,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.39368749999999997,
    "MUSR": 7.044270833333335,
    "MMLU-PRO Raw": 0.29911901595744683,
    "MMLU-PRO": 22.12433510638298,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-01",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-6B"
  },
  {
    "eval_name": "01-ai_Yi-6B-200K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-200K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-6B-200K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-6B-200K",
    "Model sha": "4a74338e778a599f313e9fa8f5bc08c717604420",
    "Average ‚¨ÜÔ∏è": 11.93315771474422,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 173,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5632119903092258,
    "IFEval Raw": 0.08433068702154728,
    "IFEval": 8.433068702154728,
    "BBH Raw": 0.42892948109603307,
    "BBH": 20.148020103768047,
    "MATH Lvl 5 Raw": 0.01435045317220544,
    "MATH Lvl 5": 1.435045317220544,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.45873958333333337,
    "MUSR": 16.842447916666668,
    "MMLU-PRO Raw": 0.2844082446808511,
    "MMLU-PRO": 20.489804964539008,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-06",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-6B-200K"
  },
  {
    "eval_name": "01-ai_Yi-6B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-6B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-6B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-6B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-6B-Chat",
    "Model sha": "01f7fabb6cfb26efeb764da4a0a19cad2c754232",
    "Average ‚¨ÜÔ∏è": 14.004356953877243,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 63,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5553328361218006,
    "IFEval Raw": 0.33952135888331847,
    "IFEval": 33.95213588833185,
    "BBH Raw": 0.41326019207548687,
    "BBH": 17.00016656742376,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.36879166666666663,
    "MUSR": 3.5656250000000003,
    "MMLU-PRO Raw": 0.3061003989361702,
    "MMLU-PRO": 22.90004432624113,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-6B-Chat"
  },
  {
    "eval_name": "01-ai_Yi-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-9B",
    "Model sha": "b4a466d95091696285409f1dcca3028543cb39da",
    "Average ‚¨ÜÔ∏è": 17.77410301729289,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 185,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7653319807085085,
    "IFEval Raw": 0.2708779372066118,
    "IFEval": 27.08779372066118,
    "BBH Raw": 0.49396075125308075,
    "BBH": 27.626956112077934,
    "MATH Lvl 5 Raw": 0.053625377643504536,
    "MATH Lvl 5": 5.362537764350454,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.40540624999999997,
    "MUSR": 8.909114583333334,
    "MMLU-PRO Raw": 0.35738031914893614,
    "MMLU-PRO": 28.59781323877068,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-01",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-9B"
  },
  {
    "eval_name": "01-ai_Yi-9B-200K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-9B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-9B-200K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-9B-200K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-9B-200K",
    "Model sha": "8c93accd5589dbb74ee938e103613508c4a9b88d",
    "Average ‚¨ÜÔ∏è": 17.74213990293211,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 75,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7744911718966523,
    "IFEval Raw": 0.23270921155866434,
    "IFEval": 23.270921155866432,
    "BBH Raw": 0.4793302602023641,
    "BBH": 26.49249509714754,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.42940625,
    "MUSR": 12.109114583333332,
    "MMLU-PRO Raw": 0.36220079787234044,
    "MMLU-PRO": 29.1334219858156,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-15",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "01-ai/Yi-9B-200K"
  },
  {
    "eval_name": "01-ai_Yi-Coder-9B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/01-ai/Yi-Coder-9B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">01-ai/Yi-Coder-9B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/01-ai__Yi-Coder-9B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "01-ai/Yi-Coder-9B-Chat",
    "Model sha": "356a1f8d4e4a606d0b879e54191ca809918576b8",
    "Average ‚¨ÜÔ∏è": 16.87269626350437,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 190,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9097661147960838,
    "IFEval Raw": 0.4817041006750976,
    "IFEval": 48.17041006750976,
    "BBH Raw": 0.48142000339111674,
    "BBH": 25.94315294491389,
    "MATH Lvl 5 Raw": 0.03323262839879154,
    "MATH Lvl 5": 3.3232628398791544,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.3991770833333333,
    "MUSR": 7.963802083333333,
    "MMLU-PRO Raw": 0.24251994680851063,
    "MMLU-PRO": 15.83554964539007,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "01-ai/Yi-Coder-9B"
  },
  {
    "eval_name": "152334H_miqu-1-70b-sf_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/152334H/miqu-1-70b-sf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">152334H/miqu-1-70b-sf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/152334H__miqu-1-70b-sf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "152334H/miqu-1-70b-sf",
    "Model sha": "1dca4cce36f01f2104ee2e6b97bac6ff7bb300c1",
    "Average ‚¨ÜÔ∏è": 29.05964337578347,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 219,
    "#Params (B)": 68,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.098985976914155,
    "IFEval Raw": 0.5181740005407873,
    "IFEval": 51.81740005407873,
    "BBH Raw": 0.6102361685099691,
    "BBH": 43.807147003691966,
    "MATH Lvl 5 Raw": 0.12235649546827795,
    "MATH Lvl 5": 12.235649546827796,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.45820833333333333,
    "MUSR": 17.209374999999998,
    "MMLU-PRO Raw": 0.42278922872340424,
    "MMLU-PRO": 35.865469858156025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "152334H/miqu-1-70b-sf"
  },
  {
    "eval_name": "1TuanPham_T-VisStar-7B-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/1TuanPham/T-VisStar-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">1TuanPham/T-VisStar-7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/1TuanPham__T-VisStar-7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "1TuanPham/T-VisStar-7B-v0.1",
    "Model sha": "b111b59971c14b46c888b96723ff7f3c7b6fd92f",
    "Average ‚¨ÜÔ∏è": 19.04410367635357,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2695126379214015,
    "IFEval Raw": 0.36070404305021786,
    "IFEval": 36.07040430502179,
    "BBH Raw": 0.5052203113352468,
    "BBH": 30.24383447882599,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.4375,
    "MUSR": 13.554166666666669,
    "MMLU-PRO Raw": 0.3210605053191489,
    "MMLU-PRO": 24.562278368794324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-22",
    "Generation": 0,
    "Base Model": "1TuanPham/T-VisStar-7B-v0.1"
  },
  {
    "eval_name": "1TuanPham_T-VisStar-v0.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/1TuanPham/T-VisStar-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">1TuanPham/T-VisStar-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/1TuanPham__T-VisStar-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "1TuanPham/T-VisStar-v0.1",
    "Model sha": "c9779bd9630a533f7e42fd8effcca69623d48c9c",
    "Average ‚¨ÜÔ∏è": 19.04410367635357,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6243844135682026,
    "IFEval Raw": 0.36070404305021786,
    "IFEval": 36.07040430502179,
    "BBH Raw": 0.5052203113352468,
    "BBH": 30.24383447882599,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.4375,
    "MUSR": 13.554166666666669,
    "MMLU-PRO Raw": 0.3210605053191489,
    "MMLU-PRO": 24.562278368794324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-20",
    "Generation": 0,
    "Base Model": "1TuanPham/T-VisStar-v0.1"
  },
  {
    "eval_name": "3rd-Degree-Burn_L-3.1-Science-Writer-8B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/3rd-Degree-Burn/L-3.1-Science-Writer-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">3rd-Degree-Burn/L-3.1-Science-Writer-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/3rd-Degree-Burn__L-3.1-Science-Writer-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "3rd-Degree-Burn/L-3.1-Science-Writer-8B",
    "Model sha": "d9bb11fb02f8eca3aec408912278e513377115da",
    "Average ‚¨ÜÔ∏è": 21.07861986755232,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7096782743049184,
    "IFEval Raw": 0.42625012743963797,
    "IFEval": 42.625012743963794,
    "BBH Raw": 0.5041306326216103,
    "BBH": 29.19930078641656,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3959479166666666,
    "MUSR": 11.693489583333331,
    "MMLU-PRO Raw": 0.36494348404255317,
    "MMLU-PRO": 29.43816489361702,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-19",
    "Generation": 1,
    "Base Model": "3rd-Degree-Burn/L-3.1-Science-Writer-8B (Merge)"
  },
  {
    "eval_name": "3rd-Degree-Burn_Llama-3.1-8B-Squareroot_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/3rd-Degree-Burn/Llama-3.1-8B-Squareroot\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">3rd-Degree-Burn/Llama-3.1-8B-Squareroot</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/3rd-Degree-Burn__Llama-3.1-8B-Squareroot-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "3rd-Degree-Burn/Llama-3.1-8B-Squareroot",
    "Model sha": "2bec01c2c5d53276eac2222c80190eb44ab2e6af",
    "Average ‚¨ÜÔ∏è": 10.581746835612421,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.987049861129138,
    "IFEval Raw": 0.22134381219608418,
    "IFEval": 22.134381219608414,
    "BBH Raw": 0.34609423326328875,
    "BBH": 8.618063681935197,
    "MATH Lvl 5 Raw": 0.2273413897280967,
    "MATH Lvl 5": 22.73413897280967,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.3089166666666667,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.17495013297872342,
    "MMLU-PRO": 8.32779255319149,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "3rd-Degree-Burn/Llama-3.1-8B-Squareroot (Merge)"
  },
  {
    "eval_name": "3rd-Degree-Burn_Llama-3.1-8B-Squareroot-v1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/3rd-Degree-Burn/Llama-3.1-8B-Squareroot-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">3rd-Degree-Burn/Llama-3.1-8B-Squareroot-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/3rd-Degree-Burn__Llama-3.1-8B-Squareroot-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "3rd-Degree-Burn/Llama-3.1-8B-Squareroot-v1",
    "Model sha": "09339d9c3b118ae3c6e7beab8b84347471990988",
    "Average ‚¨ÜÔ∏è": 7.5973619193682245,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7727492723959822,
    "IFEval Raw": 0.2892381104358657,
    "IFEval": 28.923811043586575,
    "BBH Raw": 0.33427703119251256,
    "BBH": 6.515144725982737,
    "MATH Lvl 5 Raw": 0.061933534743202415,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.3340625,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-10",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "3rd-Degree-Burn_Llama-Squared-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/3rd-Degree-Burn/Llama-Squared-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">3rd-Degree-Burn/Llama-Squared-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/3rd-Degree-Burn__Llama-Squared-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "3rd-Degree-Burn/Llama-Squared-8B",
    "Model sha": "f30737e92b3a3fa0ef2a3f3ade487cc94ad34400",
    "Average ‚¨ÜÔ∏è": 12.23354432505113,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0111115247414244,
    "IFEval Raw": 0.27552449722292405,
    "IFEval": 27.55244972229241,
    "BBH Raw": 0.4431025683868353,
    "BBH": 21.277103190106818,
    "MATH Lvl 5 Raw": 0.04531722054380666,
    "MATH Lvl 5": 4.5317220543806656,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.30894791666666666,
    "MUSR": 1.9518229166666672,
    "MMLU-PRO Raw": 0.2366190159574468,
    "MMLU-PRO": 15.179890661938533,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-08",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "4season_final_model_test_v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/4season/final_model_test_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">4season/final_model_test_v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/4season__final_model_test_v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "4season/final_model_test_v2",
    "Model sha": "cf690c35d9cf0b0b6bf034fa16dbf88c56fe861c",
    "Average ‚¨ÜÔ∏è": 21.91554017959572,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 21,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0810383598407012,
    "IFEval Raw": 0.3191132860809319,
    "IFEval": 31.911328608093193,
    "BBH Raw": 0.6342049783295018,
    "BBH": 47.410670136906425,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.4314479166666667,
    "MUSR": 12.430989583333334,
    "MMLU-PRO Raw": 0.3528091755319149,
    "MMLU-PRO": 28.08990839243498,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-20",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "4season/final_model_test_v2"
  },
  {
    "eval_name": "AALF_FuseChat-Llama-3.1-8B-Instruct-preview_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AALF/FuseChat-Llama-3.1-8B-Instruct-preview\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AALF/FuseChat-Llama-3.1-8B-Instruct-preview</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AALF__FuseChat-Llama-3.1-8B-Instruct-preview-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AALF/FuseChat-Llama-3.1-8B-Instruct-preview",
    "Model sha": "f740497979293c90fa1cfaa7c446016e107cc2c1",
    "Average ‚¨ÜÔ∏è": 25.610367506775574,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6886191280830397,
    "IFEval Raw": 0.7189579205397235,
    "IFEval": 71.89579205397234,
    "BBH Raw": 0.5119887898349903,
    "BBH": 30.84806521622957,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.38200000000000006,
    "MUSR": 6.150000000000006,
    "MMLU-PRO Raw": 0.3732546542553192,
    "MMLU-PRO": 30.361628250591018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "AALF_FuseChat-Llama-3.1-8B-SFT-preview_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AALF/FuseChat-Llama-3.1-8B-SFT-preview\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AALF/FuseChat-Llama-3.1-8B-SFT-preview</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AALF__FuseChat-Llama-3.1-8B-SFT-preview-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AALF/FuseChat-Llama-3.1-8B-SFT-preview",
    "Model sha": "601f2b8c448acc5686656d3979ed732ce050b827",
    "Average ‚¨ÜÔ∏è": 27.374838927617827,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6843075383802905,
    "IFEval Raw": 0.7280504616639405,
    "IFEval": 72.80504616639405,
    "BBH Raw": 0.5240303130445233,
    "BBH": 32.53678156315301,
    "MATH Lvl 5 Raw": 0.11404833836858005,
    "MATH Lvl 5": 11.404833836858005,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.40199999999999997,
    "MUSR": 9.749999999999998,
    "MMLU-PRO Raw": 0.37433510638297873,
    "MMLU-PRO": 30.481678486997637,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-21",
    "Generation": 1,
    "Base Model": "AALF/FuseChat-Llama-3.1-8B-SFT-preview (Merge)"
  },
  {
    "eval_name": "AALF_gemma-2-27b-it-SimPO-37K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AALF/gemma-2-27b-it-SimPO-37K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AALF/gemma-2-27b-it-SimPO-37K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AALF__gemma-2-27b-it-SimPO-37K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AALF/gemma-2-27b-it-SimPO-37K",
    "Model sha": "27f15219df2000a16955c9403c3f38b5f3413b3d",
    "Average ‚¨ÜÔ∏è": 9.298079394862366,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.997721625412398,
    "IFEval Raw": 0.24065257959990605,
    "IFEval": 24.065257959990603,
    "BBH Raw": 0.3911343917952534,
    "BBH": 15.307880971954303,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3487604166666667,
    "MUSR": 1.5950520833333328,
    "MMLU-PRO Raw": 0.1971409574468085,
    "MMLU-PRO": 10.793439716312056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-05",
    "Generation": 2,
    "Base Model": "google/gemma-2-27b"
  },
  {
    "eval_name": "AALF_gemma-2-27b-it-SimPO-37K-100steps_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AALF/gemma-2-27b-it-SimPO-37K-100steps\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AALF/gemma-2-27b-it-SimPO-37K-100steps</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AALF__gemma-2-27b-it-SimPO-37K-100steps-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AALF/gemma-2-27b-it-SimPO-37K-100steps",
    "Model sha": "d5cbf18b2eb90b77f5ddbb74cfcaeedfa692c90c",
    "Average ‚¨ÜÔ∏è": 9.89433609242818,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.85673547469955,
    "IFEval Raw": 0.2567642743476199,
    "IFEval": 25.67642743476199,
    "BBH Raw": 0.39308230769885016,
    "BBH": 15.261078322847055,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.3329166666666667,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.21251662234042554,
    "MMLU-PRO": 12.501846926713947,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-21",
    "Generation": 2,
    "Base Model": "google/gemma-2-27b"
  },
  {
    "eval_name": "AELLM_gemma-2-aeria-infinity-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AELLM/gemma-2-aeria-infinity-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AELLM/gemma-2-aeria-infinity-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AELLM__gemma-2-aeria-infinity-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AELLM/gemma-2-aeria-infinity-9b",
    "Model sha": "24e1de07258925d5ddb52134b66e2eb0d698dc11",
    "Average ‚¨ÜÔ∏è": 28.344028639678886,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.003789344178632,
    "IFEval Raw": 0.759399504426034,
    "IFEval": 75.93995044260342,
    "BBH Raw": 0.5983336669577649,
    "BBH": 42.0902142313775,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3338926174496644,
    "GPQA": 11.185682326621922,
    "MUSR Raw": 0.40196875,
    "MUSR": 9.046093750000004,
    "MMLU-PRO Raw": 0.38622007978723405,
    "MMLU-PRO": 31.80223108747045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "AELLM/gemma-2-aeria-infinity-9b (Merge)"
  },
  {
    "eval_name": "AELLM_gemma-2-lyco-infinity-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AELLM/gemma-2-lyco-infinity-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AELLM/gemma-2-lyco-infinity-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AELLM__gemma-2-lyco-infinity-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AELLM/gemma-2-lyco-infinity-9b",
    "Model sha": "2941a682fcbcfea3f1485c9e0691cc1d9edc742e",
    "Average ‚¨ÜÔ∏è": 27.204936939401076,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9785200672827754,
    "IFEval Raw": 0.7316475839660989,
    "IFEval": 73.1647583966099,
    "BBH Raw": 0.5839534871023703,
    "BBH": 39.78753882674737,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.40063541666666663,
    "MUSR": 8.912760416666671,
    "MMLU-PRO Raw": 0.378656914893617,
    "MMLU-PRO": 30.961879432624112,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "AELLM/gemma-2-lyco-infinity-9b (Merge)"
  },
  {
    "eval_name": "AGI-0_Artificium-llama3.1-8B-001_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AGI-0/Artificium-llama3.1-8B-001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AGI-0/Artificium-llama3.1-8B-001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AGI-0__Artificium-llama3.1-8B-001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AGI-0/Artificium-llama3.1-8B-001",
    "Model sha": "6bf3dcca3b75a06a4e04e5f944e709cccf4673fd",
    "Average ‚¨ÜÔ∏è": 19.063821952936436,
    "Hub License": "unknown",
    "Hub ‚ù§Ô∏è": 33,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8603265036927161,
    "IFEval Raw": 0.5247687247614108,
    "IFEval": 52.47687247614108,
    "BBH Raw": 0.42562150225923556,
    "BBH": 19.34889807323965,
    "MATH Lvl 5 Raw": 0.11027190332326281,
    "MATH Lvl 5": 11.027190332326281,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3794583333333333,
    "MUSR": 5.165625000000001,
    "MMLU-PRO Raw": 0.3181515957446808,
    "MMLU-PRO": 24.23906619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "AGI-0/Artificium-llama3.1-8B-001"
  },
  {
    "eval_name": "AI-MO_NuminaMath-7B-CoT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AI-MO/NuminaMath-7B-CoT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-MO/NuminaMath-7B-CoT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AI-MO__NuminaMath-7B-CoT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AI-MO/NuminaMath-7B-CoT",
    "Model sha": "ff7e3044218efe64128bd9c21f9ec66c3de04324",
    "Average ‚¨ÜÔ∏è": 13.097309181769297,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7459889577192211,
    "IFEval Raw": 0.2688544173903022,
    "IFEval": 26.88544173903022,
    "BBH Raw": 0.4314193495860012,
    "BBH": 19.152364282090307,
    "MATH Lvl 5 Raw": 0.08836858006042296,
    "MATH Lvl 5": 8.836858006042297,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.33034375,
    "MUSR": 0.8263020833333333,
    "MMLU-PRO Raw": 0.28681848404255317,
    "MMLU-PRO": 20.75760933806146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-15",
    "Submission Date": "2024-09-10",
    "Generation": 1,
    "Base Model": "deepseek-ai/deepseek-math-7b-base"
  },
  {
    "eval_name": "AI-MO_NuminaMath-7B-TIR_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AI-MO/NuminaMath-7B-TIR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-MO/NuminaMath-7B-TIR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AI-MO__NuminaMath-7B-TIR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AI-MO/NuminaMath-7B-TIR",
    "Model sha": "c6e394cc0579423c9cde6df6cc192c07dae73388",
    "Average ‚¨ÜÔ∏è": 11.815723181441308,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 320,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0741097874941254,
    "IFEval Raw": 0.27562423259174545,
    "IFEval": 27.562423259174547,
    "BBH Raw": 0.41436913375897894,
    "BBH": 16.87354725795866,
    "MATH Lvl 5 Raw": 0.0188821752265861,
    "MATH Lvl 5": 1.8882175226586102,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.35092708333333333,
    "MUSR": 4.199218749999999,
    "MMLU-PRO Raw": 0.2732712765957447,
    "MMLU-PRO": 19.252364066193852,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-04",
    "Submission Date": "2024-07-11",
    "Generation": 1,
    "Base Model": "deepseek-ai/deepseek-math-7b-base"
  },
  {
    "eval_name": "AI-Sweden-Models_Llama-3-8B-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/Llama-3-8B-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/Llama-3-8B-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AI-Sweden-Models__Llama-3-8B-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AI-Sweden-Models/Llama-3-8B-instruct",
    "Model sha": "4e1c955228bdb4d69c1c4560e8d5872312a8f033",
    "Average ‚¨ÜÔ∏è": 13.777204414945189,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1661108808675174,
    "IFEval Raw": 0.24012841482821137,
    "IFEval": 24.012841482821138,
    "BBH Raw": 0.4173460154515302,
    "BBH": 18.388095615027524,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.47709375000000004,
    "MUSR": 19.936718749999997,
    "MMLU-PRO Raw": 0.25972406914893614,
    "MMLU-PRO": 17.747118794326237,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-01",
    "Submission Date": "2024-06-27",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "AI-Sweden-Models_gpt-sw3-40b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AI-Sweden-Models/gpt-sw3-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AI-Sweden-Models/gpt-sw3-40b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AI-Sweden-Models__gpt-sw3-40b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AI-Sweden-Models/gpt-sw3-40b",
    "Model sha": "1af27994df1287a7fac1b10d60e40ca43a22a385",
    "Average ‚¨ÜÔ∏è": 4.7344332002937195,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 39,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9598193650570765,
    "IFEval Raw": 0.1470298807164989,
    "IFEval": 14.702988071649889,
    "BBH Raw": 0.3267744702957652,
    "BBH": 6.894934050796576,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2348993288590604,
    "GPQA": 0.0,
    "MUSR Raw": 0.36323958333333334,
    "MUSR": 2.8382812499999996,
    "MMLU-PRO Raw": 0.12757646276595744,
    "MMLU-PRO": 3.064051418439715,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-02-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "AI-Sweden-Models/gpt-sw3-40b"
  },
  {
    "eval_name": "AbacusResearch_Jallabi-34B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AbacusResearch/Jallabi-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AbacusResearch/Jallabi-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AbacusResearch__Jallabi-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AbacusResearch/Jallabi-34B",
    "Model sha": "f65696da4ed82c9a20e94b200d9dccffa07af682",
    "Average ‚¨ÜÔ∏è": 25.97208393481522,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.286492339784732,
    "IFEval Raw": 0.3528604103777976,
    "IFEval": 35.28604103777975,
    "BBH Raw": 0.6023380603196266,
    "BBH": 43.61576498719506,
    "MATH Lvl 5 Raw": 0.03927492447129909,
    "MATH Lvl 5": 3.927492447129909,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.48217708333333337,
    "MUSR": 20.23880208333333,
    "MMLU-PRO Raw": 0.4681682180851064,
    "MMLU-PRO": 40.90757978723404,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-01",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "AbacusResearch/Jallabi-34B"
  },
  {
    "eval_name": "Alibaba-NLP_gte-Qwen2-7B-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Alibaba-NLP/gte-Qwen2-7B-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Alibaba-NLP__gte-Qwen2-7B-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Alibaba-NLP/gte-Qwen2-7B-instruct",
    "Model sha": "e26182b2122f4435e8b3ebecbf363990f409b45b",
    "Average ‚¨ÜÔ∏è": 13.406180086719312,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 217,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1721133485799347,
    "IFEval Raw": 0.22554045488193547,
    "IFEval": 22.55404548819355,
    "BBH Raw": 0.4495144990818469,
    "BBH": 21.92548248566236,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.35585416666666664,
    "MUSR": 6.315104166666665,
    "MMLU-PRO Raw": 0.33211436170212766,
    "MMLU-PRO": 25.790484633569736,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-15",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "Alibaba-NLP/gte-Qwen2-7B-instruct"
  },
  {
    "eval_name": "ArliAI_ArliAI-RPMax-12B-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ArliAI/ArliAI-RPMax-12B-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ArliAI/ArliAI-RPMax-12B-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ArliAI__ArliAI-RPMax-12B-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ArliAI/ArliAI-RPMax-12B-v1.1",
    "Model sha": "645db1cf8ad952eb57854a133e8e15303b898b04",
    "Average ‚¨ÜÔ∏è": 20.812694393455782,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8334023053279451,
    "IFEval Raw": 0.5348852156721942,
    "IFEval": 53.48852156721942,
    "BBH Raw": 0.475181760840119,
    "BBH": 24.80906331793277,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.36184375,
    "MUSR": 5.563802083333336,
    "MMLU-PRO Raw": 0.3384308510638298,
    "MMLU-PRO": 26.492316784869978,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-31",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "ArliAI/ArliAI-RPMax-12B-v1.1"
  },
  {
    "eval_name": "ArliAI_Llama-3.1-8B-ArliAI-RPMax-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ArliAI/Llama-3.1-8B-ArliAI-RPMax-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ArliAI/Llama-3.1-8B-ArliAI-RPMax-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ArliAI__Llama-3.1-8B-ArliAI-RPMax-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ArliAI/Llama-3.1-8B-ArliAI-RPMax-v1.1",
    "Model sha": "540bd352e59c63900af91b95a932b33aaee70c76",
    "Average ‚¨ÜÔ∏è": 23.91696703468777,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 29,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8927447585384559,
    "IFEval Raw": 0.6359016298975606,
    "IFEval": 63.59016298975607,
    "BBH Raw": 0.5015613456039083,
    "BBH": 28.787014099442825,
    "MATH Lvl 5 Raw": 0.1299093655589124,
    "MATH Lvl 5": 12.990936555891238,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3576875,
    "MUSR": 5.3109375000000005,
    "MMLU-PRO Raw": 0.35513630319148937,
    "MMLU-PRO": 28.34847813238771,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "ArliAI/Llama-3.1-8B-ArliAI-RPMax-v1.1"
  },
  {
    "eval_name": "Artples_L-MChat-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Artples/L-MChat-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Artples/L-MChat-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Artples__L-MChat-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Artples/L-MChat-7b",
    "Model sha": "e10137f5cbfc1b73068d6473e4a87241cca0b3f4",
    "Average ‚¨ÜÔ∏è": 21.22590532742486,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5922261857023506,
    "IFEval Raw": 0.5296646231997766,
    "IFEval": 52.96646231997766,
    "BBH Raw": 0.46003301674679414,
    "BBH": 24.20155738881327,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.4028645833333333,
    "MUSR": 8.124739583333334,
    "MMLU-PRO Raw": 0.3298703457446808,
    "MMLU-PRO": 25.541149527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-02",
    "Submission Date": "2024-07-07",
    "Generation": 1,
    "Base Model": "Artples/L-MChat-7b (Merge)"
  },
  {
    "eval_name": "Artples_L-MChat-Small_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Artples/L-MChat-Small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Artples/L-MChat-Small</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Artples__L-MChat-Small-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Artples/L-MChat-Small",
    "Model sha": "52484c277f6062c12dc6d6b6397ee0d0c21b0126",
    "Average ‚¨ÜÔ∏è": 14.891448828550741,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4655105951838824,
    "IFEval Raw": 0.32870561222002065,
    "IFEval": 32.87056122200207,
    "BBH Raw": 0.48225627665257265,
    "BBH": 26.856515500031353,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.36959375,
    "MUSR": 9.265885416666663,
    "MMLU-PRO Raw": 0.24642619680851063,
    "MMLU-PRO": 16.269577423167846,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-11",
    "Submission Date": "2024-07-07",
    "Generation": 1,
    "Base Model": "Artples/L-MChat-Small (Merge)"
  },
  {
    "eval_name": "Aryanne_SuperHeart_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Aryanne/SuperHeart\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Aryanne/SuperHeart</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Aryanne__SuperHeart-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Aryanne/SuperHeart",
    "Model sha": "02b5050d7e600ce3db81a19638f6043c895d60cf",
    "Average ‚¨ÜÔ∏è": 25.26767259596917,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9039592768236691,
    "IFEval Raw": 0.5192234382549413,
    "IFEval": 51.92234382549414,
    "BBH Raw": 0.5215375046264326,
    "BBH": 31.893554212659296,
    "MATH Lvl 5 Raw": 0.13897280966767372,
    "MATH Lvl 5": 13.897280966767372,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.44357291666666665,
    "MUSR": 14.713281249999996,
    "MMLU-PRO Raw": 0.3912067819148936,
    "MMLU-PRO": 32.35630910165484,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "Aryanne/SuperHeart (Merge)"
  },
  {
    "eval_name": "AtAndDev_Qwen2.5-1.5B-continuous-learnt_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AtAndDev/Qwen2.5-1.5B-continuous-learnt\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AtAndDev/Qwen2.5-1.5B-continuous-learnt</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AtAndDev__Qwen2.5-1.5B-continuous-learnt-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AtAndDev/Qwen2.5-1.5B-continuous-learnt",
    "Model sha": "01c0981db9cf0f146fe050065f17343af75a8aa6",
    "Average ‚¨ÜÔ∏è": 16.518524239214223,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.673034773667099,
    "IFEval Raw": 0.4605214165081982,
    "IFEval": 46.05214165081983,
    "BBH Raw": 0.42577470857933336,
    "BBH": 19.53766599736009,
    "MATH Lvl 5 Raw": 0.07477341389728097,
    "MATH Lvl 5": 7.477341389728097,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3636458333333333,
    "MUSR": 3.7890625000000013,
    "MMLU-PRO Raw": 0.28116688829787234,
    "MMLU-PRO": 20.12965425531915,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-13",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "AtAndDev_Qwen2.5-1.5B-continuous-learnt_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/AtAndDev/Qwen2.5-1.5B-continuous-learnt\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">AtAndDev/Qwen2.5-1.5B-continuous-learnt</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/AtAndDev__Qwen2.5-1.5B-continuous-learnt-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "AtAndDev/Qwen2.5-1.5B-continuous-learnt",
    "Model sha": "01c0981db9cf0f146fe050065f17343af75a8aa6",
    "Average ‚¨ÜÔ∏è": 16.45133031140572,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6885846057983909,
    "IFEval Raw": 0.45105431366551857,
    "IFEval": 45.105431366551855,
    "BBH Raw": 0.42746984992662185,
    "BBH": 19.766408804078655,
    "MATH Lvl 5 Raw": 0.08534743202416918,
    "MATH Lvl 5": 8.534743202416918,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.36228124999999994,
    "MUSR": 2.551822916666667,
    "MMLU-PRO Raw": 0.28058510638297873,
    "MMLU-PRO": 20.06501182033097,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Aurel9_testmerge-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Aurel9/testmerge-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Aurel9/testmerge-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Aurel9__testmerge-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Aurel9/testmerge-7b",
    "Model sha": "b5f0a72d981b5b2c6bd6294093c6956d88477a3e",
    "Average ‚¨ÜÔ∏è": 20.994478367994606,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4764643651040789,
    "IFEval Raw": 0.3979984219648311,
    "IFEval": 39.79984219648311,
    "BBH Raw": 0.5189590919105128,
    "BBH": 32.79279332763635,
    "MATH Lvl 5 Raw": 0.06722054380664652,
    "MATH Lvl 5": 6.722054380664652,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4658645833333333,
    "MUSR": 17.133072916666666,
    "MMLU-PRO Raw": 0.3052692819148936,
    "MMLU-PRO": 22.807697990543733,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "Aurel9/testmerge-7b (Merge)"
  },
  {
    "eval_name": "Azure99_blossom-v5-32b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Azure99/blossom-v5-32b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Azure99/blossom-v5-32b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Azure99__blossom-v5-32b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Azure99/blossom-v5-32b",
    "Model sha": "ccd4d86e3de01187043683dea1e28df904f7408e",
    "Average ‚¨ÜÔ∏è": 26.35255520693019,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.688000330906401,
    "IFEval Raw": 0.5235441960664371,
    "IFEval": 52.3544196066437,
    "BBH Raw": 0.5954545257004673,
    "BBH": 42.883055884713976,
    "MATH Lvl 5 Raw": 0.10422960725075531,
    "MATH Lvl 5": 10.42296072507553,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.40199999999999997,
    "MUSR": 8.350000000000001,
    "MMLU-PRO Raw": 0.4234541223404255,
    "MMLU-PRO": 35.93934692671394,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "Azure99/blossom-v5-32b"
  },
  {
    "eval_name": "Azure99_blossom-v5-llama3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Azure99/blossom-v5-llama3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Azure99/blossom-v5-llama3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Azure99__blossom-v5-llama3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Azure99/blossom-v5-llama3-8b",
    "Model sha": "91ea35e2e65516988021e4bb3b908e3e497e05c2",
    "Average ‚¨ÜÔ∏è": 14.473081640821198,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8721528927447473,
    "IFEval Raw": 0.434293230849701,
    "IFEval": 43.4293230849701,
    "BBH Raw": 0.4184909197087261,
    "BBH": 18.306535405618444,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.36702083333333335,
    "MUSR": 5.3109375000000005,
    "MMLU-PRO Raw": 0.2205784574468085,
    "MMLU-PRO": 13.397606382978722,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-20",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "Azure99/blossom-v5-llama3-8b"
  },
  {
    "eval_name": "Azure99_blossom-v5.1-34b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Azure99/blossom-v5.1-34b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Azure99/blossom-v5.1-34b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Azure99__blossom-v5.1-34b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Azure99/blossom-v5.1-34b",
    "Model sha": "2c803204f5dbf4ce37e2df98eb0205cdc53de10d",
    "Average ‚¨ÜÔ∏è": 28.599285920507427,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.591483456808453,
    "IFEval Raw": 0.5696562897556262,
    "IFEval": 56.96562897556262,
    "BBH Raw": 0.6109110096611161,
    "BBH": 44.14770458838461,
    "MATH Lvl 5 Raw": 0.15709969788519637,
    "MATH Lvl 5": 15.709969788519636,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.39279166666666665,
    "MUSR": 7.298958333333334,
    "MMLU-PRO Raw": 0.4557845744680851,
    "MMLU-PRO": 39.531619385342786,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-07-27",
    "Generation": 0,
    "Base Model": "Azure99/blossom-v5.1-34b"
  },
  {
    "eval_name": "Azure99_blossom-v5.1-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Azure99/blossom-v5.1-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Azure99/blossom-v5.1-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Azure99__blossom-v5.1-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Azure99/blossom-v5.1-9b",
    "Model sha": "6044a3dc1e04529fe883aa513d37f266a320d793",
    "Average ‚¨ÜÔ∏è": 24.87150357178057,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2157204712428955,
    "IFEval Raw": 0.5085816744016985,
    "IFEval": 50.85816744016986,
    "BBH Raw": 0.5343292377916368,
    "BBH": 34.20124449031171,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.39939583333333334,
    "MUSR": 8.024479166666667,
    "MMLU-PRO Raw": 0.39793882978723405,
    "MMLU-PRO": 33.10431442080379,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-07-24",
    "Generation": 0,
    "Base Model": "Azure99/blossom-v5.1-9b"
  },
  {
    "eval_name": "BAAI_Gemma2-9B-IT-Simpo-Infinity-Preference_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Gemma2-9B-IT-Simpo-Infinity-Preference\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Gemma2-9B-IT-Simpo-Infinity-Preference</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Gemma2-9B-IT-Simpo-Infinity-Preference-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Gemma2-9B-IT-Simpo-Infinity-Preference",
    "Model sha": "028a91b1a4f14d365c6db08093b03348455c7bad",
    "Average ‚¨ÜÔ∏è": 20.984068550093443,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.863460482150236,
    "IFEval Raw": 0.31763831079314,
    "IFEval": 31.763831079313995,
    "BBH Raw": 0.5979459664230056,
    "BBH": 42.19084405906616,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33976510067114096,
    "GPQA": 11.968680089485462,
    "MUSR Raw": 0.39657291666666666,
    "MUSR": 8.104947916666669,
    "MMLU-PRO Raw": 0.3868849734042553,
    "MMLU-PRO": 31.876108156028373,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-28",
    "Submission Date": "2024-09-05",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0613-Llama3-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0613-Llama3-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0613-Llama3-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0613-Llama3-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0613-Llama3-70B",
    "Model sha": "9fc53668064bdda22975ca72c5a287f8241c95b3",
    "Average ‚¨ÜÔ∏è": 34.69707473198297,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.526906967407813,
    "IFEval Raw": 0.6821134589555713,
    "IFEval": 68.21134589555712,
    "BBH Raw": 0.6641614484348598,
    "BBH": 51.327160982522116,
    "MATH Lvl 5 Raw": 0.1623867069486405,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.35822147651006714,
    "GPQA": 14.429530201342287,
    "MUSR Raw": 0.45226041666666666,
    "MUSR": 16.53255208333333,
    "MMLU-PRO Raw": 0.47298869680851063,
    "MMLU-PRO": 41.44318853427896,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0613-Llama3-70B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0613-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0613-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0613-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0613-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0613-Mistral-7B",
    "Model sha": "c7a742e539ec264b9eaeefe2aed29e92e8a7ebd6",
    "Average ‚¨ÜÔ∏è": 22.180236956648173,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9493746287459065,
    "IFEval Raw": 0.5319873491225504,
    "IFEval": 53.19873491225504,
    "BBH Raw": 0.49582333763258896,
    "BBH": 28.992936470320583,
    "MATH Lvl 5 Raw": 0.07477341389728098,
    "MATH Lvl 5": 7.477341389728098,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.4350833333333333,
    "MUSR": 13.252083333333333,
    "MMLU-PRO Raw": 0.31607380319148937,
    "MMLU-PRO": 24.00820035460993,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-21",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0613-Mistral-7B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0625-Llama3-70B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Llama3-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0625-Llama3-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0625-Llama3-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0625-Llama3-70B",
    "Model sha": "6d8ceada57e55cff3503191adc4d6379ff321fe2",
    "Average ‚¨ÜÔ∏è": 36.14221684255645,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.43095514061939,
    "IFEval Raw": 0.7442120240960651,
    "IFEval": 74.42120240960651,
    "BBH Raw": 0.6670337872930245,
    "BBH": 52.02816164280523,
    "MATH Lvl 5 Raw": 0.17900302114803626,
    "MATH Lvl 5": 17.900302114803626,
    "GPQA Raw": 0.3573825503355705,
    "GPQA": 14.317673378076066,
    "MUSR Raw": 0.46165625000000005,
    "MUSR": 18.34036458333333,
    "MMLU-PRO Raw": 0.4586103723404255,
    "MMLU-PRO": 39.84559692671394,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0625-Llama3-70B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0625-Llama3-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0625-Llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0625-Llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0625-Llama3-8B",
    "Model sha": "7be7c0ff1e35c3bb781c47222da99a1724f5f1da",
    "Average ‚¨ÜÔ∏è": 21.60935975334111,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8580037811784041,
    "IFEval Raw": 0.6050268842227512,
    "IFEval": 60.50268842227513,
    "BBH Raw": 0.4954985723563075,
    "BBH": 28.9882222457564,
    "MATH Lvl 5 Raw": 0.061178247734138984,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.37120833333333336,
    "MUSR": 5.6677083333333345,
    "MMLU-PRO Raw": 0.3252160904255319,
    "MMLU-PRO": 25.02401004728132,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-07-13",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0625-Llama3-8B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0625-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0625-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0625-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0625-Mistral-7B",
    "Model sha": "302e3ae0bcc50dae3fb69fc1b08b518398e8c407",
    "Average ‚¨ÜÔ∏è": 22.843425216048672,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7857971913732913,
    "IFEval Raw": 0.5867420666054957,
    "IFEval": 58.67420666054956,
    "BBH Raw": 0.4939670574681802,
    "BBH": 28.82328942958971,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.42723958333333334,
    "MUSR": 12.23828125,
    "MMLU-PRO Raw": 0.3229720744680851,
    "MMLU-PRO": 24.774674940898343,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0625-Mistral-7B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0625-Qwen2-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Qwen2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0625-Qwen2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0625-Qwen2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0625-Qwen2-7B",
    "Model sha": "503c24156d7682458686a7b5324f7f886e63470d",
    "Average ‚¨ÜÔ∏è": 24.135356853188735,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3300779744804525,
    "IFEval Raw": 0.5553930238434022,
    "IFEval": 55.53930238434022,
    "BBH Raw": 0.5345911997776569,
    "BBH": 34.65682860864863,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.38876041666666666,
    "MUSR": 6.461718749999999,
    "MMLU-PRO Raw": 0.39602726063829785,
    "MMLU-PRO": 32.89191784869976,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0625-Qwen2-7B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-3M-0625-Yi-1.5-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-3M-0625-Yi-1.5-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B",
    "Model sha": "a42c86c61b98ca4fdf238d688fe6ea11cf414d29",
    "Average ‚¨ÜÔ∏è": 27.943550542340137,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1168012405532666,
    "IFEval Raw": 0.5185984299436606,
    "IFEval": 51.85984299436606,
    "BBH Raw": 0.5509115146247398,
    "BBH": 35.37870748220464,
    "MATH Lvl 5 Raw": 0.15181268882175225,
    "MATH Lvl 5": 15.181268882175225,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.45753125,
    "MUSR": 16.724739583333335,
    "MMLU-PRO Raw": 0.41181848404255317,
    "MMLU-PRO": 34.64649822695036,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-3M-0625-Yi-1.5-9B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-7M-0729-Llama3_1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-7M-0729-Llama3_1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-7M-0729-Llama3_1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-7M-0729-Llama3_1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-7M-0729-Llama3_1-8B",
    "Model sha": "0aca33fd7500a781d041e8bf7e5e3789b03f54f4",
    "Average ‚¨ÜÔ∏è": 23.094956406239415,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8668051031809566,
    "IFEval Raw": 0.6131952109292234,
    "IFEval": 61.31952109292233,
    "BBH Raw": 0.5077335431381055,
    "BBH": 30.888804607565575,
    "MATH Lvl 5 Raw": 0.10649546827794563,
    "MATH Lvl 5": 10.649546827794563,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.35784375,
    "MUSR": 5.297135416666668,
    "MMLU-PRO Raw": 0.3223902925531915,
    "MMLU-PRO": 24.710032505910167,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-02",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-7M-0729-Llama3_1-8B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-7M-0729-mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-7M-0729-mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-7M-0729-mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-7M-0729-mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-7M-0729-mistral-7B",
    "Model sha": "36651591cb13346ecbde23832013e024029700fa",
    "Average ‚¨ÜÔ∏è": 22.914334542962376,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7992612170109323,
    "IFEval Raw": 0.6161928128476886,
    "IFEval": 61.61928128476886,
    "BBH Raw": 0.4963813586525743,
    "BBH": 28.697915491520025,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4061875,
    "MUSR": 10.04010416666667,
    "MMLU-PRO Raw": 0.3273769946808511,
    "MMLU-PRO": 25.26411052009456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-7M-0729-mistral-7B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-7M-Gen-Llama3_1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-7M-Gen-Llama3_1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B",
    "Model sha": "1ef63c4993a8c723c9695c827295c17080a64435",
    "Average ‚¨ÜÔ∏è": 37.10681013594593,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.069121275425136,
    "IFEval Raw": 0.7335458804859993,
    "IFEval": 73.35458804859994,
    "BBH Raw": 0.6695200461367471,
    "BBH": 52.49894685232329,
    "MATH Lvl 5 Raw": 0.22960725075528704,
    "MATH Lvl 5": 22.960725075528703,
    "GPQA Raw": 0.37583892617449666,
    "GPQA": 16.778523489932887,
    "MUSR Raw": 0.45390625,
    "MUSR": 16.97161458333333,
    "MMLU-PRO Raw": 0.460688164893617,
    "MMLU-PRO": 40.076462765957444,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-09-26",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-7M-Gen-Llama3_1-70B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-7M-Gen-Llama3_1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-7M-Gen-Llama3_1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B",
    "Model sha": "56f9c2845ae024eb8b1dd9ea0d8891cbaf33c596",
    "Average ‚¨ÜÔ∏è": 23.094956406239415,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9171398085400156,
    "IFEval Raw": 0.6131952109292234,
    "IFEval": 61.31952109292233,
    "BBH Raw": 0.5077335431381055,
    "BBH": 30.888804607565575,
    "MATH Lvl 5 Raw": 0.10649546827794563,
    "MATH Lvl 5": 10.649546827794563,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.35784375,
    "MUSR": 5.297135416666668,
    "MMLU-PRO Raw": 0.3223902925531915,
    "MMLU-PRO": 24.710032505910167,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-02",
    "Submission Date": "2024-08-29",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-7M-Gen-Llama3_1-8B"
  },
  {
    "eval_name": "BAAI_Infinity-Instruct-7M-Gen-mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/Infinity-Instruct-7M-Gen-mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/Infinity-Instruct-7M-Gen-mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__Infinity-Instruct-7M-Gen-mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/Infinity-Instruct-7M-Gen-mistral-7B",
    "Model sha": "82c83d670a8954f4250547b53a057dea1fbd460d",
    "Average ‚¨ÜÔ∏è": 22.888938962938408,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.824635222613095,
    "IFEval Raw": 0.6146690780462506,
    "IFEval": 61.46690780462506,
    "BBH Raw": 0.4963813586525743,
    "BBH": 28.697915491520025,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4061875,
    "MUSR": 10.04010416666667,
    "MMLU-PRO Raw": 0.3273769946808511,
    "MMLU-PRO": 25.26411052009456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-08-29",
    "Generation": 0,
    "Base Model": "BAAI/Infinity-Instruct-7M-Gen-mistral-7B"
  },
  {
    "eval_name": "BAAI_OPI-Llama-3.1-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BAAI/OPI-Llama-3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BAAI/OPI-Llama-3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BAAI__OPI-Llama-3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BAAI/OPI-Llama-3.1-8B-Instruct",
    "Model sha": "48504799d009b4e1b29e6d2948a7cde68acdc3b0",
    "Average ‚¨ÜÔ∏è": 8.305018294278616,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6716568094274465,
    "IFEval Raw": 0.20745510800232272,
    "IFEval": 20.745510800232275,
    "BBH Raw": 0.3551224419497605,
    "BBH": 9.76871171424153,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3233020833333333,
    "MUSR": 3.579427083333334,
    "MMLU-PRO Raw": 0.21243351063829788,
    "MMLU-PRO": 12.492612293144207,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-06",
    "Submission Date": "2024-09-21",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "BEE-spoke-data_Meta-Llama-3-8Bee_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/Meta-Llama-3-8Bee\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/Meta-Llama-3-8Bee</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__Meta-Llama-3-8Bee-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/Meta-Llama-3-8Bee",
    "Model sha": "8143e34e77a49a30ec2617c5c9cc22cb3cda2287",
    "Average ‚¨ÜÔ∏è": 14.544518826320806,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8303799292781471,
    "IFEval Raw": 0.19506575885317623,
    "IFEval": 19.506575885317623,
    "BBH Raw": 0.46263641905752745,
    "BBH": 24.19903269979749,
    "MATH Lvl 5 Raw": 0.04154078549848943,
    "MATH Lvl 5": 4.1540785498489425,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.36540625,
    "MUSR": 6.242447916666666,
    "MMLU-PRO Raw": 0.32197473404255317,
    "MMLU-PRO": 24.66385933806146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-28",
    "Submission Date": "2024-07-04",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "BEE-spoke-data_smol_llama-101M-GQA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-101M-GQA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__smol_llama-101M-GQA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/smol_llama-101M-GQA",
    "Model sha": "bb26643db413bada7e0c3c50752bf9da82403dba",
    "Average ‚¨ÜÔ∏è": 3.918894933596659,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.11960565265859534,
    "IFEval Raw": 0.13843712460715346,
    "IFEval": 13.843712460715347,
    "BBH Raw": 0.3017560771912554,
    "BBH": 3.1980040943527936,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.3712708333333334,
    "MUSR": 4.275520833333334,
    "MMLU-PRO Raw": 0.11070478723404255,
    "MMLU-PRO": 1.1894208037825047,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-26",
    "Submission Date": "2024-07-06",
    "Generation": 0,
    "Base Model": "BEE-spoke-data/smol_llama-101M-GQA"
  },
  {
    "eval_name": "BEE-spoke-data_smol_llama-220M-GQA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__smol_llama-220M-GQA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/smol_llama-220M-GQA",
    "Model sha": "8845b1d3c0bc73522ef2700aab467183cbdca9f7",
    "Average ‚¨ÜÔ∏è": 6.401567328738996,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.16361327346860904,
    "IFEval Raw": 0.23860468002677343,
    "IFEval": 23.860468002677344,
    "BBH Raw": 0.30316731388708956,
    "BBH": 3.03784275772053,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.405875,
    "MUSR": 9.067708333333334,
    "MMLU-PRO Raw": 0.1149434840425532,
    "MMLU-PRO": 1.6603871158392434,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "BEE-spoke-data/smol_llama-220M-GQA"
  },
  {
    "eval_name": "BEE-spoke-data_smol_llama-220M-GQA-fineweb_edu_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-GQA-fineweb_edu\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-GQA-fineweb_edu</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__smol_llama-220M-GQA-fineweb_edu-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/smol_llama-220M-GQA-fineweb_edu",
    "Model sha": "dec16b41d5e94070dbc1f8449a554373fd4cc1d1",
    "Average ‚¨ÜÔ∏è": 6.516557780898393,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.16187600416170722,
    "IFEval Raw": 0.19881248420856662,
    "IFEval": 19.88124842085666,
    "BBH Raw": 0.29290517164510593,
    "BBH": 2.314902449149024,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.4367604166666667,
    "MUSR": 14.261718750000002,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "BEE-spoke-data/smol_llama-220M-GQA"
  },
  {
    "eval_name": "BEE-spoke-data_smol_llama-220M-openhermes_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/smol_llama-220M-openhermes\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/smol_llama-220M-openhermes</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__smol_llama-220M-openhermes-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/smol_llama-220M-openhermes",
    "Model sha": "fb4bcd4b7eee363baacb4176a26cea2aaeb173f4",
    "Average ‚¨ÜÔ∏è": 4.761771603157545,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.15442589759558073,
    "IFEval Raw": 0.1555229014570229,
    "IFEval": 15.552290145702292,
    "BBH Raw": 0.30275191401927726,
    "BBH": 3.107692077087363,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3847291666666666,
    "MUSR": 6.224479166666669,
    "MMLU-PRO Raw": 0.11203457446808511,
    "MMLU-PRO": 1.337174940898345,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "BEE-spoke-data/smol_llama-220M-GQA"
  },
  {
    "eval_name": "BEE-spoke-data_tFINE-900m-e16-d32-flan_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/tFINE-900m-e16-d32-flan\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/tFINE-900m-e16-d32-flan</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__tFINE-900m-e16-d32-flan-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/tFINE-900m-e16-d32-flan",
    "Model sha": "d9ffec9798402d13d8f2c56ec3de3ad092445297",
    "Average ‚¨ÜÔ∏è": 4.433887493354263,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4560062883721847,
    "IFEval Raw": 0.15057713533424646,
    "IFEval": 15.057713533424646,
    "BBH Raw": 0.30280434847620613,
    "BBH": 4.411893932611097,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2332214765100671,
    "GPQA": 0.0,
    "MUSR Raw": 0.3724166666666667,
    "MUSR": 3.7187500000000013,
    "MMLU-PRO Raw": 0.1307347074468085,
    "MMLU-PRO": 3.4149674940898342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-06",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "pszemraj/tFINE-900m-e16-d32-1024ctx"
  },
  {
    "eval_name": "BEE-spoke-data_tFINE-900m-e16-d32-flan-infinity-instruct-7m-T2T_en-1024_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/tFINE-900m-e16-d32-flan-infinity-instruct-7m-T2T_en-1024\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/tFINE-900m-e16-d32-flan-infinity-instruct-7m-T2T_en-1024</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__tFINE-900m-e16-d32-flan-infinity-instruct-7m-T2T_en-1024-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/tFINE-900m-e16-d32-flan-infinity-instruct-7m-T2T_en-1024",
    "Model sha": "b1e2f12f5224be9f7da0cb5ff30e1bbb3f10f6ca",
    "Average ‚¨ÜÔ∏è": 5.823652685243958,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.6006079481174478,
    "IFEval Raw": 0.13206735905176042,
    "IFEval": 13.206735905176043,
    "BBH Raw": 0.3137786304497592,
    "BBH": 4.737018282627999,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.43927083333333333,
    "MUSR": 13.808854166666668,
    "MMLU-PRO Raw": 0.12367021276595745,
    "MMLU-PRO": 2.6300236406619386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-14",
    "Generation": 2,
    "Base Model": "pszemraj/tFINE-900m-e16-d32-1024ctx"
  },
  {
    "eval_name": "BEE-spoke-data_tFINE-900m-e16-d32-instruct_2e_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/tFINE-900m-e16-d32-instruct_2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/tFINE-900m-e16-d32-instruct_2e</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__tFINE-900m-e16-d32-instruct_2e-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/tFINE-900m-e16-d32-instruct_2e",
    "Model sha": "4c626138c9f4e0c3eafe74b2755eb89334c7ca59",
    "Average ‚¨ÜÔ∏è": 5.681552326682065,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.5166186803577384,
    "IFEval Raw": 0.1402855534426433,
    "IFEval": 14.02855534426433,
    "BBH Raw": 0.31345674638809023,
    "BBH": 5.013070335904381,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.42069791666666667,
    "MUSR": 11.187239583333335,
    "MMLU-PRO Raw": 0.12367021276595745,
    "MMLU-PRO": 2.6300236406619386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-22",
    "Generation": 3,
    "Base Model": "pszemraj/tFINE-900m-e16-d32-1024ctx"
  },
  {
    "eval_name": "BEE-spoke-data_tFINE-900m-instruct-orpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BEE-spoke-data/tFINE-900m-instruct-orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BEE-spoke-data/tFINE-900m-instruct-orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BEE-spoke-data__tFINE-900m-instruct-orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BEE-spoke-data/tFINE-900m-instruct-orpo",
    "Model sha": "e0a21c79bac74442252d36e2c01403afa3f0971b",
    "Average ‚¨ÜÔ∏è": 3.4319574717820847,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.5749619320293293,
    "IFEval Raw": 0.13299157346950535,
    "IFEval": 13.299157346950535,
    "BBH Raw": 0.30220933767045094,
    "BBH": 3.267300577931774,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3408541666666667,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.11519281914893617,
    "MMLU-PRO": 1.6880910165484628,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "BEE-spoke-data/tFINE-900m-instruct-orpo"
  },
  {
    "eval_name": "Ba2han_Llama-Phi-3_DoRA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Ba2han/Llama-Phi-3_DoRA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Ba2han/Llama-Phi-3_DoRA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Ba2han__Llama-Phi-3_DoRA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Ba2han/Llama-Phi-3_DoRA",
    "Model sha": "36f99064a7be8ba475c2ee5c5424e95c263ccb87",
    "Average ‚¨ÜÔ∏è": 25.31883759934171,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5331363500192292,
    "IFEval Raw": 0.5130531434371911,
    "IFEval": 51.30531434371911,
    "BBH Raw": 0.5514558259029191,
    "BBH": 37.24916418079274,
    "MATH Lvl 5 Raw": 0.11253776435045316,
    "MATH Lvl 5": 11.253776435045317,
    "GPQA Raw": 0.3263422818791946,
    "GPQA": 10.17897091722595,
    "MUSR Raw": 0.40692708333333333,
    "MUSR": 9.532552083333336,
    "MMLU-PRO Raw": 0.39153922872340424,
    "MMLU-PRO": 32.393247635933804,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-15",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Ba2han/Llama-Phi-3_DoRA"
  },
  {
    "eval_name": "BenevolenceMessiah_Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BenevolenceMessiah/Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BenevolenceMessiah/Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BenevolenceMessiah__Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BenevolenceMessiah/Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0",
    "Model sha": "d90f6e36584dc9b367461701e83c833bdeb736f2",
    "Average ‚¨ÜÔ∏è": 15.09626789118606,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 28,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.3347971250257986,
    "IFEval Raw": 0.3011531624977283,
    "IFEval": 30.115316249772825,
    "BBH Raw": 0.4908666248538678,
    "BBH": 26.877991478721707,
    "MATH Lvl 5 Raw": 0.04305135951661632,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.4079791666666666,
    "MUSR": 8.930729166666667,
    "MMLU-PRO Raw": 0.26803523936170215,
    "MMLU-PRO": 18.670582151300238,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "BenevolenceMessiah/Yi-Coder-9B-Chat-Instruct-TIES-MoE-v1.0 (Merge)"
  },
  {
    "eval_name": "BlackBeenie_Bloslain-8B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/Bloslain-8B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/Bloslain-8B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__Bloslain-8B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/Bloslain-8B-v0.2",
    "Model sha": "ebcb7f9f30bc172523a827d1ddefeb52b1aba494",
    "Average ‚¨ÜÔ∏è": 23.803914231544336,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6917628513734966,
    "IFEval Raw": 0.5023371321427147,
    "IFEval": 50.233713214271475,
    "BBH Raw": 0.511087946253543,
    "BBH": 30.662901797340655,
    "MATH Lvl 5 Raw": 0.14501510574018128,
    "MATH Lvl 5": 14.501510574018129,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4075729166666667,
    "MUSR": 10.446614583333334,
    "MMLU-PRO Raw": 0.3653590425531915,
    "MMLU-PRO": 29.484338061465724,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-19",
    "Submission Date": "2024-11-19",
    "Generation": 1,
    "Base Model": "BlackBeenie/Bloslain-8B-v0.2 (Merge)"
  },
  {
    "eval_name": "BlackBeenie_Llama-3.1-8B-pythonic-passthrough-merge_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/Llama-3.1-8B-pythonic-passthrough-merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/Llama-3.1-8B-pythonic-passthrough-merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__Llama-3.1-8B-pythonic-passthrough-merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/Llama-3.1-8B-pythonic-passthrough-merge",
    "Model sha": "3ec46616f5b34821b3b928938931295f92e49213",
    "Average ‚¨ÜÔ∏è": 7.311462119128536,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 20,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.5832904715342573,
    "IFEval Raw": 0.23158552640327662,
    "IFEval": 23.158552640327663,
    "BBH Raw": 0.3453848032699584,
    "BBH": 9.359904687487282,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.37781249999999994,
    "MUSR": 4.593229166666666,
    "MMLU-PRO Raw": 0.1332280585106383,
    "MMLU-PRO": 3.6920065011820316,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "BlackBeenie/Llama-3.1-8B-pythonic-passthrough-merge (Merge)"
  },
  {
    "eval_name": "BlackBeenie_Neos-Gemma-2-9b_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/Neos-Gemma-2-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/Neos-Gemma-2-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__Neos-Gemma-2-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/Neos-Gemma-2-9b",
    "Model sha": "56dbbb4f972be887e5b57311a8a32e148e98d154",
    "Average ‚¨ÜÔ∏è": 25.211313042795183,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.6790921193617887,
    "IFEval Raw": 0.5875665456544192,
    "IFEval": 58.75665456544192,
    "BBH Raw": 0.5502975126048852,
    "BBH": 35.638851313766615,
    "MATH Lvl 5 Raw": 0.0823262839879154,
    "MATH Lvl 5": 8.23262839879154,
    "GPQA Raw": 0.32298657718120805,
    "GPQA": 9.731543624161072,
    "MUSR Raw": 0.36175,
    "MUSR": 5.785416666666667,
    "MMLU-PRO Raw": 0.39810505319148937,
    "MMLU-PRO": 33.12278368794326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "BlackBeenie/Neos-Gemma-2-9b (Merge)"
  },
  {
    "eval_name": "BlackBeenie_Neos-Llama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/Neos-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/Neos-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__Neos-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/Neos-Llama-3.1-8B",
    "Model sha": "9b48520ec1a777be0f1fd88f95454d85ac568407",
    "Average ‚¨ÜÔ∏è": 19.461824592709412,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7938669535501479,
    "IFEval Raw": 0.49439376410147295,
    "IFEval": 49.43937641014729,
    "BBH Raw": 0.4424998411442879,
    "BBH": 21.080122945815933,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3749895833333334,
    "MUSR": 5.740364583333334,
    "MMLU-PRO Raw": 0.32621343085106386,
    "MMLU-PRO": 25.134825650118202,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "BlackBeenie/Neos-Llama-3.1-8B (Merge)"
  },
  {
    "eval_name": "BlackBeenie_Neos-Llama-3.1-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/Neos-Llama-3.1-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/Neos-Llama-3.1-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__Neos-Llama-3.1-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/Neos-Llama-3.1-base",
    "Model sha": "d4af4d73ba5fea0275fd1e3ba5102a79ac8009db",
    "Average ‚¨ÜÔ∏è": 3.9687947544913142,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 4,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4092847029992945,
    "IFEval Raw": 0.17508211545366295,
    "IFEval": 17.508211545366294,
    "BBH Raw": 0.29303397468240516,
    "BBH": 2.2214471263806472,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23741610738255034,
    "GPQA": 0.0,
    "MUSR Raw": 0.34990625000000003,
    "MUSR": 2.8382812499999996,
    "MMLU-PRO Raw": 0.11120345744680851,
    "MMLU-PRO": 1.2448286052009452,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 0,
    "Base Model": "BlackBeenie/Neos-Llama-3.1-base"
  },
  {
    "eval_name": "BlackBeenie_llama-3-luminous-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/llama-3-luminous-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/llama-3-luminous-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__llama-3-luminous-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/llama-3-luminous-merged",
    "Model sha": "64288dd8e3305f2dc11d84fe0c653f351b2e8a9d",
    "Average ‚¨ÜÔ∏è": 21.480107983834134,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.763853564537999,
    "IFEval Raw": 0.43234506664538974,
    "IFEval": 43.234506664538976,
    "BBH Raw": 0.5153924501559338,
    "BBH": 30.643687228787254,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.4148958333333333,
    "MUSR": 10.628645833333335,
    "MMLU-PRO Raw": 0.3773271276595745,
    "MMLU-PRO": 30.814125295508273,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "BlackBeenie/llama-3-luminous-merged (Merge)"
  },
  {
    "eval_name": "BlackBeenie_llama-3.1-8B-Galore-openassistant-guanaco_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BlackBeenie/llama-3.1-8B-Galore-openassistant-guanaco\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BlackBeenie/llama-3.1-8B-Galore-openassistant-guanaco</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BlackBeenie__llama-3.1-8B-Galore-openassistant-guanaco-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BlackBeenie/llama-3.1-8B-Galore-openassistant-guanaco",
    "Model sha": "828fa03c10e9085700b7abbe26f95067fab010fd",
    "Average ‚¨ÜÔ∏è": 18.072100566743075,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8568197982016399,
    "IFEval Raw": 0.2634842218646525,
    "IFEval": 26.348422186465246,
    "BBH Raw": 0.5213365363748029,
    "BBH": 31.444704759068383,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.44062500000000004,
    "MUSR": 14.578124999999995,
    "MMLU-PRO Raw": 0.32064494680851063,
    "MMLU-PRO": 24.516105200945624,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-19",
    "Generation": 0,
    "Base Model": "BlackBeenie/llama-3.1-8B-Galore-openassistant-guanaco"
  },
  {
    "eval_name": "BoltMonkey_DreadMix_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BoltMonkey/DreadMix\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BoltMonkey/DreadMix</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BoltMonkey__DreadMix-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BoltMonkey/DreadMix",
    "Model sha": "ab5dbaaff606538db73b6fd89aa169760104a566",
    "Average ‚¨ÜÔ∏è": 28.661026786914743,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6142045816984325,
    "IFEval Raw": 0.7094908176970438,
    "IFEval": 70.94908176970438,
    "BBH Raw": 0.5435097438362475,
    "BBH": 34.84501521605123,
    "MATH Lvl 5 Raw": 0.14954682779456197,
    "MATH Lvl 5": 14.954682779456196,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.42121875,
    "MUSR": 13.619010416666663,
    "MMLU-PRO Raw": 0.37898936170212766,
    "MMLU-PRO": 30.99881796690307,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "BoltMonkey/DreadMix (Merge)"
  },
  {
    "eval_name": "BoltMonkey_NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BoltMonkey__NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated",
    "Model sha": "969e4c9b41e733a367f5ea18ed50a6171b5e2357",
    "Average ‚¨ÜÔ∏è": 27.726281547801026,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6405126109472798,
    "IFEval Raw": 0.7998909559967553,
    "IFEval": 79.98909559967552,
    "BBH Raw": 0.5151987922850448,
    "BBH": 30.75990006920939,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.401875,
    "MUSR": 9.467708333333329,
    "MMLU-PRO Raw": 0.37333776595744683,
    "MMLU-PRO": 30.37086288416076,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated (Merge)"
  },
  {
    "eval_name": "BoltMonkey_NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BoltMonkey__NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated",
    "Model sha": "969e4c9b41e733a367f5ea18ed50a6171b5e2357",
    "Average ‚¨ÜÔ∏è": 21.345510590269537,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7743185740676325,
    "IFEval Raw": 0.45902316963434797,
    "IFEval": 45.9023169634348,
    "BBH Raw": 0.5185441912447182,
    "BBH": 30.793784752659274,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.4082604166666666,
    "MUSR": 9.532552083333336,
    "MMLU-PRO Raw": 0.3631150265957447,
    "MMLU-PRO": 29.235002955082745,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "BoltMonkey/NeuralDaredevil-SuperNova-Lite-7B-DARETIES-abliterated (Merge)"
  },
  {
    "eval_name": "BoltMonkey_SuperNeuralDreadDevil-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BoltMonkey/SuperNeuralDreadDevil-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BoltMonkey/SuperNeuralDreadDevil-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BoltMonkey__SuperNeuralDreadDevil-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BoltMonkey/SuperNeuralDreadDevil-8b",
    "Model sha": "804d5864127e603abec179a159b43f446246fafc",
    "Average ‚¨ÜÔ∏è": 21.847726101261202,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.4053312293695264,
    "IFEval Raw": 0.48580100799212755,
    "IFEval": 48.58010079921276,
    "BBH Raw": 0.515107801571382,
    "BBH": 30.60671378966961,
    "MATH Lvl 5 Raw": 0.09063444108761329,
    "MATH Lvl 5": 9.06344410876133,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.41594791666666664,
    "MUSR": 10.426822916666667,
    "MMLU-PRO Raw": 0.3494015957446808,
    "MMLU-PRO": 27.711288416075647,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "BoltMonkey/SuperNeuralDreadDevil-8b (Merge)"
  },
  {
    "eval_name": "BrainWave-ML_llama3.2-3B-maths-orpo_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BrainWave-ML/llama3.2-3B-maths-orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BrainWave-ML/llama3.2-3B-maths-orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BrainWave-ML__llama3.2-3B-maths-orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BrainWave-ML/llama3.2-3B-maths-orpo",
    "Model sha": "d149d83d8e8f3883421d800848fec85766181923",
    "Average ‚¨ÜÔ∏è": 5.07608283209792,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7072190583239772,
    "IFEval Raw": 0.20490742341431845,
    "IFEval": 20.490742341431847,
    "BBH Raw": 0.2911778102988436,
    "BBH": 2.3470409575204094,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.35753125,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-24",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "BramVanroy_GEITje-7B-ultra_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BramVanroy/GEITje-7B-ultra\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BramVanroy/GEITje-7B-ultra</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BramVanroy__GEITje-7B-ultra-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BramVanroy/GEITje-7B-ultra",
    "Model sha": "d4552cdc6f015754646464d8411aa4f6bcdba8e8",
    "Average ‚¨ÜÔ∏è": 10.909605767857023,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 37,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6195232209986878,
    "IFEval Raw": 0.3723442687624392,
    "IFEval": 37.234426876243916,
    "BBH Raw": 0.37761612997305494,
    "BBH": 12.879913010035898,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.32897916666666666,
    "MUSR": 1.5223958333333336,
    "MMLU-PRO Raw": 0.20113031914893617,
    "MMLU-PRO": 11.236702127659573,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-27",
    "Submission Date": "2024-10-28",
    "Generation": 3,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "BramVanroy_fietje-2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BramVanroy/fietje-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BramVanroy/fietje-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BramVanroy__fietje-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BramVanroy/fietje-2",
    "Model sha": "3abe75d01094b713368e3d911ffb78a2d66ead22",
    "Average ‚¨ÜÔ∏è": 9.027007426451306,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3125386444113754,
    "IFEval Raw": 0.20980332185268422,
    "IFEval": 20.980332185268423,
    "BBH Raw": 0.40356695178386187,
    "BBH": 15.603676192567876,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.3695625,
    "MUSR": 5.161979166666666,
    "MMLU-PRO Raw": 0.19855385638297873,
    "MMLU-PRO": 10.950428486997636,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-09",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "BramVanroy_fietje-2-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BramVanroy/fietje-2-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BramVanroy/fietje-2-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BramVanroy__fietje-2-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BramVanroy/fietje-2-chat",
    "Model sha": "364e785d90438b787b94e33741a930c9932353c0",
    "Average ‚¨ÜÔ∏è": 10.388869107538872,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.39903270817090575,
    "IFEval Raw": 0.2917359273394593,
    "IFEval": 29.17359273394593,
    "BBH Raw": 0.4149753717401999,
    "BBH": 17.718965848323496,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.23993288590604026,
    "GPQA": 0.0,
    "MUSR Raw": 0.3527604166666667,
    "MUSR": 3.195052083333334,
    "MMLU-PRO Raw": 0.20545212765957446,
    "MMLU-PRO": 11.71690307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-10-28",
    "Generation": 3,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "BramVanroy_fietje-2-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/BramVanroy/fietje-2-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">BramVanroy/fietje-2-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/BramVanroy__fietje-2-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "BramVanroy/fietje-2-instruct",
    "Model sha": "b7b44797cd52eda1182667217e8371dbdfee4976",
    "Average ‚¨ÜÔ∏è": 10.196191687548895,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.32439509291769864,
    "IFEval Raw": 0.2789963962286732,
    "IFEval": 27.899639622867326,
    "BBH Raw": 0.41360714173029806,
    "BBH": 17.57247980884759,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.2332214765100671,
    "GPQA": 0.0,
    "MUSR Raw": 0.3369166666666667,
    "MUSR": 2.9145833333333346,
    "MMLU-PRO Raw": 0.2103557180851064,
    "MMLU-PRO": 12.26174645390071,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-27",
    "Submission Date": "2024-10-28",
    "Generation": 2,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "Casual-Autopsy_L3-Umbral-Mind-RP-v2.0-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Casual-Autopsy/L3-Umbral-Mind-RP-v2.0-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Casual-Autopsy/L3-Umbral-Mind-RP-v2.0-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Casual-Autopsy__L3-Umbral-Mind-RP-v2.0-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Casual-Autopsy/L3-Umbral-Mind-RP-v2.0-8B",
    "Model sha": "b46c066ea8387264858dc3461f382e7b42fd9c48",
    "Average ‚¨ÜÔ∏è": 25.911926926034422,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.988384908126601,
    "IFEval Raw": 0.7122634609502786,
    "IFEval": 71.22634609502786,
    "BBH Raw": 0.5262406145493724,
    "BBH": 32.48627762381486,
    "MATH Lvl 5 Raw": 0.11027190332326282,
    "MATH Lvl 5": 11.027190332326283,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.3686666666666667,
    "MUSR": 5.550000000000002,
    "MMLU-PRO Raw": 0.3723404255319149,
    "MMLU-PRO": 30.26004728132387,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-07-02",
    "Generation": 1,
    "Base Model": "Casual-Autopsy/L3-Umbral-Mind-RP-v2.0-8B (Merge)"
  },
  {
    "eval_name": "CausalLM_14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CausalLM/14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CausalLM/14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CausalLM__14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CausalLM/14B",
    "Model sha": "cc054cf5953252d0709cb3267d1a85246e489e95",
    "Average ‚¨ÜÔ∏è": 16.643938664163283,
    "Hub License": "wtfpl",
    "Hub ‚ù§Ô∏è": 304,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9964144636052077,
    "IFEval Raw": 0.2788213052478535,
    "IFEval": 27.882130524785346,
    "BBH Raw": 0.4700462397700626,
    "BBH": 24.780942674518666,
    "MATH Lvl 5 Raw": 0.04003021148036254,
    "MATH Lvl 5": 4.003021148036254,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4154791666666667,
    "MUSR": 11.468229166666669,
    "MMLU-PRO Raw": 0.3221409574468085,
    "MMLU-PRO": 24.682328605200944,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "CausalLM/14B"
  },
  {
    "eval_name": "CausalLM_34b-beta_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CausalLM/34b-beta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CausalLM/34b-beta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CausalLM__34b-beta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CausalLM/34b-beta",
    "Model sha": "0429951eb30ccdfff3515e711aaa7649a8a7364c",
    "Average ‚¨ÜÔ∏è": 23.285244928604158,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 62,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9265963407641995,
    "IFEval Raw": 0.3043247472262486,
    "IFEval": 30.43247472262486,
    "BBH Raw": 0.5590996102136266,
    "BBH": 36.677226262739055,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.37486458333333333,
    "MUSR": 6.924739583333334,
    "MMLU-PRO Raw": 0.5324966755319149,
    "MMLU-PRO": 48.05518617021277,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-06",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "CausalLM/34b-beta"
  },
  {
    "eval_name": "Changgil_K2S3-14b-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Changgil/K2S3-14b-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Changgil/K2S3-14b-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Changgil__K2S3-14b-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Changgil/K2S3-14b-v0.2",
    "Model sha": "b4f0e1eed2640df2b75847ff37e6ebb1be217b6c",
    "Average ‚¨ÜÔ∏è": 15.187667943527268,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6246302524295038,
    "IFEval Raw": 0.3242840108689389,
    "IFEval": 32.428401086893885,
    "BBH Raw": 0.4613311786298187,
    "BBH": 24.283946726650168,
    "MATH Lvl 5 Raw": 0.05211480362537764,
    "MATH Lvl 5": 5.211480362537764,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.3922604166666666,
    "MUSR": 6.7992187500000005,
    "MMLU-PRO Raw": 0.2643783244680851,
    "MMLU-PRO": 18.26425827423168,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-17",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "Changgil/K2S3-14b-v0.2"
  },
  {
    "eval_name": "Changgil_K2S3-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Changgil/K2S3-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Changgil/K2S3-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Changgil__K2S3-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Changgil/K2S3-v0.1",
    "Model sha": "d544e389f091983bb4f11314edb526d81753c919",
    "Average ‚¨ÜÔ∏è": 14.839283995827836,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2498823440050633,
    "IFEval Raw": 0.32765617450586665,
    "IFEval": 32.76561745058666,
    "BBH Raw": 0.46554920672286154,
    "BBH": 24.559557672503786,
    "MATH Lvl 5 Raw": 0.04607250755287009,
    "MATH Lvl 5": 4.607250755287009,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.40140624999999996,
    "MUSR": 7.842447916666667,
    "MMLU-PRO Raw": 0.2562333776595745,
    "MMLU-PRO": 17.359264184397162,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "Changgil/K2S3-v0.1"
  },
  {
    "eval_name": "ClaudioItaly_Albacus_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ClaudioItaly/Albacus\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ClaudioItaly/Albacus</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ClaudioItaly__Albacus-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ClaudioItaly/Albacus",
    "Model sha": "a53faf62d0f99b67478ed9d262872c821a3ba83c",
    "Average ‚¨ÜÔ∏è": 20.49298594556629,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7539389363535288,
    "IFEval Raw": 0.4667415790103592,
    "IFEval": 46.674157901035926,
    "BBH Raw": 0.5113043406568835,
    "BBH": 31.63886474402479,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.41353124999999996,
    "MUSR": 10.658072916666663,
    "MMLU-PRO Raw": 0.31648936170212766,
    "MMLU-PRO": 24.054373522458626,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-08",
    "Submission Date": "2024-09-08",
    "Generation": 1,
    "Base Model": "ClaudioItaly/Albacus (Merge)"
  },
  {
    "eval_name": "ClaudioItaly_Book-Gut12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ClaudioItaly/Book-Gut12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ClaudioItaly/Book-Gut12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ClaudioItaly__Book-Gut12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ClaudioItaly/Book-Gut12B",
    "Model sha": "ae54351faca8170c93bf1de3a51bf16650f5bcf5",
    "Average ‚¨ÜÔ∏è": 23.343745645021073,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4522482233195457,
    "IFEval Raw": 0.39984685080032095,
    "IFEval": 39.984685080032094,
    "BBH Raw": 0.5417370194443233,
    "BBH": 34.63219258973313,
    "MATH Lvl 5 Raw": 0.09894259818731117,
    "MATH Lvl 5": 9.894259818731117,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.4635416666666667,
    "MUSR": 18.276041666666668,
    "MMLU-PRO Raw": 0.3670212765957447,
    "MMLU-PRO": 29.669030732860524,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "ClaudioItaly/Book-Gut12B (Merge)"
  },
  {
    "eval_name": "ClaudioItaly_Evolutionstory-7B-v2.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ClaudioItaly/Evolutionstory-7B-v2.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ClaudioItaly/Evolutionstory-7B-v2.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ClaudioItaly__Evolutionstory-7B-v2.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ClaudioItaly/Evolutionstory-7B-v2.2",
    "Model sha": "9f838721d24a5195bed59a5ed8d9af536f7f2459",
    "Average ‚¨ÜÔ∏è": 20.798247215757183,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.560231988335538,
    "IFEval Raw": 0.4813794066410457,
    "IFEval": 48.137940664104576,
    "BBH Raw": 0.5108043406568835,
    "BBH": 31.62386474402479,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.41353124999999996,
    "MUSR": 10.658072916666663,
    "MMLU-PRO Raw": 0.31590757978723405,
    "MMLU-PRO": 23.98973108747045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-09-01",
    "Generation": 1,
    "Base Model": "ClaudioItaly/Evolutionstory-7B-v2.2 (Merge)"
  },
  {
    "eval_name": "CohereForAI_aya-23-35B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/aya-23-35B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/aya-23-35B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__aya-23-35B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/aya-23-35B",
    "Model sha": "31d6fd858f20539a55401c7ad913086f54d9ca2c",
    "Average ‚¨ÜÔ∏è": 24.67987974603332,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 266,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 16.985317019483638,
    "IFEval Raw": 0.6461932117891638,
    "IFEval": 64.61932117891638,
    "BBH Raw": 0.5399551450731271,
    "BBH": 34.85836046775463,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.4309895833333333,
    "MUSR": 13.47369791666666,
    "MMLU-PRO Raw": 0.33560505319148937,
    "MMLU-PRO": 26.178339243498815,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "CohereForAI/aya-23-35B"
  },
  {
    "eval_name": "CohereForAI_aya-23-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/aya-23-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/aya-23-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__aya-23-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/aya-23-8B",
    "Model sha": "ec151d218a24031eb039d92fb83d10445427efc9",
    "Average ‚¨ÜÔ∏è": 15.998395031351,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 393,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1951720813209588,
    "IFEval Raw": 0.4698887839820565,
    "IFEval": 46.98887839820566,
    "BBH Raw": 0.4296161519220307,
    "BBH": 20.203760646739372,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.3940625,
    "MUSR": 8.424479166666664,
    "MMLU-PRO Raw": 0.2278091755319149,
    "MMLU-PRO": 14.2010195035461,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "CohereForAI/aya-23-8B"
  },
  {
    "eval_name": "CohereForAI_aya-expanse-32b_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/aya-expanse-32b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/aya-expanse-32b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__aya-expanse-32b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/aya-expanse-32b",
    "Model sha": "08b69cfa4240e2009c80ad304f000b491d1b8c38",
    "Average ‚¨ÜÔ∏è": 29.391219089316227,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 169,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.517735099910102,
    "IFEval Raw": 0.7301737168490716,
    "IFEval": 73.01737168490715,
    "BBH Raw": 0.5648670099212114,
    "BBH": 38.70961143301418,
    "MATH Lvl 5 Raw": 0.1336858006042296,
    "MATH Lvl 5": 13.36858006042296,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.3872708333333333,
    "MUSR": 6.408854166666668,
    "MMLU-PRO Raw": 0.41298204787234044,
    "MMLU-PRO": 34.77578309692671,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 0,
    "Base Model": "CohereForAI/aya-expanse-32b"
  },
  {
    "eval_name": "CohereForAI_aya-expanse-8b_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/aya-expanse-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/aya-expanse-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__aya-expanse-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/aya-expanse-8b",
    "Model sha": "b9848575c8731981dfcf2e1f3bfbcb917a2e585d",
    "Average ‚¨ÜÔ∏è": 22.142223244821295,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 285,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1696890688757047,
    "IFEval Raw": 0.6358517622131501,
    "IFEval": 63.585176221315,
    "BBH Raw": 0.4977203055736406,
    "BBH": 28.52348250428851,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.37288541666666664,
    "MUSR": 4.410677083333334,
    "MMLU-PRO Raw": 0.3003656914893617,
    "MMLU-PRO": 22.26285460992908,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 0,
    "Base Model": "CohereForAI/aya-expanse-8b"
  },
  {
    "eval_name": "CohereForAI_c4ai-command-r-plus_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/c4ai-command-r-plus\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/c4ai-command-r-plus</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__c4ai-command-r-plus-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/c4ai-command-r-plus",
    "Model sha": "fa1bd7fb1572ceb861bbbbecfa8af83b29fa8cca",
    "Average ‚¨ÜÔ∏è": 30.961246846253957,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1681,
    "#Params (B)": 103,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 28.631531711670778,
    "IFEval Raw": 0.7664186580495308,
    "IFEval": 76.64186580495308,
    "BBH Raw": 0.581542357407793,
    "BBH": 39.91995423143177,
    "MATH Lvl 5 Raw": 0.08157099697885196,
    "MATH Lvl 5": 8.157099697885197,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.48071875000000003,
    "MUSR": 20.42317708333333,
    "MMLU-PRO Raw": 0.3991855053191489,
    "MMLU-PRO": 33.242833924349874,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "CohereForAI/c4ai-command-r-plus"
  },
  {
    "eval_name": "CohereForAI_c4ai-command-r-plus-08-2024_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/c4ai-command-r-plus-08-2024\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/c4ai-command-r-plus-08-2024</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__c4ai-command-r-plus-08-2024-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/c4ai-command-r-plus-08-2024",
    "Model sha": "2d8cf3ab0af78b9e43546486b096f86adf3ba4d0",
    "Average ‚¨ÜÔ∏è": 33.58453401148939,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 184,
    "#Params (B)": 103,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 22.31887666767308,
    "IFEval Raw": 0.7539539532883859,
    "IFEval": 75.39539532883859,
    "BBH Raw": 0.5995999913027185,
    "BBH": 42.83686540770696,
    "MATH Lvl 5 Raw": 0.1200906344410876,
    "MATH Lvl 5": 12.00906344410876,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.48294791666666664,
    "MUSR": 19.835156249999994,
    "MMLU-PRO Raw": 0.44207114361702127,
    "MMLU-PRO": 38.0079048463357,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "CohereForAI/c4ai-command-r-plus-08-2024"
  },
  {
    "eval_name": "CohereForAI_c4ai-command-r-v01_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "CohereForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CohereForAI/c4ai-command-r-v01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CohereForAI/c4ai-command-r-v01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CohereForAI__c4ai-command-r-v01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CohereForAI/c4ai-command-r-v01",
    "Model sha": "16881ccde1c68bbc7041280e6a66637bc46bfe88",
    "Average ‚¨ÜÔ∏è": 25.34997846133653,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1066,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.395437266989825,
    "IFEval Raw": 0.6748194789824333,
    "IFEval": 67.48194789824333,
    "BBH Raw": 0.5406415512767856,
    "BBH": 34.556659257058264,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.45169791666666664,
    "MUSR": 16.12890625,
    "MMLU-PRO Raw": 0.3369348404255319,
    "MMLU-PRO": 26.326093380614658,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-11",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "CohereForAI/c4ai-command-r-v01"
  },
  {
    "eval_name": "Columbia-NLP_LION-Gemma-2b-dpo-v1.0_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-Gemma-2b-dpo-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-Gemma-2b-dpo-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-Gemma-2b-dpo-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-Gemma-2b-dpo-v1.0",
    "Model sha": "a5f780075831374f8850324448acf94976dea504",
    "Average ‚¨ÜÔ∏è": 11.483994762243412,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9796484742504703,
    "IFEval Raw": 0.3278312654866864,
    "IFEval": 32.78312654866864,
    "BBH Raw": 0.39199563613207467,
    "BBH": 14.585976093815775,
    "MATH Lvl 5 Raw": 0.04305135951661632,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.41201041666666666,
    "MUSR": 9.834635416666668,
    "MMLU-PRO Raw": 0.16655585106382978,
    "MMLU-PRO": 7.395094562647753,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-Gemma-2b-dpo-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-Gemma-2b-dpo-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-Gemma-2b-dpo-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-Gemma-2b-dpo-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-Gemma-2b-dpo-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-Gemma-2b-dpo-v1.0",
    "Model sha": "a5f780075831374f8850324448acf94976dea504",
    "Average ‚¨ÜÔ∏è": 11.148799545265172,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9945691201747504,
    "IFEval Raw": 0.3102457036219453,
    "IFEval": 31.02457036219453,
    "BBH Raw": 0.38810309159554507,
    "BBH": 14.243045647726928,
    "MATH Lvl 5 Raw": 0.04682779456193355,
    "MATH Lvl 5": 4.682779456193355,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.4080729166666666,
    "MUSR": 9.109114583333335,
    "MMLU-PRO Raw": 0.16647273936170212,
    "MMLU-PRO": 7.385859929078014,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-Gemma-2b-dpo-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-Gemma-2b-odpo-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-Gemma-2b-odpo-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-Gemma-2b-odpo-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-Gemma-2b-odpo-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-Gemma-2b-odpo-v1.0",
    "Model sha": "090d9f59c3b47ab8dd099ddd278c058aa6d2d529",
    "Average ‚¨ÜÔ∏è": 11.456794764322686,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.962068098338517,
    "IFEval Raw": 0.30664858131978706,
    "IFEval": 30.664858131978704,
    "BBH Raw": 0.3895836210706875,
    "BBH": 14.023921665416339,
    "MATH Lvl 5 Raw": 0.04305135951661631,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.42791666666666667,
    "MUSR": 12.056250000000004,
    "MMLU-PRO Raw": 0.1692154255319149,
    "MMLU-PRO": 7.690602836879433,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-13",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-Gemma-2b-odpo-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-Gemma-2b-sft-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-Gemma-2b-sft-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-Gemma-2b-sft-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-Gemma-2b-sft-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-Gemma-2b-sft-v1.0",
    "Model sha": "44d6f26fa7e3b0d238064d844569bf8a07b7515e",
    "Average ‚¨ÜÔ∏è": 12.489957398856303,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9608088129774139,
    "IFEval Raw": 0.3692469314751526,
    "IFEval": 36.924693147515256,
    "BBH Raw": 0.387877927616119,
    "BBH": 14.11717086360044,
    "MATH Lvl 5 Raw": 0.06117824773413898,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.4027395833333333,
    "MUSR": 8.309114583333335,
    "MMLU-PRO Raw": 0.17819148936170212,
    "MMLU-PRO": 8.687943262411347,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-02",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-Gemma-2b-sft-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-LLaMA-3-8b-dpo-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-LLaMA-3-8b-dpo-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-LLaMA-3-8b-dpo-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-LLaMA-3-8b-dpo-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-LLaMA-3-8b-dpo-v1.0",
    "Model sha": "3cddd4a6f5939a0a4db1092a0275342b7b9912f3",
    "Average ‚¨ÜÔ∏è": 21.470701394631913,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6968492201522741,
    "IFEval Raw": 0.4957424079220912,
    "IFEval": 49.57424079220912,
    "BBH Raw": 0.5028481044452986,
    "BBH": 30.356398875749075,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.40971874999999996,
    "MUSR": 10.281510416666665,
    "MMLU-PRO Raw": 0.3218916223404255,
    "MMLU-PRO": 24.654624704491724,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-LLaMA-3-8b-dpo-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-LLaMA-3-8b-odpo-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-LLaMA-3-8b-odpo-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-LLaMA-3-8b-odpo-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-LLaMA-3-8b-odpo-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-LLaMA-3-8b-odpo-v1.0",
    "Model sha": "e2cec0d68a67092951e9205dfe634a59f2f4a2dd",
    "Average ‚¨ÜÔ∏è": 19.4629764706478,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7186968825693228,
    "IFEval Raw": 0.39679938119744496,
    "IFEval": 39.6799381197445,
    "BBH Raw": 0.5023929881802022,
    "BBH": 30.457173008350704,
    "MATH Lvl 5 Raw": 0.08308157099697884,
    "MATH Lvl 5": 8.308157099697883,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40575,
    "MUSR": 9.718749999999998,
    "MMLU-PRO Raw": 0.3152426861702128,
    "MMLU-PRO": 23.91585401891253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-LLaMA-3-8b-odpo-v1.0"
  },
  {
    "eval_name": "Columbia-NLP_LION-LLaMA-3-8b-sft-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Columbia-NLP/LION-LLaMA-3-8b-sft-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Columbia-NLP/LION-LLaMA-3-8b-sft-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Columbia-NLP__LION-LLaMA-3-8b-sft-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Columbia-NLP/LION-LLaMA-3-8b-sft-v1.0",
    "Model sha": "822eddb2fd127178d9fb7bb9f4fca0e93ada2836",
    "Average ‚¨ÜÔ∏è": 20.459335612901622,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7536131379003268,
    "IFEval Raw": 0.38171163623629745,
    "IFEval": 38.17116362362975,
    "BBH Raw": 0.5087766443418147,
    "BBH": 30.88426036029,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.45027083333333334,
    "MUSR": 15.483854166666665,
    "MMLU-PRO Raw": 0.32372007978723405,
    "MMLU-PRO": 24.857786643026003,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-02",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "Columbia-NLP/LION-LLaMA-3-8b-sft-v1.0"
  },
  {
    "eval_name": "CombinHorizon_Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CombinHorizon/Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CombinHorizon/Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CombinHorizon__Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CombinHorizon/Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES",
    "Model sha": "52d6f6308eba9c3a0b9116706fbb1ddc448e6101",
    "Average ‚¨ÜÔ∏è": 27.146690191441554,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.045560869633253,
    "IFEval Raw": 0.7564019025075688,
    "IFEval": 75.64019025075689,
    "BBH Raw": 0.5402085849577634,
    "BBH": 34.954070231174235,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.40330208333333334,
    "MUSR": 8.779427083333333,
    "MMLU-PRO Raw": 0.4341755319148936,
    "MMLU-PRO": 37.1306146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "CombinHorizon/Rombos-Qwen2.5-7B-Inst-BaseMerge-TIES (Merge)"
  },
  {
    "eval_name": "CombinHorizon_YiSM-blossom5.1-34B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CombinHorizon/YiSM-blossom5.1-34B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CombinHorizon/YiSM-blossom5.1-34B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CombinHorizon__YiSM-blossom5.1-34B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CombinHorizon/YiSM-blossom5.1-34B-SLERP",
    "Model sha": "ebd8d6507623008567a0548cd0ff9e28cbd6a656",
    "Average ‚¨ÜÔ∏è": 31.392518322054485,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.0708142239631253,
    "IFEval Raw": 0.5033112142448702,
    "IFEval": 50.33112142448702,
    "BBH Raw": 0.6207548093635428,
    "BBH": 46.397612796396146,
    "MATH Lvl 5 Raw": 0.21601208459214502,
    "MATH Lvl 5": 21.6012084592145,
    "GPQA Raw": 0.35570469798657717,
    "GPQA": 14.093959731543624,
    "MUSR Raw": 0.44134375,
    "MUSR": 14.367968750000003,
    "MMLU-PRO Raw": 0.4740691489361702,
    "MMLU-PRO": 41.56323877068557,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-08-27",
    "Generation": 1,
    "Base Model": "CombinHorizon/YiSM-blossom5.1-34B-SLERP (Merge)"
  },
  {
    "eval_name": "CoolSpring_Qwen2-0.5B-Abyme_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CoolSpring/Qwen2-0.5B-Abyme\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CoolSpring/Qwen2-0.5B-Abyme</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CoolSpring__Qwen2-0.5B-Abyme-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CoolSpring/Qwen2-0.5B-Abyme",
    "Model sha": "a48b7c04b854e5c60fe3464f96904bfc53c8640c",
    "Average ‚¨ÜÔ∏è": 4.798584382133771,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.177797308542579,
    "IFEval Raw": 0.19151850423542865,
    "IFEval": 19.151850423542868,
    "BBH Raw": 0.2861834296481826,
    "BBH": 2.2764835705971893,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.35421875,
    "MUSR": 1.47734375,
    "MMLU-PRO Raw": 0.13331117021276595,
    "MMLU-PRO": 3.701241134751772,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-18",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-0.5B"
  },
  {
    "eval_name": "CoolSpring_Qwen2-0.5B-Abyme-merge2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CoolSpring/Qwen2-0.5B-Abyme-merge2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CoolSpring/Qwen2-0.5B-Abyme-merge2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CoolSpring__Qwen2-0.5B-Abyme-merge2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CoolSpring/Qwen2-0.5B-Abyme-merge2",
    "Model sha": "02c4c601453f7ecbfab5c95bf5afa889350026ba",
    "Average ‚¨ÜÔ∏è": 6.1188482937755735,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6096954442103679,
    "IFEval Raw": 0.2021846478454944,
    "IFEval": 20.21846478454944,
    "BBH Raw": 0.29942723009138733,
    "BBH": 3.7090413943355123,
    "MATH Lvl 5 Raw": 0.021148036253776436,
    "MATH Lvl 5": 2.1148036253776437,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3687291666666667,
    "MUSR": 3.8911458333333346,
    "MMLU-PRO Raw": 0.14893617021276595,
    "MMLU-PRO": 5.437352245862883,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "CoolSpring/Qwen2-0.5B-Abyme-merge2 (Merge)"
  },
  {
    "eval_name": "CoolSpring_Qwen2-0.5B-Abyme-merge3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CoolSpring/Qwen2-0.5B-Abyme-merge3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CoolSpring/Qwen2-0.5B-Abyme-merge3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CoolSpring__Qwen2-0.5B-Abyme-merge3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CoolSpring/Qwen2-0.5B-Abyme-merge3",
    "Model sha": "86fed893893cc2a6240f0ea09ce2eeda1a5178cc",
    "Average ‚¨ÜÔ∏è": 6.7069030316516836,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6101713317128061,
    "IFEval Raw": 0.23860468002677343,
    "IFEval": 23.860468002677344,
    "BBH Raw": 0.30031404525933675,
    "BBH": 4.301149162861492,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.35009375000000004,
    "MUSR": 2.1283854166666667,
    "MMLU-PRO Raw": 0.15001662234042554,
    "MMLU-PRO": 5.557402482269504,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "CoolSpring/Qwen2-0.5B-Abyme-merge3 (Merge)"
  },
  {
    "eval_name": "Corianas_llama-3-reactor_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Corianas/llama-3-reactor\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Corianas/llama-3-reactor</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Corianas__llama-3-reactor-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Corianas/llama-3-reactor",
    "Model sha": "bef2eac42fd89baa0064badbc9c7958ad9ccbed3",
    "Average ‚¨ÜÔ∏è": 14.02064596068111,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": -1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8211648206321267,
    "IFEval Raw": 0.23001192391742797,
    "IFEval": 23.001192391742798,
    "BBH Raw": 0.4457148560545015,
    "BBH": 21.88855981925079,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.39771874999999995,
    "MUSR": 8.014843750000002,
    "MMLU-PRO Raw": 0.2800864361702128,
    "MMLU-PRO": 20.00960401891253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-20",
    "Submission Date": "2024-07-23",
    "Generation": 0,
    "Base Model": "Corianas/llama-3-reactor"
  },
  {
    "eval_name": "CortexLM_btlm-7b-base-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CortexLM/btlm-7b-base-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CortexLM/btlm-7b-base-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CortexLM__btlm-7b-base-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CortexLM/btlm-7b-base-v0.2",
    "Model sha": "eda8b4298365a26c8981316e09427c237b11217f",
    "Average ‚¨ÜÔ∏è": 8.869902463410988,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.711358350403807,
    "IFEval Raw": 0.14832865685270635,
    "IFEval": 14.832865685270637,
    "BBH Raw": 0.4006411985841813,
    "BBH": 16.19327709708517,
    "MATH Lvl 5 Raw": 0.012084592145015107,
    "MATH Lvl 5": 1.2084592145015107,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.38460416666666664,
    "MUSR": 5.542187500000001,
    "MMLU-PRO Raw": 0.2349567819148936,
    "MMLU-PRO": 14.995197990543732,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-13",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "CortexLM/btlm-7b-base-v0.2"
  },
  {
    "eval_name": "Cran-May_T.E-8.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Cran-May/T.E-8.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Cran-May/T.E-8.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Cran-May__T.E-8.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Cran-May/T.E-8.1",
    "Model sha": "5f84709710dcce7cc05fa12473e8bb207fe25849",
    "Average ‚¨ÜÔ∏è": 29.405456831659393,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0906328003583743,
    "IFEval Raw": 0.7076922565459647,
    "IFEval": 70.76922565459647,
    "BBH Raw": 0.5581754708123893,
    "BBH": 37.02437662584371,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.4505208333333333,
    "MUSR": 15.31510416666667,
    "MMLU-PRO Raw": 0.4432347074468085,
    "MMLU-PRO": 38.13718971631205,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "Cran-May/T.E-8.1 (Merge)"
  },
  {
    "eval_name": "CultriX_Qwen2.5-14B-MegaMerge-pt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/Qwen2.5-14B-MegaMerge-pt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/Qwen2.5-14B-MegaMerge-pt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__Qwen2.5-14B-MegaMerge-pt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/Qwen2.5-14B-MegaMerge-pt2",
    "Model sha": "20397f6cafc09c2cb74f105867cd99b3c68c71dc",
    "Average ‚¨ÜÔ∏è": 36.69431425267112,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.250433784833411,
    "IFEval Raw": 0.568307645935008,
    "IFEval": 56.8307645935008,
    "BBH Raw": 0.6577703330510146,
    "BBH": 50.9079030473653,
    "MATH Lvl 5 Raw": 0.27341389728096677,
    "MATH Lvl 5": 27.341389728096676,
    "GPQA Raw": 0.37919463087248323,
    "GPQA": 17.225950782997764,
    "MUSR Raw": 0.472875,
    "MUSR": 18.74270833333333,
    "MMLU-PRO Raw": 0.5420545212765957,
    "MMLU-PRO": 49.11716903073285,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "CultriX/Qwen2.5-14B-MegaMerge-pt2 (Merge)"
  },
  {
    "eval_name": "CultriX_Qwen2.5-14B-MergeStock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/Qwen2.5-14B-MergeStock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/Qwen2.5-14B-MergeStock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__Qwen2.5-14B-MergeStock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/Qwen2.5-14B-MergeStock",
    "Model sha": "fa00543296f2731793dfb0aac667571ccf1abb5b",
    "Average ‚¨ÜÔ∏è": 36.390258554758304,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 4.430605654472236,
    "IFEval Raw": 0.5685326046002386,
    "IFEval": 56.85326046002385,
    "BBH Raw": 0.6579336391923106,
    "BBH": 51.00939101332111,
    "MATH Lvl 5 Raw": 0.27341389728096677,
    "MATH Lvl 5": 27.341389728096676,
    "GPQA Raw": 0.3733221476510067,
    "GPQA": 16.442953020134222,
    "MUSR Raw": 0.4676354166666667,
    "MUSR": 17.854427083333334,
    "MMLU-PRO Raw": 0.539561170212766,
    "MMLU-PRO": 48.84013002364066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "CultriX/Qwen2.5-14B-MergeStock (Merge)"
  },
  {
    "eval_name": "CultriX_Qwen2.5-14B-Wernicke_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/Qwen2.5-14B-Wernicke\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/Qwen2.5-14B-Wernicke</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__Qwen2.5-14B-Wernicke-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/Qwen2.5-14B-Wernicke",
    "Model sha": "622c0a58ecb0c0c679d7381a823d2ae5ac2b8ce1",
    "Average ‚¨ÜÔ∏è": 36.99924187858995,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2222344879921874,
    "IFEval Raw": 0.5234699486252034,
    "IFEval": 52.34699486252033,
    "BBH Raw": 0.6568359662501574,
    "BBH": 50.642876092422625,
    "MATH Lvl 5 Raw": 0.324773413897281,
    "MATH Lvl 5": 32.477341389728096,
    "GPQA Raw": 0.3934563758389262,
    "GPQA": 19.12751677852349,
    "MUSR Raw": 0.46890625,
    "MUSR": 18.24661458333333,
    "MMLU-PRO Raw": 0.5423869680851063,
    "MMLU-PRO": 49.15410756501182,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "CultriX/Qwen2.5-14B-Wernicke (Merge)"
  },
  {
    "eval_name": "CultriX_Qwen2.5-14B-Wernicke-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/Qwen2.5-14B-Wernicke-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/Qwen2.5-14B-Wernicke-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__Qwen2.5-14B-Wernicke-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/Qwen2.5-14B-Wernicke-SFT",
    "Model sha": "3b68dfba2cf79e4a15e8f4271f7d4b62d2ab9f26",
    "Average ‚¨ÜÔ∏è": 33.524335557588614,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3930127022938419,
    "IFEval Raw": 0.4937443760333692,
    "IFEval": 49.37443760333692,
    "BBH Raw": 0.6460586236565512,
    "BBH": 49.33057176327372,
    "MATH Lvl 5 Raw": 0.3580060422960725,
    "MATH Lvl 5": 35.80060422960725,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.38999999999999996,
    "MUSR": 7.549999999999996,
    "MMLU-PRO Raw": 0.5069813829787234,
    "MMLU-PRO": 45.2201536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "CultriX/Qwen2.5-14B-Wernicke-SFT (Merge)"
  },
  {
    "eval_name": "CultriX_Qwen2.5-14B-Wernicke-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/Qwen2.5-14B-Wernicke-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/Qwen2.5-14B-Wernicke-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__Qwen2.5-14B-Wernicke-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/Qwen2.5-14B-Wernicke-SLERP",
    "Model sha": "180175561e8061be067fc349ad4491270f19976f",
    "Average ‚¨ÜÔ∏è": 30.63982515783321,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1559876923816415,
    "IFEval Raw": 0.5588904107767391,
    "IFEval": 55.88904107767391,
    "BBH Raw": 0.6440929009604598,
    "BBH": 49.372327095724025,
    "MATH Lvl 5 Raw": 0.09441087613293052,
    "MATH Lvl 5": 9.441087613293051,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.41403125,
    "MUSR": 11.120572916666662,
    "MMLU-PRO Raw": 0.5093916223404256,
    "MMLU-PRO": 45.48795803782507,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-25",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "CultriX_SeQwence-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/SeQwence-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/SeQwence-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__SeQwence-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/SeQwence-14B",
    "Model sha": "f4a147b717ba0e9392f96e343250b00239196a22",
    "Average ‚¨ÜÔ∏è": 36.65968676601578,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7963825527781585,
    "IFEval Raw": 0.5351600420218354,
    "IFEval": 53.51600420218354,
    "BBH Raw": 0.6505665291288972,
    "BBH": 50.16357763465521,
    "MATH Lvl 5 Raw": 0.33987915407854985,
    "MATH Lvl 5": 33.987915407854985,
    "GPQA Raw": 0.36073825503355705,
    "GPQA": 14.76510067114094,
    "MUSR Raw": 0.46661458333333333,
    "MUSR": 18.426822916666662,
    "MMLU-PRO Raw": 0.5418882978723404,
    "MMLU-PRO": 49.09869976359338,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "CultriX/SeQwence-14B"
  },
  {
    "eval_name": "CultriX_SeQwence-14B-v5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/CultriX/SeQwence-14B-v5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">CultriX/SeQwence-14B-v5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/CultriX__SeQwence-14B-v5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "CultriX/SeQwence-14B-v5",
    "Model sha": "9f43ad41542be56f6a18f31bfa60086318735ed5",
    "Average ‚¨ÜÔ∏è": 37.26866293158711,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8651600460737783,
    "IFEval Raw": 0.5919881470055011,
    "IFEval": 59.1988147005501,
    "BBH Raw": 0.6517093605796943,
    "BBH": 49.99573116031767,
    "MATH Lvl 5 Raw": 0.3104229607250755,
    "MATH Lvl 5": 31.042296072507554,
    "GPQA Raw": 0.3699664429530201,
    "GPQA": 15.99552572706935,
    "MUSR Raw": 0.47141666666666665,
    "MUSR": 18.327083333333334,
    "MMLU-PRO Raw": 0.5414727393617021,
    "MMLU-PRO": 49.05252659574468,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DUAL-GPO_zephyr-7b-ipo-0k-15k-i1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DUAL-GPO/zephyr-7b-ipo-0k-15k-i1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DUAL-GPO/zephyr-7b-ipo-0k-15k-i1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DUAL-GPO__zephyr-7b-ipo-0k-15k-i1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DUAL-GPO/zephyr-7b-ipo-0k-15k-i1",
    "Model sha": "564d269c67dfcc5c07a4fbc270a6a48da1929d30",
    "Average ‚¨ÜÔ∏è": 15.492947659683166,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9714233821373284,
    "IFEval Raw": 0.27562423259174545,
    "IFEval": 27.562423259174547,
    "BBH Raw": 0.4472712447565954,
    "BBH": 22.658642660096362,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.41734374999999996,
    "MUSR": 10.567968750000002,
    "MMLU-PRO Raw": 0.31299867021276595,
    "MMLU-PRO": 23.66651891252955,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "DUAL-GPO/zephyr-7b-ipo-qlora-v0-merged"
  },
  {
    "eval_name": "DZgas_GIGABATEMAN-7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DZgas/GIGABATEMAN-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DZgas/GIGABATEMAN-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DZgas__GIGABATEMAN-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DZgas/GIGABATEMAN-7B",
    "Model sha": "edf2840350e7fd55895d9df560b489ac10ecb95e",
    "Average ‚¨ÜÔ∏è": 20.44629273972021,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6303373271199019,
    "IFEval Raw": 0.46074637517342876,
    "IFEval": 46.07463751734288,
    "BBH Raw": 0.5032184342862756,
    "BBH": 29.827516654013994,
    "MATH Lvl 5 Raw": 0.053625377643504536,
    "MATH Lvl 5": 5.362537764350454,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.43284374999999997,
    "MUSR": 11.972135416666667,
    "MMLU-PRO Raw": 0.3176529255319149,
    "MMLU-PRO": 24.183658392434985,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "DZgas/GIGABATEMAN-7B (Merge)"
  },
  {
    "eval_name": "Dampfinchen_Llama-3.1-8B-Ultra-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dampfinchen/Llama-3.1-8B-Ultra-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dampfinchen/Llama-3.1-8B-Ultra-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dampfinchen__Llama-3.1-8B-Ultra-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dampfinchen/Llama-3.1-8B-Ultra-Instruct",
    "Model sha": "46662d14130cfd34f7d90816540794f24a301f86",
    "Average ‚¨ÜÔ∏è": 29.12705144283578,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8364786652077056,
    "IFEval Raw": 0.8081091503876381,
    "IFEval": 80.81091503876381,
    "BBH Raw": 0.5257532452246574,
    "BBH": 32.49458680420566,
    "MATH Lvl 5 Raw": 0.1586102719033233,
    "MATH Lvl 5": 15.86102719033233,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.40032291666666664,
    "MUSR": 8.607031250000002,
    "MMLU-PRO Raw": 0.382563164893617,
    "MMLU-PRO": 31.395907210401887,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-08-26",
    "Generation": 1,
    "Base Model": "Dampfinchen/Llama-3.1-8B-Ultra-Instruct (Merge)"
  },
  {
    "eval_name": "Danielbrdz_Barcenas-14b-Phi-3-medium-ORPO_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Danielbrdz/Barcenas-14b-Phi-3-medium-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Danielbrdz/Barcenas-14b-Phi-3-medium-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Danielbrdz__Barcenas-14b-Phi-3-medium-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Danielbrdz/Barcenas-14b-Phi-3-medium-ORPO",
    "Model sha": "b749dbcb19901b8fd0e9f38c923a24533569f895",
    "Average ‚¨ÜÔ∏è": 31.73844758399777,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5723151838835296,
    "IFEval Raw": 0.4799055395240185,
    "IFEval": 47.990553952401854,
    "BBH Raw": 0.6536184886648629,
    "BBH": 51.029418403280296,
    "MATH Lvl 5 Raw": 0.19335347432024172,
    "MATH Lvl 5": 19.33534743202417,
    "GPQA Raw": 0.3263422818791946,
    "GPQA": 10.17897091722595,
    "MUSR Raw": 0.48075,
    "MUSR": 20.527083333333334,
    "MMLU-PRO Raw": 0.47232380319148937,
    "MMLU-PRO": 41.36931146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-15",
    "Submission Date": "2024-08-13",
    "Generation": 0,
    "Base Model": "Danielbrdz/Barcenas-14b-Phi-3-medium-ORPO"
  },
  {
    "eval_name": "Danielbrdz_Barcenas-Llama3-8b-ORPO_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Danielbrdz/Barcenas-Llama3-8b-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Danielbrdz/Barcenas-Llama3-8b-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Danielbrdz__Barcenas-Llama3-8b-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Danielbrdz/Barcenas-Llama3-8b-ORPO",
    "Model sha": "66c848c4526d3db1ec41468c0f73ac4448c6abe9",
    "Average ‚¨ÜÔ∏è": 26.51900505359198,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7741591856978718,
    "IFEval Raw": 0.737242738156979,
    "IFEval": 73.7242738156979,
    "BBH Raw": 0.49865578559911244,
    "BBH": 28.600623499981847,
    "MATH Lvl 5 Raw": 0.06570996978851965,
    "MATH Lvl 5": 6.570996978851965,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.4189583333333333,
    "MUSR": 11.169791666666669,
    "MMLU-PRO Raw": 0.3829787234042553,
    "MMLU-PRO": 31.44208037825059,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-06-29",
    "Generation": 0,
    "Base Model": "Danielbrdz/Barcenas-Llama3-8b-ORPO"
  },
  {
    "eval_name": "Dans-DiscountModels_Dans-Instruct-CoreCurriculum-12b-ChatML_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Dans-Instruct-CoreCurriculum-12b-ChatML\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Dans-Instruct-CoreCurriculum-12b-ChatML</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Dans-Instruct-CoreCurriculum-12b-ChatML-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Dans-Instruct-CoreCurriculum-12b-ChatML",
    "Model sha": "56925fafe6a543e224db36864dd0927171542776",
    "Average ‚¨ÜÔ∏è": 12.91345183517749,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.2346441839869495,
    "IFEval Raw": 0.21110209798889168,
    "IFEval": 21.11020979888917,
    "BBH Raw": 0.4791864789096407,
    "BBH": 26.046417064819565,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3606354166666667,
    "MUSR": 5.712760416666669,
    "MMLU-PRO Raw": 0.2805019946808511,
    "MMLU-PRO": 20.05577718676123,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "Dans-DiscountModels_Dans-Instruct-Mix-8b-ChatML_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Dans-Instruct-Mix-8b-ChatML-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML",
    "Model sha": "029d84d4f4a618aa798490c046753b12801158e2",
    "Average ‚¨ÜÔ∏è": 13.496180237831885,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7985691981832919,
    "IFEval Raw": 0.08250774611364513,
    "IFEval": 8.250774611364513,
    "BBH Raw": 0.4738171816307924,
    "BBH": 26.336393544053255,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3918229166666667,
    "MUSR": 9.677864583333335,
    "MMLU-PRO Raw": 0.32878989361702127,
    "MMLU-PRO": 25.42109929078014,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-14",
    "Generation": 1,
    "Base Model": "Dans-DiscountModels/Meta-Llama-3.1-8B-ChatML"
  },
  {
    "eval_name": "Dans-DiscountModels_Dans-Instruct-Mix-8b-ChatML-V0.1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Dans-Instruct-Mix-8b-ChatML-V0.1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.0",
    "Model sha": "9367c1273b0025793531fcf3a2c15416539f5d81",
    "Average ‚¨ÜÔ∏è": 12.974201831840233,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8146989006651093,
    "IFEval Raw": 0.06682048076880455,
    "IFEval": 6.682048076880455,
    "BBH Raw": 0.47747656219777285,
    "BBH": 26.737651950701505,
    "MATH Lvl 5 Raw": 0.06117824773413897,
    "MATH Lvl 5": 6.117824773413897,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3785833333333333,
    "MUSR": 8.122916666666667,
    "MMLU-PRO Raw": 0.328374335106383,
    "MMLU-PRO": 25.37492612293144,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-20",
    "Generation": 1,
    "Base Model": "Dans-DiscountModels/Meta-Llama-3.1-8B-ChatML"
  },
  {
    "eval_name": "Dans-DiscountModels_Dans-Instruct-Mix-8b-ChatML-V0.1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Dans-Instruct-Mix-8b-ChatML-V0.1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.1.1",
    "Model sha": "a6188cd1807d0d72e55adc371ddd198d7e9aa7ae",
    "Average ‚¨ÜÔ∏è": 13.311582656183205,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7905887457213416,
    "IFEval Raw": 0.09105063453857985,
    "IFEval": 9.105063453857985,
    "BBH Raw": 0.4748653313732898,
    "BBH": 26.412550636134668,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.3824895833333333,
    "MUSR": 7.811197916666667,
    "MMLU-PRO Raw": 0.327875664893617,
    "MMLU-PRO": 25.319518321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "Dans-DiscountModels/Meta-Llama-3.1-8B-ChatML"
  },
  {
    "eval_name": "Dans-DiscountModels_Dans-Instruct-Mix-8b-ChatML-V0.2.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.2.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.2.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Dans-Instruct-Mix-8b-ChatML-V0.2.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Dans-Instruct-Mix-8b-ChatML-V0.2.0",
    "Model sha": "15a9988381fdba15281f1bd6b04c34f3f96120cc",
    "Average ‚¨ÜÔ∏è": 18.59091937554763,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8437165845529616,
    "IFEval Raw": 0.5064085515321569,
    "IFEval": 50.64085515321569,
    "BBH Raw": 0.4624263551503409,
    "BBH": 24.734770612245033,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.3644479166666667,
    "MUSR": 3.755989583333333,
    "MMLU-PRO Raw": 0.2999501329787234,
    "MMLU-PRO": 22.21668144208038,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "Dans-DiscountModels/Meta-Llama-3.1-8B-ChatML"
  },
  {
    "eval_name": "Dans-DiscountModels_Mistral-7b-v0.3-Test-E0.7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Dans-DiscountModels/Mistral-7b-v0.3-Test-E0.7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Dans-DiscountModels/Mistral-7b-v0.3-Test-E0.7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Dans-DiscountModels__Mistral-7b-v0.3-Test-E0.7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Dans-DiscountModels/Mistral-7b-v0.3-Test-E0.7",
    "Model sha": "e91ad0ada3f0d906bacd3c0ad41da4f65ce77b08",
    "Average ‚¨ÜÔ∏è": 19.144688021668408,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.437885590110603,
    "IFEval Raw": 0.5123538876846767,
    "IFEval": 51.235388768467665,
    "BBH Raw": 0.4750220653053363,
    "BBH": 26.820762256757714,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40051041666666665,
    "MUSR": 8.030468749999999,
    "MMLU-PRO Raw": 0.2744348404255319,
    "MMLU-PRO": 19.38164893617021,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "Dans-DiscountModels/Mistral-7b-v0.3-Test-E0.7 (Merge)"
  },
  {
    "eval_name": "Darkknight535_OpenCrystal-12B-L3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Darkknight535/OpenCrystal-12B-L3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Darkknight535/OpenCrystal-12B-L3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Darkknight535__OpenCrystal-12B-L3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Darkknight535/OpenCrystal-12B-L3",
    "Model sha": "974d2d453afdde40f6a993601bbbbf9d97b43606",
    "Average ‚¨ÜÔ∏è": 20.67288826630491,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 11,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.0122850168801376,
    "IFEval Raw": 0.4070909630890482,
    "IFEval": 40.70909630890482,
    "BBH Raw": 0.5222598504945516,
    "BBH": 31.84449091545611,
    "MATH Lvl 5 Raw": 0.08912386706948641,
    "MATH Lvl 5": 8.912386706948642,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.36565625,
    "MUSR": 5.740364583333334,
    "MMLU-PRO Raw": 0.3640292553191489,
    "MMLU-PRO": 29.336583924349874,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-25",
    "Submission Date": "2024-08-26",
    "Generation": 0,
    "Base Model": "Darkknight535/OpenCrystal-12B-L3"
  },
  {
    "eval_name": "DavidAU_L3-Dark-Planet-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-Dark-Planet-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-Dark-Planet-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-Dark-Planet-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-Dark-Planet-8B",
    "Model sha": "462c9307ba4cfcb0c1edcceac5e06f4007bc803e",
    "Average ‚¨ÜÔ∏è": 20.519536523526952,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9391407069339803,
    "IFEval Raw": 0.4134108609600305,
    "IFEval": 41.34108609600305,
    "BBH Raw": 0.5084081453197787,
    "BBH": 29.78962694499553,
    "MATH Lvl 5 Raw": 0.0853474320241692,
    "MATH Lvl 5": 8.53474320241692,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.36159375,
    "MUSR": 6.332552083333334,
    "MMLU-PRO Raw": 0.37367021276595747,
    "MMLU-PRO": 30.40780141843972,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "DavidAU/L3-Dark-Planet-8B (Merge)"
  },
  {
    "eval_name": "DavidAU_L3-Jamet-12.2B-MK.V-Blackroot-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-Jamet-12.2B-MK.V-Blackroot-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-Jamet-12.2B-MK.V-Blackroot-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-Jamet-12.2B-MK.V-Blackroot-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-Jamet-12.2B-MK.V-Blackroot-Instruct",
    "Model sha": "db4ae3d7b608fd0e7490d2fcfa0436e56e21af33",
    "Average ‚¨ÜÔ∏è": 17.857043217965458,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4375216123892207,
    "IFEval Raw": 0.3961998608137519,
    "IFEval": 39.619986081375195,
    "BBH Raw": 0.4765717717789398,
    "BBH": 25.869793144697738,
    "MATH Lvl 5 Raw": 0.04078549848942598,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.40196875,
    "MUSR": 8.312760416666668,
    "MMLU-PRO Raw": 0.3291223404255319,
    "MMLU-PRO": 25.458037825059098,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "DavidAU/L3-Jamet-12.2B-MK.V-Blackroot-Instruct (Merge)"
  },
  {
    "eval_name": "DavidAU_L3-Lumimaid-12.2B-v0.1-OAS-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-Lumimaid-12.2B-v0.1-OAS-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-Lumimaid-12.2B-v0.1-OAS-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-Lumimaid-12.2B-v0.1-OAS-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-Lumimaid-12.2B-v0.1-OAS-Instruct",
    "Model sha": "65a9e957dc4211aa3d87fdf588767823af5cde3f",
    "Average ‚¨ÜÔ∏è": 17.743438847757332,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4247071736907972,
    "IFEval Raw": 0.3924032677739509,
    "IFEval": 39.24032677739509,
    "BBH Raw": 0.46930207579694677,
    "BBH": 24.504815583181394,
    "MATH Lvl 5 Raw": 0.04078549848942599,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.41942708333333334,
    "MUSR": 11.26171875,
    "MMLU-PRO Raw": 0.31416223404255317,
    "MMLU-PRO": 23.795803782505907,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "DavidAU/L3-Lumimaid-12.2B-v0.1-OAS-Instruct (Merge)"
  },
  {
    "eval_name": "DavidAU_L3-SMB-Instruct-12.2B-F32_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-SMB-Instruct-12.2B-F32\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-SMB-Instruct-12.2B-F32</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-SMB-Instruct-12.2B-F32-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-SMB-Instruct-12.2B-F32",
    "Model sha": "ac5e205a41b17a7b05b1b62f352aacc7e65b2f13",
    "Average ‚¨ÜÔ∏è": 18.863874539984916,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3823971864376314,
    "IFEval Raw": 0.4303215468290802,
    "IFEval": 43.032154682908015,
    "BBH Raw": 0.4786412360346213,
    "BBH": 26.130957088441544,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.40872916666666664,
    "MUSR": 9.624479166666669,
    "MMLU-PRO Raw": 0.3312001329787234,
    "MMLU-PRO": 25.688903664302597,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-25",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "DavidAU/L3-SMB-Instruct-12.2B-F32 (Merge)"
  },
  {
    "eval_name": "DavidAU_L3-Stheno-Maid-Blackroot-Grand-HORROR-16B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-Stheno-Maid-Blackroot-Grand-HORROR-16B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B",
    "Model sha": "7b626e50b6c35fcb064b8b039fcf30eae01c3fae",
    "Average ‚¨ÜÔ∏è": 17.0967863641751,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.922798596582253,
    "IFEval Raw": 0.34389309254998957,
    "IFEval": 34.389309254998956,
    "BBH Raw": 0.4736328900737677,
    "BBH": 26.692021341537863,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.40311458333333333,
    "MUSR": 8.555989583333334,
    "MMLU-PRO Raw": 0.3570478723404255,
    "MMLU-PRO": 28.560874704491717,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "DavidAU/L3-Stheno-Maid-Blackroot-Grand-HORROR-16B (Merge)"
  },
  {
    "eval_name": "DavidAU_L3-Stheno-v3.2-12.2B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DavidAU/L3-Stheno-v3.2-12.2B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DavidAU/L3-Stheno-v3.2-12.2B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DavidAU__L3-Stheno-v3.2-12.2B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DavidAU/L3-Stheno-v3.2-12.2B-Instruct",
    "Model sha": "8271fc32a601a4fa5efbe58c41a0ef4181ad8836",
    "Average ‚¨ÜÔ∏è": 18.790032752524105,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3976996008626377,
    "IFEval Raw": 0.4027945850343755,
    "IFEval": 40.279458503437546,
    "BBH Raw": 0.4845980190500647,
    "BBH": 27.36962320894452,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.41025,
    "MUSR": 10.314583333333333,
    "MMLU-PRO Raw": 0.3345246010638298,
    "MMLU-PRO": 26.0582890070922,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "DavidAU/L3-Stheno-v3.2-12.2B-Instruct (Merge)"
  },
  {
    "eval_name": "Deci_DeciLM-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "DeciLMForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Deci__DeciLM-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Deci/DeciLM-7B",
    "Model sha": "c3c9f4226801dc0433f32aebffe0aac68ee2f051",
    "Average ‚¨ÜÔ∏è": 14.9605373563486,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 225,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6421373474626886,
    "IFEval Raw": 0.28129474239462404,
    "IFEval": 28.129474239462404,
    "BBH Raw": 0.44228566674266495,
    "BBH": 21.252729791067395,
    "MATH Lvl 5 Raw": 0.024924471299093653,
    "MATH Lvl 5": 2.4924471299093653,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.43585416666666665,
    "MUSR": 13.0484375,
    "MMLU-PRO Raw": 0.26919880319148937,
    "MMLU-PRO": 18.799867021276594,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-10",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Deci/DeciLM-7B"
  },
  {
    "eval_name": "Deci_DeciLM-7B-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "DeciLMForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Deci/DeciLM-7B-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Deci/DeciLM-7B-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Deci__DeciLM-7B-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Deci/DeciLM-7B-instruct",
    "Model sha": "4adc7aa9efe61b47b0a98b2cc94527d9c45c3b4f",
    "Average ‚¨ÜÔ∏è": 17.45750410417531,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 96,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6386493323361571,
    "IFEval Raw": 0.4880239985460799,
    "IFEval": 48.802399854608,
    "BBH Raw": 0.4589748654047652,
    "BBH": 23.887149044184596,
    "MATH Lvl 5 Raw": 0.029456193353474325,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.38841666666666663,
    "MUSR": 5.985416666666667,
    "MMLU-PRO Raw": 0.26080452127659576,
    "MMLU-PRO": 17.86716903073286,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-10",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Deci/DeciLM-7B-instruct"
  },
  {
    "eval_name": "DeepAutoAI_Explore_Llama-3.1-8B-Inst_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/Explore_Llama-3.1-8B-Inst\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/Explore_Llama-3.1-8B-Inst</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__Explore_Llama-3.1-8B-Inst-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/Explore_Llama-3.1-8B-Inst",
    "Model sha": "9752180fafd8f584625eb649c0cba36b91bdc3ce",
    "Average ‚¨ÜÔ∏è": 28.788231408436953,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7502392589909805,
    "IFEval Raw": 0.7794828831943688,
    "IFEval": 77.94828831943687,
    "BBH Raw": 0.511742159482904,
    "BBH": 30.393262902042363,
    "MATH Lvl 5 Raw": 0.19259818731117825,
    "MATH Lvl 5": 19.259818731117825,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3909583333333333,
    "MUSR": 9.63645833333333,
    "MMLU-PRO Raw": 0.379155585106383,
    "MMLU-PRO": 31.017287234042552,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "DeepAutoAI/Explore_Llama-3.1-8B-Inst (Merge)"
  },
  {
    "eval_name": "DeepAutoAI_Explore_Llama-3.2-1B-Inst_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/Explore_Llama-3.2-1B-Inst\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/Explore_Llama-3.2-1B-Inst</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__Explore_Llama-3.2-1B-Inst-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/Explore_Llama-3.2-1B-Inst",
    "Model sha": "9fd790df246b8979c02173f7698819a7805fb04e",
    "Average ‚¨ÜÔ∏è": 13.708555118467507,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.891845751897302,
    "IFEval Raw": 0.5648856146136695,
    "IFEval": 56.48856146136695,
    "BBH Raw": 0.35048085637770016,
    "BBH": 8.292273657131942,
    "MATH Lvl 5 Raw": 0.06344410876132932,
    "MATH Lvl 5": 6.344410876132931,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.31834375,
    "MUSR": 1.359635416666667,
    "MMLU-PRO Raw": 0.18085106382978725,
    "MMLU-PRO": 8.983451536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "DeepAutoAI/Explore_Llama-3.2-1B-Inst (Merge)"
  },
  {
    "eval_name": "DeepAutoAI_Explore_Llama-3.2-1B-Inst_v0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/Explore_Llama-3.2-1B-Inst_v0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/Explore_Llama-3.2-1B-Inst_v0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__Explore_Llama-3.2-1B-Inst_v0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v0",
    "Model sha": "9509dee6b01fff1a11dc26cf58d7eecbe3d9d9c4",
    "Average ‚¨ÜÔ∏è": 13.182851431329885,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.46718936940837497,
    "IFEval Raw": 0.5597148898256625,
    "IFEval": 55.97148898256626,
    "BBH Raw": 0.33650903200352716,
    "BBH": 7.042771972349901,
    "MATH Lvl 5 Raw": 0.049093655589123875,
    "MATH Lvl 5": 4.909365558912388,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.3103125,
    "MUSR": 0.45572916666666624,
    "MMLU-PRO Raw": 0.18035239361702127,
    "MMLU-PRO": 8.928043735224584,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 0,
    "Base Model": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v0"
  },
  {
    "eval_name": "DeepAutoAI_Explore_Llama-3.2-1B-Inst_v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__Explore_Llama-3.2-1B-Inst_v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1",
    "Model sha": "3f8b0fb6dcc1e9725ba52dd086241d5d9e413100",
    "Average ‚¨ÜÔ∏è": 10.619318805675972,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.46996594887693766,
    "IFEval Raw": 0.4998891829235318,
    "IFEval": 49.98891829235318,
    "BBH Raw": 0.3141475230443668,
    "BBH": 4.257780193079653,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.37809374999999995,
    "MUSR": 5.1950520833333345,
    "MMLU-PRO Raw": 0.12691156914893617,
    "MMLU-PRO": 2.990174349881796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1 (Merge)"
  },
  {
    "eval_name": "DeepAutoAI_Explore_Llama-3.2-1B-Inst_v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__Explore_Llama-3.2-1B-Inst_v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1.1",
    "Model sha": "158b977bca89e073871e2313740a7c75eb1291af",
    "Average ‚¨ÜÔ∏è": 14.211124391175877,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.912911836713503,
    "IFEval Raw": 0.5844193406827218,
    "IFEval": 58.44193406827218,
    "BBH Raw": 0.3512662445055541,
    "BBH": 8.818154144791238,
    "MATH Lvl 5 Raw": 0.06570996978851965,
    "MATH Lvl 5": 6.570996978851965,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3117083333333333,
    "MUSR": 0.6635416666666667,
    "MMLU-PRO Raw": 0.18184840425531915,
    "MMLU-PRO": 9.094267139479904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "DeepAutoAI/Explore_Llama-3.2-1B-Inst_v1.1 (Merge)"
  },
  {
    "eval_name": "DeepAutoAI_causal_gpt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/causal_gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/causal_gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__causal_gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/causal_gpt2",
    "Model sha": "995f029f6645dde1ef830406001754b904c49775",
    "Average ‚¨ÜÔ∏è": 5.9817068336666175,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.12586520974159543,
    "IFEval Raw": 0.1812767900282362,
    "IFEval": 18.12767900282362,
    "BBH Raw": 0.30257073962835446,
    "BBH": 2.6333438399574587,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.42695833333333333,
    "MUSR": 12.103125,
    "MMLU-PRO Raw": 0.11311502659574468,
    "MMLU-PRO": 1.457225177304964,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-10-17",
    "Generation": 0,
    "Base Model": "DeepAutoAI/causal_gpt2"
  },
  {
    "eval_name": "DeepAutoAI_d2nwg_Llama-3.1-8B-Instruct-v0.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/d2nwg_Llama-3.1-8B-Instruct-v0.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/d2nwg_Llama-3.1-8B-Instruct-v0.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__d2nwg_Llama-3.1-8B-Instruct-v0.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/d2nwg_Llama-3.1-8B-Instruct-v0.0",
    "Model sha": "8bad8800d04a06f3f906728ee223cab2f50453a0",
    "Average ‚¨ÜÔ∏è": 27.727686541835755,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8561779968441444,
    "IFEval Raw": 0.7892746800711002,
    "IFEval": 78.92746800711004,
    "BBH Raw": 0.5080411642065981,
    "BBH": 30.51007603826353,
    "MATH Lvl 5 Raw": 0.0838368580060423,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.41346875,
    "MUSR": 10.983593749999997,
    "MMLU-PRO Raw": 0.3877160904255319,
    "MMLU-PRO": 31.96845449172577,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-10",
    "Generation": 0,
    "Base Model": "DeepAutoAI/d2nwg_Llama-3.1-8B-Instruct-v0.0"
  },
  {
    "eval_name": "DeepAutoAI_d2nwg_causal_gpt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/d2nwg_causal_gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/d2nwg_causal_gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__d2nwg_causal_gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/d2nwg_causal_gpt2",
    "Model sha": "eab065cba5a7a9b08f8b264d61d504c4ecbb611b",
    "Average ‚¨ÜÔ∏è": 6.292853205279464,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.1299073091960183,
    "IFEval Raw": 0.19161823960425006,
    "IFEval": 19.161823960425007,
    "BBH Raw": 0.30268984588252307,
    "BBH": 2.850573557678692,
    "MATH Lvl 5 Raw": 0.003776435045317221,
    "MATH Lvl 5": 0.37764350453172213,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.42971875,
    "MUSR": 12.681510416666669,
    "MMLU-PRO Raw": 0.11510970744680851,
    "MMLU-PRO": 1.6788563829787229,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "DeepAutoAI/d2nwg_causal_gpt2"
  },
  {
    "eval_name": "DeepAutoAI_d2nwg_causal_gpt2_v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/d2nwg_causal_gpt2_v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/d2nwg_causal_gpt2_v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__d2nwg_causal_gpt2_v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/d2nwg_causal_gpt2_v1",
    "Model sha": "3f40c3dcb3eb591dec80ff03573eec7928a7feaa",
    "Average ‚¨ÜÔ∏è": 6.381801392161105,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2304064688014811,
    "IFEval Raw": 0.1988623518929773,
    "IFEval": 19.886235189297732,
    "BBH Raw": 0.29918984588252306,
    "BBH": 2.3872783507070148,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.4336875,
    "MUSR": 13.244270833333333,
    "MMLU-PRO Raw": 0.11353058510638298,
    "MMLU-PRO": 1.5033983451536632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-19",
    "Generation": 0,
    "Base Model": "DeepAutoAI/d2nwg_causal_gpt2_v1"
  },
  {
    "eval_name": "DeepAutoAI_ldm_soup_Llama-3.1-8B-Inst_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/ldm_soup_Llama-3.1-8B-Inst\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/ldm_soup_Llama-3.1-8B-Inst</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__ldm_soup_Llama-3.1-8B-Inst-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Inst",
    "Model sha": "0f04c5ad830f8ae0828191a4670fd4ba361b63d2",
    "Average ‚¨ÜÔ∏è": 28.763891945519962,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.704023495922865,
    "IFEval Raw": 0.803263119633683,
    "IFEval": 80.32631196336831,
    "BBH Raw": 0.512116784464076,
    "BBH": 31.101628224178786,
    "MATH Lvl 5 Raw": 0.12311178247734139,
    "MATH Lvl 5": 12.311178247734139,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.41613541666666665,
    "MUSR": 11.51692708333333,
    "MMLU-PRO Raw": 0.38863031914893614,
    "MMLU-PRO": 32.070035460992905,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Inst (Merge)"
  },
  {
    "eval_name": "DeepAutoAI_ldm_soup_Llama-3.1-8B-Instruct-v0.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__ldm_soup_Llama-3.1-8B-Instruct-v0.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.0",
    "Model sha": "210a97b4dadbda63cc9fe459e8415d4cd3bbaf99",
    "Average ‚¨ÜÔ∏è": 28.37572771437952,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.860454719743755,
    "IFEval Raw": 0.7889499860370484,
    "IFEval": 78.89499860370483,
    "BBH Raw": 0.5125175335277464,
    "BBH": 31.162649496607866,
    "MATH Lvl 5 Raw": 0.11027190332326285,
    "MATH Lvl 5": 11.027190332326285,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.41213541666666664,
    "MUSR": 11.51692708333333,
    "MMLU-PRO Raw": 0.38954454787234044,
    "MMLU-PRO": 32.171616430260045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.0"
  },
  {
    "eval_name": "DeepAutoAI_ldm_soup_Llama-3.1-8B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepAutoAI__ldm_soup_Llama-3.1-8B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.1",
    "Model sha": "ecd140c95985b4292c896e25a94a7629d2924ad1",
    "Average ‚¨ÜÔ∏è": 28.37572771437952,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8284458065320179,
    "IFEval Raw": 0.7889499860370484,
    "IFEval": 78.89499860370483,
    "BBH Raw": 0.5125175335277464,
    "BBH": 31.162649496607866,
    "MATH Lvl 5 Raw": 0.11027190332326285,
    "MATH Lvl 5": 11.027190332326285,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.41213541666666664,
    "MUSR": 11.51692708333333,
    "MMLU-PRO Raw": 0.38954454787234044,
    "MMLU-PRO": 32.171616430260045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "DeepAutoAI/ldm_soup_Llama-3.1-8B-Instruct-v0.1"
  },
  {
    "eval_name": "DeepMount00_Lexora-Lite-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Lexora-Lite-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Lexora-Lite-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Lexora-Lite-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Lexora-Lite-3B",
    "Model sha": "2cf39db7ecac17edca0bf4e0973b7fb58c40c22c",
    "Average ‚¨ÜÔ∏è": 21.618583347735385,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2934995121401753,
    "IFEval Raw": 0.572104238974809,
    "IFEval": 57.21042389748091,
    "BBH Raw": 0.47971311097374714,
    "BBH": 27.199848445980507,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3955520833333333,
    "MUSR": 7.877343750000001,
    "MMLU-PRO Raw": 0.3523105053191489,
    "MMLU-PRO": 28.034500591016542,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-10-20",
    "Generation": 0,
    "Base Model": "DeepMount00/Lexora-Lite-3B"
  },
  {
    "eval_name": "DeepMount00_Lexora-Medium-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Lexora-Medium-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Lexora-Medium-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Lexora-Medium-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Lexora-Medium-7B",
    "Model sha": "c53d166f4f2996a5b7f161529f1ea6548b54a2b2",
    "Average ‚¨ÜÔ∏è": 24.65391504607781,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7349111000222281,
    "IFEval Raw": 0.4103379034295669,
    "IFEval": 41.03379034295669,
    "BBH Raw": 0.5144844494250328,
    "BBH": 32.6953311808552,
    "MATH Lvl 5 Raw": 0.1510574018126888,
    "MATH Lvl 5": 15.105740181268882,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.44394791666666666,
    "MUSR": 14.760156250000001,
    "MMLU-PRO Raw": 0.43251329787234044,
    "MMLU-PRO": 36.9459219858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 0,
    "Base Model": "DeepMount00/Lexora-Medium-7B"
  },
  {
    "eval_name": "DeepMount00_Llama-3-8b-Ita_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Llama-3-8b-Ita\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Llama-3-8b-Ita</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Llama-3-8b-Ita-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Llama-3-8b-Ita",
    "Model sha": "d40847d2981b588690c1dc21d5157d3f4afb2978",
    "Average ‚¨ÜÔ∏è": 26.733875824898078,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7782584486500115,
    "IFEval Raw": 0.7530297388706411,
    "IFEval": 75.3029738870641,
    "BBH Raw": 0.493576505761469,
    "BBH": 28.077745566893725,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.4267708333333333,
    "MUSR": 11.6796875,
    "MMLU-PRO Raw": 0.38522273936170215,
    "MMLU-PRO": 31.691415484633573,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-01",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "DeepMount00_Llama-3.1-8b-ITA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Llama-3.1-8b-ITA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Llama-3.1-8b-ITA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Llama-3.1-8b-ITA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Llama-3.1-8b-ITA",
    "Model sha": "5ede1e388b6b15bc06acd364a8f805fe9ed16db9",
    "Average ‚¨ÜÔ∏è": 28.228097653849485,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.5075735912307806,
    "IFEval Raw": 0.7916727616058724,
    "IFEval": 79.16727616058725,
    "BBH Raw": 0.5109356715302854,
    "BBH": 30.933181176860177,
    "MATH Lvl 5 Raw": 0.10876132930513595,
    "MATH Lvl 5": 10.876132930513595,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.41359375,
    "MUSR": 11.399218749999998,
    "MMLU-PRO Raw": 0.38763297872340424,
    "MMLU-PRO": 31.959219858156025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-10-28",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "DeepMount00_Llama-3.1-Distilled_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Llama-3.1-Distilled\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Llama-3.1-Distilled</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Llama-3.1-Distilled-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Llama-3.1-Distilled",
    "Model sha": "0a94c7ddb196107e8bf1b02e31488ff8c17b9eb3",
    "Average ‚¨ÜÔ∏è": 28.838347109439926,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8390000546863493,
    "IFEval Raw": 0.7843787816327346,
    "IFEval": 78.43787816327344,
    "BBH Raw": 0.5100875314179011,
    "BBH": 30.84142128641545,
    "MATH Lvl 5 Raw": 0.1555891238670695,
    "MATH Lvl 5": 15.55891238670695,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.40581249999999996,
    "MUSR": 10.126562499999997,
    "MMLU-PRO Raw": 0.3781582446808511,
    "MMLU-PRO": 30.906471631205672,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "DeepMount00_Qwen2.5-7B-Instruct-MathCoder_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/Qwen2.5-7B-Instruct-MathCoder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/Qwen2.5-7B-Instruct-MathCoder</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__Qwen2.5-7B-Instruct-MathCoder-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/Qwen2.5-7B-Instruct-MathCoder",
    "Model sha": "90df996cdb1f3d5f051513c50df4cdfda858b5f2",
    "Average ‚¨ÜÔ∏è": 4.384321585834787,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2926795771746673,
    "IFEval Raw": 0.15302508455342934,
    "IFEval": 15.302508455342934,
    "BBH Raw": 0.2998444769655102,
    "BBH": 2.636670587150039,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3806354166666666,
    "MUSR": 5.379427083333334,
    "MMLU-PRO Raw": 0.11178523936170212,
    "MMLU-PRO": 1.309471040189124,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-24",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DeepMount00_mergekit-ties-okvgjfz_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DeepMount00/mergekit-ties-okvgjfz\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DeepMount00/mergekit-ties-okvgjfz</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DeepMount00__mergekit-ties-okvgjfz-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DeepMount00/mergekit-ties-okvgjfz",
    "Model sha": "90df996cdb1f3d5f051513c50df4cdfda858b5f2",
    "Average ‚¨ÜÔ∏è": 4.384321585834787,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2888213463878304,
    "IFEval Raw": 0.15302508455342934,
    "IFEval": 15.302508455342934,
    "BBH Raw": 0.2998444769655102,
    "BBH": 2.636670587150039,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3806354166666666,
    "MUSR": 5.379427083333334,
    "MMLU-PRO Raw": 0.11178523936170212,
    "MMLU-PRO": 1.309471040189124,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-24",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Delta-Vector_Baldur-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Delta-Vector/Baldur-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delta-Vector/Baldur-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Delta-Vector__Baldur-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Delta-Vector/Baldur-8B",
    "Model sha": "97f5d321a8346551a5ed704997dd1e93c59883f3",
    "Average ‚¨ÜÔ∏è": 24.128795136031925,
    "Hub License": "agpl-3.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2942323567330214,
    "IFEval Raw": 0.47818233398493776,
    "IFEval": 47.818233398493774,
    "BBH Raw": 0.5305842954529679,
    "BBH": 32.54183409581636,
    "MATH Lvl 5 Raw": 0.13972809667673716,
    "MATH Lvl 5": 13.972809667673717,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.43715624999999997,
    "MUSR": 14.011197916666665,
    "MMLU-PRO Raw": 0.3654421542553192,
    "MMLU-PRO": 29.493572695035457,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "Delta-Vector/Baldur-8B (Merge)"
  },
  {
    "eval_name": "Delta-Vector_Darkens-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Delta-Vector/Darkens-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delta-Vector/Darkens-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Delta-Vector__Darkens-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Delta-Vector/Darkens-8B",
    "Model sha": "e82be0389bfcecd1998dba1c3bb35b8d95d01bf2",
    "Average ‚¨ÜÔ∏è": 18.874474820285695,
    "Hub License": "agpl-3.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1997430790836436,
    "IFEval Raw": 0.25476624245889795,
    "IFEval": 25.476624245889795,
    "BBH Raw": 0.5250590567372793,
    "BBH": 32.88379503743108,
    "MATH Lvl 5 Raw": 0.05513595166163143,
    "MATH Lvl 5": 5.513595166163143,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.4105520833333333,
    "MUSR": 9.019010416666667,
    "MMLU-PRO Raw": 0.3735871010638298,
    "MMLU-PRO": 30.398566784869974,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "Delta-Vector/Darkens-8B (Merge)"
  },
  {
    "eval_name": "Delta-Vector_Henbane-7b-attempt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Delta-Vector/Henbane-7b-attempt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delta-Vector/Henbane-7b-attempt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Delta-Vector__Henbane-7b-attempt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Delta-Vector/Henbane-7b-attempt2",
    "Model sha": "448ef54e5af03e13f16f3db8ad8d1481479ac12e",
    "Average ‚¨ÜÔ∏è": 23.801361863210996,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1338381580565904,
    "IFEval Raw": 0.4157335868828043,
    "IFEval": 41.573358688280436,
    "BBH Raw": 0.5061177974093075,
    "BBH": 30.865849451121658,
    "MATH Lvl 5 Raw": 0.22658610271903326,
    "MATH Lvl 5": 22.658610271903328,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.39734375000000005,
    "MUSR": 8.701302083333335,
    "MMLU-PRO Raw": 0.4027593085106383,
    "MMLU-PRO": 33.63992316784869,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "Delta-Vector_Odin-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Delta-Vector/Odin-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delta-Vector/Odin-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Delta-Vector__Odin-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Delta-Vector/Odin-9B",
    "Model sha": "9ff20f5dd427e751ada834319bfdd9ea60b5e89c",
    "Average ‚¨ÜÔ∏è": 24.91417210813067,
    "Hub License": "agpl-3.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.708161677190859,
    "IFEval Raw": 0.3691970637907419,
    "IFEval": 36.91970637907419,
    "BBH Raw": 0.5440253444823155,
    "BBH": 34.83242280758616,
    "MATH Lvl 5 Raw": 0.14123867069486407,
    "MATH Lvl 5": 14.123867069486407,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.46478125,
    "MUSR": 17.56432291666666,
    "MMLU-PRO Raw": 0.4046708776595745,
    "MMLU-PRO": 33.85231973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-10-06",
    "Generation": 0,
    "Base Model": "Delta-Vector/Odin-9B"
  },
  {
    "eval_name": "Delta-Vector_Tor-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Delta-Vector/Tor-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Delta-Vector/Tor-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Delta-Vector__Tor-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Delta-Vector/Tor-8B",
    "Model sha": "d30a7a121c2ef5dc14004cfdf3fd13208dfbdb4f",
    "Average ‚¨ÜÔ∏è": 18.41946722561781,
    "Hub License": "agpl-3.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2520533401781035,
    "IFEval Raw": 0.23815476269631244,
    "IFEval": 23.815476269631244,
    "BBH Raw": 0.5209108776928992,
    "BBH": 31.73822449849867,
    "MATH Lvl 5 Raw": 0.05966767371601209,
    "MATH Lvl 5": 5.966767371601208,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.40921874999999996,
    "MUSR": 8.819010416666666,
    "MMLU-PRO Raw": 0.37300531914893614,
    "MMLU-PRO": 30.33392434988179,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "Delta-Vector/Tor-8B (Merge)"
  },
  {
    "eval_name": "DreadPoor_Aspire-8B-model_stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Aspire-8B-model_stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Aspire-8B-model_stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Aspire-8B-model_stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Aspire-8B-model_stock",
    "Model sha": "5c23cb2aff877d0b7bdcfa4de43d1bc8a1852de0",
    "Average ‚¨ÜÔ∏è": 28.52316487341844,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8431278895149513,
    "IFEval Raw": 0.7140620221013578,
    "IFEval": 71.40620221013577,
    "BBH Raw": 0.5278251846388996,
    "BBH": 32.534270073092834,
    "MATH Lvl 5 Raw": 0.14425981873111784,
    "MATH Lvl 5": 14.425981873111784,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.42124999999999996,
    "MUSR": 13.456249999999997,
    "MMLU-PRO Raw": 0.37632978723404253,
    "MMLU-PRO": 30.703309692671393,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "DreadPoor/Aspire-8B-model_stock (Merge)"
  },
  {
    "eval_name": "DreadPoor_Aspire_1.3-8B_model-stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Aspire_1.3-8B_model-stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Aspire_1.3-8B_model-stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Aspire_1.3-8B_model-stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Aspire_1.3-8B_model-stock",
    "Model sha": "d36f5540e8c5654a9fdd8ece9ba8e88af26e5c40",
    "Average ‚¨ÜÔ∏è": 28.38880221155723,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7157813898750875,
    "IFEval Raw": 0.7061685217445268,
    "IFEval": 70.61685217445267,
    "BBH Raw": 0.5301644606574212,
    "BBH": 32.66185070730422,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4104583333333333,
    "MUSR": 12.240624999999996,
    "MMLU-PRO Raw": 0.37159242021276595,
    "MMLU-PRO": 30.17693557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-01",
    "Generation": 1,
    "Base Model": "DreadPoor/Aspire_1.3-8B_model-stock (Merge)"
  },
  {
    "eval_name": "DreadPoor_Aurora_faustus-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Aurora_faustus-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Aurora_faustus-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Aurora_faustus-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Aurora_faustus-8B-LINEAR",
    "Model sha": "76acf1ac703eb827d2541d07a8d4a7cba4b731d4",
    "Average ‚¨ÜÔ∏è": 29.5695557027932,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7673769769612673,
    "IFEval Raw": 0.7281003293483512,
    "IFEval": 72.81003293483514,
    "BBH Raw": 0.5515538279425277,
    "BBH": 36.26348248348271,
    "MATH Lvl 5 Raw": 0.16767371601208458,
    "MATH Lvl 5": 16.76737160120846,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.4145833333333333,
    "MUSR": 12.389583333333329,
    "MMLU-PRO Raw": 0.3842253989361702,
    "MMLU-PRO": 31.580599881796683,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "DreadPoor/Aurora_faustus-8B-LINEAR (Merge)"
  },
  {
    "eval_name": "DreadPoor_Aurora_faustus-8B-LORABLATED_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Aurora_faustus-8B-LORABLATED\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Aurora_faustus-8B-LORABLATED</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Aurora_faustus-8B-LORABLATED-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Aurora_faustus-8B-LORABLATED",
    "Model sha": "97746081f7c681dcf7fad10c57de9a341aa10db1",
    "Average ‚¨ÜÔ∏è": 29.064263146797785,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8008501327903969,
    "IFEval Raw": 0.7527050448365891,
    "IFEval": 75.27050448365891,
    "BBH Raw": 0.539159616655651,
    "BBH": 34.19993531370101,
    "MATH Lvl 5 Raw": 0.14501510574018128,
    "MATH Lvl 5": 14.501510574018129,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.42385416666666664,
    "MUSR": 13.781770833333331,
    "MMLU-PRO Raw": 0.36727061170212766,
    "MMLU-PRO": 29.696734633569736,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "DreadPoor/Aurora_faustus-8B-LORABLATED (Merge)"
  },
  {
    "eval_name": "DreadPoor_Aurora_faustus-8B-LORABLATED_ALT_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Aurora_faustus-8B-LORABLATED_ALT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Aurora_faustus-8B-LORABLATED_ALT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Aurora_faustus-8B-LORABLATED_ALT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Aurora_faustus-8B-LORABLATED_ALT",
    "Model sha": "3ca36587d26bfd936aa1358adc1eabf377aa1e98",
    "Average ‚¨ÜÔ∏è": 28.93415264630571,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7951160978369094,
    "IFEval Raw": 0.7377923908562614,
    "IFEval": 73.77923908562614,
    "BBH Raw": 0.5387670721191214,
    "BBH": 34.21152011815682,
    "MATH Lvl 5 Raw": 0.15407854984894262,
    "MATH Lvl 5": 15.407854984894263,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4225208333333333,
    "MUSR": 13.781770833333331,
    "MMLU-PRO Raw": 0.36943151595744683,
    "MMLU-PRO": 29.93683510638298,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "DreadPoor/Aurora_faustus-8B-LORABLATED_ALT (Merge)"
  },
  {
    "eval_name": "DreadPoor_BaeZel-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/BaeZel-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/BaeZel-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__BaeZel-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/BaeZel-8B-LINEAR",
    "Model sha": "1deac3287de191794c50543d69d523f43654a803",
    "Average ‚¨ÜÔ∏è": 30.296459444597232,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6650688201069717,
    "IFEval Raw": 0.7377923908562614,
    "IFEval": 73.77923908562614,
    "BBH Raw": 0.5463800554321383,
    "BBH": 35.53537606986396,
    "MATH Lvl 5 Raw": 0.1782477341389728,
    "MATH Lvl 5": 17.82477341389728,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.4227083333333333,
    "MUSR": 13.338541666666666,
    "MMLU-PRO Raw": 0.3861369680851064,
    "MMLU-PRO": 31.792996453900706,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "DreadPoor/BaeZel-8B-LINEAR (Merge)"
  },
  {
    "eval_name": "DreadPoor_CoolerCoder-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/CoolerCoder-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/CoolerCoder-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__CoolerCoder-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/CoolerCoder-8B-LINEAR",
    "Model sha": "db14b0fa821b0b6b07802111fd19ba722344a32b",
    "Average ‚¨ÜÔ∏è": 19.148010879752526,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4440211221606587,
    "IFEval Raw": 0.4519286603988528,
    "IFEval": 45.19286603988529,
    "BBH Raw": 0.4761504835496542,
    "BBH": 26.365382993393997,
    "MATH Lvl 5 Raw": 0.061933534743202415,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.3963541666666666,
    "MUSR": 7.777604166666667,
    "MMLU-PRO Raw": 0.31590757978723405,
    "MMLU-PRO": 23.98973108747045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "DreadPoor/CoolerCoder-8B-LINEAR"
  },
  {
    "eval_name": "DreadPoor_Damasteel-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Damasteel-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Damasteel-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Damasteel-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Damasteel-8B-LINEAR",
    "Model sha": "cfc389c15e614b14f1d8d16740dcc183047b435a",
    "Average ‚¨ÜÔ∏è": 28.96489055799835,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6746590709993813,
    "IFEval Raw": 0.7384417789243651,
    "IFEval": 73.84417789243652,
    "BBH Raw": 0.5388142176959776,
    "BBH": 34.10613777622061,
    "MATH Lvl 5 Raw": 0.1661631419939577,
    "MATH Lvl 5": 16.61631419939577,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.42124999999999996,
    "MUSR": 11.856249999999998,
    "MMLU-PRO Raw": 0.3779089095744681,
    "MMLU-PRO": 30.87876773049646,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-01",
    "Generation": 1,
    "Base Model": "DreadPoor/Damasteel-8B-LINEAR (Merge)"
  },
  {
    "eval_name": "DreadPoor_Emu_Eggs-9B-Model_Stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Emu_Eggs-9B-Model_Stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Emu_Eggs-9B-Model_Stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Emu_Eggs-9B-Model_Stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Emu_Eggs-9B-Model_Stock",
    "Model sha": "3fb1b2da72f3618f6943aedfd1600df27886792a",
    "Average ‚¨ÜÔ∏è": 29.61171479576701,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.088349583676092,
    "IFEval Raw": 0.7606982805622415,
    "IFEval": 76.06982805622415,
    "BBH Raw": 0.6051657213517168,
    "BBH": 42.783674159620205,
    "MATH Lvl 5 Raw": 0.0256797583081571,
    "MATH Lvl 5": 2.56797583081571,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.4070833333333333,
    "MUSR": 9.318750000000003,
    "MMLU-PRO Raw": 0.4227061170212766,
    "MMLU-PRO": 35.85623522458629,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "DreadPoor/Emu_Eggs-9B-Model_Stock"
  },
  {
    "eval_name": "DreadPoor_Eunoia_Vespera-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Eunoia_Vespera-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Eunoia_Vespera-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Eunoia_Vespera-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Eunoia_Vespera-8B-LINEAR",
    "Model sha": "c674956327af664735cf39b20c7a8276dfa579f9",
    "Average ‚¨ÜÔ∏è": 28.93115613549158,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8132600469745505,
    "IFEval Raw": 0.7235291249440374,
    "IFEval": 72.35291249440374,
    "BBH Raw": 0.5399310621081937,
    "BBH": 34.21610348917685,
    "MATH Lvl 5 Raw": 0.15256797583081572,
    "MATH Lvl 5": 15.256797583081571,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.4184895833333333,
    "MUSR": 12.611197916666663,
    "MMLU-PRO Raw": 0.38389295212765956,
    "MMLU-PRO": 31.543661347517727,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "DreadPoor/Eunoia_Vespera-8B-LINEAR (Merge)"
  },
  {
    "eval_name": "DreadPoor_Heart_Stolen-8B-Model_Stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Heart_Stolen-8B-Model_Stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Heart_Stolen-8B-Model_Stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Heart_Stolen-8B-Model_Stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Heart_Stolen-8B-Model_Stock",
    "Model sha": "6d77987af7115c7455ddb072c48316815b018999",
    "Average ‚¨ÜÔ∏è": 29.247390406660518,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7493010463585114,
    "IFEval Raw": 0.7244533393617822,
    "IFEval": 72.44533393617823,
    "BBH Raw": 0.5395443745186658,
    "BBH": 34.44482164620488,
    "MATH Lvl 5 Raw": 0.1623867069486405,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.41622916666666665,
    "MUSR": 12.361979166666666,
    "MMLU-PRO Raw": 0.37940492021276595,
    "MMLU-PRO": 31.044991134751776,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-10",
    "Generation": 1,
    "Base Model": "DreadPoor/Heart_Stolen-8B-Model_Stock (Merge)"
  },
  {
    "eval_name": "DreadPoor_Heart_Stolen-ALT-8B-Model_Stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Heart_Stolen-ALT-8B-Model_Stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Heart_Stolen-ALT-8B-Model_Stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Heart_Stolen-ALT-8B-Model_Stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Heart_Stolen-ALT-8B-Model_Stock",
    "Model sha": "03d1d70cb7eb5a743468b97c9c580028df487564",
    "Average ‚¨ÜÔ∏è": 27.754545133229385,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.735627274295549,
    "IFEval Raw": 0.7183584001560305,
    "IFEval": 71.83584001560305,
    "BBH Raw": 0.526338467747489,
    "BBH": 32.354424456472486,
    "MATH Lvl 5 Raw": 0.14954682779456194,
    "MATH Lvl 5": 14.954682779456194,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.40549999999999997,
    "MUSR": 9.754166666666663,
    "MMLU-PRO Raw": 0.37724401595744683,
    "MMLU-PRO": 30.80489066193854,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-11",
    "Generation": 1,
    "Base Model": "DreadPoor/Heart_Stolen-ALT-8B-Model_Stock (Merge)"
  },
  {
    "eval_name": "DreadPoor_Irina-8B-model_stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Irina-8B-model_stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Irina-8B-model_stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Irina-8B-model_stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Irina-8B-model_stock",
    "Model sha": "b282e3ab449d71a31f48b8c13eb43a4435968728",
    "Average ‚¨ÜÔ∏è": 25.32468041749377,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7455864450908245,
    "IFEval Raw": 0.6799403360860294,
    "IFEval": 67.99403360860295,
    "BBH Raw": 0.5236638956084764,
    "BBH": 32.08833034979686,
    "MATH Lvl 5 Raw": 0.10045317220543808,
    "MATH Lvl 5": 10.045317220543808,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.40029166666666666,
    "MUSR": 8.636458333333332,
    "MMLU-PRO Raw": 0.35738031914893614,
    "MMLU-PRO": 28.59781323877068,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DreadPoor_L3.1-BaeZel-8B-Della_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/L3.1-BaeZel-8B-Della\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/L3.1-BaeZel-8B-Della</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__L3.1-BaeZel-8B-Della-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/L3.1-BaeZel-8B-Della",
    "Model sha": "ec61b6f5355a7f3975d80f1afac69e0407e612e5",
    "Average ‚¨ÜÔ∏è": 26.16755491004078,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6589947411870546,
    "IFEval Raw": 0.5180243974875552,
    "IFEval": 51.802439748755525,
    "BBH Raw": 0.5448449542185521,
    "BBH": 35.15745504522053,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.4199791666666666,
    "MUSR": 11.597395833333332,
    "MMLU-PRO Raw": 0.3902094414893617,
    "MMLU-PRO": 32.245493498817964,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-15",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DreadPoor_ONeil-model_stock-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/ONeil-model_stock-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/ONeil-model_stock-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__ONeil-model_stock-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/ONeil-model_stock-8B",
    "Model sha": "d4b84956211fd57b85122fe0c6f88b2cb9a9c86a",
    "Average ‚¨ÜÔ∏è": 26.935908369361425,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7644487666399009,
    "IFEval Raw": 0.6785662043378236,
    "IFEval": 67.85662043378235,
    "BBH Raw": 0.5548337982400763,
    "BBH": 36.4126125295027,
    "MATH Lvl 5 Raw": 0.1012084592145015,
    "MATH Lvl 5": 10.120845921450151,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.41734374999999996,
    "MUSR": 10.967968749999995,
    "MMLU-PRO Raw": 0.35987367021276595,
    "MMLU-PRO": 28.874852245862886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-07-15",
    "Generation": 1,
    "Base Model": "DreadPoor/ONeil-model_stock-8B (Merge)"
  },
  {
    "eval_name": "DreadPoor_Promissum_Mane-8B-LINEAR_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Promissum_Mane-8B-LINEAR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Promissum_Mane-8B-LINEAR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Promissum_Mane-8B-LINEAR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Promissum_Mane-8B-LINEAR",
    "Model sha": "ff399e7004040e1807e8d08b4d0967206fc50afa",
    "Average ‚¨ÜÔ∏è": 29.049296605169243,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8286465221144887,
    "IFEval Raw": 0.7150361042035134,
    "IFEval": 71.50361042035134,
    "BBH Raw": 0.5457684398146738,
    "BBH": 35.253190231117536,
    "MATH Lvl 5 Raw": 0.15256797583081572,
    "MATH Lvl 5": 15.256797583081571,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42004166666666665,
    "MUSR": 13.338541666666666,
    "MMLU-PRO Raw": 0.38505651595744683,
    "MMLU-PRO": 31.672946217494097,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "DreadPoor/Promissum_Mane-8B-LINEAR (Merge)"
  },
  {
    "eval_name": "DreadPoor_Promissum_Mane-8B-LINEAR-lorablated_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Promissum_Mane-8B-LINEAR-lorablated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Promissum_Mane-8B-LINEAR-lorablated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Promissum_Mane-8B-LINEAR-lorablated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Promissum_Mane-8B-LINEAR-lorablated",
    "Model sha": "34c4a30b7462704810e35e033aa5ef33b075a97b",
    "Average ‚¨ÜÔ∏è": 28.810739050078933,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7923424980400432,
    "IFEval Raw": 0.7156356245872064,
    "IFEval": 71.56356245872064,
    "BBH Raw": 0.5435183631990302,
    "BBH": 34.60910725048443,
    "MATH Lvl 5 Raw": 0.15256797583081574,
    "MATH Lvl 5": 15.256797583081575,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.4197916666666666,
    "MUSR": 13.840624999999998,
    "MMLU-PRO Raw": 0.37391954787234044,
    "MMLU-PRO": 30.43550531914893,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "DreadPoor/Promissum_Mane-8B-LINEAR-lorablated (Merge)"
  },
  {
    "eval_name": "DreadPoor_Sellen-8B-model_stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Sellen-8B-model_stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Sellen-8B-model_stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Sellen-8B-model_stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Sellen-8B-model_stock",
    "Model sha": "accde7145d81a428c782695ea61eebc608efd980",
    "Average ‚¨ÜÔ∏è": 26.362467133905938,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8074705227603425,
    "IFEval Raw": 0.7112893788481229,
    "IFEval": 71.1289378848123,
    "BBH Raw": 0.5231680557624704,
    "BBH": 31.3609793143707,
    "MATH Lvl 5 Raw": 0.13217522658610273,
    "MATH Lvl 5": 13.217522658610273,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3960416666666666,
    "MUSR": 10.671874999999995,
    "MMLU-PRO Raw": 0.35696476063829785,
    "MMLU-PRO": 28.551640070921984,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-27",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DreadPoor_Trinas_Nectar-8B-model_stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/Trinas_Nectar-8B-model_stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/Trinas_Nectar-8B-model_stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__Trinas_Nectar-8B-model_stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/Trinas_Nectar-8B-model_stock",
    "Model sha": "cb46b8431872557904d83fc5aa1b90dabeb74acc",
    "Average ‚¨ÜÔ∏è": 27.535042278193572,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.866723896694403,
    "IFEval Raw": 0.7259272064788096,
    "IFEval": 72.59272064788095,
    "BBH Raw": 0.5256123853406084,
    "BBH": 31.97509368554489,
    "MATH Lvl 5 Raw": 0.15332326283987915,
    "MATH Lvl 5": 15.332326283987916,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4067708333333333,
    "MUSR": 11.413020833333329,
    "MMLU-PRO Raw": 0.36178523936170215,
    "MMLU-PRO": 29.08724881796691,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-08-27",
    "Generation": 1,
    "Base Model": "DreadPoor/Trinas_Nectar-8B-model_stock (Merge)"
  },
  {
    "eval_name": "DreadPoor_WIP_Damascus-8B-TIES_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/WIP_Damascus-8B-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/WIP_Damascus-8B-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__WIP_Damascus-8B-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/WIP_Damascus-8B-TIES",
    "Model sha": "c7720a0b0a8d24e62bf71b0e955b1aca8e62f1cb",
    "Average ‚¨ÜÔ∏è": 24.731381344370785,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8181123053355764,
    "IFEval Raw": 0.4776326812856554,
    "IFEval": 47.76326812856554,
    "BBH Raw": 0.5410672913070808,
    "BBH": 34.52230581565854,
    "MATH Lvl 5 Raw": 0.1510574018126888,
    "MATH Lvl 5": 15.105740181268882,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.41185416666666663,
    "MUSR": 12.715104166666663,
    "MMLU-PRO Raw": 0.37608045212765956,
    "MMLU-PRO": 30.67560579196217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-29",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "DreadPoor_felix_dies-mistral-7B-model_stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/DreadPoor/felix_dies-mistral-7B-model_stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">DreadPoor/felix_dies-mistral-7B-model_stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/DreadPoor__felix_dies-mistral-7B-model_stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "DreadPoor/felix_dies-mistral-7B-model_stock",
    "Model sha": "bb317aa7565625327e18c5158aebd4710aa1d925",
    "Average ‚¨ÜÔ∏è": 18.101828101742136,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6615716673635748,
    "IFEval Raw": 0.30077860077926566,
    "IFEval": 30.077860077926566,
    "BBH Raw": 0.49009180735274227,
    "BBH": 28.890798050964488,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4518229166666667,
    "MUSR": 15.477864583333336,
    "MMLU-PRO Raw": 0.3109208776595745,
    "MMLU-PRO": 23.43565307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EleutherAI_gpt-j-6b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTJForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-j-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-j-6b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__gpt-j-6b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/gpt-j-6b",
    "Model sha": "47e169305d2e8376be1d31e765533382721b2cc1",
    "Average ‚¨ÜÔ∏è": 6.557823652110813,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1443,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7674321563855381,
    "IFEval Raw": 0.2522185578708937,
    "IFEval": 25.221855787089368,
    "BBH Raw": 0.3191044431037278,
    "BBH": 4.912818068323685,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.36575,
    "MUSR": 5.252083333333334,
    "MMLU-PRO Raw": 0.12408577127659574,
    "MMLU-PRO": 2.6761968085106376,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-08-19",
    "Generation": 0,
    "Base Model": "EleutherAI/gpt-j-6b"
  },
  {
    "eval_name": "EleutherAI_gpt-neo-1.3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-1.3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__gpt-neo-1.3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/gpt-neo-1.3B",
    "Model sha": "dbe59a7f4a88d01d1ba9798d78dbe3fe038792c8",
    "Average ‚¨ÜÔ∏è": 5.340738381554636,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 266,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3594240615190476,
    "IFEval Raw": 0.20790502533278366,
    "IFEval": 20.790502533278367,
    "BBH Raw": 0.30392315869356407,
    "BBH": 3.024569180930987,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.38165625,
    "MUSR": 4.873697916666666,
    "MMLU-PRO Raw": 0.1163563829787234,
    "MMLU-PRO": 1.8173758865248217,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "EleutherAI/gpt-neo-1.3B"
  },
  {
    "eval_name": "EleutherAI_gpt-neo-125m_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-125m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-125m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__gpt-neo-125m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/gpt-neo-125m",
    "Model sha": "21def0189f5705e2521767faed922f1f15e7d7db",
    "Average ‚¨ÜÔ∏è": 4.382145673978601,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 182,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2029024865932594,
    "IFEval Raw": 0.19054442213327305,
    "IFEval": 19.054442213327306,
    "BBH Raw": 0.3115156885791523,
    "BBH": 3.436738951426704,
    "MATH Lvl 5 Raw": 0.004531722054380664,
    "MATH Lvl 5": 0.4531722054380664,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.3593333333333333,
    "MUSR": 2.6166666666666654,
    "MMLU-PRO Raw": 0.10255984042553191,
    "MMLU-PRO": 0.28442671394798985,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "EleutherAI/gpt-neo-125m"
  },
  {
    "eval_name": "EleutherAI_gpt-neo-2.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neo-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neo-2.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__gpt-neo-2.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/gpt-neo-2.7B",
    "Model sha": "e24fa291132763e59f4a5422741b424fb5d59056",
    "Average ‚¨ÜÔ∏è": 6.355519100081103,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 442,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5083814016232907,
    "IFEval Raw": 0.2589628851447493,
    "IFEval": 25.896288514474925,
    "BBH Raw": 0.3139516033315253,
    "BBH": 4.178602667081014,
    "MATH Lvl 5 Raw": 0.006042296072507554,
    "MATH Lvl 5": 0.6042296072507554,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3553645833333334,
    "MUSR": 3.5205729166666675,
    "MMLU-PRO Raw": 0.11627327127659574,
    "MMLU-PRO": 1.8081412529550822,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "EleutherAI/gpt-neo-2.7B"
  },
  {
    "eval_name": "EleutherAI_gpt-neox-20b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/gpt-neox-20b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/gpt-neox-20b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__gpt-neox-20b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/gpt-neox-20b",
    "Model sha": "c292233c833e336628618a88a648727eb3dff0a7",
    "Average ‚¨ÜÔ∏è": 6.003229101114816,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 537,
    "#Params (B)": 20,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.146736373615041,
    "IFEval Raw": 0.2586880587951081,
    "IFEval": 25.86880587951081,
    "BBH Raw": 0.31650380320877564,
    "BBH": 4.929114201526899,
    "MATH Lvl 5 Raw": 0.006797583081570996,
    "MATH Lvl 5": 0.6797583081570996,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.36466666666666664,
    "MUSR": 2.816666666666666,
    "MMLU-PRO Raw": 0.1155252659574468,
    "MMLU-PRO": 1.725029550827422,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-04-07",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "EleutherAI/gpt-neox-20b"
  },
  {
    "eval_name": "EleutherAI_pythia-12b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/pythia-12b",
    "Model sha": "35c9d7f32fbb108fb8b5bdd574eb03369d1eed49",
    "Average ‚¨ÜÔ∏è": 5.9339603247654615,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 131,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1180071531663731,
    "IFEval Raw": 0.24714756845170813,
    "IFEval": 24.71475684517081,
    "BBH Raw": 0.3179653957935337,
    "BBH": 4.987531038290507,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.3646979166666667,
    "MUSR": 3.7872395833333345,
    "MMLU-PRO Raw": 0.11087101063829788,
    "MMLU-PRO": 1.2078900709219857,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-02-28",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "EleutherAI/pythia-12b"
  },
  {
    "eval_name": "EleutherAI_pythia-160m_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-160m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-160m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-160m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/pythia-160m",
    "Model sha": "50f5173d932e8e61f858120bcb800b97af589f46",
    "Average ‚¨ÜÔ∏è": 5.6171015655565055,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2353385413040577,
    "IFEval Raw": 0.18155161637787737,
    "IFEval": 18.155161637787735,
    "BBH Raw": 0.2970437484241321,
    "BBH": 2.198832279508135,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.4179375,
    "MUSR": 10.675520833333332,
    "MMLU-PRO Raw": 0.11195146276595745,
    "MMLU-PRO": 1.3279403073286051,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-02-08",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "EleutherAI/pythia-160m"
  },
  {
    "eval_name": "EleutherAI_pythia-2.8b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-2.8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-2.8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-2.8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/pythia-2.8b",
    "Model sha": "2a259cdd96a4beb1cdf467512e3904197345f6a9",
    "Average ‚¨ÜÔ∏è": 5.454241347061218,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7539020016215645,
    "IFEval Raw": 0.21732226049105263,
    "IFEval": 21.73222604910526,
    "BBH Raw": 0.3224085936276087,
    "BBH": 5.077786161905462,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.3485729166666667,
    "MUSR": 3.6382812500000004,
    "MMLU-PRO Raw": 0.11369680851063829,
    "MMLU-PRO": 1.521867612293143,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-02-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "EleutherAI/pythia-2.8b"
  },
  {
    "eval_name": "EleutherAI_pythia-410m_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-410m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-410m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-410m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/pythia-410m",
    "Model sha": "9879c9b5f8bea9051dcb0e68dff21493d67e9d4f",
    "Average ‚¨ÜÔ∏è": 5.113779260124896,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 21,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.37708182721725797,
    "IFEval Raw": 0.21954525104500505,
    "IFEval": 21.954525104500505,
    "BBH Raw": 0.302813387064426,
    "BBH": 2.7154281203357473,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.35781250000000003,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.11278257978723404,
    "MMLU-PRO": 1.4202866430260035,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-02-13",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "EleutherAI/pythia-410m"
  },
  {
    "eval_name": "EleutherAI_pythia-6.9b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EleutherAI/pythia-6.9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EleutherAI/pythia-6.9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EleutherAI__pythia-6.9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EleutherAI/pythia-6.9b",
    "Model sha": "f271943e880e60c0c715fd10e4dc74ec4e31eb44",
    "Average ‚¨ÜÔ∏è": 5.865841840200201,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 48,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8688669144579199,
    "IFEval Raw": 0.22811362739752744,
    "IFEval": 22.811362739752745,
    "BBH Raw": 0.3232287869322383,
    "BBH": 5.88163197981621,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.3590520833333333,
    "MUSR": 3.8148437499999996,
    "MMLU-PRO Raw": 0.1146941489361702,
    "MMLU-PRO": 1.6326832151300221,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-02-14",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "EleutherAI/pythia-6.9b"
  },
  {
    "eval_name": "Enno-Ai_EnnoAi-Pro-French-Llama-3-8B-v0.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Enno-Ai/EnnoAi-Pro-French-Llama-3-8B-v0.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Enno-Ai/EnnoAi-Pro-French-Llama-3-8B-v0.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Enno-Ai__EnnoAi-Pro-French-Llama-3-8B-v0.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Enno-Ai/EnnoAi-Pro-French-Llama-3-8B-v0.4",
    "Model sha": "328722ae96e3a112ec900dbe77d410788a526c5c",
    "Average ‚¨ÜÔ∏è": 15.180944731273977,
    "Hub License": "creativeml-openrail-m",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.009127965308664,
    "IFEval Raw": 0.4188807918545016,
    "IFEval": 41.88807918545016,
    "BBH Raw": 0.4074954889367559,
    "BBH": 16.875928374989595,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.41700000000000004,
    "MUSR": 10.758333333333335,
    "MMLU-PRO Raw": 0.2634640957446808,
    "MMLU-PRO": 18.162677304964536,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "Enno-Ai/EnnoAi-Pro-French-Llama-3-8B-v0.4"
  },
  {
    "eval_name": "Enno-Ai_EnnoAi-Pro-Llama-3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Enno-Ai/EnnoAi-Pro-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Enno-Ai/EnnoAi-Pro-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Enno-Ai__EnnoAi-Pro-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Enno-Ai/EnnoAi-Pro-Llama-3-8B",
    "Model sha": "6a5d745bdd304753244fe601e2a958d37d13cd71",
    "Average ‚¨ÜÔ∏è": 12.17466675780287,
    "Hub License": "creativeml-openrail-m",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1843373398046615,
    "IFEval Raw": 0.31953771548380516,
    "IFEval": 31.953771548380523,
    "BBH Raw": 0.4151575806137866,
    "BBH": 17.507545086854382,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.4070520833333333,
    "MUSR": 9.081510416666667,
    "MMLU-PRO Raw": 0.21509308510638298,
    "MMLU-PRO": 12.788120567375886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-01",
    "Submission Date": "2024-07-08",
    "Generation": 0,
    "Base Model": "Enno-Ai/EnnoAi-Pro-Llama-3-8B"
  },
  {
    "eval_name": "Enno-Ai_EnnoAi-Pro-Llama-3-8B-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Enno-Ai/EnnoAi-Pro-Llama-3-8B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Enno-Ai/EnnoAi-Pro-Llama-3-8B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Enno-Ai__EnnoAi-Pro-Llama-3-8B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Enno-Ai/EnnoAi-Pro-Llama-3-8B-v0.3",
    "Model sha": "cf29b8b484a909132e3a1f85ce891d28347c0d13",
    "Average ‚¨ÜÔ∏è": 17.524057891231795,
    "Hub License": "creativeml-openrail-m",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4708355820828696,
    "IFEval Raw": 0.5082569803676467,
    "IFEval": 50.82569803676467,
    "BBH Raw": 0.4100577461090639,
    "BBH": 16.668385554519382,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.42357291666666663,
    "MUSR": 12.313281249999996,
    "MMLU-PRO Raw": 0.2990359042553192,
    "MMLU-PRO": 22.11510047281324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Enno-Ai/EnnoAi-Pro-Llama-3-8B-v0.3"
  },
  {
    "eval_name": "Enno-Ai_EnnoAi-Pro-Llama-3.1-8B-v0.9_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Enno-Ai/EnnoAi-Pro-Llama-3.1-8B-v0.9\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Enno-Ai/EnnoAi-Pro-Llama-3.1-8B-v0.9</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Enno-Ai__EnnoAi-Pro-Llama-3.1-8B-v0.9-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Enno-Ai/EnnoAi-Pro-Llama-3.1-8B-v0.9",
    "Model sha": "c740871122fd471a1a225cf2b4368e333752d74c",
    "Average ‚¨ÜÔ∏è": 14.945694080269645,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9325712452005441,
    "IFEval Raw": 0.4689147018799009,
    "IFEval": 46.891470187990095,
    "BBH Raw": 0.41602720836190127,
    "BBH": 17.49829637438283,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3831770833333333,
    "MUSR": 5.43046875,
    "MMLU-PRO Raw": 0.2595578457446808,
    "MMLU-PRO": 17.728649527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "Enno-Ai/EnnoAi-Pro-Llama-3.1-8B-v0.9"
  },
  {
    "eval_name": "EnnoAi_EnnoAi-Pro-Llama-3.1-8B-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EnnoAi/EnnoAi-Pro-Llama-3.1-8B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EnnoAi/EnnoAi-Pro-Llama-3.1-8B-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EnnoAi__EnnoAi-Pro-Llama-3.1-8B-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EnnoAi/EnnoAi-Pro-Llama-3.1-8B-v1.0",
    "Model sha": "c740871122fd471a1a225cf2b4368e333752d74c",
    "Average ‚¨ÜÔ∏è": 14.97108966029361,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9456415922819899,
    "IFEval Raw": 0.4704384366813389,
    "IFEval": 47.04384366813389,
    "BBH Raw": 0.41602720836190127,
    "BBH": 17.49829637438283,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3831770833333333,
    "MUSR": 5.43046875,
    "MMLU-PRO Raw": 0.2595578457446808,
    "MMLU-PRO": 17.728649527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "EnnoAi/EnnoAi-Pro-Llama-3.1-8B-v1.0"
  },
  {
    "eval_name": "Epiculous_Azure_Dusk-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Epiculous/Azure_Dusk-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Epiculous/Azure_Dusk-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Epiculous__Azure_Dusk-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Epiculous/Azure_Dusk-v0.2",
    "Model sha": "ebddf1b2efbe7f9cae066d263b0991ded89c88e8",
    "Average ‚¨ÜÔ∏è": 14.050827219560906,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.991411391398455,
    "IFEval Raw": 0.346715603487635,
    "IFEval": 34.67156034876351,
    "BBH Raw": 0.4119721873553597,
    "BBH": 17.396414392379338,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3834583333333333,
    "MUSR": 6.365625000000002,
    "MMLU-PRO Raw": 0.3034408244680851,
    "MMLU-PRO": 22.604536052009458,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Epiculous/Azure_Dusk-v0.2"
  },
  {
    "eval_name": "Epiculous_Crimson_Dawn-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Epiculous/Crimson_Dawn-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Epiculous/Crimson_Dawn-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Epiculous__Crimson_Dawn-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Epiculous/Crimson_Dawn-v0.2",
    "Model sha": "4cceb1e25026afef241ad5325097e88eccd8f37a",
    "Average ‚¨ÜÔ∏è": 14.884540880721502,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.4923839709753426,
    "IFEval Raw": 0.3103454389907667,
    "IFEval": 31.034543899076674,
    "BBH Raw": 0.44823796489645434,
    "BBH": 21.68824851395527,
    "MATH Lvl 5 Raw": 0.030966767371601214,
    "MATH Lvl 5": 3.0966767371601214,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.4151770833333333,
    "MUSR": 10.897135416666666,
    "MMLU-PRO Raw": 0.27210771276595747,
    "MMLU-PRO": 19.123079196217496,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-02",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "Epiculous/Crimson_Dawn-v0.2"
  },
  {
    "eval_name": "Epiculous_NovaSpark_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Epiculous/NovaSpark\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Epiculous/NovaSpark</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Epiculous__NovaSpark-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Epiculous/NovaSpark",
    "Model sha": "a46340895859e470c3e69661f0b894677cf4c5cb",
    "Average ‚¨ÜÔ∏è": 25.22856175673255,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8181850247084421,
    "IFEval Raw": 0.6408473960203371,
    "IFEval": 64.08473960203372,
    "BBH Raw": 0.5063958663768304,
    "BBH": 29.52691068844395,
    "MATH Lvl 5 Raw": 0.15030211480362538,
    "MATH Lvl 5": 15.030211480362537,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.3881979166666667,
    "MUSR": 6.924739583333334,
    "MMLU-PRO Raw": 0.3648603723404255,
    "MMLU-PRO": 29.428930260047277,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "Epiculous/NovaSpark (Merge)"
  },
  {
    "eval_name": "Epiculous_Violet_Twilight-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Epiculous/Violet_Twilight-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Epiculous/Violet_Twilight-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Epiculous__Violet_Twilight-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Epiculous/Violet_Twilight-v0.2",
    "Model sha": "30c8bad3c1f565150afbf2fc90cacf4f45d096f6",
    "Average ‚¨ÜÔ∏è": 18.55277348742638,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7704359904032256,
    "IFEval Raw": 0.45317756885064964,
    "IFEval": 45.317756885064966,
    "BBH Raw": 0.4614552476845888,
    "BBH": 23.94053725590186,
    "MATH Lvl 5 Raw": 0.02870090634441088,
    "MATH Lvl 5": 2.870090634441088,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.42993750000000003,
    "MUSR": 13.608854166666665,
    "MMLU-PRO Raw": 0.3110871010638298,
    "MMLU-PRO": 23.45412234042553,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "Epiculous/Violet_Twilight-v0.2"
  },
  {
    "eval_name": "EpistemeAI_Alpaca-Llama3.1-8B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Alpaca-Llama3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Alpaca-Llama3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Alpaca-Llama3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Alpaca-Llama3.1-8B",
    "Model sha": "3152dfa17322dff7c6af6dbf3daceaf5db51e230",
    "Average ‚¨ÜÔ∏è": 13.922105768332225,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9208525873271449,
    "IFEval Raw": 0.15986914719610634,
    "IFEval": 15.986914719610633,
    "BBH Raw": 0.47552608539742874,
    "BBH": 25.93522655511771,
    "MATH Lvl 5 Raw": 0.04682779456193354,
    "MATH Lvl 5": 4.682779456193354,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.3402604166666667,
    "MUSR": 6.599218750000001,
    "MMLU-PRO Raw": 0.3246343085106383,
    "MMLU-PRO": 24.959367612293143,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-08-13",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Athena-gemma-2-2b-it_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Athena-gemma-2-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Athena-gemma-2-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Athena-gemma-2-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Athena-gemma-2-2b-it",
    "Model sha": "661c1dc6a1a096222e33416e099bd02b7b970405",
    "Average ‚¨ÜÔ∏è": 14.294329392765809,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.036798426888711,
    "IFEval Raw": 0.3134172883504657,
    "IFEval": 31.341728835046567,
    "BBH Raw": 0.42642293591146,
    "BBH": 19.417817674461514,
    "MATH Lvl 5 Raw": 0.033987915407854986,
    "MATH Lvl 5": 3.3987915407854987,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.43505208333333334,
    "MUSR": 13.348177083333333,
    "MMLU-PRO Raw": 0.2421875,
    "MMLU-PRO": 15.79861111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-29",
    "Submission Date": "2024-09-06",
    "Generation": 2,
    "Base Model": "unsloth/gemma-2-9b-it-bnb-4bit"
  },
  {
    "eval_name": "EpistemeAI_Athena-gemma-2-2b-it-Philos_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Athena-gemma-2-2b-it-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Athena-gemma-2-2b-it-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Athena-gemma-2-2b-it-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Athena-gemma-2-2b-it-Philos",
    "Model sha": "dea2b35d496bd32ed3c88d42ff3022654153f2e1",
    "Average ‚¨ÜÔ∏è": 15.122657277237366,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1285928174304563,
    "IFEval Raw": 0.4620950189940469,
    "IFEval": 46.20950189940469,
    "BBH Raw": 0.37947768790586744,
    "BBH": 13.212088152695856,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.43136458333333333,
    "MUSR": 12.853906250000001,
    "MMLU-PRO Raw": 0.22481715425531915,
    "MMLU-PRO": 13.86857269503546,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-05",
    "Generation": 1,
    "Base Model": "unsloth/gemma-2-2b-it-bnb-4bit"
  },
  {
    "eval_name": "EpistemeAI_Athene-codegemma-2-7b-it-alpaca-v1.3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Athene-codegemma-2-7b-it-alpaca-v1.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Athene-codegemma-2-7b-it-alpaca-v1.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Athene-codegemma-2-7b-it-alpaca-v1.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Athene-codegemma-2-7b-it-alpaca-v1.3",
    "Model sha": "9c26e1242a11178b53937bc0e9a744ef6141e05a",
    "Average ‚¨ÜÔ∏è": 17.314021588433324,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9719780087939942,
    "IFEval Raw": 0.40299405577201824,
    "IFEval": 40.299405577201824,
    "BBH Raw": 0.4331916189482215,
    "BBH": 20.87379456667128,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.4503020833333333,
    "MUSR": 14.854427083333334,
    "MMLU-PRO Raw": 0.25872672872340424,
    "MMLU-PRO": 17.636303191489358,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-06",
    "Submission Date": "2024-09-06",
    "Generation": 2,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_FineLlama3.1-8B-Instruct_4bit",
    "Precision": "4bit",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/FineLlama3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/FineLlama3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__FineLlama3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/FineLlama3.1-8B-Instruct",
    "Model sha": "a8b0fc584b10e0110e04f9d21c7f10d24391c1d5",
    "Average ‚¨ÜÔ∏è": 11.100786696591038,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3549609828071594,
    "IFEval Raw": 0.08000992921005155,
    "IFEval": 8.000992921005155,
    "BBH Raw": 0.45573635384163325,
    "BBH": 23.506618815003453,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3481666666666667,
    "MUSR": 4.954166666666666,
    "MMLU-PRO Raw": 0.3112533244680851,
    "MMLU-PRO": 23.472591607565015,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-12B",
    "Model sha": "e2ed12c3244f2502321fb20e76dfc72ad7817d6e",
    "Average ‚¨ÜÔ∏è": 15.5093551197229,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6185208209874729,
    "IFEval Raw": 0.1833501775289565,
    "IFEval": 18.335017752895652,
    "BBH Raw": 0.5110893652548262,
    "BBH": 30.666711502632058,
    "MATH Lvl 5 Raw": 0.0392749244712991,
    "MATH Lvl 5": 3.92749244712991,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.42363541666666665,
    "MUSR": 12.521093749999997,
    "MMLU-PRO Raw": 0.3343583776595745,
    "MMLU-PRO": 26.03981973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-20",
    "Submission Date": "2024-08-21",
    "Generation": 2,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-12B-v1.13a-philosophers_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-12B-v1.13a-philosophers\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-12B-v1.13a-philosophers</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-12B-v1.13a-philosophers-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-12B-v1.13a-philosophers",
    "Model sha": "7fa824d4a40abca3f8c75d432ea151dc0d1d67d6",
    "Average ‚¨ÜÔ∏è": 14.440864617241166,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6626633956315184,
    "IFEval Raw": 0.08755324760524298,
    "IFEval": 8.755324760524298,
    "BBH Raw": 0.5102697700597862,
    "BBH": 30.336232640303574,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4080729166666666,
    "MUSR": 9.975781249999995,
    "MMLU-PRO Raw": 0.3366855053191489,
    "MMLU-PRO": 26.29838947990544,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-28",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Alpaca-Llama-3.1-8B-Philos-DPO-200_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Alpaca-Llama-3.1-8B-Philos-DPO-200\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Alpaca-Llama-3.1-8B-Philos-DPO-200</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Alpaca-Llama-3.1-8B-Philos-DPO-200-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Alpaca-Llama-3.1-8B-Philos-DPO-200",
    "Model sha": "27d67626304954db71f21fec9e7fc516421274ec",
    "Average ‚¨ÜÔ∏è": 21.066973898540912,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9223811247844631,
    "IFEval Raw": 0.4577243934981405,
    "IFEval": 45.77243934981405,
    "BBH Raw": 0.4838398624677178,
    "BBH": 26.377774027551386,
    "MATH Lvl 5 Raw": 0.11933534743202419,
    "MATH Lvl 5": 11.933534743202419,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.39445833333333336,
    "MUSR": 6.907291666666666,
    "MMLU-PRO Raw": 0.35829454787234044,
    "MMLU-PRO": 28.69939420803782,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-16",
    "Generation": 3,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-KTO-beta_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-KTO-beta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-KTO-beta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-KTO-beta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-KTO-beta",
    "Model sha": "2851384717556dd6ac14c00ed87aac1f267eb263",
    "Average ‚¨ÜÔ∏è": 25.17928731284014,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8856449282711565,
    "IFEval Raw": 0.7274010735958367,
    "IFEval": 72.74010735958367,
    "BBH Raw": 0.48648902139668476,
    "BBH": 26.897964171299805,
    "MATH Lvl 5 Raw": 0.1487915407854985,
    "MATH Lvl 5": 14.879154078549849,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3619375,
    "MUSR": 4.275520833333334,
    "MMLU-PRO Raw": 0.3543051861702128,
    "MMLU-PRO": 28.25613179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-14",
    "Generation": 4,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R2",
    "Model sha": "b19336101aa5f4807d1574f4c11eebc1c1a1c34e",
    "Average ‚¨ÜÔ∏è": 22.537888510423016,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8117431345814908,
    "IFEval Raw": 0.46731561146646455,
    "IFEval": 46.731561146646456,
    "BBH Raw": 0.4932027479020209,
    "BBH": 28.24700927539328,
    "MATH Lvl 5 Raw": 0.12311178247734139,
    "MATH Lvl 5": 12.311178247734139,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.46236458333333336,
    "MUSR": 16.995572916666667,
    "MMLU-PRO Raw": 0.3351894946808511,
    "MMLU-PRO": 26.13216607565012,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-14",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-0.001-128K-auto_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-0.001-128K-auto\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-0.001-128K-auto</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-0.001-128K-auto-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-0.001-128K-auto",
    "Model sha": "19b23c434b6c4524e2146926cdbf4f0e927ae3ab",
    "Average ‚¨ÜÔ∏è": 21.57995002453019,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6949942794064364,
    "IFEval Raw": 0.44318630123627534,
    "IFEval": 44.31863012362753,
    "BBH Raw": 0.4823644760491404,
    "BBH": 26.832966985874886,
    "MATH Lvl 5 Raw": 0.1336858006042296,
    "MATH Lvl 5": 13.36858006042296,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.4066458333333333,
    "MUSR": 8.730729166666668,
    "MMLU-PRO Raw": 0.3515625,
    "MMLU-PRO": 27.95138888888889,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-15",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K",
    "Model sha": "b4a88fb5fb27fc5d8a503303cdb7aaeff373fd92",
    "Average ‚¨ÜÔ∏è": 20.62716790652169,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8147864660837938,
    "IFEval Raw": 0.4457339858242796,
    "IFEval": 44.573398582427956,
    "BBH Raw": 0.48973199216860547,
    "BBH": 28.02516078188715,
    "MATH Lvl 5 Raw": 0.12084592145015106,
    "MATH Lvl 5": 12.084592145015106,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.37622916666666667,
    "MUSR": 4.895312499999999,
    "MMLU-PRO Raw": 0.3543051861702128,
    "MMLU-PRO": 28.25613179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code",
    "Model sha": "8e8f1569a8a01ed3d6588f2669c730d4993355b5",
    "Average ‚¨ÜÔ∏è": 23.89694960766511,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8543177809047187,
    "IFEval Raw": 0.5975334335119704,
    "IFEval": 59.753343351197046,
    "BBH Raw": 0.4904191122627008,
    "BBH": 28.171887782172757,
    "MATH Lvl 5 Raw": 0.13141993957703926,
    "MATH Lvl 5": 13.141993957703926,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.40103125,
    "MUSR": 8.462239583333334,
    "MMLU-PRO Raw": 0.34225398936170215,
    "MMLU-PRO": 26.917109929078016,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-05",
    "Generation": 2,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds",
    "Model sha": "8b73dd02349f0544c48c581cc73ada5cac6ff946",
    "Average ‚¨ÜÔ∏è": 22.993108060977832,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7167339445503813,
    "IFEval Raw": 0.669099101495144,
    "IFEval": 66.9099101495144,
    "BBH Raw": 0.4668070143164938,
    "BBH": 24.462654168996078,
    "MATH Lvl 5 Raw": 0.1246223564954683,
    "MATH Lvl 5": 12.46223564954683,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.34178125,
    "MUSR": 4.555989583333334,
    "MMLU-PRO Raw": 0.33892952127659576,
    "MMLU-PRO": 26.547724586288417,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-15",
    "Generation": 4,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto",
    "Model sha": "f18598c62a783bcc0d436a35df0c8a335e8ee5d7",
    "Average ‚¨ÜÔ∏è": 23.74994070118673,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2853056830713743,
    "IFEval Raw": 0.7304984108831234,
    "IFEval": 73.04984108831235,
    "BBH Raw": 0.46492466713692354,
    "BBH": 24.58673708035273,
    "MATH Lvl 5 Raw": 0.13972809667673716,
    "MATH Lvl 5": 13.972809667673717,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.32088541666666665,
    "MUSR": 1.2106770833333331,
    "MMLU-PRO Raw": 0.34798869680851063,
    "MMLU-PRO": 27.554299645390067,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-ds-auto (Merge)"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-COT_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-COT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-COT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-COT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-COT",
    "Model sha": "bb90c19dc7c4a509e7bd73f4620dca818b58be25",
    "Average ‚¨ÜÔ∏è": 20.832251171540538,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8390365655346463,
    "IFEval Raw": 0.4578241288669619,
    "IFEval": 45.78241288669619,
    "BBH Raw": 0.4760520079608936,
    "BBH": 25.82086537586569,
    "MATH Lvl 5 Raw": 0.13670694864048338,
    "MATH Lvl 5": 13.670694864048338,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.3881354166666667,
    "MUSR": 6.450260416666667,
    "MMLU-PRO Raw": 0.3470744680851064,
    "MMLU-PRO": 27.452718676122934,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-11",
    "Generation": 3,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto",
    "Model sha": "db5ddb161ed26bc16baa814e31892dbe2f22b7a0",
    "Average ‚¨ÜÔ∏è": 23.760965489012914,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7451309547743399,
    "IFEval Raw": 0.7204816553411615,
    "IFEval": 72.04816553411615,
    "BBH Raw": 0.4817795525811035,
    "BBH": 26.452059604470918,
    "MATH Lvl 5 Raw": 0.13670694864048338,
    "MATH Lvl 5": 13.670694864048338,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.33,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.35480385638297873,
    "MMLU-PRO": 28.311539598108748,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.004-128K-code-ds-auto (Merge)"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.1-8B-Instruct-Math_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Math\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Math</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.1-8B-Instruct-Math-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.1-8B-Instruct-Math",
    "Model sha": "677c97b4f92bfc330d4fae628e9a1df1ef606dcc",
    "Average ‚¨ÜÔ∏è": 20.545340873212893,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9102715980909137,
    "IFEval Raw": 0.46229559790245434,
    "IFEval": 46.22955979024543,
    "BBH Raw": 0.49829504320793055,
    "BBH": 28.95934409387967,
    "MATH Lvl 5 Raw": 0.10725075528700907,
    "MATH Lvl 5": 10.725075528700907,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.3640729166666667,
    "MUSR": 5.97578125,
    "MMLU-PRO Raw": 0.33311170212765956,
    "MMLU-PRO": 25.901300236406616,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Meta-Llama-3.2-8B-Instruct-agent-003-128k-code-DPO",
    "Model sha": "b3c0fce7daa359cd8ed5be6595dd1a76ca2cfea2",
    "Average ‚¨ÜÔ∏è": 21.205444597479687,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.833575782418707,
    "IFEval Raw": 0.46109655713506825,
    "IFEval": 46.109655713506825,
    "BBH Raw": 0.48010141537970213,
    "BBH": 26.317877757648734,
    "MATH Lvl 5 Raw": 0.12009063444108761,
    "MATH Lvl 5": 12.009063444108762,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.3998229166666667,
    "MUSR": 8.077864583333334,
    "MMLU-PRO Raw": 0.35206117021276595,
    "MMLU-PRO": 28.00679669030733,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-09",
    "Generation": 3,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Fireball-Mistral-Nemo-Base-2407-v1-DPO2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Fireball-Mistral-Nemo-Base-2407-v1-DPO2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Fireball-Mistral-Nemo-Base-2407-v1-DPO2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Fireball-Mistral-Nemo-Base-2407-v1-DPO2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Fireball-Mistral-Nemo-Base-2407-v1-DPO2",
    "Model sha": "2cf732fbffefdf37341b946edd7995f14d3f9487",
    "Average ‚¨ÜÔ∏è": 15.276399716219549,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7712690994345468,
    "IFEval Raw": 0.18607295309778055,
    "IFEval": 18.607295309778056,
    "BBH Raw": 0.49677687590350894,
    "BBH": 28.567824892702276,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4040104166666667,
    "MUSR": 9.501302083333336,
    "MMLU-PRO Raw": 0.33527260638297873,
    "MMLU-PRO": 26.14140070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-19",
    "Submission Date": "2024-08-19",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI_Llama-3.2-3B-Agent007-Coder_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Llama-3.2-3B-Agent007-Coder\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Llama-3.2-3B-Agent007-Coder</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Llama-3.2-3B-Agent007-Coder-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Llama-3.2-3B-Agent007-Coder",
    "Model sha": "7ff4e77796b6d308e96d0150e1a01081c0b82e01",
    "Average ‚¨ÜÔ∏è": 18.901973692089324,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7108156711648944,
    "IFEval Raw": 0.5399562050913798,
    "IFEval": 53.99562050913798,
    "BBH Raw": 0.4303758760727905,
    "BBH": 19.02580948500905,
    "MATH Lvl 5 Raw": 0.11027190332326285,
    "MATH Lvl 5": 11.027190332326285,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.36680208333333336,
    "MUSR": 7.783593750000001,
    "MMLU-PRO Raw": 0.28515625,
    "MMLU-PRO": 20.572916666666664,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "EpistemeAI_Mistral-Nemo-Instruct-12B-Philosophy-Math_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI/Mistral-Nemo-Instruct-12B-Philosophy-Math\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI/Mistral-Nemo-Instruct-12B-Philosophy-Math</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI__Mistral-Nemo-Instruct-12B-Philosophy-Math-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI/Mistral-Nemo-Instruct-12B-Philosophy-Math",
    "Model sha": "1ac4205f8da109326b4a5cf173e5491a20087d76",
    "Average ‚¨ÜÔ∏è": 16.566232425191405,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3636074640143,
    "IFEval Raw": 0.06946790072563022,
    "IFEval": 6.946790072563022,
    "BBH Raw": 0.5364928342081372,
    "BBH": 33.835810999564586,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.42921875,
    "MUSR": 12.885677083333329,
    "MMLU-PRO Raw": 0.32962101063829785,
    "MMLU-PRO": 25.513445626477537,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"
  },
  {
    "eval_name": "EpistemeAI2_Athene-codegemma-2-7b-it-alpaca-v1.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Athene-codegemma-2-7b-it-alpaca-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Athene-codegemma-2-7b-it-alpaca-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Athene-codegemma-2-7b-it-alpaca-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Athene-codegemma-2-7b-it-alpaca-v1.2",
    "Model sha": "21b31062334a316b50680e8c3a141a72e4c30b61",
    "Average ‚¨ÜÔ∏è": 15.693214614615044,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.969634842541196,
    "IFEval Raw": 0.4351177098986245,
    "IFEval": 43.511770989862455,
    "BBH Raw": 0.41754154460978427,
    "BBH": 18.971369774912947,
    "MATH Lvl 5 Raw": 0.04078549848942599,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.41696875000000005,
    "MUSR": 10.387760416666667,
    "MMLU-PRO Raw": 0.22972074468085107,
    "MMLU-PRO": 14.413416075650117,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-08-26",
    "Generation": 2,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-12B-v1.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-12B-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-12B-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-12B-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-12B-v1.2",
    "Model sha": "57af42edf8232189ee99e9a21e33a0c306e3f561",
    "Average ‚¨ÜÔ∏è": 15.162522233382598,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.872564688227262,
    "IFEval Raw": 0.13553925805750963,
    "IFEval": 13.553925805750964,
    "BBH Raw": 0.5018583230653281,
    "BBH": 29.776014226579218,
    "MATH Lvl 5 Raw": 0.03927492447129909,
    "MATH Lvl 5": 3.927492447129909,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4173125,
    "MUSR": 11.264062499999996,
    "MMLU-PRO Raw": 0.33369348404255317,
    "MMLU-PRO": 25.965942671394792,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1-8B-Philos_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1-8B-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1-8B-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1-8B-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1-8B-Philos",
    "Model sha": "3dcca4cf9bdd9003c8dc91f5c78cefef1d4ae0d7",
    "Average ‚¨ÜÔ∏è": 22.539085461792496,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8483321380464708,
    "IFEval Raw": 0.498640274471735,
    "IFEval": 49.8640274471735,
    "BBH Raw": 0.4977581192690881,
    "BBH": 29.259226071264724,
    "MATH Lvl 5 Raw": 0.1178247734138973,
    "MATH Lvl 5": 11.782477341389729,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.42766666666666664,
    "MUSR": 11.891666666666667,
    "MMLU-PRO Raw": 0.3405917553191489,
    "MMLU-PRO": 26.73241725768321,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-29",
    "Submission Date": "2024-08-29",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.01-8B-Philos_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.01-8B-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.01-8B-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.01-8B-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.01-8B-Philos",
    "Model sha": "f97293ed5cec7fb9482b16600259967c6c923e4b",
    "Average ‚¨ÜÔ∏è": 21.567143968773337,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8705716876801622,
    "IFEval Raw": 0.42117913802045237,
    "IFEval": 42.11791380204524,
    "BBH Raw": 0.49561092312727917,
    "BBH": 28.628475325906475,
    "MATH Lvl 5 Raw": 0.13595166163141995,
    "MATH Lvl 5": 13.595166163141995,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.43706249999999996,
    "MUSR": 13.432812500000002,
    "MMLU-PRO Raw": 0.33834773936170215,
    "MMLU-PRO": 26.483082151300234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-03",
    "Submission Date": "2024-09-03",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.03-8B-Philos_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.03-8B-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.03-8B-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.03-8B-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.03-8B-Philos",
    "Model sha": "6e60f783f80f7d126b8e4f2b417e14dea63d2c4f",
    "Average ‚¨ÜÔ∏è": 20.299749591553304,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7975230737879972,
    "IFEval Raw": 0.3880814017916905,
    "IFEval": 38.808140179169044,
    "BBH Raw": 0.49508699339363266,
    "BBH": 27.99254879661235,
    "MATH Lvl 5 Raw": 0.12990936555891236,
    "MATH Lvl 5": 12.990936555891237,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.42801041666666667,
    "MUSR": 12.034635416666667,
    "MMLU-PRO Raw": 0.3355219414893617,
    "MMLU-PRO": 26.16910460992908,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-04",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.04-8B-Philos_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.04-8B-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.04-8B-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.04-8B-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.04-8B-Philos",
    "Model sha": "efd0c251373e1a2fa2bc8cead502c03ff6dc7c8b",
    "Average ‚¨ÜÔ∏è": 21.031576913237085,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7652479183229487,
    "IFEval Raw": 0.40843960690966635,
    "IFEval": 40.84396069096664,
    "BBH Raw": 0.4930009712421776,
    "BBH": 27.963797525362725,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.43721875,
    "MUSR": 13.685677083333337,
    "MMLU-PRO Raw": 0.3402593085106383,
    "MMLU-PRO": 26.695478723404253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-05",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.06-8B-Philos-dpo_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.06-8B-Philos-dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.06-8B-Philos-dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.06-8B-Philos-dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.06-8B-Philos-dpo",
    "Model sha": "3e76f190b505b515479cc25e92f8229c2b05159f",
    "Average ‚¨ÜÔ∏è": 21.829867402445903,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9347739599951074,
    "IFEval Raw": 0.4865756193566404,
    "IFEval": 48.657561935664035,
    "BBH Raw": 0.48807730539009225,
    "BBH": 27.207176615070413,
    "MATH Lvl 5 Raw": 0.1283987915407855,
    "MATH Lvl 5": 12.83987915407855,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.3931875,
    "MUSR": 6.848437499999999,
    "MMLU-PRO Raw": 0.3614527925531915,
    "MMLU-PRO": 29.050310283687946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-09",
    "Generation": 4,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.07-8B-Philos-Math_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.07-8B-Philos-Math-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.07-8B-Philos-Math",
    "Model sha": "0b2842bddfa6c308f67eb5a20daf04536a4e6d1a",
    "Average ‚¨ÜÔ∏è": 21.870165100447196,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9020296328434971,
    "IFEval Raw": 0.5079079065767719,
    "IFEval": 50.79079065767719,
    "BBH Raw": 0.4847020640542447,
    "BBH": 26.901201452028392,
    "MATH Lvl 5 Raw": 0.11404833836858007,
    "MATH Lvl 5": 11.404833836858007,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40630208333333334,
    "MUSR": 7.854427083333334,
    "MMLU-PRO Raw": 0.35305851063829785,
    "MMLU-PRO": 28.117612293144205,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-10",
    "Generation": 3,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.08-8B-C-R1-KTO-Reflection_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-C-R1-KTO-Reflection\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-C-R1-KTO-Reflection</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.08-8B-C-R1-KTO-Reflection-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-C-R1-KTO-Reflection",
    "Model sha": "dc900138b4406353b7e84251bc8649d70c16f13f",
    "Average ‚¨ÜÔ∏è": 20.882036817993153,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8839740092124098,
    "IFEval Raw": 0.39522577871159636,
    "IFEval": 39.52257787115964,
    "BBH Raw": 0.49553052334314723,
    "BBH": 27.571611204577167,
    "MATH Lvl 5 Raw": 0.12386706948640484,
    "MATH Lvl 5": 12.386706948640484,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.4048125,
    "MUSR": 10.4015625,
    "MMLU-PRO Raw": 0.35929188829787234,
    "MMLU-PRO": 28.8102098108747,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-16",
    "Generation": 5,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Alpaca-Llama3.1.08-8B-Philos-C-R1",
    "Model sha": "c57c786426123635baf6c8b4d30638d2053f4565",
    "Average ‚¨ÜÔ∏è": 22.41048331094016,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9097590647761513,
    "IFEval Raw": 0.5316382753316755,
    "IFEval": 53.16382753316755,
    "BBH Raw": 0.4827931104634334,
    "BBH": 26.76368521382555,
    "MATH Lvl 5 Raw": 0.11782477341389726,
    "MATH Lvl 5": 11.782477341389725,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4103020833333333,
    "MUSR": 8.454427083333332,
    "MMLU-PRO Raw": 0.3523105053191489,
    "MMLU-PRO": 28.034500591016542,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-13",
    "Generation": 3,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Llama-3.1-8B-Philos-Reflection_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Llama-3.1-8B-Philos-Reflection\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Llama-3.1-8B-Philos-Reflection</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Llama-3.1-8B-Philos-Reflection-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Llama-3.1-8B-Philos-Reflection",
    "Model sha": "4b0b75d9235886e8a947c45b94f87c5a65a81467",
    "Average ‚¨ÜÔ∏è": 20.389309021116734,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.894943365376195,
    "IFEval Raw": 0.3596047376516532,
    "IFEval": 35.96047376516532,
    "BBH Raw": 0.4897693552241443,
    "BBH": 27.76979570236311,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.3957291666666667,
    "MUSR": 9.632812500000002,
    "MMLU-PRO Raw": 0.3550531914893617,
    "MMLU-PRO": 28.33924349881796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-17",
    "Generation": 4,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-MathMistral-Nemo-Base-2407-v2dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-MathMistral-Nemo-Base-2407-v2dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-MathMistral-Nemo-Base-2407-v2dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-MathMistral-Nemo-Base-2407-v2dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-MathMistral-Nemo-Base-2407-v2dpo",
    "Model sha": "6b7d851c66359f39d16da6fbcf810b816dc6e4bc",
    "Average ‚¨ÜÔ∏è": 11.332218395902872,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8814256364904496,
    "IFEval Raw": 0.30972043067948596,
    "IFEval": 30.972043067948597,
    "BBH Raw": 0.43276373285682107,
    "BBH": 21.145528378150857,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.4029583333333333,
    "MUSR": 8.96979166666667,
    "MMLU-PRO Raw": 0.11477726063829788,
    "MMLU-PRO": 1.6419178486997636,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-08-24",
    "Generation": 2,
    "Base Model": "unsloth/Mistral-Nemo-Base-2407-bnb-4bit"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.003-128K-code-math",
    "Model sha": "aa21037cf0984cb293facb69c41895e7fccb1340",
    "Average ‚¨ÜÔ∏è": 22.677604835020144,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7916827345659109,
    "IFEval Raw": 0.5515465631191904,
    "IFEval": 55.15465631191904,
    "BBH Raw": 0.48075580310342053,
    "BBH": 26.743767165585172,
    "MATH Lvl 5 Raw": 0.13217522658610273,
    "MATH Lvl 5": 13.217522658610273,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.36925,
    "MUSR": 6.789583333333333,
    "MMLU-PRO Raw": 0.3420046542553192,
    "MMLU-PRO": 26.889406028368796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-12",
    "Generation": 3,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Meta-Llama-3.1-8B-Instruct-Agent-0.005-128K-code-COT",
    "Model sha": "cf8b99d4aa00c18fdaebfb24fa3c674ee6defa1a",
    "Average ‚¨ÜÔ∏è": 20.99999448145977,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8008176147484493,
    "IFEval Raw": 0.4633195476890207,
    "IFEval": 46.331954768902065,
    "BBH Raw": 0.4790834283312441,
    "BBH": 26.400991557555106,
    "MATH Lvl 5 Raw": 0.11480362537764352,
    "MATH Lvl 5": 11.480362537764352,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.37743750000000004,
    "MUSR": 5.013020833333332,
    "MMLU-PRO Raw": 0.3564660904255319,
    "MMLU-PRO": 28.496232269503547,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-11",
    "Generation": 3,
    "Base Model": "Removed"
  },
  {
    "eval_name": "EpistemeAI2_Fireball-Phi-3-medium-4k-inst-Philos_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/EpistemeAI2/Fireball-Phi-3-medium-4k-inst-Philos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">EpistemeAI2/Fireball-Phi-3-medium-4k-inst-Philos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/EpistemeAI2__Fireball-Phi-3-medium-4k-inst-Philos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "EpistemeAI2/Fireball-Phi-3-medium-4k-inst-Philos",
    "Model sha": "147715051102034fac98091e2a0cae6cade15ae0",
    "Average ‚¨ÜÔ∏è": 29.17284225733607,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7718137674592886,
    "IFEval Raw": 0.5312880933700359,
    "IFEval": 53.128809337003595,
    "BBH Raw": 0.6177842639287514,
    "BBH": 46.20887281141656,
    "MATH Lvl 5 Raw": 0.1404833836858006,
    "MATH Lvl 5": 14.04833836858006,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.41390625,
    "MUSR": 10.704947916666669,
    "MMLU-PRO Raw": 0.45985704787234044,
    "MMLU-PRO": 39.984116430260045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-20",
    "Generation": 1,
    "Base Model": "unsloth/phi-3-medium-4k-instruct-bnb-4bit"
  },
  {
    "eval_name": "Eric111_CatunaMayo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Eric111/CatunaMayo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Eric111/CatunaMayo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Eric111__CatunaMayo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Eric111/CatunaMayo",
    "Model sha": "23337893381293975cbcc35f75b634954fbcefaf",
    "Average ‚¨ÜÔ∏è": 21.29915504404221,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5508246280271973,
    "IFEval Raw": 0.4074156571231,
    "IFEval": 40.741565712310006,
    "BBH Raw": 0.5243635518600797,
    "BBH": 33.299425909067885,
    "MATH Lvl 5 Raw": 0.08610271903323263,
    "MATH Lvl 5": 8.610271903323262,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.45398958333333334,
    "MUSR": 15.348697916666667,
    "MMLU-PRO Raw": 0.3178191489361702,
    "MMLU-PRO": 24.202127659574465,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-15",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "Eric111/CatunaMayo"
  },
  {
    "eval_name": "Eric111_CatunaMayo-DPO_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Eric111/CatunaMayo-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Eric111/CatunaMayo-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Eric111__CatunaMayo-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Eric111/CatunaMayo-DPO",
    "Model sha": "6bdbe06c10d57d152dd8a79a71edd8e30135b689",
    "Average ‚¨ÜÔ∏è": 21.255120617115054,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5540226403673537,
    "IFEval Raw": 0.4214539643700936,
    "IFEval": 42.14539643700936,
    "BBH Raw": 0.5223991323844243,
    "BBH": 33.08995159999345,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.44503125,
    "MUSR": 14.662239583333337,
    "MMLU-PRO Raw": 0.3169880319148936,
    "MMLU-PRO": 24.109781323877066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-21",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "Eric111/CatunaMayo-DPO"
  },
  {
    "eval_name": "Etherll_Chocolatine-3B-Instruct-DPO-Revised-Ties_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Chocolatine-3B-Instruct-DPO-Revised-Ties-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties",
    "Model sha": "8a9c3d745e0805e769b544622b3f5c039abc9b07",
    "Average ‚¨ÜÔ∏è": 24.402767356336923,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6354969306598104,
    "IFEval Raw": 0.3724694920588483,
    "IFEval": 37.24694920588483,
    "BBH Raw": 0.5410649663618229,
    "BBH": 35.58334267696659,
    "MATH Lvl 5 Raw": 0.1283987915407855,
    "MATH Lvl 5": 12.83987915407855,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4649375,
    "MUSR": 17.817187499999996,
    "MMLU-PRO Raw": 0.39777260638297873,
    "MMLU-PRO": 33.08584515366431,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Etherll_Chocolatine-3B-Instruct-DPO-Revised-Ties-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Chocolatine-3B-Instruct-DPO-Revised-Ties-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Chocolatine-3B-Instruct-DPO-Revised-Ties-v2",
    "Model sha": "121b0831361743558e1a56fd89ae3d3c03272cc4",
    "Average ‚¨ÜÔ∏è": 24.42816293636089,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6312955389348048,
    "IFEval Raw": 0.37399322686028624,
    "IFEval": 37.399322686028626,
    "BBH Raw": 0.5410649663618229,
    "BBH": 35.58334267696659,
    "MATH Lvl 5 Raw": 0.1283987915407855,
    "MATH Lvl 5": 12.83987915407855,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4649375,
    "MUSR": 17.817187499999996,
    "MMLU-PRO Raw": 0.39777260638297873,
    "MMLU-PRO": 33.08584515366431,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-29",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Etherll_Herplete-LLM-Llama-3.1-8b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Herplete-LLM-Llama-3.1-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Herplete-LLM-Llama-3.1-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Herplete-LLM-Llama-3.1-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Herplete-LLM-Llama-3.1-8b",
    "Model sha": "b3829cf437216f099c031a9ab5e4c8ec974766dd",
    "Average ‚¨ÜÔ∏è": 19.58870802333299,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9736847295609784,
    "IFEval Raw": 0.46719149634082013,
    "IFEval": 46.71914963408201,
    "BBH Raw": 0.5013428726325629,
    "BBH": 28.952590926070883,
    "MATH Lvl 5 Raw": 0.027945619335347432,
    "MATH Lvl 5": 2.794561933534743,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.38599999999999995,
    "MUSR": 6.6833333333333345,
    "MMLU-PRO Raw": 0.34815492021276595,
    "MMLU-PRO": 27.57276891252955,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-08-29",
    "Generation": 1,
    "Base Model": "Etherll/Herplete-LLM-Llama-3.1-8b (Merge)"
  },
  {
    "eval_name": "Etherll_Herplete-LLM-Llama-3.1-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Herplete-LLM-Llama-3.1-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Herplete-LLM-Llama-3.1-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Herplete-LLM-Llama-3.1-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Herplete-LLM-Llama-3.1-8b",
    "Model sha": "d1383d993fad005d515be4d815797019601c679f",
    "Average ‚¨ÜÔ∏è": 26.260139418825688,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8548066413081815,
    "IFEval Raw": 0.6105976586568084,
    "IFEval": 61.05976586568083,
    "BBH Raw": 0.5347253355929804,
    "BBH": 33.206608363709044,
    "MATH Lvl 5 Raw": 0.15483383685800603,
    "MATH Lvl 5": 15.483383685800604,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.3990520833333333,
    "MUSR": 8.614843749999995,
    "MMLU-PRO Raw": 0.375249335106383,
    "MMLU-PRO": 30.583259456264773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-10-18",
    "Generation": 1,
    "Base Model": "Etherll/Herplete-LLM-Llama-3.1-8b (Merge)"
  },
  {
    "eval_name": "Etherll_Herplete-LLM-Llama-3.1-8b-Ties_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Herplete-LLM-Llama-3.1-8b-Ties\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Herplete-LLM-Llama-3.1-8b-Ties</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Herplete-LLM-Llama-3.1-8b-Ties-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Herplete-LLM-Llama-3.1-8b-Ties",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 26.571055637376002,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8622014709516758,
    "IFEval Raw": 0.6163679038285084,
    "IFEval": 61.63679038285083,
    "BBH Raw": 0.5337975953250876,
    "BBH": 33.07089034564546,
    "MATH Lvl 5 Raw": 0.1623867069486405,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.40171874999999996,
    "MUSR": 8.948177083333329,
    "MMLU-PRO Raw": 0.375249335106383,
    "MMLU-PRO": 30.583259456264773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "Etherll/Herplete-LLM-Llama-3.1-8b-Ties (Merge)"
  },
  {
    "eval_name": "Etherll_Qwen2.5-7B-della-test_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Qwen2.5-7B-della-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Qwen2.5-7B-della-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Qwen2.5-7B-della-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Qwen2.5-7B-della-test",
    "Model sha": "c2b2ffc38627e68e7b43a1b596dc16ee93c1c63b",
    "Average ‚¨ÜÔ∏è": 27.65946775351119,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3857420447130364,
    "IFEval Raw": 0.7624968417133207,
    "IFEval": 76.24968417133206,
    "BBH Raw": 0.5447331985391859,
    "BBH": 35.54689390845198,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.40469791666666666,
    "MUSR": 8.987239583333334,
    "MMLU-PRO Raw": 0.4360871010638298,
    "MMLU-PRO": 37.34301122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "Etherll/Qwen2.5-7B-della-test (Merge)"
  },
  {
    "eval_name": "Etherll_Qwen2.5-Coder-7B-Instruct-Ties_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Qwen2.5-Coder-7B-Instruct-Ties\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Qwen2.5-Coder-7B-Instruct-Ties</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Qwen2.5-Coder-7B-Instruct-Ties-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Qwen2.5-Coder-7B-Instruct-Ties",
    "Model sha": "d8c1624a2fa60f05030e34a128af391b5d8be332",
    "Average ‚¨ÜÔ∏è": 24.474445146440242,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1971812617596305,
    "IFEval Raw": 0.5005385709916355,
    "IFEval": 50.05385709916355,
    "BBH Raw": 0.4895144464043051,
    "BBH": 28.008294264156472,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.43728125,
    "MUSR": 13.426822916666667,
    "MMLU-PRO Raw": 0.3503158244680851,
    "MMLU-PRO": 27.812869385342786,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "Etherll/Qwen2.5-Coder-7B-Instruct-Ties (Merge)"
  },
  {
    "eval_name": "Etherll_Replete-LLM-V3-Llama-3.1-8b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/Replete-LLM-V3-Llama-3.1-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/Replete-LLM-V3-Llama-3.1-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__Replete-LLM-V3-Llama-3.1-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/Replete-LLM-V3-Llama-3.1-8b",
    "Model sha": "e79849d72f70ef74677ed81a8885403973b2470c",
    "Average ‚¨ÜÔ∏è": 17.927882200113448,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7893293211423049,
    "IFEval Raw": 0.5262924595628488,
    "IFEval": 52.629245956284876,
    "BBH Raw": 0.4543377420594779,
    "BBH": 22.902455222412783,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3516458333333334,
    "MUSR": 2.0557291666666675,
    "MMLU-PRO Raw": 0.34699135638297873,
    "MMLU-PRO": 27.44348404255319,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-08-26",
    "Generation": 1,
    "Base Model": "Etherll/Replete-LLM-V3-Llama-3.1-8b (Merge)"
  },
  {
    "eval_name": "Etherll_SuperHermes_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Etherll/SuperHermes\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Etherll/SuperHermes</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Etherll__SuperHermes-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Etherll/SuperHermes",
    "Model sha": "7edd56cb37722d09b0334826e0532b223d334939",
    "Average ‚¨ÜÔ∏è": 26.604602322368482,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.750015414802549,
    "IFEval Raw": 0.5459015412438996,
    "IFEval": 54.590154124389954,
    "BBH Raw": 0.5289531792679852,
    "BBH": 32.84031674117277,
    "MATH Lvl 5 Raw": 0.14652567975830816,
    "MATH Lvl 5": 14.652567975830816,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.44004166666666666,
    "MUSR": 14.938541666666666,
    "MMLU-PRO Raw": 0.39486369680851063,
    "MMLU-PRO": 32.7626329787234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-27",
    "Generation": 1,
    "Base Model": "Etherll/SuperHermes (Merge)"
  },
  {
    "eval_name": "Eurdem_Defne-llama3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Eurdem/Defne-llama3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Eurdem/Defne-llama3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Eurdem__Defne-llama3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Eurdem/Defne-llama3.1-8B",
    "Model sha": "7832ba3066636bf4dab3e7d658c0b3ded12491ae",
    "Average ‚¨ÜÔ∏è": 25.095429177394767,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7203004935938937,
    "IFEval Raw": 0.5036115285220991,
    "IFEval": 50.36115285220991,
    "BBH Raw": 0.5320979090308238,
    "BBH": 32.822381370434904,
    "MATH Lvl 5 Raw": 0.15861027190332327,
    "MATH Lvl 5": 15.861027190332328,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.43309375,
    "MUSR": 13.536718749999999,
    "MMLU-PRO Raw": 0.3865525265957447,
    "MMLU-PRO": 31.839169621749413,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-08-14",
    "Generation": 0,
    "Base Model": "Eurdem/Defne-llama3.1-8B"
  },
  {
    "eval_name": "FallenMerick_Chewy-Lemon-Cookie-11B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FallenMerick/Chewy-Lemon-Cookie-11B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FallenMerick/Chewy-Lemon-Cookie-11B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FallenMerick__Chewy-Lemon-Cookie-11B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FallenMerick/Chewy-Lemon-Cookie-11B",
    "Model sha": "0f5d0d6d218b3ef034f58eba32d6fe7ac4c237ae",
    "Average ‚¨ÜÔ∏è": 22.018549420148144,
    "Hub License": "cc-by-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8572737776360095,
    "IFEval Raw": 0.4875242135312083,
    "IFEval": 48.75242135312082,
    "BBH Raw": 0.5251122307375103,
    "BBH": 33.01430008846961,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.45455208333333336,
    "MUSR": 15.952343750000002,
    "MMLU-PRO Raw": 0.3267121010638298,
    "MMLU-PRO": 25.190233451536642,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-06",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "FallenMerick/Chewy-Lemon-Cookie-11B (Merge)"
  },
  {
    "eval_name": "Felladrin_Llama-160M-Chat-v1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Felladrin/Llama-160M-Chat-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Felladrin/Llama-160M-Chat-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Felladrin__Llama-160M-Chat-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Felladrin/Llama-160M-Chat-v1",
    "Model sha": "e7f50665676821867ee7dfad32d0ca9fb68fc6bc",
    "Average ‚¨ÜÔ∏è": 4.10106118080753,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.18158060913953425,
    "IFEval Raw": 0.15754642127333254,
    "IFEval": 15.754642127333252,
    "BBH Raw": 0.30360811146348365,
    "BBH": 3.166755569392556,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.366125,
    "MUSR": 3.165625,
    "MMLU-PRO Raw": 0.11361369680851063,
    "MMLU-PRO": 1.512632978723403,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-20",
    "Submission Date": "2024-07-23",
    "Generation": 1,
    "Base Model": "JackFram/llama-160m"
  },
  {
    "eval_name": "Felladrin_Minueza-32M-UltraChat_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Felladrin/Minueza-32M-UltraChat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Felladrin/Minueza-32M-UltraChat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Felladrin__Minueza-32M-UltraChat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Felladrin/Minueza-32M-UltraChat",
    "Model sha": "28506b99c5902d2215eb378ec91d4226a7396c49",
    "Average ‚¨ÜÔ∏è": 3.8487272872743543,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.16806688951705662,
    "IFEval Raw": 0.13756277787381924,
    "IFEval": 13.756277787381924,
    "BBH Raw": 0.2941478734048925,
    "BBH": 2.4372895622895623,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.37418749999999995,
    "MUSR": 4.640104166666668,
    "MMLU-PRO Raw": 0.11328125,
    "MMLU-PRO": 1.4756944444444438,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-27",
    "Submission Date": "2024-07-23",
    "Generation": 1,
    "Base Model": "Felladrin/Minueza-32M-Base"
  },
  {
    "eval_name": "FuJhen_ft-openhermes-25-mistral-7b-irca-dpo-pairs_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FuJhen/ft-openhermes-25-mistral-7b-irca-dpo-pairs\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FuJhen/ft-openhermes-25-mistral-7b-irca-dpo-pairs</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FuJhen__ft-openhermes-25-mistral-7b-irca-dpo-pairs-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FuJhen/ft-openhermes-25-mistral-7b-irca-dpo-pairs",
    "Model sha": "24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33",
    "Average ‚¨ÜÔ∏è": 19.615524943255753,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0020482177522776,
    "IFEval Raw": 0.5420041046645123,
    "IFEval": 54.20041046645124,
    "BBH Raw": 0.47730323895548116,
    "BBH": 26.596860970431894,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.417375,
    "MUSR": 11.20520833333333,
    "MMLU-PRO Raw": 0.2956283244680851,
    "MMLU-PRO": 21.736480496453904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "FuJhen/ft-openhermes-25-mistral-7b-irca-dpo-pairs (Merge)"
  },
  {
    "eval_name": "FuJhen_mistral-instruct-7B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FuJhen/mistral-instruct-7B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FuJhen/mistral-instruct-7B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FuJhen__mistral-instruct-7B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FuJhen/mistral-instruct-7B-DPO",
    "Model sha": "e0bc86c23ce5aae1db576c8cca6f06f1f73af2db",
    "Average ‚¨ÜÔ∏è": 19.01694266531292,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.009646735054172,
    "IFEval Raw": 0.49684171332065585,
    "IFEval": 49.68417133206558,
    "BBH Raw": 0.46239050561386214,
    "BBH": 24.925827194936442,
    "MATH Lvl 5 Raw": 0.0377643504531722,
    "MATH Lvl 5": 3.7764350453172204,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4015625,
    "MUSR": 9.428645833333329,
    "MMLU-PRO Raw": 0.30335771276595747,
    "MMLU-PRO": 22.595301418439718,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "FuJhen/mistral-instruct-7B-DPO (Merge)"
  },
  {
    "eval_name": "FuJhen_mistral_7b_v0.1_structedData_e2e_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FuJhen/mistral_7b_v0.1_structedData_e2e\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FuJhen/mistral_7b_v0.1_structedData_e2e</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FuJhen__mistral_7b_v0.1_structedData_e2e-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FuJhen/mistral_7b_v0.1_structedData_e2e",
    "Model sha": "7231864981174d9bee8c7687c24c8344414eae6b",
    "Average ‚¨ÜÔ∏è": 10.871546697990665,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0802461386460063,
    "IFEval Raw": 0.17268403391889076,
    "IFEval": 17.268403391889077,
    "BBH Raw": 0.4113914854984489,
    "BBH": 18.06242392393546,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.3722916666666667,
    "MUSR": 5.6364583333333345,
    "MMLU-PRO Raw": 0.2810837765957447,
    "MMLU-PRO": 20.12041962174941,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "FuJhen/mistral_7b_v0.1_structedData_e2e (Merge)"
  },
  {
    "eval_name": "FuJhen_mistral_7b_v0.1_structedData_viggo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FuJhen/mistral_7b_v0.1_structedData_viggo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FuJhen/mistral_7b_v0.1_structedData_viggo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FuJhen__mistral_7b_v0.1_structedData_viggo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FuJhen/mistral_7b_v0.1_structedData_viggo",
    "Model sha": "7231864981174d9bee8c7687c24c8344414eae6b",
    "Average ‚¨ÜÔ∏è": 12.352465693057807,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0761136511060907,
    "IFEval Raw": 0.17832905579418165,
    "IFEval": 17.832905579418167,
    "BBH Raw": 0.45238634545986817,
    "BBH": 23.960171694414896,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.37381250000000005,
    "MUSR": 3.9265625000000015,
    "MMLU-PRO Raw": 0.2942154255319149,
    "MMLU-PRO": 21.579491725768317,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "FuJhen/mistral_7b_v0.1_structedData_viggo (Merge)"
  },
  {
    "eval_name": "FuseAI_FuseChat-7B-v2.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/FuseAI/FuseChat-7B-v2.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">FuseAI/FuseChat-7B-v2.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/FuseAI__FuseChat-7B-v2.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "FuseAI/FuseChat-7B-v2.0",
    "Model sha": "65fdb310c09f56b9aca01b89a849f06f39faeb75",
    "Average ‚¨ÜÔ∏è": 20.184131787930824,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44330611098919076,
    "IFEval Raw": 0.3423194900641409,
    "IFEval": 34.23194900641409,
    "BBH Raw": 0.4954212795868764,
    "BBH": 29.341638180782923,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.4796666666666667,
    "MUSR": 20.224999999999994,
    "MMLU-PRO Raw": 0.3162400265957447,
    "MMLU-PRO": 24.02666962174941,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-11-21",
    "Generation": 1,
    "Base Model": "openchat/openchat_3.5"
  },
  {
    "eval_name": "GalrionSoftworks_MN-LooseCannon-12B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GalrionSoftworks/MN-LooseCannon-12B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GalrionSoftworks/MN-LooseCannon-12B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GalrionSoftworks__MN-LooseCannon-12B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GalrionSoftworks/MN-LooseCannon-12B-v1",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 21.885253033854905,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5290196825448796,
    "IFEval Raw": 0.5417791459992819,
    "IFEval": 54.177914599928194,
    "BBH Raw": 0.5128183808679557,
    "BBH": 29.976062092951295,
    "MATH Lvl 5 Raw": 0.07099697885196375,
    "MATH Lvl 5": 7.099697885196375,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.41384375,
    "MUSR": 10.963802083333329,
    "MMLU-PRO Raw": 0.3195644946808511,
    "MMLU-PRO": 24.396054964539008,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-09",
    "Submission Date": "2024-09-05",
    "Generation": 1,
    "Base Model": "GalrionSoftworks/MN-LooseCannon-12B-v1 (Merge)"
  },
  {
    "eval_name": "GalrionSoftworks_MagnusIntellectus-12B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GalrionSoftworks/MagnusIntellectus-12B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GalrionSoftworks/MagnusIntellectus-12B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GalrionSoftworks__MagnusIntellectus-12B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GalrionSoftworks/MagnusIntellectus-12B-v1",
    "Model sha": "fc83cb3eec2f8328448c5fe3cb830fc77983a6b9",
    "Average ‚¨ÜÔ∏è": 21.622238130213123,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6242638088286627,
    "IFEval Raw": 0.4421368635221213,
    "IFEval": 44.21368635221213,
    "BBH Raw": 0.5323010476246133,
    "BBH": 33.26225439614359,
    "MATH Lvl 5 Raw": 0.055891238670694864,
    "MATH Lvl 5": 5.589123867069486,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4428020833333333,
    "MUSR": 15.183593749999995,
    "MMLU-PRO Raw": 0.34208776595744683,
    "MMLU-PRO": 26.898640661938533,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-05",
    "Generation": 1,
    "Base Model": "GalrionSoftworks/MagnusIntellectus-12B-v1 (Merge)"
  },
  {
    "eval_name": "GoToCompany_gemma2-9b-cpt-sahabatai-v1-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GoToCompany__gemma2-9b-cpt-sahabatai-v1-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct",
    "Model sha": "ca19cec82a7d2bdba20020e1bebf296417cfc3ee",
    "Average ‚¨ÜÔ∏è": 32.3423789936126,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 21,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9310946507345252,
    "IFEval Raw": 0.6550607942481504,
    "IFEval": 65.50607942481504,
    "BBH Raw": 0.5954551751157878,
    "BBH": 41.86650373118869,
    "MATH Lvl 5 Raw": 0.19788519637462235,
    "MATH Lvl 5": 19.788519637462233,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4778645833333333,
    "MUSR": 19.333072916666662,
    "MMLU-PRO Raw": 0.4263630319148936,
    "MMLU-PRO": 36.26255910165484,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "GoToCompany/gemma2-9b-cpt-sahabatai-v1-instruct (Merge)"
  },
  {
    "eval_name": "GoToCompany_llama3-8b-cpt-sahabatai-v1-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GoToCompany__llama3-8b-cpt-sahabatai-v1-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct",
    "Model sha": "20fd3cff1dc86553d11b5c4b2fdbb6f2dd1ede55",
    "Average ‚¨ÜÔ∏è": 22.908341620621332,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6734109213144589,
    "IFEval Raw": 0.523844510343666,
    "IFEval": 52.38445103436659,
    "BBH Raw": 0.4951292004509417,
    "BBH": 28.539529393915917,
    "MATH Lvl 5 Raw": 0.11858006042296072,
    "MATH Lvl 5": 11.858006042296072,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.44884375,
    "MUSR": 15.172135416666663,
    "MMLU-PRO Raw": 0.3453291223404255,
    "MMLU-PRO": 27.258791371158388,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "GoToCompany/llama3-8b-cpt-sahabatai-v1-instruct (Merge)"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1",
    "Model sha": "bfc0e7dc6add02baecd9b6f84a078f7f3d164315",
    "Average ‚¨ÜÔ∏è": 8.320602658507037,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.48764240173251594,
    "IFEval Raw": 0.347189900574919,
    "IFEval": 34.7189900574919,
    "BBH Raw": 0.32683063456958195,
    "BBH": 6.845785955173547,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.32625,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.16414561170212766,
    "MMLU-PRO": 7.127290189125294,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-18",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-0.5B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-0.5B-Instruct-abliterated-v1",
    "Model sha": "bfc0e7dc6add02baecd9b6f84a078f7f3d164315",
    "Average ‚¨ÜÔ∏è": 8.415919404386559,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.49800361885240557,
    "IFEval Raw": 0.3416944817528602,
    "IFEval": 34.16944817528602,
    "BBH Raw": 0.32921013057720044,
    "BBH": 7.2211690840719855,
    "MATH Lvl 5 Raw": 0.002265861027190332,
    "MATH Lvl 5": 0.2265861027190332,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.3249166666666667,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.16381316489361702,
    "MMLU-PRO": 7.0903516548463354,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-18",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-0.5B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v1",
    "Model sha": "eca7edeba61e894597e9940348e8d90817c1ad79",
    "Average ‚¨ÜÔ∏è": 15.294145528132411,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7833808700754186,
    "IFEval Raw": 0.47685806992114255,
    "IFEval": 47.68580699211426,
    "BBH Raw": 0.418600731531926,
    "BBH": 18.306013289403676,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.3674895833333333,
    "MUSR": 4.002864583333333,
    "MMLU-PRO Raw": 0.27825797872340424,
    "MMLU-PRO": 19.806442080378247,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v2",
    "Model sha": "ff4a6eff69adb015dfcfbff7a2d2dc43b34afe89",
    "Average ‚¨ÜÔ∏è": 13.665943580793822,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7192429112316412,
    "IFEval Raw": 0.421553699738915,
    "IFEval": 42.155369973891496,
    "BBH Raw": 0.40418921704436744,
    "BBH": 16.499503211302823,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.23993288590604026,
    "GPQA": 0.0,
    "MUSR Raw": 0.37685416666666666,
    "MUSR": 4.7067708333333345,
    "MMLU-PRO Raw": 0.25615026595744683,
    "MMLU-PRO": 17.350029550827426,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-1.5B-Instruct-abliterated-v3",
    "Model sha": "03ffa6f7a6ada9d63d838707c597297f048d409b",
    "Average ‚¨ÜÔ∏è": 13.540924319659437,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7062007579348313,
    "IFEval Raw": 0.42525055740989465,
    "IFEval": 42.52505574098946,
    "BBH Raw": 0.4053446177133173,
    "BBH": 16.439711885397813,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.37018749999999995,
    "MUSR": 4.240104166666668,
    "MMLU-PRO Raw": 0.25556848404255317,
    "MMLU-PRO": 17.28538711583924,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 3,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-14B-Instruct-abliterated-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-14B-Instruct-abliterated-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-14B-Instruct-abliterated-v4",
    "Model sha": "00afd27eef16e835fcb0d8e687435dca3c185bdf",
    "Average ‚¨ÜÔ∏è": 33.511797999628094,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7471168151212961,
    "IFEval Raw": 0.8291666112581284,
    "IFEval": 82.91666112581285,
    "BBH Raw": 0.6355637424320617,
    "BBH": 48.05226992969286,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3422818791946309,
    "GPQA": 12.304250559284117,
    "MUSR Raw": 0.4286666666666667,
    "MUSR": 13.15,
    "MMLU-PRO Raw": 0.5018284574468085,
    "MMLU-PRO": 44.64760638297872,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-10-23",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_Josiefied-Qwen2.5-7B-Instruct-abliterated-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2",
    "Model sha": "ecf4024048ea1be2f0840a50080fb79b88aacde9",
    "Average ‚¨ÜÔ∏è": 27.76376328826473,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2015064279743322,
    "IFEval Raw": 0.7813811797142693,
    "IFEval": 78.13811797142694,
    "BBH Raw": 0.5309672164610734,
    "BBH": 33.33398601463088,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.43539583333333337,
    "MUSR": 13.957812500000003,
    "MMLU-PRO Raw": 0.4119847074468085,
    "MMLU-PRO": 34.66496749408983,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "Goekdeniz-Guelmez_j.o.s.i.e.v4o-1.5b-dpo-stage1-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Goekdeniz-Guelmez/j.o.s.i.e.v4o-1.5b-dpo-stage1-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Goekdeniz-Guelmez/j.o.s.i.e.v4o-1.5b-dpo-stage1-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Goekdeniz-Guelmez__j.o.s.i.e.v4o-1.5b-dpo-stage1-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Goekdeniz-Guelmez/j.o.s.i.e.v4o-1.5b-dpo-stage1-v1",
    "Model sha": "d5ddad290d83b1ba8a7612a6c1cfad6fb4346fe4",
    "Average ‚¨ÜÔ∏è": 13.567473682990121,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7911525988603724,
    "IFEval Raw": 0.41883092417009093,
    "IFEval": 41.883092417009095,
    "BBH Raw": 0.41242101633634826,
    "BBH": 17.748016873381815,
    "MATH Lvl 5 Raw": 0.029456193353474325,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3528541666666667,
    "MUSR": 1.4401041666666685,
    "MMLU-PRO Raw": 0.2554853723404255,
    "MMLU-PRO": 17.2761524822695,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-08",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "GreenNode_GreenNode-small-9B-it_float16",
    "Precision": "float16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GreenNode/GreenNode-small-9B-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GreenNode/GreenNode-small-9B-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GreenNode__GreenNode-small-9B-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GreenNode/GreenNode-small-9B-it",
    "Model sha": "1ba4ce8e2267c7fcc820961a9bfc13ab80150866",
    "Average ‚¨ÜÔ∏è": 28.28665136279506,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.645943993216027,
    "IFEval Raw": 0.7436125037123721,
    "IFEval": 74.36125037123722,
    "BBH Raw": 0.599383874005197,
    "BBH": 41.899925635193455,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.42041666666666666,
    "MUSR": 11.652083333333337,
    "MMLU-PRO Raw": 0.3927027925531915,
    "MMLU-PRO": 32.52253250591017,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "GritLM_GritLM-7B-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GritLM/GritLM-7B-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GritLM/GritLM-7B-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GritLM__GritLM-7B-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GritLM/GritLM-7B-KTO",
    "Model sha": "b5c48669508c1de18c698460c187f64e90e7df44",
    "Average ‚¨ÜÔ∏è": 19.172954327329677,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6398635932435057,
    "IFEval Raw": 0.5310132670203948,
    "IFEval": 53.10132670203948,
    "BBH Raw": 0.485293719684692,
    "BBH": 27.904317623033844,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.37102083333333336,
    "MUSR": 6.6442708333333345,
    "MMLU-PRO Raw": 0.26803523936170215,
    "MMLU-PRO": 18.670582151300238,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-16",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "GritLM/GritLM-7B-KTO"
  },
  {
    "eval_name": "GritLM_GritLM-8x7B-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/GritLM/GritLM-8x7B-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">GritLM/GritLM-8x7B-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/GritLM__GritLM-8x7B-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "GritLM/GritLM-8x7B-KTO",
    "Model sha": "938913477064fcc498757c5136d9899bb6e713ed",
    "Average ‚¨ÜÔ∏è": 25.838484970383433,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.6044632298562655,
    "IFEval Raw": 0.5714049832222946,
    "IFEval": 57.14049832222946,
    "BBH Raw": 0.5820304362331497,
    "BBH": 40.8261615594601,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.42165625,
    "MUSR": 11.673697916666669,
    "MMLU-PRO Raw": 0.36477726063829785,
    "MMLU-PRO": 29.419695626477544,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "GritLM/GritLM-8x7B-KTO"
  },
  {
    "eval_name": "Gryphe_Pantheon-RP-1.0-8b-Llama-3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gryphe/Pantheon-RP-1.0-8b-Llama-3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gryphe/Pantheon-RP-1.0-8b-Llama-3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gryphe__Pantheon-RP-1.0-8b-Llama-3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gryphe/Pantheon-RP-1.0-8b-Llama-3",
    "Model sha": "70a6df202c9df9abdc6928bec5a5ab47f2667aee",
    "Average ‚¨ÜÔ∏è": 16.772417469018666,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 46,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7208364945650547,
    "IFEval Raw": 0.39325212657969744,
    "IFEval": 39.32521265796974,
    "BBH Raw": 0.4539075127777334,
    "BBH": 23.631914688111305,
    "MATH Lvl 5 Raw": 0.05740181268882175,
    "MATH Lvl 5": 5.740181268882175,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3832395833333333,
    "MUSR": 5.504947916666667,
    "MMLU-PRO Raw": 0.30668218085106386,
    "MMLU-PRO": 22.964686761229316,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-08",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Gryphe_Pantheon-RP-1.5-12b-Nemo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gryphe/Pantheon-RP-1.5-12b-Nemo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gryphe/Pantheon-RP-1.5-12b-Nemo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gryphe__Pantheon-RP-1.5-12b-Nemo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gryphe/Pantheon-RP-1.5-12b-Nemo",
    "Model sha": "00107381f05f69666772d88a1b11affe77c94a47",
    "Average ‚¨ÜÔ∏è": 21.311158523579955,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6855832005591775,
    "IFEval Raw": 0.47630841722186024,
    "IFEval": 47.63084172218603,
    "BBH Raw": 0.519582216884963,
    "BBH": 31.750144021634004,
    "MATH Lvl 5 Raw": 0.04833836858006043,
    "MATH Lvl 5": 4.833836858006043,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.44203125000000004,
    "MUSR": 15.053906250000004,
    "MMLU-PRO Raw": 0.3302027925531915,
    "MMLU-PRO": 25.57808806146572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-08-04",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "Gryphe_Pantheon-RP-1.6-12b-Nemo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gryphe/Pantheon-RP-1.6-12b-Nemo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gryphe/Pantheon-RP-1.6-12b-Nemo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gryphe__Pantheon-RP-1.6-12b-Nemo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gryphe/Pantheon-RP-1.6-12b-Nemo",
    "Model sha": "60cf38ae0367baf314e3cce748d9a199adfea557",
    "Average ‚¨ÜÔ∏è": 20.365189359475664,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7372530273811069,
    "IFEval Raw": 0.44805671174705336,
    "IFEval": 44.80567117470534,
    "BBH Raw": 0.5204007434392454,
    "BBH": 31.687343826178374,
    "MATH Lvl 5 Raw": 0.033987915407854986,
    "MATH Lvl 5": 3.3987915407854987,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4287604166666667,
    "MUSR": 12.92838541666667,
    "MMLU-PRO Raw": 0.33111702127659576,
    "MMLU-PRO": 25.67966903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-08-31",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "Gryphe_Pantheon-RP-1.6-12b-Nemo-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gryphe/Pantheon-RP-1.6-12b-Nemo-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gryphe/Pantheon-RP-1.6-12b-Nemo-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gryphe__Pantheon-RP-1.6-12b-Nemo-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gryphe/Pantheon-RP-1.6-12b-Nemo-KTO",
    "Model sha": "6cb6d8d9a7352d71f539ab5053987e058c090443",
    "Average ‚¨ÜÔ∏è": 21.40754112367979,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6820264418278403,
    "IFEval Raw": 0.4636187537954849,
    "IFEval": 46.36187537954849,
    "BBH Raw": 0.5276980814125921,
    "BBH": 33.032200369425645,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.4247916666666667,
    "MUSR": 12.165625,
    "MMLU-PRO Raw": 0.33818151595744683,
    "MMLU-PRO": 26.46461288416076,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-28",
    "Submission Date": "2024-08-31",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "Gryphe_Pantheon-RP-Pure-1.6.2-22b-Small_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gryphe/Pantheon-RP-Pure-1.6.2-22b-Small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gryphe/Pantheon-RP-Pure-1.6.2-22b-Small</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gryphe__Pantheon-RP-Pure-1.6.2-22b-Small-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gryphe/Pantheon-RP-Pure-1.6.2-22b-Small",
    "Model sha": "d031830dcb3bc5ad9634374db4dd15b3ef6ebe0f",
    "Average ‚¨ÜÔ∏è": 27.82393227565633,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.453320074147611,
    "IFEval Raw": 0.6931042965996888,
    "IFEval": 69.31042965996889,
    "BBH Raw": 0.5304537230538597,
    "BBH": 31.683163209870646,
    "MATH Lvl 5 Raw": 0.18353474320241692,
    "MATH Lvl 5": 18.35347432024169,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.37647916666666664,
    "MUSR": 4.393229166666666,
    "MMLU-PRO Raw": 0.39419880319148937,
    "MMLU-PRO": 32.68875591016548,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-15",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Small-Instruct-2409"
  },
  {
    "eval_name": "Gunulhona_Gemma-Ko-Merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gunulhona/Gemma-Ko-Merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gunulhona/Gemma-Ko-Merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gunulhona__Gemma-Ko-Merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gunulhona/Gemma-Ko-Merge",
    "Model sha": "ca6b0eb1405f21db6a7a9cce3b112d21fcfdde97",
    "Average ‚¨ÜÔ∏è": 25.935393515635266,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.137247537892627,
    "IFEval Raw": 0.6415721397004392,
    "IFEval": 64.15721397004393,
    "BBH Raw": 0.5813027258981727,
    "BBH": 38.78719707326869,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.40469791666666666,
    "MUSR": 9.120572916666667,
    "MMLU-PRO Raw": 0.3878823138297872,
    "MMLU-PRO": 31.98692375886525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "Gunulhona/Gemma-Ko-Merge (Merge)"
  },
  {
    "eval_name": "Gunulhona_Gemma-Ko-Merge-PEFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gunulhona/Gemma-Ko-Merge-PEFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gunulhona/Gemma-Ko-Merge-PEFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gunulhona__Gemma-Ko-Merge-PEFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gunulhona/Gemma-Ko-Merge-PEFT",
    "Model sha": "ca6b0eb1405f21db6a7a9cce3b112d21fcfdde97",
    "Average ‚¨ÜÔ∏è": 18.16949453226467,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 20,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.876476720258237,
    "IFEval Raw": 0.28803906966847964,
    "IFEval": 28.80390696684797,
    "BBH Raw": 0.5154093999781059,
    "BBH": 30.18627333134207,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.40801041666666665,
    "MUSR": 8.76796875,
    "MMLU-PRO Raw": 0.38173204787234044,
    "MMLU-PRO": 31.303560874704488,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-17",
    "Generation": 0,
    "Base Model": "Gunulhona/Gemma-Ko-Merge-PEFT"
  },
  {
    "eval_name": "Gunulhona_Gemma-Ko-Merge-PEFT_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Gunulhona/Gemma-Ko-Merge-PEFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Gunulhona/Gemma-Ko-Merge-PEFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Gunulhona__Gemma-Ko-Merge-PEFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Gunulhona/Gemma-Ko-Merge-PEFT",
    "Model sha": "ca6b0eb1405f21db6a7a9cce3b112d21fcfdde97",
    "Average ‚¨ÜÔ∏è": 18.06624017039761,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 20,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.394333552078164,
    "IFEval Raw": 0.4441348954108433,
    "IFEval": 44.413489541084324,
    "BBH Raw": 0.4862989687822461,
    "BBH": 26.015069295888747,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.3985833333333333,
    "MUSR": 7.056249999999999,
    "MMLU-PRO Raw": 0.3097573138297872,
    "MMLU-PRO": 23.30636820330969,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "Gunulhona/Gemma-Ko-Merge-PEFT"
  },
  {
    "eval_name": "HPAI-BSC_Llama3-Aloe-8B-Alpha_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HPAI-BSC/Llama3-Aloe-8B-Alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HPAI-BSC/Llama3-Aloe-8B-Alpha</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HPAI-BSC__Llama3-Aloe-8B-Alpha-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HPAI-BSC/Llama3-Aloe-8B-Alpha",
    "Model sha": "f0bce5c1fee5ea2a6679bb3dc9de8548e7262c9e",
    "Average ‚¨ÜÔ∏è": 20.104566088787408,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 52,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7952447886881179,
    "IFEval Raw": 0.5081073773144147,
    "IFEval": 50.81073773144146,
    "BBH Raw": 0.48308532966126966,
    "BBH": 27.145977577581778,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3672708333333334,
    "MUSR": 5.8755208333333355,
    "MMLU-PRO Raw": 0.3295378989361702,
    "MMLU-PRO": 25.5042109929078,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-10-29",
    "Generation": 0,
    "Base Model": "HPAI-BSC/Llama3-Aloe-8B-Alpha"
  },
  {
    "eval_name": "HPAI-BSC_Llama3.1-Aloe-Beta-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HPAI-BSC/Llama3.1-Aloe-Beta-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HPAI-BSC/Llama3.1-Aloe-Beta-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HPAI-BSC__Llama3.1-Aloe-Beta-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HPAI-BSC/Llama3.1-Aloe-Beta-8B",
    "Model sha": "3f2f0bbfb03cb0a8310efa50659688c1f2c02da0",
    "Average ‚¨ÜÔ∏è": 23.75480948277468,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3986971326678141,
    "IFEval Raw": 0.7253276860951166,
    "IFEval": 72.53276860951165,
    "BBH Raw": 0.5092760762748857,
    "BBH": 30.369624781344758,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3834583333333333,
    "MUSR": 6.8322916666666655,
    "MMLU-PRO Raw": 0.35804521276595747,
    "MMLU-PRO": 28.671690307328607,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-07",
    "Generation": 0,
    "Base Model": "HPAI-BSC/Llama3.1-Aloe-Beta-8B"
  },
  {
    "eval_name": "Hastagaras_Llama-3.1-Jamet-8B-MK.I_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Hastagaras/Llama-3.1-Jamet-8B-MK.I\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Hastagaras/Llama-3.1-Jamet-8B-MK.I</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Hastagaras__Llama-3.1-Jamet-8B-MK.I-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Hastagaras/Llama-3.1-Jamet-8B-MK.I",
    "Model sha": "26cb97042b04fee7d0140375a7babbf92278f8ac",
    "Average ‚¨ÜÔ∏è": 25.39862978385939,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7187398980561769,
    "IFEval Raw": 0.7338207068356406,
    "IFEval": 73.38207068356405,
    "BBH Raw": 0.5048666433733161,
    "BBH": 29.503904748319474,
    "MATH Lvl 5 Raw": 0.12537764350453173,
    "MATH Lvl 5": 12.537764350453173,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3726041666666667,
    "MUSR": 6.142187500000003,
    "MMLU-PRO Raw": 0.3482380319148936,
    "MMLU-PRO": 27.582003546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 0,
    "Base Model": "Hastagaras/Llama-3.1-Jamet-8B-MK.I"
  },
  {
    "eval_name": "Hastagaras_Zabuza-8B-Llama-3.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Hastagaras/Zabuza-8B-Llama-3.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Hastagaras/Zabuza-8B-Llama-3.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Hastagaras__Zabuza-8B-Llama-3.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Hastagaras/Zabuza-8B-Llama-3.1",
    "Model sha": "57ffa92f229b8308916aae1d64d8f0dc9baa0a34",
    "Average ‚¨ÜÔ∏è": 19.71182897088656,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6752873084543018,
    "IFEval Raw": 0.6265342624237025,
    "IFEval": 62.653426242370244,
    "BBH Raw": 0.4538915742546196,
    "BBH": 23.220320849670458,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3567916666666667,
    "MUSR": 4.8989583333333355,
    "MMLU-PRO Raw": 0.29230385638297873,
    "MMLU-PRO": 21.3670951536643,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-05",
    "Generation": 1,
    "Base Model": "Hastagaras/Zabuza-8B-Llama-3.1 (Merge)"
  },
  {
    "eval_name": "HiroseKoichi_Llama-Salad-4x8B-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HiroseKoichi/Llama-Salad-4x8B-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HiroseKoichi/Llama-Salad-4x8B-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HiroseKoichi__Llama-Salad-4x8B-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HiroseKoichi/Llama-Salad-4x8B-V3",
    "Model sha": "a343915429779efbd1478f01ba1f7fd9d8d226c0",
    "Average ‚¨ÜÔ∏è": 24.93528984101707,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1376952021514803,
    "IFEval Raw": 0.6653523761397536,
    "IFEval": 66.53523761397537,
    "BBH Raw": 0.5244649789001753,
    "BBH": 31.92884881074505,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.37403125,
    "MUSR": 6.453906249999996,
    "MMLU-PRO Raw": 0.351811835106383,
    "MMLU-PRO": 27.979092789598102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-17",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "HiroseKoichi/Llama-Salad-4x8B-V3"
  },
  {
    "eval_name": "HuggingFaceH4_zephyr-7b-alpha_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceH4/zephyr-7b-alpha</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-7b-alpha-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceH4/zephyr-7b-alpha",
    "Model sha": "2ce2d025864af849b3e5029e2ec9d568eeda892d",
    "Average ‚¨ÜÔ∏è": 18.571864220384587,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1101,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7956751991931201,
    "IFEval Raw": 0.5191480826429429,
    "IFEval": 51.914808264294294,
    "BBH Raw": 0.4587863505904412,
    "BBH": 23.955291427068445,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.3949583333333333,
    "MUSR": 7.503125000000001,
    "MMLU-PRO Raw": 0.2795046542553192,
    "MMLU-PRO": 19.94496158392435,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-09",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "HuggingFaceH4_zephyr-7b-beta_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceH4/zephyr-7b-beta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-7b-beta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceH4/zephyr-7b-beta",
    "Model sha": "b70e0c9a2d9e14bd1e812d3c398e5f313e93b473",
    "Average ‚¨ÜÔ∏è": 17.76706099373502,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1609,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5550230908694535,
    "IFEval Raw": 0.49504315216957673,
    "IFEval": 49.50431521695767,
    "BBH Raw": 0.431582191918003,
    "BBH": 21.487542182806738,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.3925416666666666,
    "MUSR": 7.7343749999999964,
    "MMLU-PRO Raw": 0.2780917553191489,
    "MMLU-PRO": 19.78797281323877,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-26",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "HuggingFaceH4_zephyr-7b-gemma-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceH4/zephyr-7b-gemma-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-7b-gemma-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceH4/zephyr-7b-gemma-v0.1",
    "Model sha": "03b3427d0ed07d2e0f86c0a7e53d82d4beef9540",
    "Average ‚¨ÜÔ∏è": 15.929338407709793,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 121,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.481775141628969,
    "IFEval Raw": 0.3363741539116212,
    "IFEval": 33.637415391162115,
    "BBH Raw": 0.4623735014679749,
    "BBH": 23.751162749201274,
    "MATH Lvl 5 Raw": 0.0755287009063444,
    "MATH Lvl 5": 7.552870090634441,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.37396874999999996,
    "MUSR": 4.179427083333334,
    "MMLU-PRO Raw": 0.2847406914893617,
    "MMLU-PRO": 20.526743498817968,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-01",
    "Submission Date": "2024-06-12",
    "Generation": 2,
    "Base Model": "google/gemma-7b"
  },
  {
    "eval_name": "HuggingFaceH4_zephyr-orpo-141b-A35b-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceH4__zephyr-orpo-141b-A35b-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1",
    "Model sha": "a3be084543d278e61b64cd600f28157afc79ffd3",
    "Average ‚¨ÜÔ∏è": 34.063022800582324,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 261,
    "#Params (B)": 140,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 42.06778642306777,
    "IFEval Raw": 0.6510891102275296,
    "IFEval": 65.10891102275296,
    "BBH Raw": 0.6290439728524093,
    "BBH": 47.503796286541196,
    "MATH Lvl 5 Raw": 0.2009063444108761,
    "MATH Lvl 5": 20.09063444108761,
    "GPQA Raw": 0.3783557046979866,
    "GPQA": 17.114093959731544,
    "MUSR Raw": 0.4465208333333333,
    "MUSR": 14.715104166666668,
    "MMLU-PRO Raw": 0.4586103723404255,
    "MMLU-PRO": 39.84559692671394,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-10",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistral-community/Mixtral-8x22B-v0.1"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-1.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-1.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-1.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-1.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-1.7B",
    "Model sha": "673a07602ca1191e5bc2ddda428e2f608a0a14c0",
    "Average ‚¨ÜÔ∏è": 5.425398534456355,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 161,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3243072565176619,
    "IFEval Raw": 0.23615673080759053,
    "IFEval": 23.615673080759052,
    "BBH Raw": 0.3180516538964782,
    "BBH": 4.4111278515492005,
    "MATH Lvl 5 Raw": 0.007552870090634441,
    "MATH Lvl 5": 0.755287009063444,
    "GPQA Raw": 0.24161073825503357,
    "GPQA": 0.0,
    "MUSR Raw": 0.34209375000000003,
    "MUSR": 2.1283854166666667,
    "MMLU-PRO Raw": 0.11477726063829788,
    "MMLU-PRO": 1.6419178486997636,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-18",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM-1.7B"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-1.7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-1.7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-1.7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-1.7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-1.7B-Instruct",
    "Model sha": "0ad161e59935a9a691dfde2818df8b98786f30a7",
    "Average ‚¨ÜÔ∏è": 5.138221532759036,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 103,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3170226634267443,
    "IFEval Raw": 0.23478259905938464,
    "IFEval": 23.478259905938465,
    "BBH Raw": 0.28851114363217695,
    "BBH": 2.0803742908537424,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3486666666666667,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.11660571808510638,
    "MMLU-PRO": 1.8450797872340412,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-15",
    "Submission Date": "2024-07-18",
    "Generation": 1,
    "Base Model": "HuggingFaceTB/SmolLM-1.7B"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-135M_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-135M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-135M</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-135M-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-135M",
    "Model sha": "eec6e461571fba3e197a57c298f60b75422eae02",
    "Average ‚¨ÜÔ∏è": 6.838196840775357,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 172,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3433775075576674,
    "IFEval Raw": 0.21247622973709757,
    "IFEval": 21.247622973709756,
    "BBH Raw": 0.3046054260062988,
    "BBH": 3.2853998220852616,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.4366041666666667,
    "MUSR": 13.342187500000001,
    "MMLU-PRO Raw": 0.11220079787234043,
    "MMLU-PRO": 1.3556442080378246,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-18",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM-135M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-135M-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-135M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-135M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-135M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-135M-Instruct",
    "Model sha": "8ca7af58e27777cae460ad8ca3ab9db15f5c160d",
    "Average ‚¨ÜÔ∏è": 3.5641708324788968,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 98,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.46780489528386504,
    "IFEval Raw": 0.12140121544169469,
    "IFEval": 12.14012154416947,
    "BBH Raw": 0.30150816789978757,
    "BBH": 2.6929580047045802,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.36345833333333327,
    "MUSR": 3.365625,
    "MMLU-PRO Raw": 0.11760305851063829,
    "MMLU-PRO": 1.9558953900709206,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-15",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "HuggingFaceTB/SmolLM-135M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-360M_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-360M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-360M</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-360M-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-360M",
    "Model sha": "318cc630b73730bfd712e5873063156ffb8936b5",
    "Average ‚¨ÜÔ∏è": 6.147595806027067,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 61,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.36525953216712603,
    "IFEval Raw": 0.2133505764704318,
    "IFEval": 21.33505764704318,
    "BBH Raw": 0.30645160333152527,
    "BBH": 3.284915303246592,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.40178125,
    "MUSR": 8.089322916666665,
    "MMLU-PRO Raw": 0.11236702127659574,
    "MMLU-PRO": 1.374113475177304,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-18",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM-360M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM-360M-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM-360M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM-360M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM-360M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM-360M-Instruct",
    "Model sha": "8e951de8c220295ea4f85d078c4e320df7137535",
    "Average ‚¨ÜÔ∏è": 4.70678415207999,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 76,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.36650102429898507,
    "IFEval Raw": 0.19516549422199764,
    "IFEval": 19.516549422199766,
    "BBH Raw": 0.28851114363217695,
    "BBH": 2.0803742908537424,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.34717708333333336,
    "MUSR": 2.897135416666666,
    "MMLU-PRO Raw": 0.11660571808510638,
    "MMLU-PRO": 1.8450797872340412,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-15",
    "Submission Date": "2024-08-20",
    "Generation": 1,
    "Base Model": "HuggingFaceTB/SmolLM-360M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-1.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-1.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-1.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-1.7B",
    "Model sha": "4fa12cab4f5f53670b05125fb9d2873af587d231",
    "Average ‚¨ÜÔ∏è": 9.495504224029132,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 71,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3250259911403082,
    "IFEval Raw": 0.2440003634800108,
    "IFEval": 24.40003634800108,
    "BBH Raw": 0.3452594377166261,
    "BBH": 9.301788459551682,
    "MATH Lvl 5 Raw": 0.021148036253776436,
    "MATH Lvl 5": 2.1148036253776437,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.3485416666666667,
    "MUSR": 4.601041666666668,
    "MMLU-PRO Raw": 0.2137632978723404,
    "MMLU-PRO": 12.640366430260045,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-1.7B"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-1.7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-1.7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-1.7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-1.7B-Instruct",
    "Model sha": "d1bb90bcfbe0f211109880f4da18da66f229c4f6",
    "Average ‚¨ÜÔ∏è": 14.745339097105633,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 345,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3249605231216809,
    "IFEval Raw": 0.5367835121920947,
    "IFEval": 53.678351219209475,
    "BBH Raw": 0.3598617531415158,
    "BBH": 10.917989226208066,
    "MATH Lvl 5 Raw": 0.04154078549848943,
    "MATH Lvl 5": 4.1540785498489425,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.342125,
    "MUSR": 4.098958333333335,
    "MMLU-PRO Raw": 0.2053690159574468,
    "MMLU-PRO": 11.707668439716311,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "HuggingFaceTB/SmolLM2-1.7B-Instruct (Merge)"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-135M_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-135M</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-135M-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-135M",
    "Model sha": "28e66ca6931668447a3bac213f23d990ad3b0e2b",
    "Average ‚¨ÜÔ∏è": 5.5576774540416665,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.33390456663015294,
    "IFEval Raw": 0.18330030984454582,
    "IFEval": 18.33003098445458,
    "BBH Raw": 0.3044234246877141,
    "BBH": 3.70807758683998,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.4111770833333333,
    "MUSR": 10.030468749999999,
    "MMLU-PRO Raw": 0.10945811170212766,
    "MMLU-PRO": 1.0509013002364058,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-135M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-135M-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-135M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-135M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-135M-Instruct",
    "Model sha": "5a33ba103645800d7b3790c4448546c1b73efc71",
    "Average ‚¨ÜÔ∏è": 6.467364720358819,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 57,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.338375628206877,
    "IFEval Raw": 0.2883138960181208,
    "IFEval": 28.83138960181208,
    "BBH Raw": 0.3124321328066677,
    "BBH": 4.720807660805284,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.23573825503355705,
    "GPQA": 0.0,
    "MUSR Raw": 0.36621875000000004,
    "MUSR": 3.6773437500000035,
    "MMLU-PRO Raw": 0.11145279255319149,
    "MMLU-PRO": 1.2725325059101646,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-135M-Instruct"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-135M-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-135M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-135M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-135M-Instruct",
    "Model sha": "5a33ba103645800d7b3790c4448546c1b73efc71",
    "Average ‚¨ÜÔ∏è": 2.992598970656367,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 57,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.348753843693507,
    "IFEval Raw": 0.05925167444602544,
    "IFEval": 5.925167444602544,
    "BBH Raw": 0.31347502947335903,
    "BBH": 4.796275744662444,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.23406040268456377,
    "GPQA": 0.0,
    "MUSR Raw": 0.3871458333333333,
    "MUSR": 6.059895833333335,
    "MMLU-PRO Raw": 0.10920877659574468,
    "MMLU-PRO": 1.0231973995271864,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-14",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-135M-Instruct"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-360M_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-360M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-360M</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-360M-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-360M",
    "Model sha": "3ce05f63c246c44616da500b47b01f082f4d3bcc",
    "Average ‚¨ÜÔ∏è": 6.100224948704614,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.38665780937234606,
    "IFEval Raw": 0.21145227995053123,
    "IFEval": 21.145227995053123,
    "BBH Raw": 0.3233478044302361,
    "BBH": 5.543603155369513,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.3954270833333333,
    "MUSR": 7.728385416666666,
    "MMLU-PRO Raw": 0.11693816489361702,
    "MMLU-PRO": 1.8820183215130022,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-360M"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-360M-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-360M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-360M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-360M-Instruct",
    "Model sha": "4873f67095301d304753fae05bc09ec766634e50",
    "Average ‚¨ÜÔ∏è": 3.1000195398620374,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.39238186927176233,
    "IFEval Raw": 0.08303191088533979,
    "IFEval": 8.303191088533978,
    "BBH Raw": 0.3052703401844317,
    "BBH": 3.2990473293233173,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.34228125000000004,
    "MUSR": 2.751822916666668,
    "MMLU-PRO Raw": 0.11261635638297872,
    "MMLU-PRO": 1.4018173758865236,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-14",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-360M-Instruct"
  },
  {
    "eval_name": "HuggingFaceTB_SmolLM2-360M-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HuggingFaceTB/SmolLM2-360M-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HuggingFaceTB/SmolLM2-360M-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HuggingFaceTB__SmolLM2-360M-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HuggingFaceTB/SmolLM2-360M-Instruct",
    "Model sha": "4873f67095301d304753fae05bc09ec766634e50",
    "Average ‚¨ÜÔ∏è": 8.001097139380912,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.37581927076429705,
    "IFEval Raw": 0.38415958545548035,
    "IFEval": 38.41595854554804,
    "BBH Raw": 0.31435050538888504,
    "BBH": 4.173863636363637,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.346125,
    "MUSR": 2.7656250000000004,
    "MMLU-PRO Raw": 0.11170212765957446,
    "MMLU-PRO": 1.300236406619384,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "HuggingFaceTB/SmolLM2-360M-Instruct"
  },
  {
    "eval_name": "HumanLLMs_Humanish-LLama3-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HumanLLMs/Humanish-LLama3-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HumanLLMs/Humanish-LLama3-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HumanLLMs__Humanish-LLama3-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HumanLLMs/Humanish-LLama3-8B-Instruct",
    "Model sha": "42f73ada2b7fb16f18a75404d72b7911bf1e65ce",
    "Average ‚¨ÜÔ∏è": 22.564910696419574,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7482781406705685,
    "IFEval Raw": 0.6497903340913221,
    "IFEval": 64.97903340913221,
    "BBH Raw": 0.49677096627896544,
    "BBH": 28.012476599572,
    "MATH Lvl 5 Raw": 0.09592145015105742,
    "MATH Lvl 5": 9.592145015105743,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.35815624999999995,
    "MUSR": 2.0028645833333325,
    "MMLU-PRO Raw": 0.37017952127659576,
    "MMLU-PRO": 30.019946808510632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "HumanLLMs_Humanish-Mistral-Nemo-Instruct-2407_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HumanLLMs/Humanish-Mistral-Nemo-Instruct-2407\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HumanLLMs/Humanish-Mistral-Nemo-Instruct-2407</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HumanLLMs__Humanish-Mistral-Nemo-Instruct-2407-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HumanLLMs/Humanish-Mistral-Nemo-Instruct-2407",
    "Model sha": "45b80bdce8d447ef494af06751904afcc607eb37",
    "Average ‚¨ÜÔ∏è": 23.0068996717315,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6202833064408968,
    "IFEval Raw": 0.5451269298793867,
    "IFEval": 54.51269298793867,
    "BBH Raw": 0.5261780772532613,
    "BBH": 32.70961342122556,
    "MATH Lvl 5 Raw": 0.08383685800604229,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.39676041666666667,
    "MUSR": 9.395052083333335,
    "MMLU-PRO Raw": 0.35206117021276595,
    "MMLU-PRO": 28.00679669030733,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-06",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "HumanLLMs_Humanish-Qwen2.5-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/HumanLLMs/Humanish-Qwen2.5-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">HumanLLMs/Humanish-Qwen2.5-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/HumanLLMs__Humanish-Qwen2.5-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "HumanLLMs/Humanish-Qwen2.5-7B-Instruct",
    "Model sha": "7d2c71d926832d6e257ad2776011494dbac2d151",
    "Average ‚¨ÜÔ∏è": 26.665374096819846,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1933927186332434,
    "IFEval Raw": 0.7284250233824031,
    "IFEval": 72.84250233824031,
    "BBH Raw": 0.5363681457807072,
    "BBH": 34.47899758661866,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.3980625,
    "MUSR": 8.424479166666668,
    "MMLU-PRO Raw": 0.4398271276595745,
    "MMLU-PRO": 37.75856973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-05",
    "Submission Date": "2024-10-05",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "IDEA-CCNL_Ziya-LLaMA-13B-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/IDEA-CCNL/Ziya-LLaMA-13B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">IDEA-CCNL/Ziya-LLaMA-13B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/IDEA-CCNL__Ziya-LLaMA-13B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "IDEA-CCNL/Ziya-LLaMA-13B-v1",
    "Model sha": "64d931f346e1a49ea3bbca07a83137075bab1c66",
    "Average ‚¨ÜÔ∏è": 3.9064248386004103,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 273,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1082574303345918,
    "IFEval Raw": 0.16968643200042555,
    "IFEval": 16.968643200042553,
    "BBH Raw": 0.28770292445409473,
    "BBH": 1.4636170460989157,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.37505208333333334,
    "MUSR": 3.881510416666668,
    "MMLU-PRO Raw": 0.11012300531914894,
    "MMLU-PRO": 1.124778368794326,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-16",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "IDEA-CCNL/Ziya-LLaMA-13B-v1"
  },
  {
    "eval_name": "Infinirc_Infinirc-Llama3-8B-2G-Release-v1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Infinirc/Infinirc-Llama3-8B-2G-Release-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Infinirc/Infinirc-Llama3-8B-2G-Release-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Infinirc__Infinirc-Llama3-8B-2G-Release-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Infinirc/Infinirc-Llama3-8B-2G-Release-v1.0",
    "Model sha": "9c542d9ec3f86e145ae445c200c6ebe9066e8cd6",
    "Average ‚¨ÜÔ∏è": 13.087132955117646,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8187226373389154,
    "IFEval Raw": 0.20243398626754788,
    "IFEval": 20.24339862675479,
    "BBH Raw": 0.43507435668237937,
    "BBH": 20.83116494676627,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.4609375,
    "MUSR": 16.750520833333336,
    "MMLU-PRO Raw": 0.21600731382978725,
    "MMLU-PRO": 12.889701536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "Infinirc/Infinirc-Llama3-8B-2G-Release-v1.0"
  },
  {
    "eval_name": "Intel_neural-chat-7b-v3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Intel/neural-chat-7b-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Intel/neural-chat-7b-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Intel__neural-chat-7b-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Intel/neural-chat-7b-v3",
    "Model sha": "fc679274dfcd28a8b6087634f71af7ed2a0659c4",
    "Average ‚¨ÜÔ∏è": 17.943646116016044,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 67,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.48929031448334476,
    "IFEval Raw": 0.27779735546128714,
    "IFEval": 27.779735546128713,
    "BBH Raw": 0.5048316221363103,
    "BBH": 30.205692320538088,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.5054895833333334,
    "MUSR": 23.019531249999996,
    "MMLU-PRO Raw": 0.26986369680851063,
    "MMLU-PRO": 18.873744089834513,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-25",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "Intel_neural-chat-7b-v3-1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Intel/neural-chat-7b-v3-1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Intel/neural-chat-7b-v3-1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Intel__neural-chat-7b-v3-1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Intel/neural-chat-7b-v3-1",
    "Model sha": "c0d379a49c1c0579529d5e6f2e936ddb759552a8",
    "Average ‚¨ÜÔ∏è": 21.004986176864318,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 545,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5636920286565241,
    "IFEval Raw": 0.4686897432146704,
    "IFEval": 46.868974321467036,
    "BBH Raw": 0.5051565464054848,
    "BBH": 29.7397523676162,
    "MATH Lvl 5 Raw": 0.03172205438066465,
    "MATH Lvl 5": 3.1722054380664653,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.49789583333333337,
    "MUSR": 22.236979166666657,
    "MMLU-PRO Raw": 0.2677859042553192,
    "MMLU-PRO": 18.642878250591018,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-14",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "Intel_neural-chat-7b-v3-2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Intel/neural-chat-7b-v3-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Intel/neural-chat-7b-v3-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Intel__neural-chat-7b-v3-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Intel/neural-chat-7b-v3-2",
    "Model sha": "0d8f77647810d21d935ea90c66d6339b85e65a75",
    "Average ‚¨ÜÔ∏è": 21.43364681812902,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 56,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5604414844805996,
    "IFEval Raw": 0.4988397452093778,
    "IFEval": 49.883974520937784,
    "BBH Raw": 0.5032226831964403,
    "BBH": 30.237457969159426,
    "MATH Lvl 5 Raw": 0.045317220543806644,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.48952083333333335,
    "MUSR": 20.056770833333335,
    "MMLU-PRO Raw": 0.26670545212765956,
    "MMLU-PRO": 18.522828014184395,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-21",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Intel/neural-chat-7b-v3-2"
  },
  {
    "eval_name": "Intel_neural-chat-7b-v3-3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Intel/neural-chat-7b-v3-3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Intel/neural-chat-7b-v3-3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Intel__neural-chat-7b-v3-3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Intel/neural-chat-7b-v3-3",
    "Model sha": "bdd31cf498d13782cc7497cba5896996ce429f91",
    "Average ‚¨ÜÔ∏è": 19.99112025734428,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 75,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5595238951778243,
    "IFEval Raw": 0.4762585495374495,
    "IFEval": 47.62585495374495,
    "BBH Raw": 0.48766180524289693,
    "BBH": 27.753850886523697,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.4859583333333333,
    "MUSR": 20.578124999999996,
    "MMLU-PRO Raw": 0.2624667553191489,
    "MMLU-PRO": 18.05186170212766,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-09",
    "Submission Date": "2024-06-12",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "IntervitensInc_internlm2_5-20b-llamafied_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/IntervitensInc/internlm2_5-20b-llamafied\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">IntervitensInc/internlm2_5-20b-llamafied</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/IntervitensInc__internlm2_5-20b-llamafied-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "IntervitensInc/internlm2_5-20b-llamafied",
    "Model sha": "0b6fc3cc0b9bf3529816061eb508483c20b77fe9",
    "Average ‚¨ÜÔ∏è": 29.204292702208424,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.381127809431825,
    "IFEval Raw": 0.3409952260003457,
    "IFEval": 34.099522600034575,
    "BBH Raw": 0.7478466526577329,
    "BBH": 63.47057952429436,
    "MATH Lvl 5 Raw": 0.17069486404833836,
    "MATH Lvl 5": 17.069486404833835,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.44754166666666667,
    "MUSR": 14.942708333333329,
    "MMLU-PRO Raw": 0.4050864361702128,
    "MMLU-PRO": 33.89849290780142,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-11-11",
    "Generation": 0,
    "Base Model": "IntervitensInc/internlm2_5-20b-llamafied"
  },
  {
    "eval_name": "Isaak-Carter_JOSIEv4o-8b-stage1-v4_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Isaak-Carter/JOSIEv4o-8b-stage1-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Isaak-Carter/JOSIEv4o-8b-stage1-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Isaak-Carter__JOSIEv4o-8b-stage1-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Isaak-Carter/JOSIEv4o-8b-stage1-v4",
    "Model sha": "a8380a7be51b547761824e524b3d95ac73203122",
    "Average ‚¨ÜÔ∏è": 15.567377233131845,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8905821050422911,
    "IFEval Raw": 0.2552660274737696,
    "IFEval": 25.526602747376963,
    "BBH Raw": 0.4724973116620121,
    "BBH": 25.7872756997585,
    "MATH Lvl 5 Raw": 0.04682779456193353,
    "MATH Lvl 5": 4.682779456193353,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.3654375,
    "MUSR": 6.0796874999999995,
    "MMLU-PRO Raw": 0.3316156914893617,
    "MMLU-PRO": 25.7350768321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "Isaak-Carter/JOSIEv4o-8b-stage1-v4"
  },
  {
    "eval_name": "Isaak-Carter_JOSIEv4o-8b-stage1-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Isaak-Carter/JOSIEv4o-8b-stage1-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Isaak-Carter/JOSIEv4o-8b-stage1-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Isaak-Carter__JOSIEv4o-8b-stage1-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Isaak-Carter/JOSIEv4o-8b-stage1-v4",
    "Model sha": "a8380a7be51b547761824e524b3d95ac73203122",
    "Average ‚¨ÜÔ∏è": 15.419272249038903,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8798822262772638,
    "IFEval Raw": 0.2476972211509905,
    "IFEval": 24.76972211509905,
    "BBH Raw": 0.4758066295235124,
    "BBH": 25.919578359413446,
    "MATH Lvl 5 Raw": 0.045317220543806644,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.3641041666666667,
    "MUSR": 6.346354166666667,
    "MMLU-PRO Raw": 0.32920545212765956,
    "MMLU-PRO": 25.467272458628837,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "Isaak-Carter/JOSIEv4o-8b-stage1-v4"
  },
  {
    "eval_name": "Isaak-Carter_Josiefied-Qwen2.5-7B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Isaak-Carter__Josiefied-Qwen2.5-7B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated",
    "Model sha": "879168f9ce9fac315a19dd4f4c7df5253bb660f2",
    "Average ‚¨ÜÔ∏è": 26.857295459909086,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0767906750734597,
    "IFEval Raw": 0.7317473193349202,
    "IFEval": 73.17473193349201,
    "BBH Raw": 0.5396376284460921,
    "BBH": 34.904315688323074,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4086666666666667,
    "MUSR": 9.616666666666667,
    "MMLU-PRO Raw": 0.4276097074468085,
    "MMLU-PRO": 36.401078605200944,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Isaak-Carter_Josiefied-Qwen2.5-7B-Instruct-abliterated-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Isaak-Carter__Josiefied-Qwen2.5-7B-Instruct-abliterated-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Isaak-Carter/Josiefied-Qwen2.5-7B-Instruct-abliterated-v2",
    "Model sha": "5d07f58562422feb9f25c9c048e40356d2cf7e4b",
    "Average ‚¨ÜÔ∏è": 27.81795957129169,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.130914646790651,
    "IFEval Raw": 0.7841039552830933,
    "IFEval": 78.41039552830932,
    "BBH Raw": 0.5310923599182072,
    "BBH": 33.2945398202129,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.43539583333333337,
    "MUSR": 13.957812500000003,
    "MMLU-PRO Raw": 0.4128158244680851,
    "MMLU-PRO": 34.757313829787236,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "J-LAB_Thynk_orpo_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/J-LAB/Thynk_orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">J-LAB/Thynk_orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/J-LAB__Thynk_orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "J-LAB/Thynk_orpo",
    "Model sha": "c6606d402f26d005b9f1a71a1cde9139d1cffb2a",
    "Average ‚¨ÜÔ∏è": 16.974407042111938,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2147641107160356,
    "IFEval Raw": 0.21017788357114678,
    "IFEval": 21.017788357114675,
    "BBH Raw": 0.44631138778709606,
    "BBH": 22.062783944144368,
    "MATH Lvl 5 Raw": 0.13066465256797585,
    "MATH Lvl 5": 13.066465256797585,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.45147916666666665,
    "MUSR": 15.201562500000001,
    "MMLU-PRO Raw": 0.32313829787234044,
    "MMLU-PRO": 24.793144208037827,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Jacoby746_Casual-Magnum-34B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Casual-Magnum-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Casual-Magnum-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Casual-Magnum-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Casual-Magnum-34B",
    "Model sha": "b628c6959441db75460cfd49536322b1ea46130e",
    "Average ‚¨ÜÔ∏è": 23.571334618424515,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.42669714201879,
    "IFEval Raw": 0.19301675110927893,
    "IFEval": 19.301675110927896,
    "BBH Raw": 0.6032046880542974,
    "BBH": 43.05156762846773,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.3724832214765101,
    "GPQA": 16.33109619686801,
    "MUSR Raw": 0.4077604166666666,
    "MUSR": 8.403385416666666,
    "MMLU-PRO Raw": 0.5183676861702128,
    "MMLU-PRO": 46.485298463356976,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "Jacoby746/Casual-Magnum-34B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Inf-Silent-Kunoichi-v0.1-2x7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Inf-Silent-Kunoichi-v0.1-2x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Inf-Silent-Kunoichi-v0.1-2x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Inf-Silent-Kunoichi-v0.1-2x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Inf-Silent-Kunoichi-v0.1-2x7B",
    "Model sha": "9ab68beb6fe16cab2ab708b9af4417c89751d297",
    "Average ‚¨ÜÔ∏è": 20.009947649745282,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.860053302725323,
    "IFEval Raw": 0.38798166642286913,
    "IFEval": 38.79816664228691,
    "BBH Raw": 0.518546209727402,
    "BBH": 32.38700420411291,
    "MATH Lvl 5 Raw": 0.06042296072507553,
    "MATH Lvl 5": 6.042296072507553,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.42804166666666665,
    "MUSR": 12.338541666666671,
    "MMLU-PRO Raw": 0.3271276595744681,
    "MMLU-PRO": 25.236406619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-20",
    "Generation": 1,
    "Base Model": "Jacoby746/Inf-Silent-Kunoichi-v0.1-2x7B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Inf-Silent-Kunoichi-v0.2-2x7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Inf-Silent-Kunoichi-v0.2-2x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Inf-Silent-Kunoichi-v0.2-2x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Inf-Silent-Kunoichi-v0.2-2x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Inf-Silent-Kunoichi-v0.2-2x7B",
    "Model sha": "711263c24f812676eb382a31a5f0fed9bd8c16e4",
    "Average ‚¨ÜÔ∏è": 19.917523364619857,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8662645383190185,
    "IFEval Raw": 0.3636019095998617,
    "IFEval": 36.36019095998617,
    "BBH Raw": 0.5209417299963208,
    "BBH": 32.259183510828905,
    "MATH Lvl 5 Raw": 0.05664652567975831,
    "MATH Lvl 5": 5.664652567975831,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.43197916666666664,
    "MUSR": 13.264062499999996,
    "MMLU-PRO Raw": 0.32721077127659576,
    "MMLU-PRO": 25.24564125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Jacoby746/Inf-Silent-Kunoichi-v0.2-2x7B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Proto-Athena-4x7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Proto-Athena-4x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Proto-Athena-4x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Proto-Athena-4x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Proto-Athena-4x7B",
    "Model sha": "450fcba7a630fb61a662f71936d37979226fced8",
    "Average ‚¨ÜÔ∏è": 19.64969645530488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6766143676427347,
    "IFEval Raw": 0.37029636918930664,
    "IFEval": 37.02963691893066,
    "BBH Raw": 0.5106547638742905,
    "BBH": 30.870822876627887,
    "MATH Lvl 5 Raw": 0.05740181268882175,
    "MATH Lvl 5": 5.740181268882175,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.43477083333333333,
    "MUSR": 13.813020833333328,
    "MMLU-PRO Raw": 0.32064494680851063,
    "MMLU-PRO": 24.516105200945624,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Jacoby746/Proto-Athena-4x7B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Proto-Athena-v0.2-4x7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Proto-Athena-v0.2-4x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Proto-Athena-v0.2-4x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Proto-Athena-v0.2-4x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Proto-Athena-v0.2-4x7B",
    "Model sha": "01feeded217ea83a8794e7968c8850859b5f0b14",
    "Average ‚¨ÜÔ∏è": 19.143897625710434,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6513719521912902,
    "IFEval Raw": 0.37524213531208306,
    "IFEval": 37.524213531208304,
    "BBH Raw": 0.5067731005424964,
    "BBH": 30.34084433030367,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.42128125,
    "MUSR": 10.960156250000002,
    "MMLU-PRO Raw": 0.3197307180851064,
    "MMLU-PRO": 24.414524231678485,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Jacoby746/Proto-Athena-v0.2-4x7B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Proto-Harpy-Blazing-Light-v0.1-2x7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Proto-Harpy-Blazing-Light-v0.1-2x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Proto-Harpy-Blazing-Light-v0.1-2x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Proto-Harpy-Blazing-Light-v0.1-2x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Proto-Harpy-Blazing-Light-v0.1-2x7B",
    "Model sha": "bbb5d7c7a0c9e999e057ffa71eaa93d59d95b36b",
    "Average ‚¨ÜÔ∏è": 22.292391864753952,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8818408275559706,
    "IFEval Raw": 0.4904719477652628,
    "IFEval": 49.04719477652628,
    "BBH Raw": 0.5186849053052595,
    "BBH": 32.63252990159268,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.44496874999999997,
    "MUSR": 14.12109375,
    "MMLU-PRO Raw": 0.33011968085106386,
    "MMLU-PRO": 25.56885342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "Jacoby746/Proto-Harpy-Blazing-Light-v0.1-2x7B (Merge)"
  },
  {
    "eval_name": "Jacoby746_Proto-Harpy-Spark-v0.1-7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jacoby746/Proto-Harpy-Spark-v0.1-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jacoby746/Proto-Harpy-Spark-v0.1-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jacoby746__Proto-Harpy-Spark-v0.1-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jacoby746/Proto-Harpy-Spark-v0.1-7B",
    "Model sha": "984cca02cd930b2e1b7b2a7d53471d32d9821cdd",
    "Average ‚¨ÜÔ∏è": 19.862588017523432,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5958045958649423,
    "IFEval Raw": 0.43326928106313467,
    "IFEval": 43.32692810631347,
    "BBH Raw": 0.4735771808296548,
    "BBH": 26.913110159424864,
    "MATH Lvl 5 Raw": 0.06268882175226585,
    "MATH Lvl 5": 6.268882175226585,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.43166666666666664,
    "MUSR": 12.29166666666667,
    "MMLU-PRO Raw": 0.30693151595744683,
    "MMLU-PRO": 22.992390661938536,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "Jacoby746/Proto-Harpy-Spark-v0.1-7B (Merge)"
  },
  {
    "eval_name": "Jimmy19991222_Llama-3-Instruct-8B-SimPO-v0.2_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/Llama-3-Instruct-8B-SimPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/Llama-3-Instruct-8B-SimPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__Llama-3-Instruct-8B-SimPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/Llama-3-Instruct-8B-SimPO-v0.2",
    "Model sha": "53a517ceaef324efc3626be44140b4f18a010591",
    "Average ‚¨ÜÔ∏è": 24.27994844284174,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.513151724588246,
    "IFEval Raw": 0.6540368444615842,
    "IFEval": 65.40368444615842,
    "BBH Raw": 0.498371102582105,
    "BBH": 29.1238227637126,
    "MATH Lvl 5 Raw": 0.04305135951661632,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.40125000000000005,
    "MUSR": 8.389583333333334,
    "MMLU-PRO Raw": 0.3686003989361702,
    "MMLU-PRO": 29.844488770685572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-bert-f1-beta10-gamma0.3-lr1.0e-6-1minus-rerun_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert-f1-beta10-gamma0.3-lr1.0e-6-1minus-rerun\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert-f1-beta10-gamma0.3-lr1.0e-6-1minus-rerun</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-bert-f1-beta10-gamma0.3-lr1.0e-6-1minus-rerun-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert-f1-beta10-gamma0.3-lr1.0e-6-1minus-rerun",
    "Model sha": "00c02a823b4ff1a6cfcded6085ba9630df633998",
    "Average ‚¨ÜÔ∏è": 23.817704278814873,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.48179090913198847,
    "IFEval Raw": 0.6717221416951467,
    "IFEval": 67.17221416951466,
    "BBH Raw": 0.48797965672899357,
    "BBH": 27.755228582197066,
    "MATH Lvl 5 Raw": 0.04078549848942599,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.4040729166666667,
    "MUSR": 8.709114583333337,
    "MMLU-PRO Raw": 0.36336436170212766,
    "MMLU-PRO": 29.262706855791958,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-bert_f1-beta10-gamma0.3-lr1.0e-6-scale-log_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_f1-beta10-gamma0.3-lr1.0e-6-scale-log\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_f1-beta10-gamma0.3-lr1.0e-6-scale-log</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-bert_f1-beta10-gamma0.3-lr1.0e-6-scale-log-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_f1-beta10-gamma0.3-lr1.0e-6-scale-log",
    "Model sha": "99d9e31df5b7e88b1da78b1bd335cac3215dfd6e",
    "Average ‚¨ÜÔ∏è": 23.756269939372,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.47853541382039944,
    "IFEval Raw": 0.6555605792630221,
    "IFEval": 65.55605792630222,
    "BBH Raw": 0.49345840367294164,
    "BBH": 28.613596677525617,
    "MATH Lvl 5 Raw": 0.03398791540785499,
    "MATH Lvl 5": 3.398791540785499,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.4000104166666667,
    "MUSR": 8.167968750000002,
    "MMLU-PRO Raw": 0.3657746010638298,
    "MMLU-PRO": 29.53051122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-bert_p-beta10-gamma0.3-lr1.0e-6-scale-log_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_p-beta10-gamma0.3-lr1.0e-6-scale-log\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_p-beta10-gamma0.3-lr1.0e-6-scale-log</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-bert_p-beta10-gamma0.3-lr1.0e-6-scale-log-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-bert_p-beta10-gamma0.3-lr1.0e-6-scale-log",
    "Model sha": "49a029ea2605d768e89b638ad78a59fd62d192ab",
    "Average ‚¨ÜÔ∏è": 22.79797888615873,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5224851991910083,
    "IFEval Raw": 0.6315055164740666,
    "IFEval": 63.150551647406665,
    "BBH Raw": 0.4916414793938901,
    "BBH": 27.666183558964235,
    "MATH Lvl 5 Raw": 0.05060422960725076,
    "MATH Lvl 5": 5.0604229607250755,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3935,
    "MUSR": 7.087500000000001,
    "MMLU-PRO Raw": 0.3611203457446808,
    "MMLU-PRO": 29.01337174940898,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-bleu-beta0.1-no-length-scale-gamma0.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-bleu-beta0.1-no-length-scale-gamma0.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-bleu-beta0.1-no-length-scale-gamma0.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-bleu-beta0.1-no-length-scale-gamma0.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-bleu-beta0.1-no-length-scale-gamma0.4",
    "Model sha": "de8bb28ad7a9d1158f318a4461dc47ad03e6e560",
    "Average ‚¨ÜÔ∏è": 22.82731178886496,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.48037110120712095,
    "IFEval Raw": 0.6284580468711907,
    "IFEval": 62.84580468711906,
    "BBH Raw": 0.4986088445592742,
    "BBH": 29.329731874817796,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.40137500000000004,
    "MUSR": 9.071875000000002,
    "MMLU-PRO Raw": 0.3544714095744681,
    "MMLU-PRO": 28.27460106382979,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-rouge2-beta10-1minus-gamma0.3-rerun_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-1minus-gamma0.3-rerun\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-1minus-gamma0.3-rerun</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-rouge2-beta10-1minus-gamma0.3-rerun-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-1minus-gamma0.3-rerun",
    "Model sha": "e9692d8dbe30273839763757aa9ef07a5fcf0c59",
    "Average ‚¨ÜÔ∏è": 24.15902614701517,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0093592036388221,
    "IFEval Raw": 0.6677504576745258,
    "IFEval": 66.77504576745258,
    "BBH Raw": 0.4940463886115545,
    "BBH": 28.390676236054286,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.3987083333333334,
    "MUSR": 8.005208333333334,
    "MMLU-PRO Raw": 0.3657746010638298,
    "MMLU-PRO": 29.53051122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-rouge2-beta10-gamma0.3-lr1.0e-6-scale-log_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-gamma0.3-lr1.0e-6-scale-log\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-gamma0.3-lr1.0e-6-scale-log</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-rouge2-beta10-gamma0.3-lr1.0e-6-scale-log-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-rouge2-beta10-gamma0.3-lr1.0e-6-scale-log",
    "Model sha": "9ff0ce408abb8dbcf7efb9b6533338f2c344a355",
    "Average ‚¨ÜÔ∏è": 23.858382787902887,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5019935256585902,
    "IFEval Raw": 0.6605063453857986,
    "IFEval": 66.05063453857986,
    "BBH Raw": 0.49160075581298046,
    "BBH": 28.075035515119442,
    "MATH Lvl 5 Raw": 0.0445619335347432,
    "MATH Lvl 5": 4.45619335347432,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.4000416666666667,
    "MUSR": 7.805208333333336,
    "MMLU-PRO Raw": 0.3664394946808511,
    "MMLU-PRO": 29.604388297872337,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Jimmy19991222_llama-3-8b-instruct-gapo-v2-rougeL-beta10-gamma0.3-lr1.0e-6-scale-log_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Jimmy19991222/llama-3-8b-instruct-gapo-v2-rougeL-beta10-gamma0.3-lr1.0e-6-scale-log\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Jimmy19991222/llama-3-8b-instruct-gapo-v2-rougeL-beta10-gamma0.3-lr1.0e-6-scale-log</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Jimmy19991222__llama-3-8b-instruct-gapo-v2-rougeL-beta10-gamma0.3-lr1.0e-6-scale-log-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Jimmy19991222/llama-3-8b-instruct-gapo-v2-rougeL-beta10-gamma0.3-lr1.0e-6-scale-log",
    "Model sha": "ec67f95c4d1813a34bbde52d0ad14824fd7111a0",
    "Average ‚¨ÜÔ∏è": 23.742269138965423,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.48658594812293093,
    "IFEval Raw": 0.649190813707629,
    "IFEval": 64.91908137076291,
    "BBH Raw": 0.4952489348573605,
    "BBH": 28.56256683836558,
    "MATH Lvl 5 Raw": 0.04531722054380665,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.3961354166666667,
    "MUSR": 7.383593750000003,
    "MMLU-PRO Raw": 0.37109375,
    "MMLU-PRO": 30.12152777777778,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "Joseph717171_Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Joseph717171/Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Joseph717171/Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Joseph717171__Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Joseph717171/Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32",
    "Model sha": "823930851c57b11fd2e25cd65b5c53f909209d0e",
    "Average ‚¨ÜÔ∏è": 23.25287703936368,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7075451204073241,
    "IFEval Raw": 0.6185410266980501,
    "IFEval": 61.854102669805016,
    "BBH Raw": 0.5177452540141246,
    "BBH": 30.724096614147953,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4369375,
    "MUSR": 13.617187499999995,
    "MMLU-PRO Raw": 0.31441156914893614,
    "MMLU-PRO": 23.823507683215126,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-25",
    "Generation": 0,
    "Base Model": "Joseph717171/Hermes-3-Llama-3.1-8B_TIES_with_Base_Embeds_Initialized_to_Special_Instruct_Toks_dtypeF32"
  },
  {
    "eval_name": "Joseph717171_Llama-3.1-SuperNova-8B-Lite_TIES_with_Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Joseph717171/Llama-3.1-SuperNova-8B-Lite_TIES_with_Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Joseph717171/Llama-3.1-SuperNova-8B-Lite_TIES_with_Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Joseph717171__Llama-3.1-SuperNova-8B-Lite_TIES_with_Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Joseph717171/Llama-3.1-SuperNova-8B-Lite_TIES_with_Base",
    "Model sha": "f1e2cad4dca10f948fd2ee9588f80df0b40d7232",
    "Average ‚¨ÜÔ∏è": 30.08138253058424,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8747313938555062,
    "IFEval Raw": 0.8096328851890761,
    "IFEval": 80.9632885189076,
    "BBH Raw": 0.5147423127141911,
    "BBH": 31.46581339489899,
    "MATH Lvl 5 Raw": 0.17371601208459217,
    "MATH Lvl 5": 17.371601208459218,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4109895833333333,
    "MUSR": 10.74036458333333,
    "MMLU-PRO Raw": 0.38804853723404253,
    "MMLU-PRO": 32.00539302600473,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "Joseph717171/Llama-3.1-SuperNova-8B-Lite_TIES_with_Base"
  },
  {
    "eval_name": "Josephgflowers_Cinder-Phi-2-V1-F16-gguf_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Josephgflowers/Cinder-Phi-2-V1-F16-gguf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Josephgflowers/Cinder-Phi-2-V1-F16-gguf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Josephgflowers__Cinder-Phi-2-V1-F16-gguf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Josephgflowers/Cinder-Phi-2-V1-F16-gguf",
    "Model sha": "85629ec9b18efee31d07630664e7a3815121badf",
    "Average ‚¨ÜÔ∏è": 10.855702876978865,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4714036094971793,
    "IFEval Raw": 0.23565694579271884,
    "IFEval": 23.565694579271884,
    "BBH Raw": 0.4396616219689493,
    "BBH": 22.45340222827221,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.34345833333333337,
    "MUSR": 1.965625,
    "MMLU-PRO Raw": 0.2160904255319149,
    "MMLU-PRO": 12.898936170212766,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-25",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Josephgflowers/Cinder-Phi-2-V1-F16-gguf"
  },
  {
    "eval_name": "Josephgflowers_Differential-Attention-Liquid-Metal-Tinyllama_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Josephgflowers__Differential-Attention-Liquid-Metal-Tinyllama-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama",
    "Model sha": "bdb6c63ff1025241e8e10b1858d67dc410f0a702",
    "Average ‚¨ÜÔ∏è": 4.709670929782421,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.17379359837669422,
    "IFEval Raw": 0.22269245601670234,
    "IFEval": 22.269245601670235,
    "BBH Raw": 0.292556113105267,
    "BBH": 2.5522242028124382,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.33555208333333336,
    "MUSR": 0.9440104166666662,
    "MMLU-PRO Raw": 0.12142619680851063,
    "MMLU-PRO": 2.3806885342789585,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-07",
    "Generation": 0,
    "Base Model": "Josephgflowers/Differential-Attention-Liquid-Metal-Tinyllama"
  },
  {
    "eval_name": "Josephgflowers_TinyLlama-Cinder-Agent-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Josephgflowers/TinyLlama-Cinder-Agent-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Josephgflowers/TinyLlama-Cinder-Agent-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Josephgflowers__TinyLlama-Cinder-Agent-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Josephgflowers/TinyLlama-Cinder-Agent-v1",
    "Model sha": "a9cd8b48bfe30f29bb1f819213da9a4c41eee67f",
    "Average ‚¨ÜÔ∏è": 5.816564399847588,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.23783151264514582,
    "IFEval Raw": 0.26695612087040166,
    "IFEval": 26.695612087040164,
    "BBH Raw": 0.31160367351776513,
    "BBH": 3.804167155031377,
    "MATH Lvl 5 Raw": 0.0037764350453172208,
    "MATH Lvl 5": 0.3776435045317221,
    "GPQA Raw": 0.24412751677852348,
    "GPQA": 0.0,
    "MUSR Raw": 0.33945833333333336,
    "MUSR": 2.232291666666667,
    "MMLU-PRO Raw": 0.11610704787234043,
    "MMLU-PRO": 1.7896719858156023,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-21",
    "Submission Date": "2024-06-26",
    "Generation": 4,
    "Base Model": "Josephgflowers/TinyLlama-3T-Cinder-v1.2"
  },
  {
    "eval_name": "Josephgflowers_TinyLlama-v1.1-Cinders-World_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Josephgflowers/TinyLlama-v1.1-Cinders-World\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Josephgflowers/TinyLlama-v1.1-Cinders-World</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Josephgflowers__TinyLlama-v1.1-Cinders-World-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Josephgflowers/TinyLlama-v1.1-Cinders-World",
    "Model sha": "11a2c305f787a7908dd87c4e5a7d0f1e314a1f05",
    "Average ‚¨ÜÔ∏è": 5.129125488958306,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.2573832662901053,
    "IFEval Raw": 0.24692260978647768,
    "IFEval": 24.692260978647766,
    "BBH Raw": 0.29979653176003074,
    "BBH": 3.1077144735021442,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.24412751677852348,
    "GPQA": 0.0,
    "MUSR Raw": 0.3356145833333333,
    "MUSR": 0.6184895833333329,
    "MMLU-PRO Raw": 0.11984707446808511,
    "MMLU-PRO": 2.2052304964539005,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-13",
    "Generation": 0,
    "Base Model": "Josephgflowers/TinyLlama-v1.1-Cinders-World"
  },
  {
    "eval_name": "Josephgflowers_TinyLlama_v1.1_math_code-world-test-1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Josephgflowers/TinyLlama_v1.1_math_code-world-test-1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Josephgflowers/TinyLlama_v1.1_math_code-world-test-1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Josephgflowers__TinyLlama_v1.1_math_code-world-test-1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Josephgflowers/TinyLlama_v1.1_math_code-world-test-1",
    "Model sha": "6f7c2aaf0b8723bc6a1dc23a4a1ff0ec24dc11ec",
    "Average ‚¨ÜÔ∏è": 1.8391657564373078,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2729437792562811,
    "IFEval Raw": 0.00784363267242029,
    "IFEval": 0.784363267242029,
    "BBH Raw": 0.31463497508928434,
    "BBH": 4.164017098724634,
    "MATH Lvl 5 Raw": 0.009818731117824775,
    "MATH Lvl 5": 0.9818731117824775,
    "GPQA Raw": 0.23406040268456377,
    "GPQA": 0.0,
    "MUSR Raw": 0.34990625000000003,
    "MUSR": 3.6382812500000004,
    "MMLU-PRO Raw": 0.11319813829787234,
    "MMLU-PRO": 1.466459810874704,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-23",
    "Submission Date": "2024-09-09",
    "Generation": 0,
    "Base Model": "Josephgflowers/TinyLlama_v1.1_math_code-world-test-1"
  },
  {
    "eval_name": "KSU-HW-SEC_Llama3-70b-SVA-FT-1415_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KSU-HW-SEC/Llama3-70b-SVA-FT-1415\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KSU-HW-SEC/Llama3-70b-SVA-FT-1415</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KSU-HW-SEC__Llama3-70b-SVA-FT-1415-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KSU-HW-SEC/Llama3-70b-SVA-FT-1415",
    "Model sha": "1c09728455567898116d2d9cfb6cbbbbd4ee730c",
    "Average ‚¨ÜÔ∏è": 36.119232723240636,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.601028865895834,
    "IFEval Raw": 0.6179913739987677,
    "IFEval": 61.79913739987677,
    "BBH Raw": 0.6650146340680478,
    "BBH": 51.328741195678994,
    "MATH Lvl 5 Raw": 0.21978851963746227,
    "MATH Lvl 5": 21.978851963746227,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.4565416666666667,
    "MUSR": 17.801041666666663,
    "MMLU-PRO Raw": 0.5242686170212766,
    "MMLU-PRO": 47.140957446808514,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "KSU-HW-SEC_Llama3-70b-SVA-FT-500_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KSU-HW-SEC/Llama3-70b-SVA-FT-500\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KSU-HW-SEC/Llama3-70b-SVA-FT-500</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KSU-HW-SEC__Llama3-70b-SVA-FT-500-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KSU-HW-SEC/Llama3-70b-SVA-FT-500",
    "Model sha": "856a23f28aeada23d1135c86a37e05524307e8ed",
    "Average ‚¨ÜÔ∏è": 35.953711985827894,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.473738241545481,
    "IFEval Raw": 0.6105223030448099,
    "IFEval": 61.052230304481,
    "BBH Raw": 0.6692236023098005,
    "BBH": 51.8870262488106,
    "MATH Lvl 5 Raw": 0.2137462235649547,
    "MATH Lvl 5": 21.374622356495472,
    "GPQA Raw": 0.3808724832214765,
    "GPQA": 17.4496644295302,
    "MUSR Raw": 0.45114583333333336,
    "MUSR": 16.993229166666666,
    "MMLU-PRO Raw": 0.522689494680851,
    "MMLU-PRO": 46.96549940898345,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "KSU-HW-SEC_Llama3-70b-SVA-FT-final_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KSU-HW-SEC/Llama3-70b-SVA-FT-final\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KSU-HW-SEC/Llama3-70b-SVA-FT-final</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KSU-HW-SEC__Llama3-70b-SVA-FT-final-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KSU-HW-SEC/Llama3-70b-SVA-FT-final",
    "Model sha": "391bbd94173b34975d1aa2c7356977a630253b75",
    "Average ‚¨ÜÔ∏è": 36.09383714321667,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.656198988776492,
    "IFEval Raw": 0.6164676391973297,
    "IFEval": 61.64676391973297,
    "BBH Raw": 0.6650146340680478,
    "BBH": 51.328741195678994,
    "MATH Lvl 5 Raw": 0.21978851963746227,
    "MATH Lvl 5": 21.978851963746227,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.4565416666666667,
    "MUSR": 17.801041666666663,
    "MMLU-PRO Raw": 0.5242686170212766,
    "MMLU-PRO": 47.140957446808514,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "KSU-HW-SEC_Llama3.1-70b-SVA-FT-1000step_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KSU-HW-SEC/Llama3.1-70b-SVA-FT-1000step\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KSU-HW-SEC/Llama3.1-70b-SVA-FT-1000step</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KSU-HW-SEC__Llama3.1-70b-SVA-FT-1000step-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KSU-HW-SEC/Llama3.1-70b-SVA-FT-1000step",
    "Model sha": "b195fea0d8f350ff29243d4e88654b1baa5af79e",
    "Average ‚¨ÜÔ∏è": 40.75025867079505,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 12.554447084741954,
    "IFEval Raw": 0.7238039512936785,
    "IFEval": 72.38039512936786,
    "BBH Raw": 0.6903120365165111,
    "BBH": 55.48536459580824,
    "MATH Lvl 5 Raw": 0.3209969788519637,
    "MATH Lvl 5": 32.09969788519637,
    "GPQA Raw": 0.3959731543624161,
    "GPQA": 19.463087248322143,
    "MUSR Raw": 0.45917708333333335,
    "MUSR": 17.830468749999998,
    "MMLU-PRO Raw": 0.5251828457446809,
    "MMLU-PRO": 47.242538416075654,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Kimargin_GPT-NEO-1.3B-wiki_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kimargin/GPT-NEO-1.3B-wiki\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kimargin/GPT-NEO-1.3B-wiki</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kimargin__GPT-NEO-1.3B-wiki-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kimargin/GPT-NEO-1.3B-wiki",
    "Model sha": "92fa51fa6589f6e8fdfcc83f085216b3dae11da5",
    "Average ‚¨ÜÔ∏è": 5.248478254584771,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8327450684953104,
    "IFEval Raw": 0.19206815693471102,
    "IFEval": 19.2068156934711,
    "BBH Raw": 0.3026339952046975,
    "BBH": 3.423611572649296,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.3882604166666666,
    "MUSR": 6.932552083333333,
    "MMLU-PRO Raw": 0.10987367021276596,
    "MMLU-PRO": 1.0970744680851066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "Kimargin/GPT-NEO-1.3B-wiki (Merge)"
  },
  {
    "eval_name": "KingNish_Qwen2.5-0.5b-Test-ft_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KingNish/Qwen2.5-0.5b-Test-ft\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KingNish/Qwen2.5-0.5b-Test-ft</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KingNish__Qwen2.5-0.5b-Test-ft-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KingNish/Qwen2.5-0.5b-Test-ft",
    "Model sha": "f905bb1d37c7853fb5c7157d8d3ad0f062b65c0f",
    "Average ‚¨ÜÔ∏è": 7.4751839774632565,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6686904299214081,
    "IFEval Raw": 0.26708134416681073,
    "IFEval": 26.708134416681073,
    "BBH Raw": 0.3231533857529747,
    "BBH": 6.058845092070314,
    "MATH Lvl 5 Raw": 0.012084592145015107,
    "MATH Lvl 5": 1.2084592145015107,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.342125,
    "MUSR": 1.432291666666666,
    "MMLU-PRO Raw": 0.16888297872340424,
    "MMLU-PRO": 7.65366430260047,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "KingNish/Qwen2.5-0.5b-Test-ft (Merge)"
  },
  {
    "eval_name": "KingNish_Reasoning-Llama-3b-v0.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/KingNish/Reasoning-Llama-3b-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">KingNish/Reasoning-Llama-3b-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/KingNish__Reasoning-Llama-3b-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "KingNish/Reasoning-Llama-3b-v0.1",
    "Model sha": "d164caf591c42a4cbc3b21d46493e72fbdbd9de8",
    "Average ‚¨ÜÔ∏è": 19.85991229689613,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6752350025810513,
    "IFEval Raw": 0.6224628430342602,
    "IFEval": 62.246284303426016,
    "BBH Raw": 0.43433592509582786,
    "BBH": 19.86245115758441,
    "MATH Lvl 5 Raw": 0.10876132930513595,
    "MATH Lvl 5": 10.876132930513595,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.31676041666666666,
    "MUSR": 2.3950520833333333,
    "MMLU-PRO Raw": 0.3029421542553192,
    "MMLU-PRO": 22.549128250591018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "Kquant03_CognitiveFusion2-4x7B-BF16_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kquant03/CognitiveFusion2-4x7B-BF16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kquant03/CognitiveFusion2-4x7B-BF16</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kquant03__CognitiveFusion2-4x7B-BF16-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kquant03/CognitiveFusion2-4x7B-BF16",
    "Model sha": "db45b86c462bb93db7ba4f2c3fe3517582c859a1",
    "Average ‚¨ÜÔ∏è": 15.591290516909313,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6660353312717722,
    "IFEval Raw": 0.35665700341759865,
    "IFEval": 35.665700341759866,
    "BBH Raw": 0.41078286111483786,
    "BBH": 17.689002759870313,
    "MATH Lvl 5 Raw": 0.05513595166163142,
    "MATH Lvl 5": 5.5135951661631415,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4145520833333333,
    "MUSR": 9.95234375,
    "MMLU-PRO Raw": 0.27925531914893614,
    "MMLU-PRO": 19.917257683215126,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-06",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "Kquant03/CognitiveFusion2-4x7B-BF16"
  },
  {
    "eval_name": "Kquant03_L3-Pneuma-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kquant03/L3-Pneuma-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kquant03/L3-Pneuma-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kquant03__L3-Pneuma-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kquant03/L3-Pneuma-8B",
    "Model sha": "257aa8d00e82f91b7a780384aa76573c2ea614a8",
    "Average ‚¨ÜÔ∏è": 16.642746333013065,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8038110075755518,
    "IFEval Raw": 0.2374056392593873,
    "IFEval": 23.74056392593873,
    "BBH Raw": 0.49550433176754827,
    "BBH": 28.820201716269676,
    "MATH Lvl 5 Raw": 0.05211480362537764,
    "MATH Lvl 5": 5.211480362537764,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.41715624999999995,
    "MUSR": 10.211197916666668,
    "MMLU-PRO Raw": 0.31840093085106386,
    "MMLU-PRO": 24.266770094562652,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Kukedlc_NeuralExperiment-7b-MagicCoder-v7.5_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralExperiment-7b-MagicCoder-v7.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralExperiment-7b-MagicCoder-v7.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralExperiment-7b-MagicCoder-v7.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralExperiment-7b-MagicCoder-v7.5",
    "Model sha": "43ea8d27d652dc15e4d27f665c5d636a5937780b",
    "Average ‚¨ÜÔ∏è": 18.031180859624474,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4548601599414557,
    "IFEval Raw": 0.4552509563513699,
    "IFEval": 45.52509563513699,
    "BBH Raw": 0.3988446544778517,
    "BBH": 16.386034485864904,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.4281979166666667,
    "MUSR": 13.058072916666665,
    "MMLU-PRO Raw": 0.2824135638297872,
    "MMLU-PRO": 20.268173758865245,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-07",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "Kukedlc/NeuralExperiment-7b-MagicCoder-v7.5"
  },
  {
    "eval_name": "Kukedlc_NeuralLLaMa-3-8b-DT-v0.1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralLLaMa-3-8b-DT-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralLLaMa-3-8b-DT-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralLLaMa-3-8b-DT-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralLLaMa-3-8b-DT-v0.1",
    "Model sha": "1fe849c1e7e4793c2fdd869fcfb51e0d1910674f",
    "Average ‚¨ÜÔ∏è": 21.259598634719698,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8529641414406882,
    "IFEval Raw": 0.4371412297149342,
    "IFEval": 43.71412297149342,
    "BBH Raw": 0.4986771544360115,
    "BBH": 28.008307823364888,
    "MATH Lvl 5 Raw": 0.08081570996978853,
    "MATH Lvl 5": 8.081570996978853,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.40711458333333334,
    "MUSR": 9.68932291666667,
    "MMLU-PRO Raw": 0.379155585106383,
    "MMLU-PRO": 31.017287234042552,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "Kukedlc/NeuralLLaMa-3-8b-DT-v0.1 (Merge)"
  },
  {
    "eval_name": "Kukedlc_NeuralLLaMa-3-8b-ORPO-v0.3_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralLLaMa-3-8b-ORPO-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralLLaMa-3-8b-ORPO-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralLLaMa-3-8b-ORPO-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralLLaMa-3-8b-ORPO-v0.3",
    "Model sha": "aa176c0db7791a1c09039135791145b0704a5f46",
    "Average ‚¨ÜÔ∏è": 17.59768422227647,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9155100842800143,
    "IFEval Raw": 0.5275912356990563,
    "IFEval": 52.759123569905626,
    "BBH Raw": 0.4557141539616392,
    "BBH": 22.391711908230842,
    "MATH Lvl 5 Raw": 0.0392749244712991,
    "MATH Lvl 5": 3.92749244712991,
    "GPQA Raw": 0.23909395973154363,
    "GPQA": 0.0,
    "MUSR Raw": 0.37003125,
    "MUSR": 3.65390625,
    "MMLU-PRO Raw": 0.3056848404255319,
    "MMLU-PRO": 22.853871158392433,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-14",
    "Submission Date": "2024-07-28",
    "Generation": 1,
    "Base Model": "Kukedlc/NeuralLLaMa-3-8b-ORPO-v0.3 (Merge)"
  },
  {
    "eval_name": "Kukedlc_NeuralSynthesis-7B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralSynthesis-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralSynthesis-7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralSynthesis-7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralSynthesis-7B-v0.1",
    "Model sha": "547a5dc8963e127a9638256bb80eb3a36da1cc5d",
    "Average ‚¨ÜÔ∏è": 19.9779124309599,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5962990298332037,
    "IFEval Raw": 0.4184563624516283,
    "IFEval": 41.84563624516284,
    "BBH Raw": 0.5144745481048844,
    "BBH": 31.83439540006779,
    "MATH Lvl 5 Raw": 0.061178247734138984,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.43328125,
    "MUSR": 13.160156250000005,
    "MMLU-PRO Raw": 0.304936835106383,
    "MMLU-PRO": 22.770759456264773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-06",
    "Submission Date": "2024-06-29",
    "Generation": 0,
    "Base Model": "Kukedlc/NeuralSynthesis-7B-v0.1"
  },
  {
    "eval_name": "Kukedlc_NeuralSynthesis-7B-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralSynthesis-7B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralSynthesis-7B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralSynthesis-7B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralSynthesis-7B-v0.3",
    "Model sha": "090fab29146f8e55066bce2f5f5859ab2d6027f4",
    "Average ‚¨ÜÔ∏è": 20.08268459117341,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5835721210587271,
    "IFEval Raw": 0.4078400865259733,
    "IFEval": 40.78400865259733,
    "BBH Raw": 0.5138078814382175,
    "BBH": 31.811748341244265,
    "MATH Lvl 5 Raw": 0.0770392749244713,
    "MATH Lvl 5": 7.7039274924471295,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.4345833333333333,
    "MUSR": 13.389583333333329,
    "MMLU-PRO Raw": 0.30501994680851063,
    "MMLU-PRO": 22.779994089834513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-07",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "Kukedlc/NeuralSynthesis-7B-v0.3"
  },
  {
    "eval_name": "Kukedlc_NeuralSynthesis-7b-v0.4-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kukedlc/NeuralSynthesis-7b-v0.4-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kukedlc/NeuralSynthesis-7b-v0.4-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kukedlc__NeuralSynthesis-7b-v0.4-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kukedlc/NeuralSynthesis-7b-v0.4-slerp",
    "Model sha": "bb3bd36fce162f472668dbd91960cd1525b45f30",
    "Average ‚¨ÜÔ∏è": 19.543100768776316,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5990642500924268,
    "IFEval Raw": 0.3947259936967247,
    "IFEval": 39.47259936967247,
    "BBH Raw": 0.5142932549151301,
    "BBH": 31.99718681136041,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.43324999999999997,
    "MUSR": 13.056250000000004,
    "MMLU-PRO Raw": 0.3042719414893617,
    "MMLU-PRO": 22.696882387706854,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-12",
    "Submission Date": "2024-07-31",
    "Generation": 1,
    "Base Model": "Kukedlc/NeuralSynthesis-7b-v0.4-slerp (Merge)"
  },
  {
    "eval_name": "Kumar955_Hemanth-llm_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Kumar955/Hemanth-llm\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Kumar955/Hemanth-llm</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Kumar955__Hemanth-llm-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Kumar955/Hemanth-llm",
    "Model sha": "871325cc04f57cd953c161a0ace49c47af8eca4c",
    "Average ‚¨ÜÔ∏è": 22.143018437127726,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9903782778128376,
    "IFEval Raw": 0.5045102550122564,
    "IFEval": 50.451025501225644,
    "BBH Raw": 0.522494907014536,
    "BBH": 33.044262388969805,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4485625,
    "MUSR": 14.503645833333335,
    "MMLU-PRO Raw": 0.3112533244680851,
    "MMLU-PRO": 23.472591607565015,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "Kumar955/Hemanth-llm (Merge)"
  },
  {
    "eval_name": "L-RAGE_3_PRYMMAL-ECE-7B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/L-RAGE/3_PRYMMAL-ECE-7B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">L-RAGE/3_PRYMMAL-ECE-7B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/L-RAGE__3_PRYMMAL-ECE-7B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "L-RAGE/3_PRYMMAL-ECE-7B-SLERP-V1",
    "Model sha": "483902db68f99affe1d7f1139755dfd115abbca5",
    "Average ‚¨ÜÔ∏è": 14.476673707758115,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.589778301043505,
    "IFEval Raw": 0.27422572108671656,
    "IFEval": 27.42257210867166,
    "BBH Raw": 0.422793974567173,
    "BBH": 19.083009480539996,
    "MATH Lvl 5 Raw": 0.08534743202416918,
    "MATH Lvl 5": 8.534743202416918,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.3841354166666667,
    "MUSR": 6.18359375,
    "MMLU-PRO Raw": 0.29247007978723405,
    "MMLU-PRO": 21.38556442080378,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "L-RAGE/3_PRYMMAL-ECE-7B-SLERP-V1 (Merge)"
  },
  {
    "eval_name": "LEESM_llama-2-7b-hf-lora-oki100p_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LEESM/llama-2-7b-hf-lora-oki100p\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LEESM/llama-2-7b-hf-lora-oki100p</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LEESM__llama-2-7b-hf-lora-oki100p-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LEESM/llama-2-7b-hf-lora-oki100p",
    "Model sha": "4bfd99888bf37e23d966f1e537fe199992c27a72",
    "Average ‚¨ÜÔ∏è": 8.70733029782081,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4838177160569967,
    "IFEval Raw": 0.25129434345314877,
    "IFEval": 25.129434345314877,
    "BBH Raw": 0.34916752720369776,
    "BBH": 10.265743141867235,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.3687291666666666,
    "MUSR": 3.5578125000000003,
    "MMLU-PRO Raw": 0.18558843085106383,
    "MMLU-PRO": 9.509825650118202,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "LEESM/llama-2-7b-hf-lora-oki100p"
  },
  {
    "eval_name": "LEESM_llama-2-7b-hf-lora-oki10p_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LEESM/llama-2-7b-hf-lora-oki10p\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LEESM/llama-2-7b-hf-lora-oki10p</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LEESM__llama-2-7b-hf-lora-oki10p-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LEESM/llama-2-7b-hf-lora-oki10p",
    "Model sha": "d6e5af01616a038ac2b5cb83f458e490e1102244",
    "Average ‚¨ÜÔ∏è": 7.090031583262745,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9836533944137831,
    "IFEval Raw": 0.22609010758121784,
    "IFEval": 22.609010758121784,
    "BBH Raw": 0.3530929513059229,
    "BBH": 9.438287176618806,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.34752083333333333,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.16788563829787234,
    "MMLU-PRO": 7.542848699763592,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "LEESM/llama-2-7b-hf-lora-oki10p"
  },
  {
    "eval_name": "LEESM_llama-3-8b-bnb-4b-kowiki231101_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LEESM/llama-3-8b-bnb-4b-kowiki231101\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LEESM/llama-3-8b-bnb-4b-kowiki231101</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LEESM__llama-3-8b-bnb-4b-kowiki231101-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LEESM/llama-3-8b-bnb-4b-kowiki231101",
    "Model sha": "63b8f715daab6a0c7196a20855be8e85fe7ddcb4",
    "Average ‚¨ÜÔ∏è": 9.271088455799534,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7568876072004526,
    "IFEval Raw": 0.16848739123303944,
    "IFEval": 16.848739123303943,
    "BBH Raw": 0.4130805653617178,
    "BBH": 16.93486814930169,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.3551458333333333,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.24251994680851063,
    "MMLU-PRO": 15.83554964539007,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "LEESM_llama-3-Korean-Bllossom-8B-trexlab-oki10p_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LEESM/llama-3-Korean-Bllossom-8B-trexlab-oki10p\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LEESM/llama-3-Korean-Bllossom-8B-trexlab-oki10p</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LEESM__llama-3-Korean-Bllossom-8B-trexlab-oki10p-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LEESM/llama-3-Korean-Bllossom-8B-trexlab-oki10p",
    "Model sha": "d105e0365510f9e5f8550558343083cab8523524",
    "Average ‚¨ÜÔ∏è": 12.943197667049484,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.758357790011608,
    "IFEval Raw": 0.21372513818889433,
    "IFEval": 21.372513818889434,
    "BBH Raw": 0.43430121169320707,
    "BBH": 19.797435760911394,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.38692708333333337,
    "MUSR": 7.665885416666669,
    "MMLU-PRO Raw": 0.3176529255319149,
    "MMLU-PRO": 24.183658392434985,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-22",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "LEESM/llama-3-Korean-Bllossom-8B-trexlab-oki10p"
  },
  {
    "eval_name": "LGAI-EXAONE_EXAONE-3.0-7.8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "ExaoneForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LGAI-EXAONE__EXAONE-3.0-7.8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
    "Model sha": "7f15baedd46858153d817445aff032f4d6cf4939",
    "Average ‚¨ÜÔ∏è": 21.403463325745587,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 377,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8251279774796095,
    "IFEval Raw": 0.7192826145737754,
    "IFEval": 71.92826145737754,
    "BBH Raw": 0.4174432647784512,
    "BBH": 17.97733539518049,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.366125,
    "MUSR": 3.298958333333333,
    "MMLU-PRO Raw": 0.35771276595744683,
    "MMLU-PRO": 28.63475177304965,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-31",
    "Submission Date": "2024-08-18",
    "Generation": 0,
    "Base Model": "LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct"
  },
  {
    "eval_name": "LLM360_K2_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/K2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/K2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LLM360__K2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LLM360/K2",
    "Model sha": "49d159b6f2b64d562e745f0ff06e65b9a4c28ead",
    "Average ‚¨ÜÔ∏è": 14.568224589032942,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 80,
    "#Params (B)": 65,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 8.838206417729227,
    "IFEval Raw": 0.2252157608478836,
    "IFEval": 22.52157608478836,
    "BBH Raw": 0.4971835676523677,
    "BBH": 28.220402834201128,
    "MATH Lvl 5 Raw": 0.02265861027190333,
    "MATH Lvl 5": 2.2658610271903328,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.39799999999999996,
    "MUSR": 8.550000000000004,
    "MMLU-PRO Raw": 0.30044880319148937,
    "MMLU-PRO": 22.272089243498815,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "LLM360/K2"
  },
  {
    "eval_name": "LLM360_K2-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LLM360/K2-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM360/K2-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LLM360__K2-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LLM360/K2-Chat",
    "Model sha": "5454f2d28031c9127e4227c873ca2f154e02e4c7",
    "Average ‚¨ÜÔ∏è": 22.93951172079261,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 33,
    "#Params (B)": 65,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 17.259828063150998,
    "IFEval Raw": 0.5151763986223221,
    "IFEval": 51.51763986223221,
    "BBH Raw": 0.5358099630242067,
    "BBH": 33.79382923599304,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.457,
    "MUSR": 16.824999999999996,
    "MMLU-PRO Raw": 0.3371010638297872,
    "MMLU-PRO": 26.34456264775413,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "LLM360/K2-Chat"
  },
  {
    "eval_name": "LLM4Binary_llm4decompile-1.3b-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LLM4Binary/llm4decompile-1.3b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LLM4Binary/llm4decompile-1.3b-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LLM4Binary__llm4decompile-1.3b-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LLM4Binary/llm4decompile-1.3b-v2",
    "Model sha": "a347dabcb1ea9f21c9339bd764c150262e993b95",
    "Average ‚¨ÜÔ∏è": 6.850908042622611,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.24758267594153469,
    "IFEval Raw": 0.22678936333373229,
    "IFEval": 22.67893633337323,
    "BBH Raw": 0.3271808417267589,
    "BBH": 5.915475430438469,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.23573825503355705,
    "GPQA": 0.0,
    "MUSR Raw": 0.4071770833333333,
    "MUSR": 9.430468750000001,
    "MMLU-PRO Raw": 0.12092752659574468,
    "MMLU-PRO": 2.3252807328605196,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-18",
    "Submission Date": "2024-11-16",
    "Generation": 0,
    "Base Model": "LLM4Binary/llm4decompile-1.3b-v2"
  },
  {
    "eval_name": "Lambent_qwen2.5-reinstruct-alternate-lumen-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lambent/qwen2.5-reinstruct-alternate-lumen-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lambent/qwen2.5-reinstruct-alternate-lumen-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lambent__qwen2.5-reinstruct-alternate-lumen-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lambent/qwen2.5-reinstruct-alternate-lumen-14B",
    "Model sha": "dac3be334098338fb6c02636349e8ed53f18c4a4",
    "Average ‚¨ÜÔ∏è": 33.966892262649466,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2646505172395983,
    "IFEval Raw": 0.47938137475232384,
    "IFEval": 47.93813747523238,
    "BBH Raw": 0.6458988582965893,
    "BBH": 48.989609006737815,
    "MATH Lvl 5 Raw": 0.21601208459214505,
    "MATH Lvl 5": 21.601208459214504,
    "GPQA Raw": 0.3766778523489933,
    "GPQA": 16.890380313199106,
    "MUSR Raw": 0.47700000000000004,
    "MUSR": 19.624999999999996,
    "MMLU-PRO Raw": 0.538813164893617,
    "MMLU-PRO": 48.757018321512994,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "Lambent/qwen2.5-reinstruct-alternate-lumen-14B (Merge)"
  },
  {
    "eval_name": "Langboat_Mengzi3-8B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Langboat/Mengzi3-8B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Langboat/Mengzi3-8B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Langboat__Mengzi3-8B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Langboat/Mengzi3-8B-Chat",
    "Model sha": "128fffd3dac7c6067ca4d1a650e836e3ef46c013",
    "Average ‚¨ÜÔ∏è": 19.82253217790367,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8519356294482479,
    "IFEval Raw": 0.513977357854936,
    "IFEval": 51.3977357854936,
    "BBH Raw": 0.4683725003203179,
    "BBH": 25.188298449475578,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.4077916666666667,
    "MUSR": 9.040625,
    "MMLU-PRO Raw": 0.31416223404255317,
    "MMLU-PRO": 23.795803782505907,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-10-21",
    "Generation": 0,
    "Base Model": "Langboat/Mengzi3-8B-Chat"
  },
  {
    "eval_name": "LenguajeNaturalAI_leniachat-gemma-2b-v0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LenguajeNaturalAI/leniachat-gemma-2b-v0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LenguajeNaturalAI/leniachat-gemma-2b-v0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LenguajeNaturalAI__leniachat-gemma-2b-v0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LenguajeNaturalAI/leniachat-gemma-2b-v0",
    "Model sha": "e5691dcc682a10dc9ef4bdbb3dc896fcf271018e",
    "Average ‚¨ÜÔ∏è": 5.598771713093911,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.966577672280824,
    "IFEval Raw": 0.21497404664069114,
    "IFEval": 21.497404664069116,
    "BBH Raw": 0.30740211895412034,
    "BBH": 4.1382969637280675,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.36590625000000004,
    "MUSR": 3.6382812500000004,
    "MMLU-PRO Raw": 0.11702127659574468,
    "MMLU-PRO": 1.891252955082742,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-09",
    "Submission Date": "2024-09-01",
    "Generation": 1,
    "Base Model": "google/gemma-2b"
  },
  {
    "eval_name": "LenguajeNaturalAI_leniachat-qwen2-1.5B-v0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LenguajeNaturalAI/leniachat-qwen2-1.5B-v0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LenguajeNaturalAI/leniachat-qwen2-1.5B-v0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LenguajeNaturalAI__leniachat-qwen2-1.5B-v0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LenguajeNaturalAI/leniachat-qwen2-1.5B-v0",
    "Model sha": "031a2efebb3cc1150e46f42ba0bea9fa7b855436",
    "Average ‚¨ÜÔ∏è": 8.543038998265203,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8445617663049534,
    "IFEval Raw": 0.22211842356059697,
    "IFEval": 22.211842356059698,
    "BBH Raw": 0.36835590195612017,
    "BBH": 12.771666354808303,
    "MATH Lvl 5 Raw": 0.010574018126888216,
    "MATH Lvl 5": 1.0574018126888216,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3749895833333334,
    "MUSR": 3.873697916666666,
    "MMLU-PRO Raw": 0.18799867021276595,
    "MMLU-PRO": 9.77763002364066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-16",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-1.5B"
  },
  {
    "eval_name": "LeroyDyer_LCARS_AI_001_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/LCARS_AI_001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/LCARS_AI_001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__LCARS_AI_001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/LCARS_AI_001",
    "Model sha": "3452e84fbfd92c62085fdce3834eff5c9cd87d4f",
    "Average ‚¨ÜÔ∏è": 14.416617622602132,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.581991823672954,
    "IFEval Raw": 0.31094495937445976,
    "IFEval": 31.094495937445977,
    "BBH Raw": 0.42578875825590146,
    "BBH": 19.460966800253622,
    "MATH Lvl 5 Raw": 0.022658610271903325,
    "MATH Lvl 5": 2.2658610271903323,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.43836458333333334,
    "MUSR": 13.328906250000003,
    "MMLU-PRO Raw": 0.2670378989361702,
    "MMLU-PRO": 18.559766548463354,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer_LCARS_AI_1x4_003_SuperAI_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/LCARS_AI_1x4_003_SuperAI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/LCARS_AI_1x4_003_SuperAI</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__LCARS_AI_1x4_003_SuperAI-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/LCARS_AI_1x4_003_SuperAI",
    "Model sha": "917c84d241bfff8b8648d9d865ae4b5bead68c6b",
    "Average ‚¨ÜÔ∏è": 19.467876582042745,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6572590101673916,
    "IFEval Raw": 0.41111251479407973,
    "IFEval": 41.11125147940797,
    "BBH Raw": 0.49198503573704794,
    "BBH": 28.423430655930204,
    "MATH Lvl 5 Raw": 0.05438066465256797,
    "MATH Lvl 5": 5.438066465256797,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4506145833333333,
    "MUSR": 15.560156250000004,
    "MMLU-PRO Raw": 0.29720744680851063,
    "MMLU-PRO": 21.911938534278956,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-08-07",
    "Generation": 1,
    "Base Model": "LeroyDyer/LCARS_AI_1x4_003_SuperAI (Merge)"
  },
  {
    "eval_name": "LeroyDyer_LCARS_AI_StarTrek_Computer_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/LCARS_AI_StarTrek_Computer\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/LCARS_AI_StarTrek_Computer</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__LCARS_AI_StarTrek_Computer-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/LCARS_AI_StarTrek_Computer",
    "Model sha": "9d4af4ab13df574ad0d40ed71de7d43c17f59a94",
    "Average ‚¨ÜÔ∏è": 14.57612922252259,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6619306009486569,
    "IFEval Raw": 0.35825609383103496,
    "IFEval": 35.8256093831035,
    "BBH Raw": 0.4446191188748297,
    "BBH": 21.781003095704886,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3950208333333333,
    "MUSR": 7.444270833333334,
    "MMLU-PRO Raw": 0.24584441489361702,
    "MMLU-PRO": 16.204934988179666,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-11",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "LeroyDyer/LCARS_AI_StarTrek_Computer"
  },
  {
    "eval_name": "LeroyDyer_LCARS_TOP_SCORE_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/LCARS_TOP_SCORE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/LCARS_TOP_SCORE</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__LCARS_TOP_SCORE-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/LCARS_TOP_SCORE",
    "Model sha": "ada3e3ac6ae162503da5158e72851053f4c7dac8",
    "Average ‚¨ÜÔ∏è": 20.322005404213282,
    "Hub License": "openrail",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6133181226037983,
    "IFEval Raw": 0.43706587410293574,
    "IFEval": 43.70658741029358,
    "BBH Raw": 0.5127371051825098,
    "BBH": 31.69912679947687,
    "MATH Lvl 5 Raw": 0.06722054380664652,
    "MATH Lvl 5": 6.722054380664652,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.42928125,
    "MUSR": 12.426822916666671,
    "MMLU-PRO Raw": 0.3031083776595745,
    "MMLU-PRO": 22.567597517730498,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-30",
    "Submission Date": "2024-08-08",
    "Generation": 1,
    "Base Model": "LeroyDyer/LCARS_TOP_SCORE (Merge)"
  },
  {
    "eval_name": "LeroyDyer_Mixtral_AI_SwahiliTron_7b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/Mixtral_AI_SwahiliTron_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/Mixtral_AI_SwahiliTron_7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__Mixtral_AI_SwahiliTron_7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/Mixtral_AI_SwahiliTron_7b",
    "Model sha": "fd997ccdee03788e7e79944d26d9c641dc4fcd4c",
    "Average ‚¨ÜÔ∏è": 4.2705450975952735,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6984955784720502,
    "IFEval Raw": 0.1533996462718919,
    "IFEval": 15.339964627189191,
    "BBH Raw": 0.3055092453201354,
    "BBH": 3.211683047233007,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.34203125,
    "MUSR": 1.920572916666666,
    "MMLU-PRO Raw": 0.12076130319148937,
    "MMLU-PRO": 2.30681146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-10",
    "Submission Date": "2024-07-12",
    "Generation": 0,
    "Base Model": "LeroyDyer/Mixtral_AI_SwahiliTron_7b"
  },
  {
    "eval_name": "LeroyDyer_SpydazWebAI_Human_AGI_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWebAI_Human_AGI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWebAI_Human_AGI</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWebAI_Human_AGI-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWebAI_Human_AGI",
    "Model sha": "0bc02d34a0b49c3473505d8df757de211af37131",
    "Average ‚¨ÜÔ∏è": 9.907408810969168,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6677153990275635,
    "IFEval Raw": 0.3388221031308041,
    "IFEval": 33.88221031308041,
    "BBH Raw": 0.3374862127508733,
    "BBH": 7.445695539873622,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.39663541666666663,
    "MUSR": 7.379427083333333,
    "MMLU-PRO Raw": 0.1478557180851064,
    "MMLU-PRO": 5.317302009456265,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "LeroyDyer/SpydazWebAI_Human_AGI (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWebAI_Human_AGI_001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWebAI_Human_AGI_001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWebAI_Human_AGI_001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWebAI_Human_AGI_001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWebAI_Human_AGI_001",
    "Model sha": "4ed76e404deb425d5c934cdbbb4b99b4c1017433",
    "Average ‚¨ÜÔ∏è": 10.120978492084495,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44240658980134534,
    "IFEval Raw": 0.31181930610779396,
    "IFEval": 31.1819306107794,
    "BBH Raw": 0.3433421938604874,
    "BBH": 8.661200013836998,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.39939583333333334,
    "MUSR": 8.224479166666667,
    "MMLU-PRO Raw": 0.14261968085106383,
    "MMLU-PRO": 4.735520094562647,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "LeroyDyer/SpydazWebAI_Human_AGI_001 (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_CyberTron_Ultra_7b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_CyberTron_Ultra_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_CyberTron_Ultra_7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_CyberTron_Ultra_7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_CyberTron_Ultra_7b",
    "Model sha": "50c69e539578ab5384eb018a60cc1268637becae",
    "Average ‚¨ÜÔ∏è": 13.478558813262525,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6565460368913401,
    "IFEval Raw": 0.15557276914143361,
    "IFEval": 15.557276914143362,
    "BBH Raw": 0.48107736108561827,
    "BBH": 27.74553183153259,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.41362499999999996,
    "MUSR": 10.303125000000003,
    "MMLU-PRO Raw": 0.2865691489361702,
    "MMLU-PRO": 20.72990543735224,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-14",
    "Submission Date": "2024-07-12",
    "Generation": 1,
    "Base Model": "LeroyDyer/Mixtral_AI_CyberTron_Ultra"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_HumanAI_001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_HumanAI_001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_HumanAI_001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_HumanAI_001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_HumanAI_001",
    "Model sha": "7d664b94eb7c50bd0314ee74b7ac564c55efa878",
    "Average ‚¨ÜÔ∏è": 7.678967220435808,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6441774683784759,
    "IFEval Raw": 0.22516589316347294,
    "IFEval": 22.516589316347293,
    "BBH Raw": 0.33440360243051986,
    "BBH": 8.06526235359233,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.38603125,
    "MUSR": 6.053906250000002,
    "MMLU-PRO Raw": 0.1270777925531915,
    "MMLU-PRO": 3.008643617021276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-17",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_HumanAI_006_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_HumanAI_006\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_HumanAI_006</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_HumanAI_006-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_HumanAI_006",
    "Model sha": "c3ef6d31d58344f6d67825769a304b9ac5e702ca",
    "Average ‚¨ÜÔ∏è": 4.75810132799894,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44891065808953184,
    "IFEval Raw": 0.14300832901146734,
    "IFEval": 14.300832901146736,
    "BBH Raw": 0.3301800420981355,
    "BBH": 6.725319981390316,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3567916666666667,
    "MUSR": 1.7656250000000016,
    "MMLU-PRO Raw": 0.11353058510638298,
    "MMLU-PRO": 1.5033983451536632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-18",
    "Generation": 2,
    "Base Model": "LeroyDyer/SpydazWeb_AI_HumanAI_005 (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_HumanAI_007_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_HumanAI_007\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_HumanAI_007</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_HumanAI_007-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_HumanAI_007",
    "Model sha": "38d8c760a50e09cc877497275701de207ed54953",
    "Average ‚¨ÜÔ∏è": 10.29773571689596,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4487411697422009,
    "IFEval Raw": 0.3351751131442351,
    "IFEval": 33.51751131442351,
    "BBH Raw": 0.3415665794743605,
    "BBH": 8.46281905839016,
    "MATH Lvl 5 Raw": 0.015105740181268883,
    "MATH Lvl 5": 1.5105740181268883,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.40962499999999996,
    "MUSR": 9.236458333333333,
    "MMLU-PRO Raw": 0.13522273936170212,
    "MMLU-PRO": 3.9136377068557904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "LeroyDyer/SpydazWeb_AI_HumanAI_007 (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_HumanAI_RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_HumanAI_RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_HumanAI_RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_HumanAI_RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_HumanAI_RP",
    "Model sha": "0569cca30df948b9f2e5145ce5c2b5a03ec025ae",
    "Average ‚¨ÜÔ∏è": 7.6689433176032935,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44370854460054904,
    "IFEval Raw": 0.2541168543907942,
    "IFEval": 25.41168543907942,
    "BBH Raw": 0.33230179059744286,
    "BBH": 7.1764945791517105,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.3882604166666666,
    "MUSR": 5.865885416666665,
    "MMLU-PRO Raw": 0.1323969414893617,
    "MMLU-PRO": 3.599660165484633,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "LeroyDyer/SpydazWeb_AI_HumanAI_RP (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_AI_HumanAI_TextVision_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_AI_HumanAI_TextVision\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_AI_HumanAI_TextVision</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_AI_HumanAI_TextVision-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_AI_HumanAI_TextVision",
    "Model sha": "ba0dcf52fec492cc5d91b3297c08c5581d893607",
    "Average ‚¨ÜÔ∏è": 9.345527494068838,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4557151627283165,
    "IFEval Raw": 0.3062740196013245,
    "IFEval": 30.62740196013245,
    "BBH Raw": 0.33536617928965984,
    "BBH": 7.525593201173674,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.39384375,
    "MUSR": 7.497135416666669,
    "MMLU-PRO Raw": 0.13871343085106383,
    "MMLU-PRO": 4.301492316784869,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-11-18",
    "Generation": 2,
    "Base Model": "LeroyDyer/SpydazWeb_AI_HumanAI_RP (Merge)"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_HumanAI_M1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_HumanAI_M1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_HumanAI_M1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_HumanAI_M1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_HumanAI_M1",
    "Model sha": "c9bb5fdc262f9c68d02b798eb867495199bf3dbf",
    "Average ‚¨ÜÔ∏è": 10.391053333653405,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6608902644526,
    "IFEval Raw": 0.3582062261466243,
    "IFEval": 35.820622614662426,
    "BBH Raw": 0.35632705798398107,
    "BBH": 10.02754339015283,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.36711458333333336,
    "MUSR": 4.289322916666667,
    "MMLU-PRO Raw": 0.1663065159574468,
    "MMLU-PRO": 7.367390661938533,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-16",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_HumanAI_M2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_HumanAI_M2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_HumanAI_M2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_HumanAI_M2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_HumanAI_M2",
    "Model sha": "82fd99df73eeaf8ce12add6e74fda7901c75f86c",
    "Average ‚¨ÜÔ∏è": 12.446350312296405,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.631059639441631,
    "IFEval Raw": 0.3750171766468526,
    "IFEval": 37.50171766468526,
    "BBH Raw": 0.39308772552915555,
    "BBH": 15.397194202296944,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.3751458333333333,
    "MUSR": 3.9932291666666657,
    "MMLU-PRO Raw": 0.2010472074468085,
    "MMLU-PRO": 11.227467494089833,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-16",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer_SpydazWeb_HumanAI_M3_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/SpydazWeb_HumanAI_M3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/SpydazWeb_HumanAI_M3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer__SpydazWeb_HumanAI_M3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/SpydazWeb_HumanAI_M3",
    "Model sha": "01dbeb9536ad2cba5a3c4fbeef77e6b3f692adc5",
    "Average ‚¨ÜÔ∏è": 5.405095635257794,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6875494454496557,
    "IFEval Raw": 0.1578711153073844,
    "IFEval": 15.787111530738443,
    "BBH Raw": 0.31272572546166244,
    "BBH": 4.765388996591294,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.3914270833333333,
    "MUSR": 7.128385416666667,
    "MMLU-PRO Raw": 0.11486037234042554,
    "MMLU-PRO": 1.6511524822695034,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-16",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_12_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_12\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_12</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_12-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_12",
    "Model sha": "675cf7fbfa36974b2eb5aef53afdf56a65ecfcfd",
    "Average ‚¨ÜÔ∏è": 6.451512878120261,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6485254035817755,
    "IFEval Raw": 0.2764985793250797,
    "IFEval": 27.64985793250797,
    "BBH Raw": 0.31633960292107943,
    "BBH": 4.495993524198578,
    "MATH Lvl 5 Raw": 0.0037764350453172208,
    "MATH Lvl 5": 0.3776435045317221,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.35815624999999995,
    "MUSR": 2.2028645833333327,
    "MMLU-PRO Raw": 0.11369680851063829,
    "MMLU-PRO": 1.521867612293143,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_14_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_14\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_14</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_14-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_14",
    "Model sha": "53e73726a0a780db48303f4befbf7574e5c04984",
    "Average ‚¨ÜÔ∏è": 4.20213088745114,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6213508564736376,
    "IFEval Raw": 0.1811770546594148,
    "IFEval": 18.11770546594148,
    "BBH Raw": 0.2988848127354542,
    "BBH": 2.162400468558809,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3395208333333333,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.11394614361702128,
    "MMLU-PRO": 1.549571513002364,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_BIBLE_002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_BIBLE_002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_BIBLE_002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_BIBLE_002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_BIBLE_002",
    "Model sha": "f47113e6352f4df8c50e9e571fc85cd7a154a07f",
    "Average ‚¨ÜÔ∏è": 6.760196792392537,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.653249878257643,
    "IFEval Raw": 0.21949538336059432,
    "IFEval": 21.949538336059433,
    "BBH Raw": 0.3289070186514165,
    "BBH": 6.349580156104774,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.34069791666666666,
    "MUSR": 2.4539062499999997,
    "MMLU-PRO Raw": 0.13680186170212766,
    "MMLU-PRO": 4.089095744680851,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "LeroyDyer/_Spydaz_Web_AI_BIBLE_002"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_ChatML_002_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_ChatML_002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_ChatML_002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_ChatML_002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_ChatML_002",
    "Model sha": "9475af8113cf4027839974283b702d6be502f7fa",
    "Average ‚¨ÜÔ∏è": 5.526903574467614,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6408049347507471,
    "IFEval Raw": 0.24122772022677608,
    "IFEval": 24.12277202267761,
    "BBH Raw": 0.3106383598957094,
    "BBH": 4.191974214495695,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.3623125,
    "MUSR": 2.7890625000000004,
    "MMLU-PRO Raw": 0.10945811170212766,
    "MMLU-PRO": 1.0509013002364058,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-01",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_ChatQA_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_ChatQA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_ChatQA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_ChatQA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_ChatQA",
    "Model sha": "9f86dd12d4c75e0290aa3084a44cf111bc975144",
    "Average ‚¨ÜÔ∏è": 4.9514879965764145,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.582079399818881,
    "IFEval Raw": 0.1414591062824417,
    "IFEval": 14.14591062824417,
    "BBH Raw": 0.32359493837413505,
    "BBH": 5.599561733978841,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.3447291666666667,
    "MUSR": 2.5578125000000003,
    "MMLU-PRO Raw": 0.14752327127659576,
    "MMLU-PRO": 5.280363475177306,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LeroyDyer__Spydaz_Web_AI_ChatQA_003_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LeroyDyer/_Spydaz_Web_AI_ChatQA_003\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LeroyDyer/_Spydaz_Web_AI_ChatQA_003</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LeroyDyer___Spydaz_Web_AI_ChatQA_003-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LeroyDyer/_Spydaz_Web_AI_ChatQA_003",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 6.13167884357284,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6298125839746871,
    "IFEval Raw": 0.22091938279321088,
    "IFEval": 22.09193827932109,
    "BBH Raw": 0.3171811407815537,
    "BBH": 4.293436202390652,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.38184375,
    "MUSR": 5.830468750000001,
    "MMLU-PRO Raw": 0.11328125,
    "MMLU-PRO": 1.4756944444444438,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-2B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-2B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-2B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-2B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-2B-SLERP-V1",
    "Model sha": "fda2d7dd2d797726ebd34cee88095e0ae6b0b093",
    "Average ‚¨ÜÔ∏è": 21.173036771512912,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4241604785653426,
    "IFEval Raw": 0.5823459531820016,
    "IFEval": 58.23459531820015,
    "BBH Raw": 0.4287069505821554,
    "BBH": 19.53491130754225,
    "MATH Lvl 5 Raw": 0.09214501510574018,
    "MATH Lvl 5": 9.214501510574017,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.43746875,
    "MUSR": 13.916927083333334,
    "MMLU-PRO Raw": 0.2677859042553192,
    "MMLU-PRO": 18.642878250591018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-07",
    "Generation": 1,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-2B-SLERP-V1 (Merge)"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-2B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-2B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-2B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-2B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-2B-SLERP-V2",
    "Model sha": "7e55d63df09ec396f39adcc426a91f2e74606bd0",
    "Average ‚¨ÜÔ∏è": 21.03618886927276,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.289832484365067,
    "IFEval Raw": 0.5542693386880144,
    "IFEval": 55.42693386880145,
    "BBH Raw": 0.43764741906109417,
    "BBH": 20.197376640583055,
    "MATH Lvl 5 Raw": 0.09214501510574018,
    "MATH Lvl 5": 9.214501510574017,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.44816666666666666,
    "MUSR": 15.620833333333335,
    "MMLU-PRO Raw": 0.2744348404255319,
    "MMLU-PRO": 19.38164893617021,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-05",
    "Generation": 1,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-2B-SLERP-V2 (Merge)"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-7B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-7B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-7B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-7B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-7B-SLERP",
    "Model sha": "fdc2ac0da72ad62ecc9677cdac32dd097bc99c3a",
    "Average ‚¨ÜÔ∏è": 30.79800521013577,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6850489839975119,
    "IFEval Raw": 0.5592649724952016,
    "IFEval": 55.92649724952015,
    "BBH Raw": 0.5556642048146725,
    "BBH": 36.482570049394035,
    "MATH Lvl 5 Raw": 0.3187311178247734,
    "MATH Lvl 5": 31.87311178247734,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.43960416666666663,
    "MUSR": 13.483854166666669,
    "MMLU-PRO Raw": 0.45071476063829785,
    "MMLU-PRO": 38.968306737588655,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-7B-SLERP (Merge)"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-7B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-7B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-7B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-7B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V1",
    "Model sha": "1f9b9683053a13b9f5c3863a6de53d9e14a2e6c5",
    "Average ‚¨ÜÔ∏è": 3.7204134260134736,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2495998696070838,
    "IFEval Raw": 0.10733742026711349,
    "IFEval": 10.733742026711349,
    "BBH Raw": 0.30525797550329686,
    "BBH": 2.7840182309259673,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3910833333333333,
    "MUSR": 7.318749999999999,
    "MMLU-PRO Raw": 0.11236702127659574,
    "MMLU-PRO": 1.374113475177304,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V1"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-7B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-7B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-7B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-7B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V2",
    "Model sha": "d633d064bcd8723da5b2337048cee1079e745766",
    "Average ‚¨ÜÔ∏è": 3.7204134260134736,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.275211435580003,
    "IFEval Raw": 0.10733742026711349,
    "IFEval": 10.733742026711349,
    "BBH Raw": 0.30525797550329686,
    "BBH": 2.7840182309259673,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3910833333333333,
    "MUSR": 7.318749999999999,
    "MMLU-PRO Raw": 0.11236702127659574,
    "MMLU-PRO": 1.374113475177304,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V2"
  },
  {
    "eval_name": "Lil-R_2_PRYMMAL-ECE-7B-SLERP-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/2_PRYMMAL-ECE-7B-SLERP-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/2_PRYMMAL-ECE-7B-SLERP-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__2_PRYMMAL-ECE-7B-SLERP-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V3",
    "Model sha": "691d98e52b8136355cf3884a4c29968bf0fc6dcf",
    "Average ‚¨ÜÔ∏è": 8.77802167872376,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2858008502868983,
    "IFEval Raw": 0.22346706738121516,
    "IFEval": 22.346706738121515,
    "BBH Raw": 0.357839880712804,
    "BBH": 10.612229209084205,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.4107083333333333,
    "MUSR": 9.738541666666668,
    "MMLU-PRO Raw": 0.18168218085106383,
    "MMLU-PRO": 9.075797872340424,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "Lil-R/2_PRYMMAL-ECE-7B-SLERP-V3 (Merge)"
  },
  {
    "eval_name": "Lil-R_PRYMMAL-ECE-1B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/PRYMMAL-ECE-1B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/PRYMMAL-ECE-1B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__PRYMMAL-ECE-1B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/PRYMMAL-ECE-1B-SLERP-V1",
    "Model sha": "5770824fbfc2f9df22f6a1442e1392b029e333ec",
    "Average ‚¨ÜÔ∏è": 14.404947865677416,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6473094756783748,
    "IFEval Raw": 0.2874395492847866,
    "IFEval": 28.743954928478658,
    "BBH Raw": 0.41904526564708194,
    "BBH": 17.999676133564762,
    "MATH Lvl 5 Raw": 0.07477341389728097,
    "MATH Lvl 5": 7.477341389728097,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.39743749999999994,
    "MUSR": 7.346354166666667,
    "MMLU-PRO Raw": 0.2925531914893617,
    "MMLU-PRO": 21.39479905437352,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "Lil-R/PRYMMAL-ECE-1B-SLERP-V1 (Merge)"
  },
  {
    "eval_name": "Lil-R_PRYMMAL-ECE-7B-SLERP-V8_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lil-R/PRYMMAL-ECE-7B-SLERP-V8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lil-R/PRYMMAL-ECE-7B-SLERP-V8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lil-R__PRYMMAL-ECE-7B-SLERP-V8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lil-R/PRYMMAL-ECE-7B-SLERP-V8",
    "Model sha": "19fa915c941013075673c2943e2d06d131afcfef",
    "Average ‚¨ÜÔ∏è": 3.222583873408301,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2781107719464087,
    "IFEval Raw": 0.1258471965495995,
    "IFEval": 12.58471965495995,
    "BBH Raw": 0.2955092966258663,
    "BBH": 2.270601109130521,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.36314583333333333,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.11278257978723404,
    "MMLU-PRO": 1.4202866430260035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "Lil-R/PRYMMAL-ECE-7B-SLERP-V8"
  },
  {
    "eval_name": "LilRg_10PRYMMAL-3B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/10PRYMMAL-3B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/10PRYMMAL-3B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__10PRYMMAL-3B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/10PRYMMAL-3B-slerp",
    "Model sha": "3e0a12c2ec82e18136fc1cf1609c66154cff8a6e",
    "Average ‚¨ÜÔ∏è": 20.382961212646297,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.7429066405461255,
    "IFEval Raw": 0.1945903535951276,
    "IFEval": 19.45903535951276,
    "BBH Raw": 0.5320377091634505,
    "BBH": 34.877917500461,
    "MATH Lvl 5 Raw": 0.10725075528700907,
    "MATH Lvl 5": 10.725075528700907,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.45290625,
    "MUSR": 15.713281250000001,
    "MMLU-PRO Raw": 0.3881316489361702,
    "MMLU-PRO": 32.014627659574465,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "LilRg/10PRYMMAL-3B-slerp (Merge)"
  },
  {
    "eval_name": "LilRg_ECE-1B-merge-PRYMMAL_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/ECE-1B-merge-PRYMMAL\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/ECE-1B-merge-PRYMMAL</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__ECE-1B-merge-PRYMMAL-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/ECE-1B-merge-PRYMMAL",
    "Model sha": "009c75039786c38e2a6168cf93c9a46a4d111fb9",
    "Average ‚¨ÜÔ∏è": 14.346595145088264,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6903570368821728,
    "IFEval Raw": 0.27122811916825135,
    "IFEval": 27.122811916825135,
    "BBH Raw": 0.42345600176908743,
    "BBH": 19.141465000010825,
    "MATH Lvl 5 Raw": 0.09214501510574018,
    "MATH Lvl 5": 9.214501510574017,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.3801041666666667,
    "MUSR": 5.279687499999999,
    "MMLU-PRO Raw": 0.2906416223404255,
    "MMLU-PRO": 21.1824024822695,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "LilRg/ECE-1B-merge-PRYMMAL (Merge)"
  },
  {
    "eval_name": "LilRg_ECE_Finetunning_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/ECE_Finetunning\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/ECE_Finetunning</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__ECE_Finetunning-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/ECE_Finetunning",
    "Model sha": "8d10549bcf802355f2d6203a33ed27e81b15b9e5",
    "Average ‚¨ÜÔ∏è": 11.911503558649864,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7701882435701863,
    "IFEval Raw": 0.04453849120334047,
    "IFEval": 4.453849120334047,
    "BBH Raw": 0.47321596790730514,
    "BBH": 26.530834895216355,
    "MATH Lvl 5 Raw": 0.04078549848942598,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.38394791666666667,
    "MUSR": 7.693489583333334,
    "MMLU-PRO Raw": 0.3191489361702128,
    "MMLU-PRO": 24.34988179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 3,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "LilRg_PRYMMAL-6B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-6B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-6B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-6B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-6B-slerp",
    "Model sha": "1ce0f5fdaae6a7866eda77df18378e9b5621af65",
    "Average ‚¨ÜÔ∏è": 3.23270592407536,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3480815434665663,
    "IFEval Raw": 0.11533065599276586,
    "IFEval": 11.533065599276586,
    "BBH Raw": 0.28676215692036117,
    "BBH": 2.2124311744899985,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.36975,
    "MUSR": 4.452083333333333,
    "MMLU-PRO Raw": 0.1107878989361702,
    "MMLU-PRO": 1.1986554373522447,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-6B-slerp (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-ECE-7B-SLERP-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-ECE-7B-SLERP-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-ECE-7B-SLERP-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-ECE-7B-SLERP-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-ECE-7B-SLERP-V3",
    "Model sha": "742eee22ab39880acb8650b7290d420065d0514b",
    "Average ‚¨ÜÔ∏è": 3.3703008780702635,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2811271075442374,
    "IFEval Raw": 0.12432346174816154,
    "IFEval": 12.432346174816153,
    "BBH Raw": 0.2957239084980124,
    "BBH": 2.2903233313527434,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36714583333333334,
    "MUSR": 3.193229166666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-ECE-7B-SLERP-V3 (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-ECE-7B-SLERP-V4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-ECE-7B-SLERP-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-ECE-7B-SLERP-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-ECE-7B-SLERP-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-ECE-7B-SLERP-V4",
    "Model sha": "5a45274282197dcce0f22b442f65df14ac75f507",
    "Average ‚¨ÜÔ∏è": 3.380292884465147,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3057103128688,
    "IFEval Raw": 0.12492298213185458,
    "IFEval": 12.492298213185457,
    "BBH Raw": 0.2957239084980124,
    "BBH": 2.2903233313527434,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36714583333333334,
    "MUSR": 3.193229166666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-ECE-7B-SLERP-V4 (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-ECE-7B-SLERP-V5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-ECE-7B-SLERP-V5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-ECE-7B-SLERP-V5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-ECE-7B-SLERP-V5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-ECE-7B-SLERP-V5",
    "Model sha": "63f1ed2c963e3cb78d6c6a89836e0712aa7c3a6f",
    "Average ‚¨ÜÔ∏è": 3.380292884465147,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2864251984084647,
    "IFEval Raw": 0.12492298213185458,
    "IFEval": 12.492298213185457,
    "BBH Raw": 0.2957239084980124,
    "BBH": 2.2903233313527434,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36714583333333334,
    "MUSR": 3.193229166666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-ECE-7B-SLERP-V5 (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-ECE-7B-SLERP-V6_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-ECE-7B-SLERP-V6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-ECE-7B-SLERP-V6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-ECE-7B-SLERP-V6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-ECE-7B-SLERP-V6",
    "Model sha": "92a8c865ef44974d0bafd22c1f991afe7889717b",
    "Average ‚¨ÜÔ∏è": 3.3703008780702635,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.247841053987101,
    "IFEval Raw": 0.12432346174816154,
    "IFEval": 12.432346174816153,
    "BBH Raw": 0.2957239084980124,
    "BBH": 2.2903233313527434,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36714583333333334,
    "MUSR": 3.193229166666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-ECE-7B-SLERP-V6 (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-ECE-7B-SLERP-V7_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-ECE-7B-SLERP-V7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-ECE-7B-SLERP-V7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-ECE-7B-SLERP-V7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-ECE-7B-SLERP-V7",
    "Model sha": "834363d4b420f85ff1af920a68149240c580726c",
    "Average ‚¨ÜÔ∏è": 3.380292884465147,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2746199079970209,
    "IFEval Raw": 0.12492298213185458,
    "IFEval": 12.492298213185457,
    "BBH Raw": 0.2957239084980124,
    "BBH": 2.2903233313527434,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36714583333333334,
    "MUSR": 3.193229166666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-ECE-7B-SLERP-V7 (Merge)"
  },
  {
    "eval_name": "LilRg_PRYMMAL-slerp-Merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LilRg/PRYMMAL-slerp-Merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LilRg/PRYMMAL-slerp-Merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LilRg__PRYMMAL-slerp-Merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LilRg/PRYMMAL-slerp-Merge",
    "Model sha": "e5597549ceb5afe56428097cb297326537d07c3e",
    "Average ‚¨ÜÔ∏è": 22.38284116689637,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4175135448002238,
    "IFEval Raw": 0.304400102838247,
    "IFEval": 30.440010283824698,
    "BBH Raw": 0.5364156271768925,
    "BBH": 35.553775523419816,
    "MATH Lvl 5 Raw": 0.09894259818731119,
    "MATH Lvl 5": 9.894259818731118,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.46347916666666666,
    "MUSR": 17.2015625,
    "MMLU-PRO Raw": 0.3863031914893617,
    "MMLU-PRO": 31.811465721040182,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "LilRg/PRYMMAL-slerp-Merge (Merge)"
  },
  {
    "eval_name": "LimYeri_CodeMind-Llama3-8B-unsloth_v2-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LimYeri/CodeMind-Llama3-8B-unsloth_v2-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LimYeri/CodeMind-Llama3-8B-unsloth_v2-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LimYeri__CodeMind-Llama3-8B-unsloth_v2-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LimYeri/CodeMind-Llama3-8B-unsloth_v2-merged",
    "Model sha": "d4ec745f8279e3ac6d41709153c21cc077e66385",
    "Average ‚¨ÜÔ∏è": 22.40996673514959,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8266246336662607,
    "IFEval Raw": 0.6946280314011268,
    "IFEval": 69.46280314011267,
    "BBH Raw": 0.48600920882996324,
    "BBH": 26.655629407381003,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3316145833333333,
    "MUSR": 2.218489583333334,
    "MMLU-PRO Raw": 0.3505651595744681,
    "MMLU-PRO": 27.840573286052013,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "unsloth/llama-3-8b-Instruct-bnb-4bit"
  },
  {
    "eval_name": "LimYeri_CodeMind-Llama3-8B-unsloth_v3-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LimYeri/CodeMind-Llama3-8B-unsloth_v3-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LimYeri/CodeMind-Llama3-8B-unsloth_v3-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LimYeri__CodeMind-Llama3-8B-unsloth_v3-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LimYeri/CodeMind-Llama3-8B-unsloth_v3-merged",
    "Model sha": "548a221b00d8056fe7090f5e6e0af58ee7c62563",
    "Average ‚¨ÜÔ∏è": 21.84577318798755,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8362810671872299,
    "IFEval Raw": 0.6762933460994606,
    "IFEval": 67.62933460994606,
    "BBH Raw": 0.4908161460506797,
    "BBH": 27.02521610839599,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.3356145833333333,
    "MUSR": 1.1518229166666667,
    "MMLU-PRO Raw": 0.34956781914893614,
    "MMLU-PRO": 27.729757683215126,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "LimYeri_CodeMind-Llama3-8B-unsloth_v4-one-DPO-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-DPO-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-DPO-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LimYeri__CodeMind-Llama3-8B-unsloth_v4-one-DPO-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-DPO-merged",
    "Model sha": "e21e4932c56cebd3f9816bf083c1792cdccbe7a7",
    "Average ‚¨ÜÔ∏è": 21.74925407748006,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7323124058624179,
    "IFEval Raw": 0.6492406813920397,
    "IFEval": 64.92406813920398,
    "BBH Raw": 0.48526582322240047,
    "BBH": 26.37017668673992,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3607916666666667,
    "MUSR": 3.565625000000001,
    "MMLU-PRO Raw": 0.3353557180851064,
    "MMLU-PRO": 26.150635342789595,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "unsloth/llama-3-8b-Instruct-bnb-4bit"
  },
  {
    "eval_name": "LimYeri_CodeMind-Llama3-8B-unsloth_v4-one-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LimYeri__CodeMind-Llama3-8B-unsloth_v4-one-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LimYeri/CodeMind-Llama3-8B-unsloth_v4-one-merged",
    "Model sha": "9c8939ccdc10beee56462eadbc16e28359a6d4c4",
    "Average ‚¨ÜÔ∏è": 17.61324636978146,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8871420735927358,
    "IFEval Raw": 0.32108693821283085,
    "IFEval": 32.10869382128308,
    "BBH Raw": 0.47387586084568856,
    "BBH": 24.57473532012109,
    "MATH Lvl 5 Raw": 0.05513595166163143,
    "MATH Lvl 5": 5.513595166163143,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.40692708333333333,
    "MUSR": 9.399218750000003,
    "MMLU-PRO Raw": 0.33527260638297873,
    "MMLU-PRO": 26.14140070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-06",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "unsloth/llama-3-8b-Instruct-bnb-4bit"
  },
  {
    "eval_name": "LimYeri_CodeMind-Llama3.1-8B-unsloth-merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/LimYeri/CodeMind-Llama3.1-8B-unsloth-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">LimYeri/CodeMind-Llama3.1-8B-unsloth-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/LimYeri__CodeMind-Llama3.1-8B-unsloth-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "LimYeri/CodeMind-Llama3.1-8B-unsloth-merged",
    "Model sha": "911ffe6614d23bfc9cb7ece0cd3afd861a65d7f0",
    "Average ‚¨ÜÔ∏è": 22.25475482964021,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8336868910313886,
    "IFEval Raw": 0.6490157227268093,
    "IFEval": 64.90157227268094,
    "BBH Raw": 0.4694777854416285,
    "BBH": 24.185738827978955,
    "MATH Lvl 5 Raw": 0.10498489425981875,
    "MATH Lvl 5": 10.498489425981875,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.37523958333333335,
    "MUSR": 6.038281250000002,
    "MMLU-PRO Raw": 0.33402593085106386,
    "MMLU-PRO": 26.002881205673763,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-31",
    "Submission Date": "2024-08-31",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Locutusque_Hercules-6.0-Llama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/Hercules-6.0-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/Hercules-6.0-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Locutusque__Hercules-6.0-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Locutusque/Hercules-6.0-Llama-3.1-8B",
    "Model sha": "f35a95aeabf9f82bbd64bfc6fd0d857df750ee83",
    "Average ‚¨ÜÔ∏è": 23.48354588229914,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8851430751496295,
    "IFEval Raw": 0.6630041622893922,
    "IFEval": 66.30041622893921,
    "BBH Raw": 0.48133037900119535,
    "BBH": 26.63965184405082,
    "MATH Lvl 5 Raw": 0.14577039274924472,
    "MATH Lvl 5": 14.577039274924472,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.362125,
    "MUSR": 2.432291666666666,
    "MMLU-PRO Raw": 0.3614527925531915,
    "MMLU-PRO": 29.050310283687946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-09-26",
    "Generation": 0,
    "Base Model": "Locutusque/Hercules-6.0-Llama-3.1-8B"
  },
  {
    "eval_name": "Locutusque_Hercules-6.1-Llama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/Hercules-6.1-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/Hercules-6.1-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Locutusque__Hercules-6.1-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Locutusque/Hercules-6.1-Llama-3.1-8B",
    "Model sha": "f4abf4385111b4acbea8bee2c6636ef84b2dac43",
    "Average ‚¨ÜÔ∏è": 22.609955510275046,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9568030034424307,
    "IFEval Raw": 0.6006806384836678,
    "IFEval": 60.068063848366776,
    "BBH Raw": 0.46562423765034017,
    "BBH": 24.151873375413786,
    "MATH Lvl 5 Raw": 0.16918429003021151,
    "MATH Lvl 5": 16.918429003021153,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.35533333333333333,
    "MUSR": 3.4166666666666674,
    "MMLU-PRO Raw": 0.36685505319148937,
    "MMLU-PRO": 29.650561465721044,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "Locutusque/Hercules-6.1-Llama-3.1-8B"
  },
  {
    "eval_name": "Locutusque_Llama-3-NeuralHercules-5.0-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/Llama-3-NeuralHercules-5.0-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/Llama-3-NeuralHercules-5.0-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Locutusque__Llama-3-NeuralHercules-5.0-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Locutusque/Llama-3-NeuralHercules-5.0-8B",
    "Model sha": "2bbb675e592a1772f2389fe2d58a5b610d479d94",
    "Average ‚¨ÜÔ∏è": 15.992123441844717,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8907147725121036,
    "IFEval Raw": 0.4489310584803876,
    "IFEval": 44.89310584803876,
    "BBH Raw": 0.3940474241916672,
    "BBH": 16.34207153663529,
    "MATH Lvl 5 Raw": 0.040030211480362544,
    "MATH Lvl 5": 4.003021148036255,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3880729166666667,
    "MUSR": 6.7757812500000005,
    "MMLU-PRO Raw": 0.29330119680851063,
    "MMLU-PRO": 21.47791075650118,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Locutusque/Llama-3-NeuralHercules-5.0-8B"
  },
  {
    "eval_name": "Locutusque_Llama-3-Yggdrasil-2.0-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/Llama-3-Yggdrasil-2.0-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/Llama-3-Yggdrasil-2.0-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Locutusque__Llama-3-Yggdrasil-2.0-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Locutusque/Llama-3-Yggdrasil-2.0-8B",
    "Model sha": "ec2329946ccc81a7c1ae36210728f717bc4f01d8",
    "Average ‚¨ÜÔ∏è": 20.359492559845098,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8149409181903315,
    "IFEval Raw": 0.5370583385417359,
    "IFEval": 53.70583385417359,
    "BBH Raw": 0.47724551424666856,
    "BBH": 26.922800957191782,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.39765625,
    "MUSR": 8.073697916666669,
    "MMLU-PRO Raw": 0.316655585106383,
    "MMLU-PRO": 24.072842789598106,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-05",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "Locutusque/Llama-3-Yggdrasil-2.0-8B (Merge)"
  },
  {
    "eval_name": "Locutusque_TinyMistral-248M-v2.5_float16",
    "Precision": "float16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Locutusque/TinyMistral-248M-v2.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Locutusque/TinyMistral-248M-v2.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Locutusque__TinyMistral-248M-v2.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Locutusque/TinyMistral-248M-v2.5",
    "Model sha": "214e48aabc01235e25c67477898756f1bebef215",
    "Average ‚¨ÜÔ∏è": 3.8717939498227243,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.2422144259484986,
    "IFEval Raw": 0.1336409615376091,
    "IFEval": 13.36409615376091,
    "BBH Raw": 0.30385761123260785,
    "BBH": 3.181881126755549,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.37815624999999997,
    "MUSR": 5.069531250000002,
    "MMLU-PRO Raw": 0.11353058510638298,
    "MMLU-PRO": 1.5033983451536632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-24",
    "Submission Date": "2024-09-17",
    "Generation": 0,
    "Base Model": "Locutusque/TinyMistral-248M-v2.5"
  },
  {
    "eval_name": "Luni_StarDust-12b-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Luni/StarDust-12b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Luni/StarDust-12b-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Luni__StarDust-12b-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Luni/StarDust-12b-v1",
    "Model sha": "91976b0c71dce1310f4a6139552e10a6149bdc31",
    "Average ‚¨ÜÔ∏è": 23.18362910123705,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.452633039239195,
    "IFEval Raw": 0.5459259210007226,
    "IFEval": 54.59259210007225,
    "BBH Raw": 0.5366139363101082,
    "BBH": 34.44627563758498,
    "MATH Lvl 5 Raw": 0.06042296072507554,
    "MATH Lvl 5": 6.042296072507554,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.43244791666666665,
    "MUSR": 13.755989583333331,
    "MMLU-PRO Raw": 0.34117353723404253,
    "MMLU-PRO": 26.797059692671393,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-29",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "Luni/StarDust-12b-v1 (Merge)"
  },
  {
    "eval_name": "Luni_StarDust-12b-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Luni/StarDust-12b-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Luni/StarDust-12b-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Luni__StarDust-12b-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Luni/StarDust-12b-v2",
    "Model sha": "75bffd7b86f37c2cebc4fdf83fbc3ab33d6c6e05",
    "Average ‚¨ÜÔ∏è": 24.089195268557347,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 31,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5325245349547734,
    "IFEval Raw": 0.5628620947973599,
    "IFEval": 56.28620947973599,
    "BBH Raw": 0.5419479534912178,
    "BBH": 34.95288411454464,
    "MATH Lvl 5 Raw": 0.06117824773413898,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.4338125,
    "MUSR": 14.259895833333331,
    "MMLU-PRO Raw": 0.3439162234042553,
    "MMLU-PRO": 27.101802600472812,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-01",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "Luni/StarDust-12b-v2 (Merge)"
  },
  {
    "eval_name": "Lyte_Llama-3.1-8B-Instruct-Reasoner-1o1_v0.3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lyte/Llama-3.1-8B-Instruct-Reasoner-1o1_v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lyte/Llama-3.1-8B-Instruct-Reasoner-1o1_v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lyte__Llama-3.1-8B-Instruct-Reasoner-1o1_v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lyte/Llama-3.1-8B-Instruct-Reasoner-1o1_v0.3",
    "Model sha": "35ab483f04afa763f36f978408f4f82e0379ee25",
    "Average ‚¨ÜÔ∏è": 25.26252458499293,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9157373416580631,
    "IFEval Raw": 0.7098155117310957,
    "IFEval": 70.98155117310957,
    "BBH Raw": 0.4949521619329585,
    "BBH": 27.83521213410715,
    "MATH Lvl 5 Raw": 0.16087613293051362,
    "MATH Lvl 5": 16.087613293051362,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.346125,
    "MUSR": 4.8989583333333355,
    "MMLU-PRO Raw": 0.36178523936170215,
    "MMLU-PRO": 29.08724881796691,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-17",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Lyte_Llama-3.2-1B-Instruct-COT-RL-Expriement1-EP04_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lyte/Llama-3.2-1B-Instruct-COT-RL-Expriement1-EP04\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lyte/Llama-3.2-1B-Instruct-COT-RL-Expriement1-EP04</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lyte__Llama-3.2-1B-Instruct-COT-RL-Expriement1-EP04-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lyte/Llama-3.2-1B-Instruct-COT-RL-Expriement1-EP04",
    "Model sha": "59d93307c6f2cb7a29c593cbc7393122d502d1b1",
    "Average ‚¨ÜÔ∏è": 14.546602836554825,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4501333052866559,
    "IFEval Raw": 0.5773503193748144,
    "IFEval": 57.73503193748144,
    "BBH Raw": 0.3515036874279285,
    "BBH": 8.894408584162113,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.32355208333333335,
    "MUSR": 2.5440104166666675,
    "MMLU-PRO Raw": 0.18425864361702127,
    "MMLU-PRO": 9.362071513002363,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Lyte_Llama-3.2-3B-Overthinker_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Lyte/Llama-3.2-3B-Overthinker\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Lyte/Llama-3.2-3B-Overthinker</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Lyte__Llama-3.2-3B-Overthinker-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Lyte/Llama-3.2-3B-Overthinker",
    "Model sha": "0e7af37fb3381365905fc2df24811c0e6d2ba5b2",
    "Average ‚¨ÜÔ∏è": 19.077846240199964,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7336396973945092,
    "IFEval Raw": 0.6407975283359264,
    "IFEval": 64.07975283359264,
    "BBH Raw": 0.4320093097952517,
    "BBH": 20.095582226457154,
    "MATH Lvl 5 Raw": 0.030966767371601214,
    "MATH Lvl 5": 3.0966767371601214,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.34190625,
    "MUSR": 3.9049479166666674,
    "MMLU-PRO Raw": 0.29853723404255317,
    "MMLU-PRO": 22.059692671394796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-10-18",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "M4-ai_TinyMistral-248M-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/M4-ai/TinyMistral-248M-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">M4-ai/TinyMistral-248M-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/M4-ai__TinyMistral-248M-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "M4-ai/TinyMistral-248M-v3",
    "Model sha": "fa23fe617768c671f0bbbff1edf4556cfe844167",
    "Average ‚¨ÜÔ∏è": 4.130107609282086,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.23418361142077865,
    "IFEval Raw": 0.16386631914431488,
    "IFEval": 16.386631914431486,
    "BBH Raw": 0.2884549938995566,
    "BBH": 1.7775539303863237,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2407718120805369,
    "GPQA": 0.0,
    "MUSR Raw": 0.3793333333333333,
    "MUSR": 5.150000000000001,
    "MMLU-PRO Raw": 0.11319813829787234,
    "MMLU-PRO": 1.466459810874704,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-05",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "M4-ai/TinyMistral-248M-v3"
  },
  {
    "eval_name": "MEscriva_ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MEscriva/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MEscriva/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MEscriva__ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MEscriva/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis",
    "Model sha": "7a9d848188a674302d64a865786d4508be19571a",
    "Average ‚¨ÜÔ∏è": 3.81803368483764,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0515562311926925,
    "IFEval Raw": 0.08662903318749807,
    "IFEval": 8.662903318749805,
    "BBH Raw": 0.305728612437881,
    "BBH": 3.237774271047842,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.40171874999999996,
    "MUSR": 8.614843750000002,
    "MMLU-PRO Raw": 0.11544215425531915,
    "MMLU-PRO": 1.7157949172576823,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "MEscriva/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis"
  },
  {
    "eval_name": "MLP-KTLim_llama-3-Korean-Bllossom-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MLP-KTLim/llama-3-Korean-Bllossom-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MLP-KTLim__llama-3-Korean-Bllossom-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MLP-KTLim/llama-3-Korean-Bllossom-8B",
    "Model sha": "8a738f9f622ffc2b0a4a6b81dabbca80406248bf",
    "Average ‚¨ÜÔ∏è": 20.333975762059037,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 281,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7747205643348448,
    "IFEval Raw": 0.5112800702136997,
    "IFEval": 51.12800702136997,
    "BBH Raw": 0.49004556470187666,
    "BBH": 26.927527973055067,
    "MATH Lvl 5 Raw": 0.09818731117824775,
    "MATH Lvl 5": 9.818731117824775,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3674583333333334,
    "MUSR": 3.6322916666666694,
    "MMLU-PRO Raw": 0.359375,
    "MMLU-PRO": 28.819444444444446,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-07-09",
    "Generation": 1,
    "Base Model": "MLP-KTLim/llama-3-Korean-Bllossom-8B (Merge)"
  },
  {
    "eval_name": "MTSAIR_MultiVerse_70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MTSAIR/MultiVerse_70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MTSAIR/MultiVerse_70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MTSAIR__MultiVerse_70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MTSAIR/MultiVerse_70B",
    "Model sha": "063430cdc4d972a0884e3e3e3d45ea4afbdf71a2",
    "Average ‚¨ÜÔ∏è": 32.00519030847487,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 39,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.601817383486374,
    "IFEval Raw": 0.5249183278146429,
    "IFEval": 52.49183278146429,
    "BBH Raw": 0.6183134284931178,
    "BBH": 46.135898982415,
    "MATH Lvl 5 Raw": 0.17824773413897282,
    "MATH Lvl 5": 17.824773413897283,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.47398958333333335,
    "MUSR": 18.815364583333327,
    "MMLU-PRO Raw": 0.48603723404255317,
    "MMLU-PRO": 42.89302600472813,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-25",
    "Submission Date": "2024-06-29",
    "Generation": 0,
    "Base Model": "MTSAIR/MultiVerse_70B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3-8B-Magpie-Align-SFT-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3-8B-Magpie-Align-SFT-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.1",
    "Model sha": "1ed587f54f70334f495efb9c027acb03e96fe24f",
    "Average ‚¨ÜÔ∏è": 15.928911353272705,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8335691386633998,
    "IFEval Raw": 0.4361416596851908,
    "IFEval": 43.61416596851908,
    "BBH Raw": 0.4615102744527366,
    "BBH": 23.990124398411343,
    "MATH Lvl 5 Raw": 0.05589123867069487,
    "MATH Lvl 5": 5.589123867069487,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.32773958333333336,
    "MUSR": 0.0,
    "MMLU-PRO Raw": 0.2863198138297872,
    "MMLU-PRO": 20.702201536643024,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-06",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3-8B-Magpie-Align-SFT-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3-8B-Magpie-Align-SFT-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3-8B-Magpie-Align-SFT-v0.3",
    "Model sha": "d2578eb754d1c20efe604749296580f680950917",
    "Average ‚¨ÜÔ∏è": 17.49028478617081,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8954201190774991,
    "IFEval Raw": 0.5063586838477463,
    "IFEval": 50.635868384774625,
    "BBH Raw": 0.45715808996720547,
    "BBH": 23.698815892387586,
    "MATH Lvl 5 Raw": 0.06948640483383685,
    "MATH Lvl 5": 6.948640483383685,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.34237500000000004,
    "MUSR": 0.3968749999999998,
    "MMLU-PRO Raw": 0.2902260638297872,
    "MMLU-PRO": 21.136229314420802,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-13",
    "Submission Date": "2024-08-06",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3-8B-Magpie-Align-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3-8B-Magpie-Align-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3-8B-Magpie-Align-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3-8B-Magpie-Align-v0.1",
    "Model sha": "a83ddac146fb2da1dd1bfa4069e336074d1439a8",
    "Average ‚¨ÜÔ∏è": 16.473094269110153,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9068485399826427,
    "IFEval Raw": 0.4118117705465941,
    "IFEval": 41.181177054659415,
    "BBH Raw": 0.4811441560714845,
    "BBH": 26.69176089392447,
    "MATH Lvl 5 Raw": 0.033987915407854986,
    "MATH Lvl 5": 3.3987915407854987,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.3046979166666667,
    "MUSR": 1.920572916666666,
    "MMLU-PRO Raw": 0.3006150265957447,
    "MMLU-PRO": 22.2905585106383,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-07-03",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3-8B-Magpie-Align-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3-8B-Magpie-Align-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3-8B-Magpie-Align-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3-8B-Magpie-Align-v0.1",
    "Model sha": "a83ddac146fb2da1dd1bfa4069e336074d1439a8",
    "Average ‚¨ÜÔ∏è": 16.307770990649317,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8495360815484534,
    "IFEval Raw": 0.4027192294223771,
    "IFEval": 40.27192294223771,
    "BBH Raw": 0.47894081019705514,
    "BBH": 26.289712088654472,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.3086979166666666,
    "MUSR": 1.920572916666666,
    "MMLU-PRO Raw": 0.30011635638297873,
    "MMLU-PRO": 22.23515070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-07-03",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3-8B-Magpie-Align-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3-8B-Magpie-Align-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3-8B-Magpie-Align-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3-8B-Magpie-Align-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3-8B-Magpie-Align-v0.3",
    "Model sha": "7e420ddd6ff48bf213dcab2a9ddb7845b80dd1aa",
    "Average ‚¨ÜÔ∏è": 16.91155823731261,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7422896278969292,
    "IFEval Raw": 0.44970566984490046,
    "IFEval": 44.97056698449004,
    "BBH Raw": 0.456960506522001,
    "BBH": 24.311446807587018,
    "MATH Lvl 5 Raw": 0.027190332326283987,
    "MATH Lvl 5": 2.719033232628399,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.34060416666666665,
    "MUSR": 3.7421875000000004,
    "MMLU-PRO Raw": 0.31341422872340424,
    "MMLU-PRO": 23.712692080378247,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-15",
    "Submission Date": "2024-08-06",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3.1-8B-Magpie-Align-SFT-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3.1-8B-Magpie-Align-SFT-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3.1-8B-Magpie-Align-SFT-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3.1-8B-Magpie-Align-SFT-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3.1-8B-Magpie-Align-SFT-v0.1",
    "Model sha": "b191916912f0e76b2bdc93c46c0af590cc87e7ae",
    "Average ‚¨ÜÔ∏è": 17.975799245688165,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8299899919845526,
    "IFEval Raw": 0.47820671374176077,
    "IFEval": 47.82067137417607,
    "BBH Raw": 0.4764157817799906,
    "BBH": 26.13667696363235,
    "MATH Lvl 5 Raw": 0.08987915407854985,
    "MATH Lvl 5": 8.987915407854985,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3397395833333334,
    "MUSR": 1.8666666666666683,
    "MMLU-PRO Raw": 0.29429853723404253,
    "MMLU-PRO": 21.588726359338057,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-23",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Magpie-Align_Llama-3.1-8B-Magpie-Align-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/Llama-3.1-8B-Magpie-Align-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/Llama-3.1-8B-Magpie-Align-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__Llama-3.1-8B-Magpie-Align-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/Llama-3.1-8B-Magpie-Align-v0.1",
    "Model sha": "dd34258a5f2bf7630b5a8e5662b050c60a088927",
    "Average ‚¨ÜÔ∏è": 16.439100559586322,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7080411077827046,
    "IFEval Raw": 0.4457838535086903,
    "IFEval": 44.57838535086903,
    "BBH Raw": 0.46223963164680143,
    "BBH": 24.04053735093787,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.31406249999999997,
    "MUSR": 3.091145833333334,
    "MMLU-PRO Raw": 0.32621343085106386,
    "MMLU-PRO": 25.134825650118202,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-09-17",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Magpie-Align_MagpieLM-8B-Chat-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/MagpieLM-8B-Chat-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/MagpieLM-8B-Chat-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__MagpieLM-8B-Chat-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/MagpieLM-8B-Chat-v0.1",
    "Model sha": "0b30eabc82a01fb42f44ba62c2dc81e1bd09cc04",
    "Average ‚¨ÜÔ∏è": 14.006706850729245,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7368765968547278,
    "IFEval Raw": 0.3700714105240761,
    "IFEval": 37.00714105240761,
    "BBH Raw": 0.4172338260055306,
    "BBH": 18.25580502860485,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3500625,
    "MUSR": 2.824479166666667,
    "MMLU-PRO Raw": 0.3194813829787234,
    "MMLU-PRO": 24.386820330969268,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Magpie-Align_MagpieLM-8B-SFT-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Magpie-Align/MagpieLM-8B-SFT-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Magpie-Align/MagpieLM-8B-SFT-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Magpie-Align__MagpieLM-8B-SFT-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Magpie-Align/MagpieLM-8B-SFT-v0.1",
    "Model sha": "b91f605a511707cb3b7f0893a8ed80c77b32d5a8",
    "Average ‚¨ÜÔ∏è": 16.91534944021143,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8004210982504402,
    "IFEval Raw": 0.4720619068515982,
    "IFEval": 47.20619068515982,
    "BBH Raw": 0.45528501595553356,
    "BBH": 23.612313350177956,
    "MATH Lvl 5 Raw": 0.02341389728096677,
    "MATH Lvl 5": 2.341389728096677,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3648854166666667,
    "MUSR": 3.877343750000002,
    "MMLU-PRO Raw": 0.2989527925531915,
    "MMLU-PRO": 22.1058658392435,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ManoloPueblo_ContentCuisine_1-7B-slerp_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ManoloPueblo/ContentCuisine_1-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ManoloPueblo/ContentCuisine_1-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ManoloPueblo__ContentCuisine_1-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ManoloPueblo/ContentCuisine_1-7B-slerp",
    "Model sha": "e811e880075a2945623040ee43e9a6972675ff2e",
    "Average ‚¨ÜÔ∏è": 21.040210378699708,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4995058365865235,
    "IFEval Raw": 0.3907044419916932,
    "IFEval": 39.070444199169316,
    "BBH Raw": 0.5188437309746964,
    "BBH": 32.78974404613455,
    "MATH Lvl 5 Raw": 0.07250755287009064,
    "MATH Lvl 5": 7.250755287009064,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.46719791666666666,
    "MUSR": 17.26640625,
    "MMLU-PRO Raw": 0.30535239361702127,
    "MMLU-PRO": 22.816932624113473,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "ManoloPueblo/ContentCuisine_1-7B-slerp (Merge)"
  },
  {
    "eval_name": "ManoloPueblo_LLM_MERGE_CC2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ManoloPueblo/LLM_MERGE_CC2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ManoloPueblo/LLM_MERGE_CC2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ManoloPueblo__LLM_MERGE_CC2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ManoloPueblo/LLM_MERGE_CC2",
    "Model sha": "a39dcd4e8175c0e2ab9bda2c7a4f377b97549644",
    "Average ‚¨ÜÔ∏è": 20.734779597442383,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5735433689320039,
    "IFEval Raw": 0.3853087585384557,
    "IFEval": 38.53087585384557,
    "BBH Raw": 0.5209367401710429,
    "BBH": 33.241073524404655,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.45929166666666665,
    "MUSR": 16.444791666666664,
    "MMLU-PRO Raw": 0.30319148936170215,
    "MMLU-PRO": 22.576832151300238,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-02",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "ManoloPueblo/LLM_MERGE_CC2"
  },
  {
    "eval_name": "ManoloPueblo_LLM_MERGE_CC3_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ManoloPueblo/LLM_MERGE_CC3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ManoloPueblo/LLM_MERGE_CC3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ManoloPueblo__LLM_MERGE_CC3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ManoloPueblo/LLM_MERGE_CC3",
    "Model sha": "79d2bd3866e363b9e700f59cfc573b2bc9de2442",
    "Average ‚¨ÜÔ∏è": 21.71640464254096,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5369304403483045,
    "IFEval Raw": 0.3958751667797001,
    "IFEval": 39.58751667797001,
    "BBH Raw": 0.5246290546274339,
    "BBH": 33.23001780763082,
    "MATH Lvl 5 Raw": 0.08157099697885196,
    "MATH Lvl 5": 8.157099697885197,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4671666666666667,
    "MUSR": 17.42916666666667,
    "MMLU-PRO Raw": 0.3155751329787234,
    "MMLU-PRO": 23.95279255319149,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-10",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "ManoloPueblo/LLM_MERGE_CC3"
  },
  {
    "eval_name": "MarinaraSpaghetti_NemoReRemix-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MarinaraSpaghetti/NemoReRemix-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MarinaraSpaghetti/NemoReRemix-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MarinaraSpaghetti__NemoReRemix-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MarinaraSpaghetti/NemoReRemix-12B",
    "Model sha": "9ebc7c2d4577b663fb050d86ed91fb676eb2e1f2",
    "Average ‚¨ÜÔ∏è": 21.682113557188703,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5770075484090382,
    "IFEval Raw": 0.33425089872649016,
    "IFEval": 33.42508987264902,
    "BBH Raw": 0.5536511805668158,
    "BBH": 36.12470152357596,
    "MATH Lvl 5 Raw": 0.06948640483383688,
    "MATH Lvl 5": 6.948640483383688,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.4501458333333333,
    "MUSR": 15.668229166666665,
    "MMLU-PRO Raw": 0.3597905585106383,
    "MMLU-PRO": 28.865617612293136,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-14",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "MarinaraSpaghetti/NemoReRemix-12B (Merge)"
  },
  {
    "eval_name": "MarinaraSpaghetti_Nemomix-v4.0-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MarinaraSpaghetti/Nemomix-v4.0-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MarinaraSpaghetti/Nemomix-v4.0-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MarinaraSpaghetti__Nemomix-v4.0-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MarinaraSpaghetti/Nemomix-v4.0-12B",
    "Model sha": "69fbd8449ce3e916fc257e982a78189308123074",
    "Average ‚¨ÜÔ∏è": 24.379859820295167,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 21,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3545484463957131,
    "IFEval Raw": 0.5574664113441224,
    "IFEval": 55.74664113441224,
    "BBH Raw": 0.5274986611124783,
    "BBH": 32.879942700903165,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.42444791666666665,
    "MUSR": 12.75598958333333,
    "MMLU-PRO Raw": 0.36128656914893614,
    "MMLU-PRO": 29.031841016548455,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-08-02",
    "Generation": 1,
    "Base Model": "MarinaraSpaghetti/Nemomix-v4.0-12B (Merge)"
  },
  {
    "eval_name": "Marsouuu_MiniMathExpert-2_61B-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/MiniMathExpert-2_61B-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/MiniMathExpert-2_61B-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__MiniMathExpert-2_61B-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/MiniMathExpert-2_61B-ECE-PRYMMAL-Martial",
    "Model sha": "df21939a22e7233ebb7d62dfaf1c854facc5c772",
    "Average ‚¨ÜÔ∏è": 12.53238447300032,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4527899773122042,
    "IFEval Raw": 0.25484159807089635,
    "IFEval": 25.484159807089632,
    "BBH Raw": 0.3952730330493959,
    "BBH": 15.297499289020854,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.40832291666666665,
    "MUSR": 9.273697916666668,
    "MMLU-PRO Raw": 0.22739361702127658,
    "MMLU-PRO": 14.154846335697396,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "Marsouuu/MiniMathExpert-2_61B-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_MiniQwenMathExpert-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/MiniQwenMathExpert-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/MiniQwenMathExpert-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__MiniQwenMathExpert-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/MiniQwenMathExpert-ECE-PRYMMAL-Martial",
    "Model sha": "0787682e65f7763ef978c4cf2e32803be8b49298",
    "Average ‚¨ÜÔ∏è": 14.880579276695444,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6891579783430338,
    "IFEval Raw": 0.2794961812435449,
    "IFEval": 27.949618124354487,
    "BBH Raw": 0.42301343044108936,
    "BBH": 19.019948525917457,
    "MATH Lvl 5 Raw": 0.10196374622356497,
    "MATH Lvl 5": 10.196374622356497,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.38673958333333336,
    "MUSR": 6.509114583333333,
    "MMLU-PRO Raw": 0.2922207446808511,
    "MMLU-PRO": 21.35786052009456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "Marsouuu/MiniQwenMathExpert-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial",
    "Model sha": "9cb9e74d2a65abd6458dffac103ad99c3b8f5154",
    "Average ‚¨ÜÔ∏è": 6.572937761865147,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9065090460174168,
    "IFEval Raw": 0.16973629968483622,
    "IFEval": 16.973629968483625,
    "BBH Raw": 0.3464368053320647,
    "BBH": 8.870227428732667,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3990833333333333,
    "MUSR": 7.8520833333333355,
    "MMLU-PRO Raw": 0.13788231382978725,
    "MMLU-PRO": 4.209145981087471,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "Marsouuu/MistralBase-4x7B-MoE-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_general3B-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/general3B-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/general3B-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__general3B-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/general3B-ECE-PRYMMAL-Martial",
    "Model sha": "42992194a835a6fcad1edf1f94527ac08a7a60fb",
    "Average ‚¨ÜÔ∏è": 22.072560326298653,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7244425979727039,
    "IFEval Raw": 0.27222658102722996,
    "IFEval": 27.222658102722995,
    "BBH Raw": 0.5394350977017502,
    "BBH": 35.70087336193955,
    "MATH Lvl 5 Raw": 0.10045317220543808,
    "MATH Lvl 5": 10.045317220543808,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.4700520833333333,
    "MUSR": 18.223177083333326,
    "MMLU-PRO Raw": 0.38763297872340424,
    "MMLU-PRO": 31.959219858156025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "Marsouuu/general3B-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_general3Bv2-ECE-PRYMMAL-Martial_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/general3Bv2-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/general3Bv2-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__general3Bv2-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/general3Bv2-ECE-PRYMMAL-Martial",
    "Model sha": "c6c5b3b0ecf9d04fc3a35bc4135df7cc08be3eb9",
    "Average ‚¨ÜÔ∏è": 31.03669110997119,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.717131726832526,
    "IFEval Raw": 0.5692817280371636,
    "IFEval": 56.928172803716365,
    "BBH Raw": 0.5636569831901026,
    "BBH": 37.66776306891555,
    "MATH Lvl 5 Raw": 0.31419939577039274,
    "MATH Lvl 5": 31.419939577039273,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.43960416666666663,
    "MUSR": 13.28385416666667,
    "MMLU-PRO Raw": 0.4498005319148936,
    "MMLU-PRO": 38.86672576832151,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "Marsouuu/general3Bv2-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_lareneg1_78B-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/lareneg1_78B-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/lareneg1_78B-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__lareneg1_78B-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/lareneg1_78B-ECE-PRYMMAL-Martial",
    "Model sha": "907a62bb805596e2105c9dca28c0e9ed1e9fd402",
    "Average ‚¨ÜÔ∏è": 14.880579276695444,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6328920886675071,
    "IFEval Raw": 0.2794961812435449,
    "IFEval": 27.949618124354487,
    "BBH Raw": 0.42301343044108936,
    "BBH": 19.019948525917457,
    "MATH Lvl 5 Raw": 0.10196374622356497,
    "MATH Lvl 5": 10.196374622356497,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.38673958333333336,
    "MUSR": 6.509114583333333,
    "MMLU-PRO Raw": 0.2922207446808511,
    "MMLU-PRO": 21.35786052009456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "Marsouuu/lareneg1_78B-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_lareneg3B-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/lareneg3B-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/lareneg3B-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__lareneg3B-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/lareneg3B-ECE-PRYMMAL-Martial",
    "Model sha": "2c8be0ac28ae27dd441298e83f19e17409d89f4e",
    "Average ‚¨ÜÔ∏è": 23.816174101276204,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.49099460376572657,
    "IFEval Raw": 0.33032908239028,
    "IFEval": 33.032908239028,
    "BBH Raw": 0.5453325807578268,
    "BBH": 36.35072191454026,
    "MATH Lvl 5 Raw": 0.14425981873111782,
    "MATH Lvl 5": 14.425981873111782,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.47246875,
    "MUSR": 18.39192708333333,
    "MMLU-PRO Raw": 0.37666223404255317,
    "MMLU-PRO": 30.740248226950357,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "Marsouuu/lareneg3B-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "Marsouuu_lareneg3Bv2-ECE-PRYMMAL-Martial_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Marsouuu/lareneg3Bv2-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Marsouuu/lareneg3Bv2-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Marsouuu__lareneg3Bv2-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Marsouuu/lareneg3Bv2-ECE-PRYMMAL-Martial",
    "Model sha": "ff92a6f314c392085af6c85f60a7da745e064653",
    "Average ‚¨ÜÔ∏è": 31.26926201417686,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6657444079520377,
    "IFEval Raw": 0.5753267995585047,
    "IFEval": 57.532679955850476,
    "BBH Raw": 0.562336014537904,
    "BBH": 37.471640114731564,
    "MATH Lvl 5 Raw": 0.3149546827794562,
    "MATH Lvl 5": 31.49546827794562,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.4369375,
    "MUSR": 12.817187500000003,
    "MMLU-PRO Raw": 0.45113031914893614,
    "MMLU-PRO": 39.01447990543734,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "Marsouuu/lareneg3Bv2-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "MaziyarPanahi_Calme-4x7B-MoE-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Calme-4x7B-MoE-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Calme-4x7B-MoE-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Calme-4x7B-MoE-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Calme-4x7B-MoE-v0.1",
    "Model sha": "e2fab90eef37977002947684043f139a1660f519",
    "Average ‚¨ÜÔ∏è": 20.023903328526014,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3609831028552992,
    "IFEval Raw": 0.4315205875964663,
    "IFEval": 43.15205875964662,
    "BBH Raw": 0.5102819889174134,
    "BBH": 31.26187805626151,
    "MATH Lvl 5 Raw": 0.08006042296072509,
    "MATH Lvl 5": 8.00604229607251,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.4198854166666666,
    "MUSR": 10.619010416666669,
    "MMLU-PRO Raw": 0.3056848404255319,
    "MMLU-PRO": 22.853871158392433,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-17",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "MaziyarPanahi/Calme-4x7B-MoE-v0.1"
  },
  {
    "eval_name": "MaziyarPanahi_Calme-4x7B-MoE-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Calme-4x7B-MoE-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Calme-4x7B-MoE-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Calme-4x7B-MoE-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Calme-4x7B-MoE-v0.2",
    "Model sha": "ffef41baf94b3f88b30cf0aeb3fd72d9e4187161",
    "Average ‚¨ÜÔ∏è": 20.163772829255382,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.415711178875455,
    "IFEval Raw": 0.429447200095746,
    "IFEval": 42.944720009574596,
    "BBH Raw": 0.5110766802558263,
    "BBH": 31.396819621762447,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.43176041666666665,
    "MUSR": 12.536718750000004,
    "MMLU-PRO Raw": 0.30576795212765956,
    "MMLU-PRO": 22.86310579196217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-17",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "MaziyarPanahi/Calme-4x7B-MoE-v0.2"
  },
  {
    "eval_name": "MaziyarPanahi_Llama-3-70B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Llama-3-70B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Llama-3-70B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Llama-3-70B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Llama-3-70B-Instruct-v0.1",
    "Model sha": "6db1cb4256525fc5429734ddc0eb941d08d0be30",
    "Average ‚¨ÜÔ∏è": 26.056974867400314,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.26398598741129,
    "IFEval Raw": 0.47143800671108216,
    "IFEval": 47.14380067110822,
    "BBH Raw": 0.5366257615951637,
    "BBH": 32.71291726367119,
    "MATH Lvl 5 Raw": 0.1638972809667674,
    "MATH Lvl 5": 16.38972809667674,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4433020833333334,
    "MUSR": 15.312760416666672,
    "MMLU-PRO Raw": 0.4617686170212766,
    "MMLU-PRO": 40.196513002364064,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-14",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "MaziyarPanahi_Llama-3-8B-Instruct-v0.10_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-v0.10\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Llama-3-8B-Instruct-v0.10</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Llama-3-8B-Instruct-v0.10-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Llama-3-8B-Instruct-v0.10",
    "Model sha": "4411eb9f6f5e4c462a6bdbc64c26dcc123100b66",
    "Average ‚¨ÜÔ∏è": 26.75963903768024,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.142130620074281,
    "IFEval Raw": 0.7667433520835827,
    "IFEval": 76.67433520835827,
    "BBH Raw": 0.4924311866686311,
    "BBH": 27.924674302120888,
    "MATH Lvl 5 Raw": 0.05513595166163144,
    "MATH Lvl 5": 5.513595166163144,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.42143749999999996,
    "MUSR": 10.813020833333333,
    "MMLU-PRO Raw": 0.38622007978723405,
    "MMLU-PRO": 31.80223108747045,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-06-26",
    "Generation": 4,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "MaziyarPanahi_Llama-3-8B-Instruct-v0.8_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-v0.8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Llama-3-8B-Instruct-v0.8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Llama-3-8B-Instruct-v0.8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Llama-3-8B-Instruct-v0.8",
    "Model sha": "94d222b8447b600b9836da4036df9490b59fe966",
    "Average ‚¨ÜÔ∏è": 26.90147249054173,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.5363378744857097,
    "IFEval Raw": 0.7527549125209998,
    "IFEval": 75.27549125209998,
    "BBH Raw": 0.49627836815949883,
    "BBH": 28.270418759783485,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.42019791666666667,
    "MUSR": 10.924739583333334,
    "MMLU-PRO Raw": 0.3853058510638298,
    "MMLU-PRO": 31.700650118203306,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-01",
    "Submission Date": "2024-07-11",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "MaziyarPanahi_Llama-3-8B-Instruct-v0.9_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Llama-3-8B-Instruct-v0.9\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Llama-3-8B-Instruct-v0.9</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Llama-3-8B-Instruct-v0.9-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Llama-3-8B-Instruct-v0.9",
    "Model sha": "ddf91fdc0a3ab5e5d76864f1c4cf44e5adacd565",
    "Average ‚¨ÜÔ∏è": 26.82440874256939,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7663568235309868,
    "IFEval Raw": 0.763046494412603,
    "IFEval": 76.3046494412603,
    "BBH Raw": 0.4936132794870085,
    "BBH": 27.903013285410182,
    "MATH Lvl 5 Raw": 0.0755287009063444,
    "MATH Lvl 5": 7.552870090634441,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4148020833333333,
    "MUSR": 9.850260416666666,
    "MMLU-PRO Raw": 0.3845578457446808,
    "MMLU-PRO": 31.61753841607564,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-08-06",
    "Generation": 3,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "MaziyarPanahi_Qwen1.5-MoE-A2.7B-Wikihow_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Qwen1.5-MoE-A2.7B-Wikihow\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Qwen1.5-MoE-A2.7B-Wikihow</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Qwen1.5-MoE-A2.7B-Wikihow-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Qwen1.5-MoE-A2.7B-Wikihow",
    "Model sha": "191cf0630b7b50fe1fc9be198e1f203935df1428",
    "Average ‚¨ÜÔ∏è": 11.507206996479937,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.306082950303546,
    "IFEval Raw": 0.29543278501043896,
    "IFEval": 29.5432785010439,
    "BBH Raw": 0.3920071454890602,
    "BBH": 15.47343942401254,
    "MATH Lvl 5 Raw": 0.03323262839879155,
    "MATH Lvl 5": 3.323262839879155,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.35021875,
    "MUSR": 2.010677083333334,
    "MMLU-PRO Raw": 0.23803191489361702,
    "MMLU-PRO": 15.336879432624112,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-30",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen1.5-MoE-A2.7B"
  },
  {
    "eval_name": "MaziyarPanahi_Qwen2-7B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Qwen2-7B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Qwen2-7B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Qwen2-7B-Instruct-v0.1",
    "Model sha": "5123ecd76cefd4ef3b6009542b13e060d03e5232",
    "Average ‚¨ÜÔ∏è": 22.98150895877193,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4535992706809813,
    "IFEval Raw": 0.33522498082864577,
    "IFEval": 33.52249808286457,
    "BBH Raw": 0.5123061019250074,
    "BBH": 31.92360727430821,
    "MATH Lvl 5 Raw": 0.22129909365558914,
    "MATH Lvl 5": 22.129909365558913,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.44347916666666665,
    "MUSR": 13.868229166666667,
    "MMLU-PRO Raw": 0.3857214095744681,
    "MMLU-PRO": 31.746823286052013,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-07-07",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_Qwen2-7B-Instruct-v0.8_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/Qwen2-7B-Instruct-v0.8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/Qwen2-7B-Instruct-v0.8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__Qwen2-7B-Instruct-v0.8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/Qwen2-7B-Instruct-v0.8",
    "Model sha": "a6f9d0e11efcba18c905554ab43b877ead187a77",
    "Average ‚¨ÜÔ∏è": 19.52037323782283,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3341063573944425,
    "IFEval Raw": 0.27747266142723526,
    "IFEval": 27.74726614272353,
    "BBH Raw": 0.4637108491317945,
    "BBH": 25.532524528361474,
    "MATH Lvl 5 Raw": 0.17447129909365558,
    "MATH Lvl 5": 17.447129909365557,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.4293125,
    "MUSR": 12.0640625,
    "MMLU-PRO Raw": 0.3566323138297872,
    "MMLU-PRO": 28.514701536643027,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-07-07",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-llama3.1-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-llama3.1-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-llama3.1-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-llama3.1-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-llama3.1-70b",
    "Model sha": "f39ad1c90b0f30379e80756d29c6533cf84c362a",
    "Average ‚¨ÜÔ∏è": 34.33985939628554,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 15.454839573803616,
    "IFEval Raw": 0.8434298771703524,
    "IFEval": 84.34298771703524,
    "BBH Raw": 0.644755327496552,
    "BBH": 48.55364600487639,
    "MATH Lvl 5 Raw": 0.01435045317220544,
    "MATH Lvl 5": 1.435045317220544,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.43803125000000004,
    "MUSR": 13.720572916666663,
    "MMLU-PRO Raw": 0.5282579787234043,
    "MMLU-PRO": 47.58421985815603,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-23",
    "Submission Date": "2024-07-24",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-phi3-4b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-phi3-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-phi3-4b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-phi3-4b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-phi3-4b",
    "Model sha": "6764c79badacba5fa3584d2d2593d762caa1d17d",
    "Average ‚¨ÜÔ∏è": 24.588084286118573,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7524686237954691,
    "IFEval Raw": 0.552520645221346,
    "IFEval": 55.2520645221346,
    "BBH Raw": 0.5595320442699866,
    "BBH": 38.1242795228128,
    "MATH Lvl 5 Raw": 0.04758308157099698,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.40153124999999995,
    "MUSR": 8.258072916666668,
    "MMLU-PRO Raw": 0.3745844414893617,
    "MMLU-PRO": 30.50938238770685,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-09",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-phi3.5-4b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-phi3.5-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-phi3.5-4b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-phi3.5-4b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-phi3.5-4b",
    "Model sha": "583b7f382a8ed35f6f7c09f2950f0f2346945a83",
    "Average ‚¨ÜÔ∏è": 27.20732708410567,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0045512528789182,
    "IFEval Raw": 0.5659095644002359,
    "IFEval": 56.59095644002358,
    "BBH Raw": 0.5483695590203843,
    "BBH": 36.110096929573025,
    "MATH Lvl 5 Raw": 0.15634441087613296,
    "MATH Lvl 5": 15.634441087613297,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.3994583333333333,
    "MUSR": 9.765624999999998,
    "MMLU-PRO Raw": 0.3935339095744681,
    "MMLU-PRO": 32.61487884160757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-08-23",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3.5-mini-instruct"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-qwen2-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-qwen2-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-qwen2-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-qwen2-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-qwen2-72b",
    "Model sha": "0369c39770f45f2464587918f2dbdb8449ea3a0d",
    "Average ‚¨ÜÔ∏è": 43.945771681032305,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.134871203475228,
    "IFEval Raw": 0.8162774770941104,
    "IFEval": 81.62774770941103,
    "BBH Raw": 0.6965560971922596,
    "BBH": 57.3258823447103,
    "MATH Lvl 5 Raw": 0.3806646525679758,
    "MATH Lvl 5": 38.066465256797585,
    "GPQA Raw": 0.3808724832214765,
    "GPQA": 17.4496644295302,
    "MUSR Raw": 0.47321875,
    "MUSR": 20.15234375,
    "MMLU-PRO Raw": 0.5414727393617021,
    "MMLU-PRO": 49.05252659574468,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-qwen2-7b",
    "Model sha": "5aac57e2290f7c49af88a9cb9883ce25b58882a1",
    "Average ‚¨ÜÔ∏è": 23.504115559635427,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4342552038335228,
    "IFEval Raw": 0.3816119008674761,
    "IFEval": 38.16119008674761,
    "BBH Raw": 0.5045925887362795,
    "BBH": 31.00709744702013,
    "MATH Lvl 5 Raw": 0.22885196374622357,
    "MATH Lvl 5": 22.885196374622357,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.44369791666666664,
    "MUSR": 13.795572916666666,
    "MMLU-PRO Raw": 0.3692652925531915,
    "MMLU-PRO": 29.918365839243506,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-qwen2.5-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-qwen2.5-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-qwen2.5-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-qwen2.5-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-qwen2.5-72b",
    "Model sha": "eb6c92dec932070ea872f39469ca5b9daf2d34e6",
    "Average ‚¨ÜÔ∏è": 38.39045817891699,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.748893638946452,
    "IFEval Raw": 0.8662360315075112,
    "IFEval": 86.62360315075111,
    "BBH Raw": 0.7261624327092416,
    "BBH": 61.65570318314716,
    "MATH Lvl 5 Raw": 0.02341389728096677,
    "MATH Lvl 5": 2.341389728096677,
    "GPQA Raw": 0.36325503355704697,
    "GPQA": 15.100671140939594,
    "MUSR Raw": 0.42984375,
    "MUSR": 13.297135416666663,
    "MMLU-PRO Raw": 0.5619182180851063,
    "MMLU-PRO": 51.3242464539007,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.1-rys-78b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.1-rys-78b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.1-rys-78b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.1-rys-78b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.1-rys-78b",
    "Model sha": "e746f5ddc0c9b31a2382d985a4ec87fa910847c7",
    "Average ‚¨ÜÔ∏è": 44.55588168517635,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.332288402738165,
    "IFEval Raw": 0.8135547015252862,
    "IFEval": 81.35547015252862,
    "BBH Raw": 0.7097861139530462,
    "BBH": 59.4700307859535,
    "MATH Lvl 5 Raw": 0.3889728096676737,
    "MATH Lvl 5": 38.897280966767376,
    "GPQA Raw": 0.39429530201342283,
    "GPQA": 19.239373601789712,
    "MUSR Raw": 0.4693125,
    "MUSR": 18.99739583333333,
    "MMLU-PRO Raw": 0.5443816489361702,
    "MMLU-PRO": 49.37573877068559,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-08",
    "Generation": 1,
    "Base Model": "dnhkng/RYS-XLarge"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-llama3-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-llama3-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-llama3-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-llama3-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-llama3-70b",
    "Model sha": "95366b974baedee4d95c1e841bc3d15e94753804",
    "Average ‚¨ÜÔ∏è": 38.29112109233707,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.628273307511309,
    "IFEval Raw": 0.8208486814984242,
    "IFEval": 82.0848681498424,
    "BBH Raw": 0.6435431762417703,
    "BBH": 48.57170594999846,
    "MATH Lvl 5 Raw": 0.24848942598187312,
    "MATH Lvl 5": 24.848942598187314,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.4445729166666667,
    "MUSR": 15.30494791666667,
    "MMLU-PRO Raw": 0.5206948138297872,
    "MMLU-PRO": 46.74386820330969,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-27",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-llama3.1-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-llama3.1-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-llama3.1-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-llama3.1-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-llama3.1-70b",
    "Model sha": "c81ac05ed2c2344e9fd366cfff197da406ef5234",
    "Average ‚¨ÜÔ∏è": 36.45048314820763,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 15.841823817829598,
    "IFEval Raw": 0.8592667455684251,
    "IFEval": 85.92667455684251,
    "BBH Raw": 0.6792920009427085,
    "BBH": 54.20646208605566,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.45415625000000004,
    "MUSR": 17.06953125,
    "MMLU-PRO Raw": 0.5414727393617021,
    "MMLU-PRO": 49.05252659574468,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-09",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-phi3-4b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-phi3-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-phi3-4b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-phi3-4b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-phi3-4b",
    "Model sha": "c0a366a4c01d7e724ceba7e2f2c19251983423fe",
    "Average ‚¨ÜÔ∏è": 23.231129959612986,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7967914667675369,
    "IFEval Raw": 0.5069083365470286,
    "IFEval": 50.69083365470286,
    "BBH Raw": 0.5529604896487258,
    "BBH": 37.733734155011526,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.3975625,
    "MUSR": 7.695312500000001,
    "MMLU-PRO Raw": 0.3813996010638298,
    "MMLU-PRO": 31.26662234042553,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-10",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-qwen2-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-qwen2-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-qwen2-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-qwen2-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-qwen2-72b",
    "Model sha": "529e9bd80a76d943409bc92bb246aa7ca63dd9e6",
    "Average ‚¨ÜÔ∏è": 43.77539284020449,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.51741457822528,
    "IFEval Raw": 0.8008151704145002,
    "IFEval": 80.08151704145003,
    "BBH Raw": 0.6939595229335245,
    "BBH": 56.79594225047665,
    "MATH Lvl 5 Raw": 0.4342900302114804,
    "MATH Lvl 5": 43.42900302114804,
    "GPQA Raw": 0.37416107382550334,
    "GPQA": 16.554809843400445,
    "MUSR Raw": 0.4508020833333333,
    "MUSR": 16.516927083333332,
    "MMLU-PRO Raw": 0.543467420212766,
    "MMLU-PRO": 49.27415780141844,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-08-06",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-qwen2-7b",
    "Model sha": "bbb1d119f75c5b2eaa8978286808bd59cae04997",
    "Average ‚¨ÜÔ∏è": 23.532966972011433,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5487495525214117,
    "IFEval Raw": 0.35972996094806226,
    "IFEval": 35.97299609480623,
    "BBH Raw": 0.5214913750127922,
    "BBH": 33.10936559556884,
    "MATH Lvl 5 Raw": 0.21148036253776437,
    "MATH Lvl 5": 21.148036253776436,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.43582291666666667,
    "MUSR": 13.277864583333338,
    "MMLU-PRO Raw": 0.3898769946808511,
    "MMLU-PRO": 32.208554964539005,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-qwen2.5-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-qwen2.5-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-qwen2.5-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-qwen2.5-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-qwen2.5-72b",
    "Model sha": "c6c7fdf70d8bf81364108975eb8ba78eecac83d4",
    "Average ‚¨ÜÔ∏è": 38.02266336132943,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.2580637745108,
    "IFEval Raw": 0.8476763875406145,
    "IFEval": 84.76763875406145,
    "BBH Raw": 0.7276399007138082,
    "BBH": 61.80360419146786,
    "MATH Lvl 5 Raw": 0.037009063444108765,
    "MATH Lvl 5": 3.7009063444108765,
    "GPQA Raw": 0.35906040268456374,
    "GPQA": 14.541387024608499,
    "MUSR Raw": 0.4206666666666667,
    "MUSR": 12.016666666666671,
    "MMLU-PRO Raw": 0.561751994680851,
    "MMLU-PRO": 51.305777186761226,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.2-rys-78b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.2-rys-78b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.2-rys-78b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.2-rys-78b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.2-rys-78b",
    "Model sha": "8d0dde25c9042705f65559446944a19259c3fc8e",
    "Average ‚¨ÜÔ∏è": 44.26045323567612,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.523356251314894,
    "IFEval Raw": 0.7986420475449585,
    "IFEval": 79.86420475449586,
    "BBH Raw": 0.7081014602379213,
    "BBH": 59.268645675184494,
    "MATH Lvl 5 Raw": 0.39954682779456197,
    "MATH Lvl 5": 39.9546827794562,
    "GPQA Raw": 0.40687919463087246,
    "GPQA": 20.917225950782996,
    "MUSR Raw": 0.45356250000000004,
    "MUSR": 16.82864583333333,
    "MMLU-PRO Raw": 0.538563829787234,
    "MMLU-PRO": 48.72931442080378,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-08",
    "Generation": 1,
    "Base Model": "dnhkng/RYS-XLarge"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-llama3-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-llama3-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-llama3-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-llama3-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-llama3-70b",
    "Model sha": "bd17453eaae0e36d1e1e17da13fdd155fce91a29",
    "Average ‚¨ÜÔ∏è": 37.15514908346956,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.636809063148661,
    "IFEval Raw": 0.8010401290797307,
    "IFEval": 80.10401290797307,
    "BBH Raw": 0.6399173489368603,
    "BBH": 48.0085850617923,
    "MATH Lvl 5 Raw": 0.23791540785498488,
    "MATH Lvl 5": 23.791540785498487,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.42612500000000003,
    "MUSR": 12.565625000000002,
    "MMLU-PRO Raw": 0.5204454787234043,
    "MMLU-PRO": 46.716164302600475,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-27",
    "Submission Date": "2024-08-30",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-llama3.1-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-llama3.1-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-llama3.1-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-llama3.1-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-llama3.1-70b",
    "Model sha": "a39c79250721b75beefa1b1763895eafd010f6f6",
    "Average ‚¨ÜÔ∏è": 40.64427352822522,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.06055787020106,
    "IFEval Raw": 0.8604657863358112,
    "IFEval": 86.04657863358113,
    "BBH Raw": 0.6871653740091753,
    "BBH": 55.58549511699308,
    "MATH Lvl 5 Raw": 0.23489425981873113,
    "MATH Lvl 5": 23.48942598187311,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.45682291666666663,
    "MUSR": 17.736197916666658,
    "MMLU-PRO Raw": 0.5363198138297872,
    "MMLU-PRO": 48.4799793144208,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-18",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-phi3-4b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-phi3-4b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-phi3-4b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-phi3-4b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-phi3-4b",
    "Model sha": "e1f70c3724c728aadd1c7c1bb279487494f7059e",
    "Average ‚¨ÜÔ∏è": 23.105983364523226,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8379308479653939,
    "IFEval Raw": 0.49264507063480456,
    "IFEval": 49.26450706348045,
    "BBH Raw": 0.5537867816134527,
    "BBH": 37.65889241962552,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.3988333333333333,
    "MUSR": 7.754166666666667,
    "MMLU-PRO Raw": 0.3828125,
    "MMLU-PRO": 31.42361111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-10",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-qwen2-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-qwen2-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-qwen2-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-qwen2-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-qwen2-72b",
    "Model sha": "12ff2e800f968e867a580c072905cf4671da066f",
    "Average ‚¨ÜÔ∏è": 30.407678856462994,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 19.44848499544426,
    "IFEval Raw": 0.3849840645044039,
    "IFEval": 38.498406450440385,
    "BBH Raw": 0.6576306700720502,
    "BBH": 51.22830430718469,
    "MATH Lvl 5 Raw": 0.16163141993957703,
    "MATH Lvl 5": 16.1631419939577,
    "GPQA Raw": 0.3716442953020134,
    "GPQA": 16.21923937360179,
    "MUSR Raw": 0.4112395833333333,
    "MUSR": 11.23828125,
    "MMLU-PRO Raw": 0.5418882978723404,
    "MMLU-PRO": 49.09869976359338,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-qwen2-7b",
    "Model sha": "ca39e60052a600a709e03fefceabd9620e0b66d7",
    "Average ‚¨ÜÔ∏è": 23.043936653898097,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8301999134771472,
    "IFEval Raw": 0.3824862476008103,
    "IFEval": 38.24862476008103,
    "BBH Raw": 0.5064049035932394,
    "BBH": 30.95608211537095,
    "MATH Lvl 5 Raw": 0.20468277945619334,
    "MATH Lvl 5": 20.468277945619334,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4422395833333333,
    "MUSR": 13.313281249999998,
    "MMLU-PRO Raw": 0.3611203457446808,
    "MMLU-PRO": 29.01337174940898,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.3-rys-78b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.3-rys-78b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.3-rys-78b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.3-rys-78b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.3-rys-78b",
    "Model sha": "a8a4e55c2f7054d25c2f0ab3a3b3d806eb915180",
    "Average ‚¨ÜÔ∏è": 44.4189045437758,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.298609611347306,
    "IFEval Raw": 0.8065854155862002,
    "IFEval": 80.65854155862002,
    "BBH Raw": 0.7107763314317289,
    "BBH": 59.57454695904105,
    "MATH Lvl 5 Raw": 0.3897280966767372,
    "MATH Lvl 5": 38.97280966767372,
    "GPQA Raw": 0.40436241610738255,
    "GPQA": 20.581655480984338,
    "MUSR Raw": 0.45492708333333337,
    "MUSR": 16.999218749999997,
    "MMLU-PRO Raw": 0.5475398936170213,
    "MMLU-PRO": 49.7266548463357,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "dnhkng/RYS-XLarge"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.4-llama3-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.4-llama3-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.4-llama3-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.4-llama3-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.4-llama3-70b",
    "Model sha": "cb03e4d810b82d86e7cb01ab146bade09a5d06d1",
    "Average ‚¨ÜÔ∏è": 32.4988133655824,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 17.743758320242858,
    "IFEval Raw": 0.5027371817887649,
    "IFEval": 50.27371817887649,
    "BBH Raw": 0.6418191966839487,
    "BBH": 48.39776612820646,
    "MATH Lvl 5 Raw": 0.24546827794561935,
    "MATH Lvl 5": 24.546827794561935,
    "GPQA Raw": 0.33976510067114096,
    "GPQA": 11.968680089485462,
    "MUSR Raw": 0.4287916666666667,
    "MUSR": 13.098958333333336,
    "MMLU-PRO Raw": 0.5203623670212766,
    "MMLU-PRO": 46.706929669030735,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-28",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.4-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.4-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.4-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.4-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.4-qwen2-7b",
    "Model sha": "d683c3ef1feb13e92227f5fd92fe5bc4b55ea4a2",
    "Average ‚¨ÜÔ∏è": 22.801088302834632,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6184903835338569,
    "IFEval Raw": 0.32995452067181746,
    "IFEval": 32.995452067181745,
    "BBH Raw": 0.5101416326251771,
    "BBH": 31.818265642234802,
    "MATH Lvl 5 Raw": 0.2001510574018127,
    "MATH Lvl 5": 20.01510574018127,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.44528125,
    "MUSR": 14.426822916666667,
    "MMLU-PRO Raw": 0.3976894946808511,
    "MMLU-PRO": 33.076610520094555,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.4-rys-78b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.4-rys-78b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.4-rys-78b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.4-rys-78b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.4-rys-78b",
    "Model sha": "0a35e51ffa9efa644c11816a2d56434804177acb",
    "Average ‚¨ÜÔ∏è": 50.71469472295214,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 42,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.976328201268458,
    "IFEval Raw": 0.8010899967641414,
    "IFEval": 80.10899967641413,
    "BBH Raw": 0.7279510956242796,
    "BBH": 62.15654929467119,
    "MATH Lvl 5 Raw": 0.4040785498489426,
    "MATH Lvl 5": 40.40785498489426,
    "GPQA Raw": 0.40268456375838924,
    "GPQA": 20.3579418344519,
    "MUSR Raw": 0.5770624999999999,
    "MUSR": 34.56614583333333,
    "MMLU-PRO Raw": 0.7002160904255319,
    "MMLU-PRO": 66.690676713948,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-09-03",
    "Generation": 2,
    "Base Model": "dnhkng/RYS-XLarge"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.5-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.5-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.5-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.5-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.5-qwen2-7b",
    "Model sha": "20fb1afc22c0722cb2c57185fff59befeba0fbec",
    "Average ‚¨ÜÔ∏è": 22.67212680186952,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3991691217972488,
    "IFEval Raw": 0.31449221399220734,
    "IFEval": 31.449221399220733,
    "BBH Raw": 0.4886561146965678,
    "BBH": 28.28099517875505,
    "MATH Lvl 5 Raw": 0.22658610271903323,
    "MATH Lvl 5": 22.658610271903324,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.45646875,
    "MUSR": 15.791927083333334,
    "MMLU-PRO Raw": 0.3681848404255319,
    "MMLU-PRO": 29.79831560283688,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.6-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.6-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.6-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.6-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.6-qwen2-7b",
    "Model sha": "ebfaae016a50f8922098a2a262ec3ca704504cae",
    "Average ‚¨ÜÔ∏è": 21.33303578900708,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6402708032103497,
    "IFEval Raw": 0.3442676542684522,
    "IFEval": 34.426765426845215,
    "BBH Raw": 0.4930243946403894,
    "BBH": 29.30841923308876,
    "MATH Lvl 5 Raw": 0.12764350453172207,
    "MATH Lvl 5": 12.764350453172208,
    "GPQA Raw": 0.2843959731543625,
    "GPQA": 4.586129753914999,
    "MUSR Raw": 0.4586145833333333,
    "MUSR": 16.560156250000002,
    "MMLU-PRO Raw": 0.3731715425531915,
    "MMLU-PRO": 30.352393617021285,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-2.7-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-2.7-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-2.7-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-2.7-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-2.7-qwen2-7b",
    "Model sha": "edc11a1baccedc04a5a4576ee4910fd8922ad47f",
    "Average ‚¨ÜÔ∏è": 22.34267903429701,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3642802885037124,
    "IFEval Raw": 0.3592301759331906,
    "IFEval": 35.92301759331906,
    "BBH Raw": 0.4883170901309997,
    "BBH": 28.912244614673995,
    "MATH Lvl 5 Raw": 0.13746223564954685,
    "MATH Lvl 5": 13.746223564954684,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.48242708333333334,
    "MUSR": 19.93671875,
    "MMLU-PRO Raw": 0.3705119680851064,
    "MMLU-PRO": 30.056885342789595,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.1-baguette-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.1-baguette-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.1-baguette-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.1-baguette-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.1-baguette-3b",
    "Model sha": "4601b18deed3931c33907ae98060898e787c7758",
    "Average ‚¨ÜÔ∏è": 22.13245790331135,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7315897206303091,
    "IFEval Raw": 0.6234369251364158,
    "IFEval": 62.343692513641585,
    "BBH Raw": 0.46833341042911075,
    "BBH": 25.50768075774144,
    "MATH Lvl 5 Raw": 0.04909365558912387,
    "MATH Lvl 5": 4.909365558912387,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.40079166666666666,
    "MUSR": 8.565625000000002,
    "MMLU-PRO Raw": 0.33992686170212766,
    "MMLU-PRO": 26.658540189125297,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.1-instruct-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.1-instruct-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.1-instruct-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.1-instruct-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.1-instruct-3b",
    "Model sha": "3bbd7f1f7949dd7c3679a29a781a95bd1085dc19",
    "Average ‚¨ÜÔ∏è": 20.286043304770676,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3936621697340073,
    "IFEval Raw": 0.43359397509718656,
    "IFEval": 43.35939750971866,
    "BBH Raw": 0.4812730148043098,
    "BBH": 27.309895959339894,
    "MATH Lvl 5 Raw": 0.1042296072507553,
    "MATH Lvl 5": 10.42296072507553,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.39520833333333333,
    "MUSR": 7.401041666666669,
    "MMLU-PRO Raw": 0.355718085106383,
    "MMLU-PRO": 28.413120567375884,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.1-llamaloi-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.1-llamaloi-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.1-llamaloi-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.1-llamaloi-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.1-llamaloi-3b",
    "Model sha": "62547548c06bb22f0b82c2bda7ac466507314a4b",
    "Average ‚¨ÜÔ∏è": 24.00523380928473,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1929176644729231,
    "IFEval Raw": 0.7375175645066203,
    "IFEval": 73.75175645066203,
    "BBH Raw": 0.4587340004998879,
    "BBH": 23.7691655758483,
    "MATH Lvl 5 Raw": 0.16767371601208458,
    "MATH Lvl 5": 16.76737160120846,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.35152083333333334,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.3204787234042553,
    "MMLU-PRO": 24.497635933806144,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.2-baguette-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.2-baguette-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.2-baguette-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.2-baguette-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.2-baguette-3b",
    "Model sha": "bba8e602432bd467b64cabf9cb62326893060e60",
    "Average ‚¨ÜÔ∏è": 22.14064810816379,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7760124975015028,
    "IFEval Raw": 0.6338282423968404,
    "IFEval": 63.38282423968404,
    "BBH Raw": 0.470862269902714,
    "BBH": 25.865746650731108,
    "MATH Lvl 5 Raw": 0.030966767371601207,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.40209374999999997,
    "MUSR": 8.595052083333334,
    "MMLU-PRO Raw": 0.3337765957446808,
    "MMLU-PRO": 25.975177304964536,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.2-instruct-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.2-instruct-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.2-instruct-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.2-instruct-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.2-instruct-3b",
    "Model sha": "12347f5991157e752de6ba9f773a1bbc22445e3a",
    "Average ‚¨ÜÔ∏è": 22.65660627287841,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7434331691693647,
    "IFEval Raw": 0.5533196363426819,
    "IFEval": 55.33196363426819,
    "BBH Raw": 0.4865641110376735,
    "BBH": 27.976798242393084,
    "MATH Lvl 5 Raw": 0.09894259818731117,
    "MATH Lvl 5": 9.894259818731117,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.40469791666666666,
    "MUSR": 8.787239583333333,
    "MMLU-PRO Raw": 0.36527593085106386,
    "MMLU-PRO": 29.47510342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.3-baguette-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.3-baguette-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.3-baguette-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.3-baguette-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.3-baguette-3b",
    "Model sha": "66f9438922503e5616b6b4488e96fd9342d5efb0",
    "Average ‚¨ÜÔ∏è": 21.06268964855956,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7469229188062569,
    "IFEval Raw": 0.6359514975819713,
    "IFEval": 63.59514975819714,
    "BBH Raw": 0.4678217295957521,
    "BBH": 25.596594106096415,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.39282291666666663,
    "MUSR": 7.136197916666667,
    "MMLU-PRO Raw": 0.3341921542553192,
    "MMLU-PRO": 26.021350472813243,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MaziyarPanahi_calme-3.3-instruct-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MaziyarPanahi/calme-3.3-instruct-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MaziyarPanahi/calme-3.3-instruct-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MaziyarPanahi__calme-3.3-instruct-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MaziyarPanahi/calme-3.3-instruct-3b",
    "Model sha": "ea7d7fb442c981ecd44c5a9060ac6b062927f231",
    "Average ‚¨ÜÔ∏è": 21.54779315312935,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7525485152012086,
    "IFEval Raw": 0.6423212631373645,
    "IFEval": 64.23212631373644,
    "BBH Raw": 0.46933409427688694,
    "BBH": 25.682137818579093,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.40742708333333333,
    "MUSR": 9.395052083333335,
    "MMLU-PRO Raw": 0.33053523936170215,
    "MMLU-PRO": 25.615026595744684,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "MultivexAI_Phi-3.5-Mini-Instruct-MultiVex-v0.25-GGUF_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/MultivexAI/Phi-3.5-Mini-Instruct-MultiVex-v0.25-GGUF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">MultivexAI/Phi-3.5-Mini-Instruct-MultiVex-v0.25-GGUF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/MultivexAI__Phi-3.5-Mini-Instruct-MultiVex-v0.25-GGUF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "MultivexAI/Phi-3.5-Mini-Instruct-MultiVex-v0.25-GGUF",
    "Model sha": "0bdec12abd74bc164fdfa432528b914e19f6a9aa",
    "Average ‚¨ÜÔ∏è": 3.7340655292605676,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6733871400406728,
    "IFEval Raw": 0.14398241111362298,
    "IFEval": 14.398241111362296,
    "BBH Raw": 0.29077474506950557,
    "BBH": 1.6023814703484736,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.3641979166666667,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11087101063829788,
    "MMLU-PRO": 1.2078900709219857,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Mxode_NanoLM-0.3B-Instruct-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Mxode/NanoLM-0.3B-Instruct-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Mxode/NanoLM-0.3B-Instruct-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Mxode__NanoLM-0.3B-Instruct-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Mxode/NanoLM-0.3B-Instruct-v1",
    "Model sha": "638cda2c122e96c7992227b56b29967d9c8fd57e",
    "Average ‚¨ÜÔ∏è": 5.498565044787242,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6062758049460221,
    "IFEval Raw": 0.1536744726215331,
    "IFEval": 15.36744726215331,
    "BBH Raw": 0.30282462164767127,
    "BBH": 3.104609898338746,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.41552083333333334,
    "MUSR": 10.440104166666668,
    "MMLU-PRO Raw": 0.11053856382978723,
    "MMLU-PRO": 1.1709515366430252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-03",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "Mxode/NanoLM-0.3B-Instruct-v1"
  },
  {
    "eval_name": "Mxode_NanoLM-0.3B-Instruct-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Mxode/NanoLM-0.3B-Instruct-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Mxode/NanoLM-0.3B-Instruct-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Mxode__NanoLM-0.3B-Instruct-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Mxode/NanoLM-0.3B-Instruct-v1.1",
    "Model sha": "7338464708c691667b193e7bb8f6b5bb3f9df27d",
    "Average ‚¨ÜÔ∏è": 5.861005557078461,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6074798791497296,
    "IFEval Raw": 0.17827918810977095,
    "IFEval": 17.827918810977096,
    "BBH Raw": 0.3014403673764691,
    "BBH": 3.0952799822018195,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.42733333333333334,
    "MUSR": 12.216666666666669,
    "MMLU-PRO Raw": 0.11211768617021277,
    "MMLU-PRO": 1.3464095744680846,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "Mxode/NanoLM-0.3B-Instruct-v1.1"
  },
  {
    "eval_name": "Mxode_NanoLM-0.3B-Instruct-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Mxode/NanoLM-0.3B-Instruct-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Mxode/NanoLM-0.3B-Instruct-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Mxode__NanoLM-0.3B-Instruct-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Mxode/NanoLM-0.3B-Instruct-v2",
    "Model sha": "40027e2a1a404144975cfc0dd7d354057b98854b",
    "Average ‚¨ÜÔ∏è": 4.900377500631554,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6065211224312973,
    "IFEval Raw": 0.1667885654507817,
    "IFEval": 16.67885654507817,
    "BBH Raw": 0.29211039456850646,
    "BBH": 2.2094810446663793,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3954583333333333,
    "MUSR": 7.565625,
    "MMLU-PRO Raw": 0.11344747340425532,
    "MMLU-PRO": 1.4941637115839235,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-07",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Mxode/NanoLM-0.3B-Instruct-v2"
  },
  {
    "eval_name": "Mxode_NanoLM-1B-Instruct-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Mxode/NanoLM-1B-Instruct-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Mxode/NanoLM-1B-Instruct-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Mxode__NanoLM-1B-Instruct-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Mxode/NanoLM-1B-Instruct-v1.1",
    "Model sha": "cad6274afcfcf33927dc6c116d63013dcc1dfc48",
    "Average ‚¨ÜÔ∏è": 6.643430190859999,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8254798681462031,
    "IFEval Raw": 0.23952889444451833,
    "IFEval": 23.952889444451834,
    "BBH Raw": 0.31835012059590373,
    "BBH": 6.106919191919192,
    "MATH Lvl 5 Raw": 0.029456193353474325,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.34327083333333336,
    "MUSR": 2.675520833333333,
    "MMLU-PRO Raw": 0.12150930851063829,
    "MMLU-PRO": 2.3899231678486985,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-07",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Mxode/NanoLM-1B-Instruct-v1.1"
  },
  {
    "eval_name": "Mxode_NanoLM-1B-Instruct-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Mxode/NanoLM-1B-Instruct-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Mxode/NanoLM-1B-Instruct-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Mxode__NanoLM-1B-Instruct-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Mxode/NanoLM-1B-Instruct-v2",
    "Model sha": "ebd8c374447985dbd4e247ffe6c5ebb5b4910418",
    "Average ‚¨ÜÔ∏è": 7.0145354420783015,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8102814750388582,
    "IFEval Raw": 0.2629844368497808,
    "IFEval": 26.298443684978082,
    "BBH Raw": 0.3123145400715591,
    "BBH": 4.910622895622894,
    "MATH Lvl 5 Raw": 0.02114803625377644,
    "MATH Lvl 5": 2.114803625377644,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.35520833333333335,
    "MUSR": 4.3343750000000005,
    "MMLU-PRO Raw": 0.12375332446808511,
    "MMLU-PRO": 2.639258274231678,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-07",
    "Submission Date": "2024-09-09",
    "Generation": 0,
    "Base Model": "Mxode/NanoLM-1B-Instruct-v2"
  },
  {
    "eval_name": "NAPS-ai_naps-gemma-2-27b-v-0.1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-gemma-2-27b-v-0.1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-gemma-2-27b-v-0.1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-gemma-2-27b-v-0.1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-gemma-2-27b-v-0.1.0",
    "Model sha": "c75cc878c364615db4b1b173b21b97ebcfb13d70",
    "Average ‚¨ÜÔ∏è": 1.6796019124036488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.224861038909133,
    "IFEval Raw": 0.0,
    "IFEval": 0.0,
    "BBH Raw": 0.2911778102988436,
    "BBH": 2.3470409575204094,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.35753125,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "NAPS-ai/naps-gemma-2-27b-v-0.1.0 (Merge)"
  },
  {
    "eval_name": "NAPS-ai_naps-gemma-2-27b-v0.1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-gemma-2-27b-v0.1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-gemma-2-27b-v0.1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-gemma-2-27b-v0.1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-gemma-2-27b-v0.1.0",
    "Model sha": "befb5b776e052a364bad4a5b3380a4d8370572dd",
    "Average ‚¨ÜÔ∏è": 1.6796019124036488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 22.664249227404174,
    "IFEval Raw": 0.0,
    "IFEval": 0.0,
    "BBH Raw": 0.2911778102988436,
    "BBH": 2.3470409575204094,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.35753125,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "NAPS-ai/naps-gemma-2-27b-v0.1.0 (Merge)"
  },
  {
    "eval_name": "NAPS-ai_naps-llama-3_1-8b-instruct-v0.3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-llama-3_1-8b-instruct-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-llama-3_1-8b-instruct-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-llama-3_1-8b-instruct-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-llama-3_1-8b-instruct-v0.3",
    "Model sha": "3dcd36be024e02de712d537f8786d868659127bb",
    "Average ‚¨ÜÔ∏è": 22.92587024976139,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9693078829654969,
    "IFEval Raw": 0.5390818583580456,
    "IFEval": 53.90818583580456,
    "BBH Raw": 0.4900525115527062,
    "BBH": 26.27454019814682,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.37870833333333337,
    "MUSR": 7.205208333333334,
    "MMLU-PRO Raw": 0.33984375,
    "MMLU-PRO": 26.649305555555554,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-02",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "NAPS-ai/naps-llama-3_1-8b-instruct-v0.3"
  },
  {
    "eval_name": "NAPS-ai_naps-llama-3_1-8b-instruct-v0.4_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-llama-3_1-8b-instruct-v0.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-llama-3_1-8b-instruct-v0.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-llama-3_1-8b-instruct-v0.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-llama-3_1-8b-instruct-v0.4",
    "Model sha": "152229e8de5270aea7b9d7689503fb2577f8911a",
    "Average ‚¨ÜÔ∏è": 27.551439019117016,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8919277904123731,
    "IFEval Raw": 0.7344202272193336,
    "IFEval": 73.44202272193337,
    "BBH Raw": 0.4861833360906734,
    "BBH": 27.832818693945708,
    "MATH Lvl 5 Raw": 0.1865558912386707,
    "MATH Lvl 5": 18.65558912386707,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.4421145833333333,
    "MUSR": 13.964322916666667,
    "MMLU-PRO Raw": 0.3474900265957447,
    "MMLU-PRO": 27.498891843971627,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "NAPS-ai/naps-llama-3_1-8b-instruct-v0.4 (Merge)"
  },
  {
    "eval_name": "NAPS-ai_naps-llama-3_1-instruct-v0.5.0_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-llama-3_1-instruct-v0.5.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-llama-3_1-instruct-v0.5.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-llama-3_1-instruct-v0.5.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-llama-3_1-instruct-v0.5.0",
    "Model sha": "bf6d3578346e80c586ec1a4a9883079523b48c11",
    "Average ‚¨ÜÔ∏è": 15.900117928030674,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1376211115182477,
    "IFEval Raw": 0.5020124381086628,
    "IFEval": 50.20124381086628,
    "BBH Raw": 0.4147584365689691,
    "BBH": 18.110133310152776,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.37127083333333327,
    "MUSR": 3.675520833333333,
    "MMLU-PRO Raw": 0.26138630319148937,
    "MMLU-PRO": 17.93181146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "NAPS-ai/naps-llama-3_1-instruct-v0.5.0"
  },
  {
    "eval_name": "NAPS-ai_naps-llama-3_1_instruct-v0.6.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NAPS-ai/naps-llama-3_1_instruct-v0.6.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NAPS-ai/naps-llama-3_1_instruct-v0.6.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NAPS-ai__naps-llama-3_1_instruct-v0.6.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NAPS-ai/naps-llama-3_1_instruct-v0.6.0",
    "Model sha": "e0ce03ea6539f9398adbe14d8f9512e5484625b4",
    "Average ‚¨ÜÔ∏è": 15.766913239603488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7877002817500637,
    "IFEval Raw": 0.3280063564675062,
    "IFEval": 32.80063564675062,
    "BBH Raw": 0.45284530156109354,
    "BBH": 22.339533869982745,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.37390624999999994,
    "MUSR": 3.971614583333333,
    "MMLU-PRO Raw": 0.3240525265957447,
    "MMLU-PRO": 24.894725177304963,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-11-13",
    "Generation": 1,
    "Base Model": "NAPS-ai/naps-llama-3_1_instruct-v0.6.0 (Merge)"
  },
  {
    "eval_name": "NLPark_AnFeng_v3.1-Avocet_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NLPark/AnFeng_v3.1-Avocet\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NLPark/AnFeng_v3.1-Avocet</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NLPark__AnFeng_v3.1-Avocet-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NLPark/AnFeng_v3.1-Avocet",
    "Model sha": "5170739731033323e6e66a0f68d34790042a3b2a",
    "Average ‚¨ÜÔ∏è": 28.277663297463416,
    "Hub License": "cc-by-nc-nd-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.172008033345858,
    "IFEval Raw": 0.5096311121158525,
    "IFEval": 50.96311121158526,
    "BBH Raw": 0.582852329074409,
    "BBH": 40.309033651453255,
    "MATH Lvl 5 Raw": 0.15256797583081574,
    "MATH Lvl 5": 15.256797583081575,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.44757291666666665,
    "MUSR": 14.979947916666662,
    "MMLU-PRO Raw": 0.44381648936170215,
    "MMLU-PRO": 38.20183215130024,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "NLPark/AnFeng_v3.1-Avocet"
  },
  {
    "eval_name": "NLPark_B-and-W_Flycatcher-3AD1E_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NLPark/B-and-W_Flycatcher-3AD1E\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NLPark/B-and-W_Flycatcher-3AD1E</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NLPark__B-and-W_Flycatcher-3AD1E-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NLPark/B-and-W_Flycatcher-3AD1E",
    "Model sha": "21044e39f6854f5a6df84c5074d449b7eb96b522",
    "Average ‚¨ÜÔ∏è": 29.284049971751234,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5513222969241838,
    "IFEval Raw": 0.49084650948372543,
    "IFEval": 49.08465094837254,
    "BBH Raw": 0.6065117528534355,
    "BBH": 43.74245801092346,
    "MATH Lvl 5 Raw": 0.16691842900302117,
    "MATH Lvl 5": 16.691842900302117,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.44227083333333334,
    "MUSR": 13.883854166666671,
    "MMLU-PRO Raw": 0.4740691489361702,
    "MMLU-PRO": 41.56323877068557,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "NLPark/B-and-W_Flycatcher-3AD1E"
  },
  {
    "eval_name": "NLPark_Shi-Ci-Robin-Test_3AD80_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NLPark/Shi-Ci-Robin-Test_3AD80\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NLPark/Shi-Ci-Robin-Test_3AD80</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NLPark__Shi-Ci-Robin-Test_3AD80-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NLPark/Shi-Ci-Robin-Test_3AD80",
    "Model sha": "995887837a259817570489183cbe8b1abffd23b1",
    "Average ‚¨ÜÔ∏è": 38.51659936266015,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.446754480356967,
    "IFEval Raw": 0.7226547782107031,
    "IFEval": 72.26547782107032,
    "BBH Raw": 0.6704805157570325,
    "BBH": 52.265661751102094,
    "MATH Lvl 5 Raw": 0.2726586102719033,
    "MATH Lvl 5": 27.26586102719033,
    "GPQA Raw": 0.3598993288590604,
    "GPQA": 14.65324384787472,
    "MUSR Raw": 0.46959375000000003,
    "MUSR": 18.86588541666667,
    "MMLU-PRO Raw": 0.5120511968085106,
    "MMLU-PRO": 45.78346631205674,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "NLPark/Shi-Ci-Robin-Test_3AD80 (Merge)"
  },
  {
    "eval_name": "NTQAI_Nxcode-CQ-7B-orpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NTQAI/Nxcode-CQ-7B-orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NTQAI/Nxcode-CQ-7B-orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NTQAI__Nxcode-CQ-7B-orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NTQAI/Nxcode-CQ-7B-orpo",
    "Model sha": "74f3b3c06de36b261af9ef857279d6e33f893336",
    "Average ‚¨ÜÔ∏è": 12.298250998539771,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 106,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8421749133935907,
    "IFEval Raw": 0.40072119753365515,
    "IFEval": 40.07211975336551,
    "BBH Raw": 0.4143023249178217,
    "BBH": 17.58000487008142,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.39396875,
    "MUSR": 7.04609375,
    "MMLU-PRO Raw": 0.16115359042553193,
    "MMLU-PRO": 6.794843380614658,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-24",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "NTQAI/Nxcode-CQ-7B-orpo"
  },
  {
    "eval_name": "NYTK_PULI-GPTrio_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-GPTrio\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-GPTrio</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NYTK__PULI-GPTrio-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NYTK/PULI-GPTrio",
    "Model sha": "16a56dd22d184e4b7b49d90461fa8d4810639463",
    "Average ‚¨ÜÔ∏è": 5.770787326967873,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7220469953476825,
    "IFEval Raw": 0.21797164855915638,
    "IFEval": 21.79716485591564,
    "BBH Raw": 0.30600290906237543,
    "BBH": 3.0152211415704975,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.38187499999999996,
    "MUSR": 5.3343750000000005,
    "MMLU-PRO Raw": 0.11369680851063829,
    "MMLU-PRO": 1.521867612293143,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-06-08",
    "Submission Date": "2024-08-24",
    "Generation": 0,
    "Base Model": "NYTK/PULI-GPTrio"
  },
  {
    "eval_name": "NYTK_PULI-LlumiX-32K_float16",
    "Precision": "float16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NYTK/PULI-LlumiX-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NYTK/PULI-LlumiX-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NYTK__PULI-LlumiX-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NYTK/PULI-LlumiX-32K",
    "Model sha": "a589894397a36b61c578d0dd4778ee6e5fe471ff",
    "Average ‚¨ÜÔ∏è": 6.4058163053555175,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8225697399628139,
    "IFEval Raw": 0.1699612583500667,
    "IFEval": 16.99612583500667,
    "BBH Raw": 0.31893582242949375,
    "BBH": 5.107047129907727,
    "MATH Lvl 5 Raw": 0.006042296072507552,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.39641666666666664,
    "MUSR": 7.718750000000001,
    "MMLU-PRO Raw": 0.16805186170212766,
    "MMLU-PRO": 7.561317966903072,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-12",
    "Submission Date": "2024-08-24",
    "Generation": 0,
    "Base Model": "NYTK/PULI-LlumiX-32K"
  },
  {
    "eval_name": "Naveenpoliasetty_llama3-8B-V2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Naveenpoliasetty/llama3-8B-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Naveenpoliasetty/llama3-8B-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Naveenpoliasetty__llama3-8B-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Naveenpoliasetty/llama3-8B-V2",
    "Model sha": "e0458381d02bc411b9e576796d185f23dcc11f71",
    "Average ‚¨ÜÔ∏è": 20.845862971267593,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.75201223968556,
    "IFEval Raw": 0.4122616878770551,
    "IFEval": 41.226168787705504,
    "BBH Raw": 0.5188657580065063,
    "BBH": 30.873209425039573,
    "MATH Lvl 5 Raw": 0.08006042296072508,
    "MATH Lvl 5": 8.006042296072508,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.40813541666666664,
    "MUSR": 9.183593750000002,
    "MMLU-PRO Raw": 0.3737533244680851,
    "MMLU-PRO": 30.417036052009454,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-18",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "Naveenpoliasetty/llama3-8B-V2 (Merge)"
  },
  {
    "eval_name": "Nekochu_Llama-3.1-8B-German-ORPO_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nekochu/Llama-3.1-8B-German-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nekochu/Llama-3.1-8B-German-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nekochu__Llama-3.1-8B-German-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nekochu/Llama-3.1-8B-German-ORPO",
    "Model sha": "463ea77e46fb6d69c86f23df21b0ab0a0b9e77cd",
    "Average ‚¨ÜÔ∏è": 21.302894109086605,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9441975363228331,
    "IFEval Raw": 0.4610710692074806,
    "IFEval": 46.10710692074806,
    "BBH Raw": 0.4982577044334462,
    "BBH": 29.419254274936463,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.46475,
    "MUSR": 16.860416666666666,
    "MMLU-PRO Raw": 0.33934507978723405,
    "MMLU-PRO": 26.593897754137114,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-24",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Nekochu_Llama-3.1-8B-french-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nekochu/Llama-3.1-8B-french-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nekochu/Llama-3.1-8B-french-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nekochu__Llama-3.1-8B-french-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nekochu/Llama-3.1-8B-french-DPO",
    "Model sha": "b0c66dd2a2814a6bfb05313ffec856fd4c6c7bd7",
    "Average ‚¨ÜÔ∏è": 20.80753601836232,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8212074656654257,
    "IFEval Raw": 0.46564227361179444,
    "IFEval": 46.56422736117945,
    "BBH Raw": 0.5110888403999433,
    "BBH": 30.03259699633449,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.4215625,
    "MUSR": 11.561979166666667,
    "MMLU-PRO Raw": 0.3414228723404255,
    "MMLU-PRO": 26.824763593380613,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "NousResearch/Meta-Llama-3.1-8B-Instruct"
  },
  {
    "eval_name": "Nekochu_Luminia-13B-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nekochu/Luminia-13B-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nekochu/Luminia-13B-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nekochu__Luminia-13B-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nekochu/Luminia-13B-v3",
    "Model sha": "602563f3af32b3c6be067ad522e6f3eaff4f8627",
    "Average ‚¨ÜÔ∏è": 11.55954793966039,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1388797430506286,
    "IFEval Raw": 0.25231829323971505,
    "IFEval": 25.231829323971507,
    "BBH Raw": 0.41121515510929624,
    "BBH": 17.690523920374847,
    "MATH Lvl 5 Raw": 0.013595166163141994,
    "MATH Lvl 5": 1.3595166163141994,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.3983333333333334,
    "MUSR": 8.891666666666667,
    "MMLU-PRO Raw": 0.22149268617021275,
    "MMLU-PRO": 13.499187352245862,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-18",
    "Submission Date": "2024-09-25",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-2-13b-chat-hf"
  },
  {
    "eval_name": "Nekochu_Luminia-8B-RP_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nekochu/Luminia-8B-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nekochu/Luminia-8B-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nekochu__Luminia-8B-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nekochu/Luminia-8B-RP",
    "Model sha": "619be17206729d86b898b9d1b3369a7135c1a9b9",
    "Average ‚¨ÜÔ∏è": 24.51738786262653,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9525977403575676,
    "IFEval Raw": 0.5574165436597118,
    "IFEval": 55.74165436597117,
    "BBH Raw": 0.5218151030627874,
    "BBH": 31.802699112572423,
    "MATH Lvl 5 Raw": 0.1299093655589124,
    "MATH Lvl 5": 12.990936555891238,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.3997604166666666,
    "MUSR": 11.070052083333328,
    "MMLU-PRO Raw": 0.3631150265957447,
    "MMLU-PRO": 29.235002955082745,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-24",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "NeverSleep_Lumimaid-v0.2-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NeverSleep/Lumimaid-v0.2-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NeverSleep/Lumimaid-v0.2-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NeverSleep__Lumimaid-v0.2-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NeverSleep/Lumimaid-v0.2-12B",
    "Model sha": "b04f4e8f9a0c64fbb271d1135b208c90c3aa0ad0",
    "Average ‚¨ÜÔ∏è": 17.79484716565742,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 79,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5641933636819907,
    "IFEval Raw": 0.10993497253952846,
    "IFEval": 10.993497253952846,
    "BBH Raw": 0.5395610525850818,
    "BBH": 34.40988943485442,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.48211458333333335,
    "MUSR": 21.297656250000003,
    "MMLU-PRO Raw": 0.3511469414893617,
    "MMLU-PRO": 27.905215721040182,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "NeverSleep/Lumimaid-v0.2-12B"
  },
  {
    "eval_name": "NeverSleep_Lumimaid-v0.2-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NeverSleep/Lumimaid-v0.2-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NeverSleep/Lumimaid-v0.2-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NeverSleep__Lumimaid-v0.2-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NeverSleep/Lumimaid-v0.2-8B",
    "Model sha": "4563201f29ef18c62d16e9f6fffd3931a63ccb51",
    "Average ‚¨ÜÔ∏è": 24.3868203470804,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 66,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7396955345562696,
    "IFEval Raw": 0.5038109992597419,
    "IFEval": 50.3810999259742,
    "BBH Raw": 0.5237767601226618,
    "BBH": 31.963373781180234,
    "MATH Lvl 5 Raw": 0.1419939577039275,
    "MATH Lvl 5": 14.19939577039275,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.4303020833333333,
    "MUSR": 12.321093750000001,
    "MMLU-PRO Raw": 0.36361369680851063,
    "MMLU-PRO": 29.290410756501185,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-08-09",
    "Generation": 0,
    "Base Model": "NeverSleep/Lumimaid-v0.2-8B"
  },
  {
    "eval_name": "Nexusflow_NexusRaven-V2-13B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nexusflow/NexusRaven-V2-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nexusflow/NexusRaven-V2-13B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nexusflow__NexusRaven-V2-13B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nexusflow/NexusRaven-V2-13B",
    "Model sha": "cdab7132db4a4fd64513123374ea1451d85a7ace",
    "Average ‚¨ÜÔ∏è": 8.31183115135613,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 464,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.089804913215833,
    "IFEval Raw": 0.1790781792311068,
    "IFEval": 17.90781792311068,
    "BBH Raw": 0.39488604640507335,
    "BBH": 15.336448395229596,
    "MATH Lvl 5 Raw": 0.018882175226586105,
    "MATH Lvl 5": 1.8882175226586104,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3736875,
    "MUSR": 3.7109374999999996,
    "MMLU-PRO Raw": 0.18716755319148937,
    "MMLU-PRO": 9.685283687943262,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-04",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "codellama/CodeLlama-13b-Instruct-hf"
  },
  {
    "eval_name": "Nitral-AI_Hathor_Stable-v0.2-L3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nitral-AI/Hathor_Stable-v0.2-L3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nitral-AI/Hathor_Stable-v0.2-L3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nitral-AI__Hathor_Stable-v0.2-L3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nitral-AI/Hathor_Stable-v0.2-L3-8B",
    "Model sha": "1c9f391c3e349f8ba51b5696290ee6db6a2b63fd",
    "Average ‚¨ÜÔ∏è": 25.842428037605917,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 58,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8084229647256214,
    "IFEval Raw": 0.7174840534226963,
    "IFEval": 71.74840534226962,
    "BBH Raw": 0.5285819178301682,
    "BBH": 32.826028565585965,
    "MATH Lvl 5 Raw": 0.10045317220543808,
    "MATH Lvl 5": 10.045317220543808,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.3780625,
    "MUSR": 5.557812500000004,
    "MMLU-PRO Raw": 0.36959773936170215,
    "MMLU-PRO": 29.955304373522463,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-09",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "Nitral-AI/Hathor_Stable-v0.2-L3-8B"
  },
  {
    "eval_name": "Nohobby_MS-Schisandra-22B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nohobby/MS-Schisandra-22B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nohobby/MS-Schisandra-22B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nohobby__MS-Schisandra-22B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nohobby/MS-Schisandra-22B-v0.1",
    "Model sha": "df698b7b740fb3b5193d61cd51e5e3a42c3b1e1c",
    "Average ‚¨ÜÔ∏è": 29.696236411462305,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6005536479261968,
    "IFEval Raw": 0.6331289866443259,
    "IFEval": 63.312898664432595,
    "BBH Raw": 0.5789949714896523,
    "BBH": 40.01139961622215,
    "MATH Lvl 5 Raw": 0.19788519637462235,
    "MATH Lvl 5": 19.788519637462233,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.39284375,
    "MUSR": 9.70546875,
    "MMLU-PRO Raw": 0.4095744680851064,
    "MMLU-PRO": 34.39716312056737,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-26",
    "Submission Date": "2024-10-30",
    "Generation": 1,
    "Base Model": "Nohobby/MS-Schisandra-22B-v0.1 (Merge)"
  },
  {
    "eval_name": "Nohobby_MS-Schisandra-22B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Nohobby/MS-Schisandra-22B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Nohobby/MS-Schisandra-22B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Nohobby__MS-Schisandra-22B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Nohobby/MS-Schisandra-22B-v0.2",
    "Model sha": "257b6d38d2f1c2a607c38a6a86336a241a81a455",
    "Average ‚¨ÜÔ∏è": 30.218548596061098,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.035007926661824,
    "IFEval Raw": 0.6382997114323329,
    "IFEval": 63.82997114323328,
    "BBH Raw": 0.5841215984231857,
    "BBH": 40.614458088552716,
    "MATH Lvl 5 Raw": 0.19939577039274925,
    "MATH Lvl 5": 19.939577039274926,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.40747916666666667,
    "MUSR": 10.668229166666668,
    "MMLU-PRO Raw": 0.4136469414893617,
    "MMLU-PRO": 34.84966016548463,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-02",
    "Submission Date": "2024-11-02",
    "Generation": 1,
    "Base Model": "Nohobby/MS-Schisandra-22B-v0.2 (Merge)"
  },
  {
    "eval_name": "NotASI_FineTome-Llama3.2-1B-0929_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NotASI/FineTome-Llama3.2-1B-0929\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NotASI/FineTome-Llama3.2-1B-0929</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NotASI__FineTome-Llama3.2-1B-0929-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NotASI/FineTome-Llama3.2-1B-0929",
    "Model sha": "61c8742238d0cfe68a0a3f61326b84cd6624ad02",
    "Average ‚¨ÜÔ∏è": 9.562948978547835,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4654084233445274,
    "IFEval Raw": 0.39907223943580805,
    "IFEval": 39.9072239435808,
    "BBH Raw": 0.3246274874705644,
    "BBH": 5.741405038838561,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3487604166666667,
    "MUSR": 2.6617187500000004,
    "MMLU-PRO Raw": 0.1428690159574468,
    "MMLU-PRO": 4.763223995271866,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-04",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-1B-Instruct"
  },
  {
    "eval_name": "NotASI_FineTome-Llama3.2-3B-1002_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NotASI/FineTome-Llama3.2-3B-1002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NotASI/FineTome-Llama3.2-3B-1002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NotASI__FineTome-Llama3.2-3B-1002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NotASI/FineTome-Llama3.2-3B-1002",
    "Model sha": "7c8497a24a381e3bfd77bc92e5685442768790d0",
    "Average ‚¨ÜÔ∏è": 16.649107253623388,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4197055211574883,
    "IFEval Raw": 0.5474496558021605,
    "IFEval": 54.744965580216046,
    "BBH Raw": 0.4319470614025341,
    "BBH": 19.52006065248879,
    "MATH Lvl 5 Raw": 0.05589123867069487,
    "MATH Lvl 5": 5.589123867069487,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3685104166666667,
    "MUSR": 3.963802083333334,
    "MMLU-PRO Raw": 0.24368351063829788,
    "MMLU-PRO": 15.96483451536643,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-05",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "NotASI_FineTome-v1.5-Llama3.2-1B-1007_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NotASI/FineTome-v1.5-Llama3.2-1B-1007\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NotASI/FineTome-v1.5-Llama3.2-1B-1007</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NotASI__FineTome-v1.5-Llama3.2-1B-1007-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NotASI/FineTome-v1.5-Llama3.2-1B-1007",
    "Model sha": "5e329d987e9f74dd2703a4fefa56ab8c72b5702b",
    "Average ‚¨ÜÔ∏è": 8.940455389326333,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4742315207715417,
    "IFEval Raw": 0.39237777984636324,
    "IFEval": 39.237777984636324,
    "BBH Raw": 0.32405671121485663,
    "BBH": 5.801724673541757,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.34745833333333337,
    "MUSR": 2.498958333333334,
    "MMLU-PRO Raw": 0.1427027925531915,
    "MMLU-PRO": 4.744754728132387,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "NotASI/FineTome-v1.5-Llama3.2-1B-1007 (Merge)"
  },
  {
    "eval_name": "NotASI_FineTome-v1.5-Llama3.2-3B-1007_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NotASI/FineTome-v1.5-Llama3.2-3B-1007\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NotASI/FineTome-v1.5-Llama3.2-3B-1007</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NotASI__FineTome-v1.5-Llama3.2-3B-1007-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NotASI/FineTome-v1.5-Llama3.2-3B-1007",
    "Model sha": "6c6e71fbcff6c00d04a3fd69084af20bf2a943c8",
    "Average ‚¨ÜÔ∏è": 16.962638810813903,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7253785254254749,
    "IFEval Raw": 0.5507719517546776,
    "IFEval": 55.077195175467764,
    "BBH Raw": 0.4312372935321582,
    "BBH": 19.457219278849333,
    "MATH Lvl 5 Raw": 0.055135951661631426,
    "MATH Lvl 5": 5.513595166163142,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3645416666666667,
    "MUSR": 4.067708333333334,
    "MMLU-PRO Raw": 0.2448470744680851,
    "MMLU-PRO": 16.094119385342786,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "NotASI/FineTome-v1.5-Llama3.2-3B-1007 (Merge)"
  },
  {
    "eval_name": "NousResearch_Hermes-2-Pro-Llama-3-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Hermes-2-Pro-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Hermes-2-Pro-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Hermes-2-Pro-Llama-3-8B",
    "Model sha": "bc265d1781299ed2045214289c927c207439a729",
    "Average ‚¨ÜÔ∏è": 21.704920333811653,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 407,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.749983206264044,
    "IFEval Raw": 0.5361839918084017,
    "IFEval": 53.61839918084017,
    "BBH Raw": 0.507112624310082,
    "BBH": 30.667993420825,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.4262395833333333,
    "MUSR": 11.246614583333335,
    "MMLU-PRO Raw": 0.30518617021276595,
    "MMLU-PRO": 22.798463356973993,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-30",
    "Submission Date": "2024-06-13",
    "Generation": 1,
    "Base Model": "NousResearch/Meta-Llama-3-8B"
  },
  {
    "eval_name": "NousResearch_Hermes-2-Pro-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Hermes-2-Pro-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Hermes-2-Pro-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Hermes-2-Pro-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Hermes-2-Pro-Mistral-7B",
    "Model sha": "09317b1d8da639b5d9af77c06aa17cde0f0f91c0",
    "Average ‚¨ÜÔ∏è": 21.702107522212724,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 487,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.47279751936003345,
    "IFEval Raw": 0.5668337788179807,
    "IFEval": 56.68337788179808,
    "BBH Raw": 0.4995435330498075,
    "BBH": 29.427578860536,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.43759375,
    "MUSR": 14.132552083333337,
    "MMLU-PRO Raw": 0.29463098404255317,
    "MMLU-PRO": 21.625664893617017,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-11",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "NousResearch_Hermes-2-Theta-Llama-3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Hermes-2-Theta-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Hermes-2-Theta-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Hermes-2-Theta-Llama-3-8B",
    "Model sha": "885173e97ab8572b444f7db1290d5d0386e26816",
    "Average ‚¨ÜÔ∏è": 24.775788343989266,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 194,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7439224175968665,
    "IFEval Raw": 0.6517883659800441,
    "IFEval": 65.17883659800441,
    "BBH Raw": 0.5206672260911865,
    "BBH": 32.046073848075835,
    "MATH Lvl 5 Raw": 0.09592145015105741,
    "MATH Lvl 5": 9.592145015105741,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.3948958333333334,
    "MUSR": 8.36197916666667,
    "MMLU-PRO Raw": 0.33685172872340424,
    "MMLU-PRO": 26.316858747044915,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-05",
    "Submission Date": "2024-07-11",
    "Generation": 2,
    "Base Model": "NousResearch/Meta-Llama-3-8B"
  },
  {
    "eval_name": "NousResearch_Hermes-3-Llama-3.1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Hermes-3-Llama-3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Hermes-3-Llama-3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Hermes-3-Llama-3.1-70B",
    "Model sha": "093242c69a91f8d9d5b8094c380b88772f9bd7f8",
    "Average ‚¨ÜÔ∏è": 37.482545094445584,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 92,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.207890920921344,
    "IFEval Raw": 0.7661438316998896,
    "IFEval": 76.61438316998897,
    "BBH Raw": 0.6755780641387483,
    "BBH": 53.76540869130056,
    "MATH Lvl 5 Raw": 0.14803625377643506,
    "MATH Lvl 5": 14.803625377643506,
    "GPQA Raw": 0.3615771812080537,
    "GPQA": 14.876957494407161,
    "MUSR Raw": 0.4948958333333333,
    "MUSR": 23.42864583333333,
    "MMLU-PRO Raw": 0.47265625,
    "MMLU-PRO": 41.40625,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "NousResearch_Hermes-3-Llama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Hermes-3-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Hermes-3-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Hermes-3-Llama-3.1-8B",
    "Model sha": "aabb745a717e133b74dcae23195d2635cf5f38cc",
    "Average ‚¨ÜÔ∏è": 23.49087671148001,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 244,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9058079001429431,
    "IFEval Raw": 0.6170172918966121,
    "IFEval": 61.70172918966122,
    "BBH Raw": 0.5177452540141246,
    "BBH": 30.724096614147953,
    "MATH Lvl 5 Raw": 0.04758308157099698,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.4369375,
    "MUSR": 13.617187499999995,
    "MMLU-PRO Raw": 0.3139128989361702,
    "MMLU-PRO": 23.768099881796687,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-28",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "NousResearch_Nous-Hermes-2-Mistral-7B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Nous-Hermes-2-Mistral-7B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Nous-Hermes-2-Mistral-7B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Nous-Hermes-2-Mistral-7B-DPO",
    "Model sha": "ebec0a691037d38955727d6949798429a63929dd",
    "Average ‚¨ÜÔ∏è": 21.037646390284717,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 168,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4745989186988177,
    "IFEval Raw": 0.5762510139762497,
    "IFEval": 57.62510139762497,
    "BBH Raw": 0.48526536654652347,
    "BBH": 27.792545658366084,
    "MATH Lvl 5 Raw": 0.04380664652567977,
    "MATH Lvl 5": 4.380664652567977,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3999791666666667,
    "MUSR": 8.330729166666668,
    "MMLU-PRO Raw": 0.3015292553191489,
    "MMLU-PRO": 22.392139479905435,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-18",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "NousResearch_Nous-Hermes-2-Mixtral-8x7B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Nous-Hermes-2-Mixtral-8x7B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO",
    "Model sha": "286ae6737d048ad1d965c2e830864df02db50f2f",
    "Average ‚¨ÜÔ∏è": 27.29024985448301,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 419,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.865143724149878,
    "IFEval Raw": 0.5896898008395501,
    "IFEval": 58.96898008395502,
    "BBH Raw": 0.5538851384033822,
    "BBH": 37.10778379133987,
    "MATH Lvl 5 Raw": 0.11858006042296075,
    "MATH Lvl 5": 11.858006042296076,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.4595416666666667,
    "MUSR": 16.676041666666666,
    "MMLU-PRO Raw": 0.3666057180851064,
    "MMLU-PRO": 29.622857565011817,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-11",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "NousResearch_Nous-Hermes-2-Mixtral-8x7B-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Nous-Hermes-2-Mixtral-8x7B-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Nous-Hermes-2-Mixtral-8x7B-SFT",
    "Model sha": "4c06af2684730f75a6874b95e8bf6058105d9612",
    "Average ‚¨ÜÔ∏è": 21.841010891461725,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 55,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.387939846172085,
    "IFEval Raw": 0.5730783210769648,
    "IFEval": 57.30783210769647,
    "BBH Raw": 0.5057868454026635,
    "BBH": 30.594312778864406,
    "MATH Lvl 5 Raw": 0.02114803625377644,
    "MATH Lvl 5": 2.114803625377644,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.421375,
    "MUSR": 11.138541666666669,
    "MMLU-PRO Raw": 0.30659906914893614,
    "MMLU-PRO": 22.95545212765957,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-26",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "NousResearch_Nous-Hermes-2-SOLAR-10.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Nous-Hermes-2-SOLAR-10.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Nous-Hermes-2-SOLAR-10.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Nous-Hermes-2-SOLAR-10.7B",
    "Model sha": "14c1fbe2f71acdcd58247b30d5439bd572d52386",
    "Average ‚¨ÜÔ∏è": 23.362190692279768,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 204,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6434441380172282,
    "IFEval Raw": 0.5278660620486975,
    "IFEval": 52.78660620486975,
    "BBH Raw": 0.5414294841140173,
    "BBH": 34.990894584465195,
    "MATH Lvl 5 Raw": 0.05438066465256798,
    "MATH Lvl 5": 5.4380664652567985,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.43728125,
    "MUSR": 13.826822916666663,
    "MMLU-PRO Raw": 0.3458277925531915,
    "MMLU-PRO": 27.314199172576835,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-01",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "upstage/SOLAR-10.7B-v1.0"
  },
  {
    "eval_name": "NousResearch_Nous-Hermes-llama-2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Nous-Hermes-llama-2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Nous-Hermes-llama-2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Nous-Hermes-llama-2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Nous-Hermes-llama-2-7b",
    "Model sha": "b7c3ec54b754175e006ef75696a2ba3802697078",
    "Average ‚¨ÜÔ∏è": 9.291539705283657,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 68,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.5580570577031625,
    "IFEval Raw": 0.17290788441335658,
    "IFEval": 17.290788441335657,
    "BBH Raw": 0.3823937686034717,
    "BBH": 13.78941955171473,
    "MATH Lvl 5 Raw": 0.007552870090634441,
    "MATH Lvl 5": 0.755287009063444,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.42571875,
    "MUSR": 11.681510416666667,
    "MMLU-PRO Raw": 0.19398271276595744,
    "MMLU-PRO": 10.442523640661937,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-25",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "NousResearch/Nous-Hermes-llama-2-7b"
  },
  {
    "eval_name": "NousResearch_Yarn-Llama-2-13b-128k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Llama-2-13b-128k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Llama-2-13b-128k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Llama-2-13b-128k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Llama-2-13b-128k",
    "Model sha": "4e3e87a067f64f8814c83dd5e3bad92dcf8a2391",
    "Average ‚¨ÜÔ∏è": 8.41861797578536,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 114,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 51.935783310852166,
    "IFEval Raw": 0.16546430138698653,
    "IFEval": 16.546430138698653,
    "BBH Raw": 0.3826816443733663,
    "BBH": 13.505319085673955,
    "MATH Lvl 5 Raw": 0.012839879154078549,
    "MATH Lvl 5": 1.2839879154078548,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.34575,
    "MUSR": 3.385416666666666,
    "MMLU-PRO Raw": 0.23204787234042554,
    "MMLU-PRO": 14.671985815602836,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-08-30",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Llama-2-13b-128k"
  },
  {
    "eval_name": "NousResearch_Yarn-Llama-2-7b-128k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Llama-2-7b-128k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Llama-2-7b-128k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Llama-2-7b-128k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Llama-2-7b-128k",
    "Model sha": "e1ceedbbf2ed28b88086794441a6c05606d15437",
    "Average ‚¨ÜÔ∏è": 6.701507629071706,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 38,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8397388368066746,
    "IFEval Raw": 0.14847825990593846,
    "IFEval": 14.847825990593847,
    "BBH Raw": 0.32480295375597734,
    "BBH": 6.1446917129934855,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.39669791666666665,
    "MUSR": 8.253906250000004,
    "MMLU-PRO Raw": 0.1791057180851064,
    "MMLU-PRO": 8.789524231678488,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-08-31",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Llama-2-7b-128k"
  },
  {
    "eval_name": "NousResearch_Yarn-Llama-2-7b-64k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Llama-2-7b-64k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Llama-2-7b-64k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Llama-2-7b-64k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Llama-2-7b-64k",
    "Model sha": "08491431ac3b50add7443f5d4c02850801d877be",
    "Average ‚¨ÜÔ∏è": 7.13476632786368,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8304013262191507,
    "IFEval Raw": 0.1699856381068897,
    "IFEval": 16.99856381068897,
    "BBH Raw": 0.3326277865253592,
    "BBH": 7.0440554144724175,
    "MATH Lvl 5 Raw": 0.01057401812688822,
    "MATH Lvl 5": 1.057401812688822,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.393875,
    "MUSR": 6.934374999999998,
    "MMLU-PRO Raw": 0.17985372340425532,
    "MMLU-PRO": 8.872635933806146,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-08-30",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Llama-2-7b-64k"
  },
  {
    "eval_name": "NousResearch_Yarn-Mistral-7b-128k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Mistral-7b-128k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Mistral-7b-128k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Mistral-7b-128k",
    "Model sha": "d09f1f8ed437d61c1aff94c1beabee554843dcdd",
    "Average ‚¨ÜÔ∏è": 13.218402925989887,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 572,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5502610317742958,
    "IFEval Raw": 0.19336693307091848,
    "IFEval": 19.33669330709185,
    "BBH Raw": 0.4314467711273296,
    "BBH": 20.633112436478672,
    "MATH Lvl 5 Raw": 0.028700906344410883,
    "MATH Lvl 5": 2.8700906344410884,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4070520833333333,
    "MUSR": 8.948177083333333,
    "MMLU-PRO Raw": 0.289311835106383,
    "MMLU-PRO": 21.034648345153663,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-31",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Mistral-7b-128k"
  },
  {
    "eval_name": "NousResearch_Yarn-Mistral-7b-64k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Mistral-7b-64k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Mistral-7b-64k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Mistral-7b-64k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Mistral-7b-64k",
    "Model sha": "0273c624561fcecc8e8f4030492a9307aa60f945",
    "Average ‚¨ÜÔ∏è": 13.502693645072753,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 50,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5411601456957255,
    "IFEval Raw": 0.2079548930171944,
    "IFEval": 20.79548930171944,
    "BBH Raw": 0.42931904551037814,
    "BBH": 20.230200209182886,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.41238541666666667,
    "MUSR": 9.881510416666666,
    "MMLU-PRO Raw": 0.2913896276595745,
    "MMLU-PRO": 21.265514184397162,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-31",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Mistral-7b-64k"
  },
  {
    "eval_name": "NousResearch_Yarn-Solar-10b-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Solar-10b-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Solar-10b-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Solar-10b-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Solar-10b-32k",
    "Model sha": "ec3158b5276ac6644ddbdb36ccf6f9a106c98ede",
    "Average ‚¨ÜÔ∏è": 15.718665312283363,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3884383632906987,
    "IFEval Raw": 0.1948153122603581,
    "IFEval": 19.481531226035806,
    "BBH Raw": 0.4986859152325069,
    "BBH": 28.99482436025671,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4146458333333333,
    "MUSR": 10.597395833333332,
    "MMLU-PRO Raw": 0.32721077127659576,
    "MMLU-PRO": 25.24564125295508,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-17",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Solar-10b-32k"
  },
  {
    "eval_name": "NousResearch_Yarn-Solar-10b-64k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NousResearch/Yarn-Solar-10b-64k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NousResearch/Yarn-Solar-10b-64k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NousResearch__Yarn-Solar-10b-64k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NousResearch/Yarn-Solar-10b-64k",
    "Model sha": "703818628a5e8ef637e48e8dbeb3662aa0497aff",
    "Average ‚¨ÜÔ∏è": 15.12428609620011,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7637528730509061,
    "IFEval Raw": 0.1988867316498003,
    "IFEval": 19.88867316498003,
    "BBH Raw": 0.49219907954226505,
    "BBH": 28.395714153595822,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.40143750000000006,
    "MUSR": 9.013020833333336,
    "MMLU-PRO Raw": 0.3148271276595745,
    "MMLU-PRO": 23.869680851063833,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-17",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "NousResearch/Yarn-Solar-10b-64k"
  },
  {
    "eval_name": "NucleusAI_nucleus-22B-token-500B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/NucleusAI/nucleus-22B-token-500B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">NucleusAI/nucleus-22B-token-500B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/NucleusAI__nucleus-22B-token-500B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "NucleusAI/nucleus-22B-token-500B",
    "Model sha": "49bb1a47c0d32b4bfa6630a4eff04a857adcd4ca",
    "Average ‚¨ÜÔ∏è": 1.6334163485881146,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 21,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.59481771866259,
    "IFEval Raw": 0.025654153202391873,
    "IFEval": 2.5654153202391874,
    "BBH Raw": 0.29198007801214715,
    "BBH": 1.8879990685708254,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.3510520833333333,
    "MUSR": 3.5481770833333326,
    "MMLU-PRO Raw": 0.11619015957446809,
    "MMLU-PRO": 1.798906619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-06",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "NucleusAI/nucleus-22B-token-500B"
  },
  {
    "eval_name": "OEvortex_HelpingAI-15B_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OEvortex/HelpingAI-15B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OEvortex/HelpingAI-15B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OEvortex__HelpingAI-15B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OEvortex/HelpingAI-15B",
    "Model sha": "fcc5d4eeee08c07680a2560a302de3eaa5d6f550",
    "Average ‚¨ÜÔ∏è": 4.515495603660534,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 15,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.227237308858675,
    "IFEval Raw": 0.2030091268944179,
    "IFEval": 20.30091268944179,
    "BBH Raw": 0.2936006977853758,
    "BBH": 1.8153805514942334,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.361875,
    "MUSR": 2.7343749999999996,
    "MMLU-PRO Raw": 0.11112034574468085,
    "MMLU-PRO": 1.2355939716312052,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-11",
    "Submission Date": "2024-07-13",
    "Generation": 0,
    "Base Model": "OEvortex/HelpingAI-15B"
  },
  {
    "eval_name": "OEvortex_HelpingAI-3B-reloaded_float16",
    "Precision": "float16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OEvortex/HelpingAI-3B-reloaded\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OEvortex/HelpingAI-3B-reloaded</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OEvortex__HelpingAI-3B-reloaded-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OEvortex/HelpingAI-3B-reloaded",
    "Model sha": "aaee653fea06ba322e7a9ed15530db605cc3b382",
    "Average ‚¨ÜÔ∏è": 14.59218689770182,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5616264484183794,
    "IFEval Raw": 0.46466819150963884,
    "IFEval": 46.466819150963886,
    "BBH Raw": 0.4128512897904065,
    "BBH": 16.98574044907848,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.3524479166666667,
    "MUSR": 4.289322916666667,
    "MMLU-PRO Raw": 0.25947473404255317,
    "MMLU-PRO": 17.719414893617017,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-10-31",
    "Generation": 0,
    "Base Model": "OEvortex/HelpingAI-3B-reloaded"
  },
  {
    "eval_name": "OEvortex_HelpingAI2-9B_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OEvortex/HelpingAI2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OEvortex/HelpingAI2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OEvortex__HelpingAI2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OEvortex/HelpingAI2-9B",
    "Model sha": "b45a18cf41d0d438d71d79687e098ec60dd0aec1",
    "Average ‚¨ÜÔ∏è": 17.418105309994427,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0406514109433187,
    "IFEval Raw": 0.44131238447319776,
    "IFEval": 44.131238447319774,
    "BBH Raw": 0.4844617641983123,
    "BBH": 27.073241609173305,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.3710833333333334,
    "MUSR": 6.318750000000001,
    "MMLU-PRO Raw": 0.28997672872340424,
    "MMLU-PRO": 21.108525413711583,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "OEvortex/HelpingAI2-9B"
  },
  {
    "eval_name": "OEvortex_HelpingAI2.5-10B_float16",
    "Precision": "float16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OEvortex/HelpingAI2.5-10B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OEvortex/HelpingAI2.5-10B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OEvortex__HelpingAI2.5-10B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OEvortex/HelpingAI2.5-10B",
    "Model sha": "25ac750b886c7e42521c769e6c2cd2b1143cfbcc",
    "Average ‚¨ÜÔ∏è": 13.371895008607561,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9393933821774075,
    "IFEval Raw": 0.32765617450586665,
    "IFEval": 32.76561745058666,
    "BBH Raw": 0.4495657491171711,
    "BBH": 21.135366144659056,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.37381250000000005,
    "MUSR": 6.259895833333336,
    "MMLU-PRO Raw": 0.25748005319148937,
    "MMLU-PRO": 17.49778368794326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "OEvortex/HelpingAI2.5-10B"
  },
  {
    "eval_name": "OliveiraJLT_Sagui-7B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OliveiraJLT/Sagui-7B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OliveiraJLT/Sagui-7B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OliveiraJLT__Sagui-7B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OliveiraJLT/Sagui-7B-Instruct-v0.1",
    "Model sha": "e3032ba89a6df12b801ab3be2a29b59068aa048d",
    "Average ‚¨ÜÔ∏è": 8.390585578374138,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0709379026425876,
    "IFEval Raw": 0.28916275482386733,
    "IFEval": 28.916275482386734,
    "BBH Raw": 0.3110678914743868,
    "BBH": 5.043571655312187,
    "MATH Lvl 5 Raw": 0.003776435045317221,
    "MATH Lvl 5": 0.37764350453172213,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.4190520833333333,
    "MUSR": 10.61484375,
    "MMLU-PRO Raw": 0.14852061170212766,
    "MMLU-PRO": 5.391179078014184,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-07-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Omkar1102_code-yi_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Omkar1102/code-yi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Omkar1102/code-yi</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Omkar1102__code-yi-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Omkar1102/code-yi",
    "Model sha": "7e875c1d64029d1f8db6813bd2b715cb5406b745",
    "Average ‚¨ÜÔ∏è": 4.921767050278931,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44252049269387844,
    "IFEval Raw": 0.21477457590304835,
    "IFEval": 21.477457590304837,
    "BBH Raw": 0.2760062695877461,
    "BBH": 1.8441580122160068,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3802291666666667,
    "MUSR": 4.695312500000001,
    "MMLU-PRO Raw": 0.11261635638297872,
    "MMLU-PRO": 1.4018173758865236,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-16",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Omkar1102_code-yi_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Omkar1102/code-yi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Omkar1102/code-yi</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Omkar1102__code-yi-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Omkar1102/code-yi",
    "Model sha": "7e875c1d64029d1f8db6813bd2b715cb5406b745",
    "Average ‚¨ÜÔ∏è": 5.170300155045384,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8511720977478499,
    "IFEval Raw": 0.2254407195131141,
    "IFEval": 22.54407195131141,
    "BBH Raw": 0.2750025242693941,
    "BBH": 1.5813991446240265,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.3761979166666667,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11228390957446809,
    "MMLU-PRO": 1.3648788416075646,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-16",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "OmnicromsBrain_NeuralStar_FusionWriter_4x7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OmnicromsBrain/NeuralStar_FusionWriter_4x7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OmnicromsBrain/NeuralStar_FusionWriter_4x7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OmnicromsBrain__NeuralStar_FusionWriter_4x7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OmnicromsBrain/NeuralStar_FusionWriter_4x7b",
    "Model sha": "fbe296d2c76acbb792cdd22e14d1c8bb13723839",
    "Average ‚¨ÜÔ∏è": 20.071645037035577,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3764648005691698,
    "IFEval Raw": 0.5963842604289951,
    "IFEval": 59.63842604289951,
    "BBH Raw": 0.47762434766958123,
    "BBH": 26.038439832659787,
    "MATH Lvl 5 Raw": 0.049093655589123875,
    "MATH Lvl 5": 4.909365558912388,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.401875,
    "MUSR": 8.201041666666667,
    "MMLU-PRO Raw": 0.2605551861702128,
    "MMLU-PRO": 17.83946513002364,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-07-01",
    "Generation": 1,
    "Base Model": "OmnicromsBrain/NeuralStar_FusionWriter_4x7b (Merge)"
  },
  {
    "eval_name": "Open-Orca_Mistral-7B-OpenOrca_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Open-Orca/Mistral-7B-OpenOrca\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Open-Orca/Mistral-7B-OpenOrca</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Open-Orca__Mistral-7B-OpenOrca-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Open-Orca/Mistral-7B-OpenOrca",
    "Model sha": "4a37328cef00f524d3791b1c0cc559a3cc6af14d",
    "Average ‚¨ÜÔ∏è": 17.696474879163762,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 674,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5335798433744053,
    "IFEval Raw": 0.4977659277384008,
    "IFEval": 49.77659277384008,
    "BBH Raw": 0.4768173517353546,
    "BBH": 25.840025395269805,
    "MATH Lvl 5 Raw": 0.03398791540785499,
    "MATH Lvl 5": 3.398791540785499,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.38578124999999996,
    "MUSR": 5.889322916666667,
    "MMLU-PRO Raw": 0.26529255319148937,
    "MMLU-PRO": 18.36583924349882,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Open-Orca/Mistral-7B-OpenOrca"
  },
  {
    "eval_name": "OpenAssistant_oasst-sft-1-pythia-12b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenAssistant/oasst-sft-1-pythia-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenAssistant__oasst-sft-1-pythia-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenAssistant/oasst-sft-1-pythia-12b",
    "Model sha": "293df535fe7711a5726987fc2f17dfc87de452a1",
    "Average ‚¨ÜÔ∏è": 3.6692423765809075,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 278,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.88805723718757,
    "IFEval Raw": 0.10553885911603435,
    "IFEval": 10.553885911603434,
    "BBH Raw": 0.314662875941371,
    "BBH": 4.778508799161477,
    "MATH Lvl 5 Raw": 0.014350453172205437,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.33269791666666665,
    "MUSR": 2.987239583333334,
    "MMLU-PRO Raw": 0.11128656914893617,
    "MMLU-PRO": 1.2540632387706852,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-03-09",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "OpenAssistant/oasst-sft-1-pythia-12b"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3-70b-v21.2-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3-70b-v21.2-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3-70b-v21.2-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3-70b-v21.2-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3-70b-v21.2-32k",
    "Model sha": "e79a2f16c052fc76eeafb5b51d16261b2b981d0f",
    "Average ‚¨ÜÔ∏è": 35.46534078043743,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.03593601516099,
    "IFEval Raw": 0.7010476646409305,
    "IFEval": 70.10476646409305,
    "BBH Raw": 0.6507443429944494,
    "BBH": 49.969365808428186,
    "MATH Lvl 5 Raw": 0.19788519637462235,
    "MATH Lvl 5": 19.788519637462233,
    "GPQA Raw": 0.3422818791946309,
    "GPQA": 12.304250559284117,
    "MUSR Raw": 0.45796875000000004,
    "MUSR": 18.04609375,
    "MMLU-PRO Raw": 0.4832114361702128,
    "MMLU-PRO": 42.579048463356976,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-12",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3-70b-v21.2-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3-8b-v21.1-8k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3-8b-v21.1-8k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3-8b-v21.1-8k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3-8b-v21.1-8k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3-8b-v21.1-8k",
    "Model sha": "658508bce03ccd61cea9657e0357bd4cd10503ba",
    "Average ‚¨ÜÔ∏è": 19.961528447552116,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 30,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.819422107889426,
    "IFEval Raw": 0.5569666263292509,
    "IFEval": 55.69666263292508,
    "BBH Raw": 0.47875007373484046,
    "BBH": 26.115045337590942,
    "MATH Lvl 5 Raw": 0.03096676737160121,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.3987708333333333,
    "MUSR": 10.346354166666663,
    "MMLU-PRO Raw": 0.2954621010638298,
    "MMLU-PRO": 21.71801122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-20",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3-8b-v21.1-8k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3-8b-v21.2-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3-8b-v21.2-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3-8b-v21.2-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3-8b-v21.2-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3-8b-v21.2-32k",
    "Model sha": "f3ea2dec2533a3dd97df32db2376b17875cafda2",
    "Average ‚¨ÜÔ∏è": 21.905833556192658,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8498893671229553,
    "IFEval Raw": 0.6191904147661538,
    "IFEval": 61.91904147661538,
    "BBH Raw": 0.4856219845879779,
    "BBH": 27.252334736558794,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.377875,
    "MUSR": 5.934375000000003,
    "MMLU-PRO Raw": 0.3298703457446808,
    "MMLU-PRO": 25.541149527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-18",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3-8b-v21.2-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3.1-70b-v22.1-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3.1-70b-v22.1-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3.1-70b-v22.1-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3.1-70b-v22.1-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3.1-70b-v22.1-131k",
    "Model sha": "43ed945180174d79a8f6c68509161c249c884dfa",
    "Average ‚¨ÜÔ∏è": 35.30788179040349,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.191623304325518,
    "IFEval Raw": 0.7332710541363582,
    "IFEval": 73.32710541363582,
    "BBH Raw": 0.6698491606025763,
    "BBH": 51.94077625159233,
    "MATH Lvl 5 Raw": 0.03851963746223565,
    "MATH Lvl 5": 3.8519637462235647,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.46295833333333336,
    "MUSR": 18.23645833333333,
    "MMLU-PRO Raw": 0.5304188829787234,
    "MMLU-PRO": 47.82432033096927,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-08-24",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3.1-70b-v22.1-131k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3.1-8b-v22.2-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3.1-8b-v22.2-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3.1-8b-v22.2-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3.1-8b-v22.2-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3.1-8b-v22.2-131k",
    "Model sha": "0d9d85c7a5e4292e07c346147de56bd3991d525c",
    "Average ‚¨ÜÔ∏è": 24.254527583378188,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7910630985256194,
    "IFEval Raw": 0.6657269378582162,
    "IFEval": 66.57269378582163,
    "BBH Raw": 0.5006515954024578,
    "BBH": 29.057538243651496,
    "MATH Lvl 5 Raw": 0.10498489425981872,
    "MATH Lvl 5": 10.498489425981871,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.40810416666666666,
    "MUSR": 9.81302083333333,
    "MMLU-PRO Raw": 0.3310339095744681,
    "MMLU-PRO": 25.67043439716312,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-28",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3.1-8b-v22.2-131k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3.1-8b-v22.3-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3.1-8b-v22.3-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k",
    "Model sha": "0097358fa1a450251b7ea1a03a5effdfded6c461",
    "Average ‚¨ÜÔ∏è": 23.07877971712109,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8342232202457845,
    "IFEval Raw": 0.5997065563815123,
    "IFEval": 59.97065563815122,
    "BBH Raw": 0.5065914870348772,
    "BBH": 30.319510884756202,
    "MATH Lvl 5 Raw": 0.10649546827794563,
    "MATH Lvl 5": 10.649546827794563,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.40146875,
    "MUSR": 8.316927083333335,
    "MMLU-PRO Raw": 0.3277094414893617,
    "MMLU-PRO": 25.30104905437352,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-08-24",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3.1-8b-v22.3-131k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3.2-1b-v23.1-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3.2-1b-v23.1-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3.2-1b-v23.1-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3.2-1b-v23.1-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3.2-1b-v23.1-131k",
    "Model sha": "71b61e0e02e55553902f0051074d2ae965413cdb",
    "Average ‚¨ÜÔ∏è": 8.959801927591569,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.44614261173181186,
    "IFEval Raw": 0.3590052172679601,
    "IFEval": 35.90052172679601,
    "BBH Raw": 0.3266563226631131,
    "BBH": 6.043619508652057,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.33421875,
    "MUSR": 1.2106770833333331,
    "MMLU-PRO Raw": 0.1840093085106383,
    "MMLU-PRO": 9.334367612293143,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-09",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3.2-1b-v23.1-131k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-llama3.2-3b-v23.2-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-llama3.2-3b-v23.2-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-llama3.2-3b-v23.2-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-llama3.2-3b-v23.2-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-llama3.2-3b-v23.2-131k",
    "Model sha": "7cd2baa3d9bb99e970d711fb7afe786753bc25ea",
    "Average ‚¨ÜÔ∏è": 13.394834151781216,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6955718223588185,
    "IFEval Raw": 0.4319450169993395,
    "IFEval": 43.19450169993395,
    "BBH Raw": 0.4072660342069299,
    "BBH": 16.588825592691695,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3263125,
    "MUSR": 0.45572916666666624,
    "MMLU-PRO Raw": 0.2479222074468085,
    "MMLU-PRO": 16.435800827423165,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-15",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-llama3.2-3b-v23.2-131k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-mixtral-7bx8-v18.1-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-mixtral-7bx8-v18.1-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k",
    "Model sha": "98596b6731058cc9cca85f3b8ac9077342cb60ae",
    "Average ‚¨ÜÔ∏è": 22.229103914926494,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.868875678055959,
    "IFEval Raw": 0.549347952322061,
    "IFEval": 54.9347952322061,
    "BBH Raw": 0.46561770563515265,
    "BBH": 24.535442968436797,
    "MATH Lvl 5 Raw": 0.10196374622356495,
    "MATH Lvl 5": 10.196374622356496,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.3830520833333333,
    "MUSR": 5.281510416666666,
    "MMLU-PRO Raw": 0.38040226063829785,
    "MMLU-PRO": 31.155806737588655,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-12",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-nemotron-70b-v23.1-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-nemotron-70b-v23.1-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-nemotron-70b-v23.1-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-nemotron-70b-v23.1-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-nemotron-70b-v23.1-131k",
    "Model sha": "d8cb98fb9281a84eb0df8216bae60beaf5181921",
    "Average ‚¨ÜÔ∏è": 39.080116730865974,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 24.294955092188385,
    "IFEval Raw": 0.7555275557742346,
    "IFEval": 75.55275557742345,
    "BBH Raw": 0.6749472828128272,
    "BBH": 53.18804887163517,
    "MATH Lvl 5 Raw": 0.27870090634441086,
    "MATH Lvl 5": 27.870090634441087,
    "GPQA Raw": 0.36325503355704697,
    "GPQA": 15.100671140939594,
    "MUSR Raw": 0.45375000000000004,
    "MUSR": 16.38541666666667,
    "MMLU-PRO Raw": 0.5174534574468085,
    "MMLU-PRO": 46.38371749408983,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-23",
    "Generation": 3,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-nemotron-70b-v23.2-131k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-nemotron-70b-v23.2-131k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-nemotron-70b-v23.2-131k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-nemotron-70b-v23.2-131k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-nemotron-70b-v23.2-131k",
    "Model sha": "7a39fd93b078189c6892344c2f01059320543e2f",
    "Average ‚¨ÜÔ∏è": 38.51659936266015,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.394907827173064,
    "IFEval Raw": 0.7226547782107031,
    "IFEval": 72.26547782107032,
    "BBH Raw": 0.6704805157570325,
    "BBH": 52.265661751102094,
    "MATH Lvl 5 Raw": 0.2726586102719033,
    "MATH Lvl 5": 27.26586102719033,
    "GPQA Raw": 0.3598993288590604,
    "GPQA": 14.65324384787472,
    "MUSR Raw": 0.46959375000000003,
    "MUSR": 18.86588541666667,
    "MMLU-PRO Raw": 0.5120511968085106,
    "MMLU-PRO": 45.78346631205674,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-24",
    "Generation": 3,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-qwen2.5llamaify-14b-v23.1-200k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.1-200k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.1-200k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-qwen2.5llamaify-14b-v23.1-200k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.1-200k",
    "Model sha": "001e14063e2702a9b2284dc6ec889d2586dc839b",
    "Average ‚¨ÜÔ∏è": 31.042901144572593,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4567499993965214,
    "IFEval Raw": 0.630880508162786,
    "IFEval": 63.088050816278596,
    "BBH Raw": 0.601319898776811,
    "BBH": 43.27649863201484,
    "MATH Lvl 5 Raw": 0.16465256797583083,
    "MATH Lvl 5": 16.465256797583084,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.42404166666666665,
    "MUSR": 11.538541666666669,
    "MMLU-PRO Raw": 0.4673371010638298,
    "MMLU-PRO": 40.81523345153664,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.1-200k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-qwen2.5llamaify-14b-v23.3-200k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.3-200k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.3-200k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-qwen2.5llamaify-14b-v23.3-200k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.3-200k",
    "Model sha": "0cef6f7719c1eb3bc1ebba133508c2c6d67e635c",
    "Average ‚¨ÜÔ∏è": 28.848771220664332,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4979490078097508,
    "IFEval Raw": 0.6131453432448126,
    "IFEval": 61.31453432448127,
    "BBH Raw": 0.6080855261046028,
    "BBH": 44.1839402106242,
    "MATH Lvl 5 Raw": 0.02416918429003021,
    "MATH Lvl 5": 2.416918429003021,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.4345833333333333,
    "MUSR": 12.722916666666668,
    "MMLU-PRO Raw": 0.4794714095744681,
    "MMLU-PRO": 42.16348995271868,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-qwen2.5llamaify-14b-v23.3-200k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-qwen2.5llamaify-7b-v23.1-200k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-qwen2.5llamaify-7b-v23.1-200k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-qwen2.5llamaify-7b-v23.1-200k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-qwen2.5llamaify-7b-v23.1-200k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-qwen2.5llamaify-7b-v23.1-200k",
    "Model sha": "91521abfec2a00f4853f6cb4dd620177617ca572",
    "Average ‚¨ÜÔ∏è": 26.843617296527373,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.843130530949716,
    "IFEval Raw": 0.5672582082208539,
    "IFEval": 56.725820822085396,
    "BBH Raw": 0.5509381466888461,
    "BBH": 36.3981275172541,
    "MATH Lvl 5 Raw": 0.12764350453172205,
    "MATH Lvl 5": 12.764350453172204,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.43632291666666667,
    "MUSR": 13.807031250000003,
    "MMLU-PRO Raw": 0.394780585106383,
    "MMLU-PRO": 32.75339834515366,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-10",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-yi1.5-34b-v21.3-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-yi1.5-34b-v21.3-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-yi1.5-34b-v21.3-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-yi1.5-34b-v21.3-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-yi1.5-34b-v21.3-32k",
    "Model sha": "966be6ad502cdd50a9af94d5f003aec040cdb0b5",
    "Average ‚¨ÜÔ∏è": 30.295297872302047,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.0383267111697756,
    "IFEval Raw": 0.5420041046645123,
    "IFEval": 54.20041046645124,
    "BBH Raw": 0.6162574860411373,
    "BBH": 45.637092606204,
    "MATH Lvl 5 Raw": 0.1404833836858006,
    "MATH Lvl 5": 14.04833836858006,
    "GPQA Raw": 0.348993288590604,
    "GPQA": 13.19910514541387,
    "MUSR Raw": 0.44394791666666666,
    "MUSR": 14.693489583333337,
    "MMLU-PRO Raw": 0.4599401595744681,
    "MMLU-PRO": 39.99335106382979,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-05",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-yi1.5-34b-v21.3-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-zero-14b-v22.3-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-zero-14b-v22.3-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-zero-14b-v22.3-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-zero-14b-v22.3-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-zero-14b-v22.3-32k",
    "Model sha": "d9a0b6bc02f283e154c9ad6db43a5a97eed97f5b",
    "Average ‚¨ÜÔ∏è": 19.267601743281492,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6887689138253539,
    "IFEval Raw": 0.37529200299649373,
    "IFEval": 37.529200299649375,
    "BBH Raw": 0.4859759816473639,
    "BBH": 26.289506846678147,
    "MATH Lvl 5 Raw": 0.0853474320241692,
    "MATH Lvl 5": 8.53474320241692,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.41660416666666666,
    "MUSR": 11.342187500000001,
    "MMLU-PRO Raw": 0.3187333776595745,
    "MMLU-PRO": 24.30370862884161,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-zero-14b-v22.3-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-zero-3b-v21.2-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-zero-3b-v21.2-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-zero-3b-v21.2-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-zero-3b-v21.2-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-zero-3b-v21.2-32k",
    "Model sha": "74e1d168c5e917219d668d1483f6355dd0464a31",
    "Average ‚¨ÜÔ∏è": 11.5496567674182,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8786187529837363,
    "IFEval Raw": 0.3802377691192702,
    "IFEval": 38.02377691192702,
    "BBH Raw": 0.3934791831798414,
    "BBH": 15.293406418468868,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3566354166666667,
    "MUSR": 2.2460937499999996,
    "MMLU-PRO Raw": 0.20337433510638298,
    "MMLU-PRO": 11.486037234042552,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-zero-3b-v21.2-32k"
  },
  {
    "eval_name": "OpenBuddy_openbuddy-zero-56b-v21.2-32k_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenBuddy/openbuddy-zero-56b-v21.2-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenBuddy/openbuddy-zero-56b-v21.2-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenBuddy__openbuddy-zero-56b-v21.2-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenBuddy/openbuddy-zero-56b-v21.2-32k",
    "Model sha": "c7a1a4a6e798f75d1d3219ab9ff9f2692e29f7d5",
    "Average ‚¨ÜÔ∏è": 28.233905159704353,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 56,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 7.573745947003327,
    "IFEval Raw": 0.5057092957796425,
    "IFEval": 50.57092957796425,
    "BBH Raw": 0.6128345897750148,
    "BBH": 44.796541615730554,
    "MATH Lvl 5 Raw": 0.14425981873111782,
    "MATH Lvl 5": 14.425981873111782,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.4305208333333333,
    "MUSR": 12.781770833333331,
    "MMLU-PRO Raw": 0.43991023936170215,
    "MMLU-PRO": 37.76780437352246,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-10",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "OpenBuddy/openbuddy-zero-56b-v21.2-32k"
  },
  {
    "eval_name": "OpenLeecher_llama3-8b-lima_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/OpenLeecher/llama3-8b-lima\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">OpenLeecher/llama3-8b-lima</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/OpenLeecher__llama3-8b-lima-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "OpenLeecher/llama3-8b-lima",
    "Model sha": "237a2bcb240eecd9355a091f839e42ba3d31bda5",
    "Average ‚¨ÜÔ∏è": 14.761081833869587,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9589293660241075,
    "IFEval Raw": 0.43706587410293574,
    "IFEval": 43.70658741029358,
    "BBH Raw": 0.4295828632822993,
    "BBH": 19.573064881964964,
    "MATH Lvl 5 Raw": 0.03474320241691844,
    "MATH Lvl 5": 3.474320241691844,
    "GPQA Raw": 0.23825503355704697,
    "GPQA": 0.0,
    "MUSR Raw": 0.37127083333333327,
    "MUSR": 3.7421874999999996,
    "MMLU-PRO Raw": 0.26263297872340424,
    "MMLU-PRO": 18.070330969267136,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "OpenLeecher/llama3-8b-lima"
  },
  {
    "eval_name": "Orenguteng_Llama-3.1-8B-Lexi-Uncensored_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Orenguteng/Llama-3.1-8B-Lexi-Uncensored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Orenguteng__Llama-3.1-8B-Lexi-Uncensored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored",
    "Model sha": "56ac439ab4c7826871493ffbe2d49f2100a98e97",
    "Average ‚¨ÜÔ∏è": 26.86041322229723,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8567350675600638,
    "IFEval Raw": 0.7776843220432896,
    "IFEval": 77.76843220432897,
    "BBH Raw": 0.5057261652642643,
    "BBH": 29.24254324176861,
    "MATH Lvl 5 Raw": 0.13821752265861026,
    "MATH Lvl 5": 13.821752265861026,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.3871145833333333,
    "MUSR": 6.422656250000002,
    "MMLU-PRO Raw": 0.37898936170212766,
    "MMLU-PRO": 30.99881796690307,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-26",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored"
  },
  {
    "eval_name": "Orenguteng_Llama-3.1-8B-Lexi-Uncensored-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Orenguteng__Llama-3.1-8B-Lexi-Uncensored-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2",
    "Model sha": "2340f8fbcd2452125a798686ca90b882a08fb0d9",
    "Average ‚¨ÜÔ∏è": 27.925120906061533,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 102,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8696863144847194,
    "IFEval Raw": 0.7791581891603169,
    "IFEval": 77.91581891603168,
    "BBH Raw": 0.5084008018783934,
    "BBH": 29.687032745631218,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3842916666666667,
    "MUSR": 7.76979166666667,
    "MMLU-PRO Raw": 0.3780751329787234,
    "MMLU-PRO": 30.897236997635936,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-09",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2"
  },
  {
    "eval_name": "Orion-zhen_Qwen2.5-7B-Instruct-Uncensored_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Orion-zhen/Qwen2.5-7B-Instruct-Uncensored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Orion-zhen/Qwen2.5-7B-Instruct-Uncensored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Orion-zhen__Qwen2.5-7B-Instruct-Uncensored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored",
    "Model sha": "33c24657b4394fc430ad90b5d413e5985ce8e292",
    "Average ‚¨ÜÔ∏è": 27.98971152356219,
    "Hub License": "gpl-3.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1168119424540062,
    "IFEval Raw": 0.7204317876567508,
    "IFEval": 72.04317876567507,
    "BBH Raw": 0.5473918652157296,
    "BBH": 35.83245286228819,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.43613541666666666,
    "MUSR": 13.58359375,
    "MMLU-PRO Raw": 0.4426529255319149,
    "MMLU-PRO": 38.07254728132387,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-10-19",
    "Generation": 1,
    "Base Model": "Orion-zhen/Qwen2.5-7B-Instruct-Uncensored (Merge)"
  },
  {
    "eval_name": "P0x0_Astra-v1-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/P0x0/Astra-v1-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">P0x0/Astra-v1-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/P0x0__Astra-v1-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "P0x0/Astra-v1-12B",
    "Model sha": "c706e253f8d8fa838b505cbec0e1a6aeec545abc",
    "Average ‚¨ÜÔ∏è": 19.674299882430986,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6056733720976346,
    "IFEval Raw": 0.28059437847134494,
    "IFEval": 28.05943784713449,
    "BBH Raw": 0.5214506484138984,
    "BBH": 31.80990734117942,
    "MATH Lvl 5 Raw": 0.10951661631419939,
    "MATH Lvl 5": 10.951661631419938,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.4051875,
    "MUSR": 11.381770833333329,
    "MMLU-PRO Raw": 0.3460771276595745,
    "MMLU-PRO": 27.341903073286055,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-21",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "PJMixers_LLaMa-3-CursedStock-v2.0-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PJMixers/LLaMa-3-CursedStock-v2.0-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PJMixers/LLaMa-3-CursedStock-v2.0-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PJMixers__LLaMa-3-CursedStock-v2.0-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PJMixers/LLaMa-3-CursedStock-v2.0-8B",
    "Model sha": "d47cc29df363f71ffaf6cd21ac4bdeefa27359db",
    "Average ‚¨ÜÔ∏è": 24.203898243796555,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4026920558645364,
    "IFEval Raw": 0.6330791189599152,
    "IFEval": 63.30791189599152,
    "BBH Raw": 0.527115950402997,
    "BBH": 32.56361170891586,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.38562500000000005,
    "MUSR": 8.036458333333336,
    "MMLU-PRO Raw": 0.3556349734042553,
    "MMLU-PRO": 28.40388593380615,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "PJMixers/LLaMa-3-CursedStock-v2.0-8B (Merge)"
  },
  {
    "eval_name": "PJMixers-Dev_LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PJMixers-Dev__LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B",
    "Model sha": "1286f51489b06fe67fa36d57aa87331fa37e698b",
    "Average ‚¨ÜÔ∏è": 22.626211240184926,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7136943710993237,
    "IFEval Raw": 0.693054428915278,
    "IFEval": 69.3054428915278,
    "BBH Raw": 0.4556166737589294,
    "BBH": 23.80830677255767,
    "MATH Lvl 5 Raw": 0.11706948640483385,
    "MATH Lvl 5": 11.706948640483384,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.37003125000000003,
    "MUSR": 4.053906250000002,
    "MMLU-PRO Raw": 0.312749335106383,
    "MMLU-PRO": 23.63881501182033,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.1-SFT-3B (Merge)"
  },
  {
    "eval_name": "PJMixers-Dev_LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PJMixers-Dev__LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B",
    "Model sha": "4c348a8dfc1be0b4985e0ed2882329515a60c19d",
    "Average ‚¨ÜÔ∏è": 21.67196919742406,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7098992379250731,
    "IFEval Raw": 0.6291573026237051,
    "IFEval": 62.9157302623705,
    "BBH Raw": 0.45814952191015346,
    "BBH": 23.341239903737886,
    "MATH Lvl 5 Raw": 0.12386706948640484,
    "MATH Lvl 5": 12.386706948640484,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.365875,
    "MUSR": 4.867708333333335,
    "MMLU-PRO Raw": 0.3115026595744681,
    "MMLU-PRO": 23.50029550827423,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-3B (Merge)"
  },
  {
    "eval_name": "PJMixers-Dev_LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PJMixers-Dev__LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B",
    "Model sha": "17b245cfcffcc6aadc90989bf08d9625455064e1",
    "Average ‚¨ÜÔ∏è": 21.687797523752916,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6790420787993628,
    "IFEval Raw": 0.6503898544750152,
    "IFEval": 65.03898544750152,
    "BBH Raw": 0.45107942950222196,
    "BBH": 22.288715309224642,
    "MATH Lvl 5 Raw": 0.11782477341389729,
    "MATH Lvl 5": 11.782477341389729,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.3687291666666667,
    "MUSR": 4.691145833333335,
    "MMLU-PRO Raw": 0.3107546542553192,
    "MMLU-PRO": 23.417183806146575,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMix-v0.2-SFT-HailMary-v0.1-KTO-3B (Merge)"
  },
  {
    "eval_name": "PJMixers-Dev_LLaMa-3.2-Instruct-JankMixBread-v0.1-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PJMixers-Dev/LLaMa-3.2-Instruct-JankMixBread-v0.1-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PJMixers-Dev/LLaMa-3.2-Instruct-JankMixBread-v0.1-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PJMixers-Dev__LLaMa-3.2-Instruct-JankMixBread-v0.1-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMixBread-v0.1-3B",
    "Model sha": "19faf7463cab41a2492cad26fc54b2fce3a05caf",
    "Average ‚¨ÜÔ∏è": 19.57365039781563,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7024275750432208,
    "IFEval Raw": 0.5040858256093831,
    "IFEval": 50.408582560938314,
    "BBH Raw": 0.4483158594793648,
    "BBH": 22.759588390933697,
    "MATH Lvl 5 Raw": 0.12084592145015106,
    "MATH Lvl 5": 12.084592145015106,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3515520833333334,
    "MUSR": 4.677343750000003,
    "MMLU-PRO Raw": 0.308344414893617,
    "MMLU-PRO": 23.149379432624112,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "PJMixers-Dev/LLaMa-3.2-Instruct-JankMixBread-v0.1-3B (Merge)"
  },
  {
    "eval_name": "Parissa3_test-model_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Parissa3/test-model\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Parissa3/test-model</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Parissa3__test-model-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Parissa3/test-model",
    "Model sha": "7021138dac98d930f1ce0ebe186583c0813d6f48",
    "Average ‚¨ÜÔ∏è": 20.75850572801048,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.47335290291140303,
    "IFEval Raw": 0.3882564927725103,
    "IFEval": 38.82564927725103,
    "BBH Raw": 0.5193916761801759,
    "BBH": 32.83903240379116,
    "MATH Lvl 5 Raw": 0.06570996978851963,
    "MATH Lvl 5": 6.570996978851963,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.46853125,
    "MUSR": 17.53307291666667,
    "MMLU-PRO Raw": 0.3056848404255319,
    "MMLU-PRO": 22.853871158392433,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "Parissa3/test-model (Merge)"
  },
  {
    "eval_name": "PocketDoc_Dans-Instruct-CoreCurriculum-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PocketDoc/Dans-Instruct-CoreCurriculum-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PocketDoc/Dans-Instruct-CoreCurriculum-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PocketDoc__Dans-Instruct-CoreCurriculum-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PocketDoc/Dans-Instruct-CoreCurriculum-12b",
    "Model sha": "c50db5ba880b7edc0efd32a7f3b9d2f051c3f4a6",
    "Average ‚¨ÜÔ∏è": 9.402823710265748,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.588538152037204,
    "IFEval Raw": 0.21914520139895477,
    "IFEval": 21.914520139895476,
    "BBH Raw": 0.3788739075240266,
    "BBH": 13.232564953040013,
    "MATH Lvl 5 Raw": 0.04909365558912387,
    "MATH Lvl 5": 4.909365558912387,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4095625,
    "MUSR": 9.561979166666669,
    "MMLU-PRO Raw": 0.1219248670212766,
    "MMLU-PRO": 2.4360963356973993,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-01",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "PocketDoc_Dans-PersonalityEngine-v1.0.0-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PocketDoc/Dans-PersonalityEngine-v1.0.0-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PocketDoc/Dans-PersonalityEngine-v1.0.0-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PocketDoc__Dans-PersonalityEngine-v1.0.0-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PocketDoc/Dans-PersonalityEngine-v1.0.0-8b",
    "Model sha": "c64612e1eee1ddb3aa064a25eba8921ec3d94325",
    "Average ‚¨ÜÔ∏è": 18.779420223036123,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9009742584093858,
    "IFEval Raw": 0.498190357141274,
    "IFEval": 49.81903571412741,
    "BBH Raw": 0.47325544259149366,
    "BBH": 25.68795976908214,
    "MATH Lvl 5 Raw": 0.05589123867069487,
    "MATH Lvl 5": 5.589123867069487,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.35415625,
    "MUSR": 3.9361979166666683,
    "MMLU-PRO Raw": 0.3065159574468085,
    "MMLU-PRO": 22.94621749408983,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "PocketDoc/Dans-PersonalityEngine-v1.0.0-8b (Merge)"
  },
  {
    "eval_name": "PranavHarshan_LaMistral-V4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PranavHarshan/LaMistral-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PranavHarshan/LaMistral-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PranavHarshan__LaMistral-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PranavHarshan/LaMistral-V4",
    "Model sha": "b373c2a1ab08823b6b119899f807793c96ef7888",
    "Average ‚¨ÜÔ∏è": 24.21076517685928,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6972323700900078,
    "IFEval Raw": 0.623861354539289,
    "IFEval": 62.38613545392891,
    "BBH Raw": 0.5184255342586473,
    "BBH": 31.091348681794813,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.3642916666666667,
    "MUSR": 5.6364583333333345,
    "MMLU-PRO Raw": 0.35987367021276595,
    "MMLU-PRO": 28.874852245862886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "PranavHarshan/LaMistral-V4 (Merge)"
  },
  {
    "eval_name": "PranavHarshan_MedNarra-X1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PranavHarshan/MedNarra-X1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PranavHarshan/MedNarra-X1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PranavHarshan__MedNarra-X1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PranavHarshan/MedNarra-X1",
    "Model sha": "9fe294e7fd69ec56f0b7fa1a23759eed070f44bf",
    "Average ‚¨ÜÔ∏è": 18.128681543434002,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6761609388675446,
    "IFEval Raw": 0.43384331351924005,
    "IFEval": 43.384331351924004,
    "BBH Raw": 0.46371668179774184,
    "BBH": 23.52349513234211,
    "MATH Lvl 5 Raw": 0.04682779456193354,
    "MATH Lvl 5": 4.682779456193354,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.35403125,
    "MUSR": 2.4539062499999997,
    "MMLU-PRO Raw": 0.34308510638297873,
    "MMLU-PRO": 27.009456264775412,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "PranavHarshan/MedNarra-X1 (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_10.7B_48Layers-Appended_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Appended\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Appended</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_10.7B_48Layers-Appended-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Appended",
    "Model sha": "1091b30480f4cc91f26cb1bd7579e527f490f8d2",
    "Average ‚¨ÜÔ∏è": 22.71069985585982,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8360996102835094,
    "IFEval Raw": 0.5960595663949432,
    "IFEval": 59.60595663949432,
    "BBH Raw": 0.4619637884426022,
    "BBH": 24.057172512288606,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.42540625,
    "MUSR": 11.77578125,
    "MMLU-PRO Raw": 0.3289561170212766,
    "MMLU-PRO": 25.439568557919618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-31",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Appended (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_10.7B_48Layers-Interleaved_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Interleaved\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Interleaved</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_10.7B_48Layers-Interleaved-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Interleaved",
    "Model sha": "dd6bd9a8a9a2223a02a4e8aa6270accbc8d4d81a",
    "Average ‚¨ÜÔ∏è": 22.65911348895351,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8365921760063044,
    "IFEval Raw": 0.5960595663949432,
    "IFEval": 59.60595663949432,
    "BBH Raw": 0.4619637884426022,
    "BBH": 24.057172512288606,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42540625,
    "MUSR": 11.77578125,
    "MMLU-PRO Raw": 0.3298703457446808,
    "MMLU-PRO": 25.541149527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-10",
    "Submission Date": "2024-08-16",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_10.7B_48Layers-Interleaved (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_32K-PoSE_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_32K-PoSE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_32K-PoSE</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_32K-PoSE-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_32K-PoSE",
    "Model sha": "da6a73abac7fba68f1df4d42485d79553e97bf91",
    "Average ‚¨ÜÔ∏è": 12.702270280359086,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.46039877635090204,
    "IFEval Raw": 0.3968991165662664,
    "IFEval": 39.68991165662664,
    "BBH Raw": 0.3471309425137119,
    "BBH": 8.828394853721203,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.42054166666666665,
    "MUSR": 11.334375000000001,
    "MMLU-PRO Raw": 0.203125,
    "MMLU-PRO": 11.458333333333332,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-02",
    "Submission Date": "2024-11-02",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_32K-PoSE (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_8.11B_36Layers-Appended_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Appended\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Appended</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_8.11B_36Layers-Appended-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Appended",
    "Model sha": "e957847e013bdd2f6e852b8a1c369ddce92fca78",
    "Average ‚¨ÜÔ∏è": 22.736095435883787,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6837071414696949,
    "IFEval Raw": 0.5975833011963811,
    "IFEval": 59.75833011963812,
    "BBH Raw": 0.4619637884426022,
    "BBH": 24.057172512288606,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.42540625,
    "MUSR": 11.77578125,
    "MMLU-PRO Raw": 0.3289561170212766,
    "MMLU-PRO": 25.439568557919618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-26",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Appended (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_8.11B_36Layers-Interleaved_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Interleaved\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Interleaved</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_8.11B_36Layers-Interleaved-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Interleaved",
    "Model sha": "485ebe835c6c001af0a1a6e0e40aab27bc195842",
    "Average ‚¨ÜÔ∏è": 22.617724600064623,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6604545756311642,
    "IFEval Raw": 0.5960595663949432,
    "IFEval": 59.60595663949432,
    "BBH Raw": 0.46213045510926887,
    "BBH": 24.075505845621944,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42407291666666663,
    "MUSR": 11.509114583333334,
    "MMLU-PRO Raw": 0.3298703457446808,
    "MMLU-PRO": 25.541149527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-10",
    "Submission Date": "2024-08-16",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_8.11B_36Layers-Interleaved (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_8.99B_40Layers-Appended_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Appended\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Appended</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_8.99B_40Layers-Appended-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Appended",
    "Model sha": "2120720b7fb2ecc27b9c03cc876316fd25b26e40",
    "Average ‚¨ÜÔ∏è": 22.71069985585982,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7192493966122806,
    "IFEval Raw": 0.5960595663949432,
    "IFEval": 59.60595663949432,
    "BBH Raw": 0.4619637884426022,
    "BBH": 24.057172512288606,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.42540625,
    "MUSR": 11.77578125,
    "MMLU-PRO Raw": 0.3289561170212766,
    "MMLU-PRO": 25.439568557919618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-26",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Appended (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_8.99B_40Layers-Interleaved_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Interleaved\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Interleaved</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_8.99B_40Layers-Interleaved-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Interleaved",
    "Model sha": "b6dfa36a99179674706d5e859714afa6b8743640",
    "Average ‚¨ÜÔ∏è": 22.643120180088587,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7278113725071576,
    "IFEval Raw": 0.5975833011963811,
    "IFEval": 59.75833011963812,
    "BBH Raw": 0.46213045510926887,
    "BBH": 24.075505845621944,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42407291666666663,
    "MUSR": 11.509114583333334,
    "MMLU-PRO Raw": 0.3298703457446808,
    "MMLU-PRO": 25.541149527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-10",
    "Submission Date": "2024-08-16",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_8.99B_40Layers-Interleaved (Merge)"
  },
  {
    "eval_name": "Pretergeek_OpenChat-3.5-0106_9.86B_44Layers-Appended_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/OpenChat-3.5-0106_9.86B_44Layers-Appended\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/OpenChat-3.5-0106_9.86B_44Layers-Appended</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__OpenChat-3.5-0106_9.86B_44Layers-Appended-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/OpenChat-3.5-0106_9.86B_44Layers-Appended",
    "Model sha": "8a7ef4a2c4faf8760650e26e44509920bace633a",
    "Average ‚¨ÜÔ∏è": 22.71069985585982,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7867630512271585,
    "IFEval Raw": 0.5960595663949432,
    "IFEval": 59.60595663949432,
    "BBH Raw": 0.4619637884426022,
    "BBH": 24.057172512288606,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.42540625,
    "MUSR": 11.77578125,
    "MMLU-PRO Raw": 0.3289561170212766,
    "MMLU-PRO": 25.439568557919618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "Pretergeek/OpenChat-3.5-0106_9.86B_44Layers-Appended (Merge)"
  },
  {
    "eval_name": "Pretergeek_openchat-3.5-0106_Rebased_Mistral-7B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Pretergeek/openchat-3.5-0106_Rebased_Mistral-7B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Pretergeek/openchat-3.5-0106_Rebased_Mistral-7B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Pretergeek__openchat-3.5-0106_Rebased_Mistral-7B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Pretergeek/openchat-3.5-0106_Rebased_Mistral-7B-v0.2",
    "Model sha": "31c11027a7320115af1e5c33b41bcace83420fe2",
    "Average ‚¨ÜÔ∏è": 16.027100154079797,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6218379317378522,
    "IFEval Raw": 0.37062106322335847,
    "IFEval": 37.06210632233585,
    "BBH Raw": 0.36271140677296004,
    "BBH": 10.910767600799835,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.4840104166666667,
    "MUSR": 20.56796875,
    "MMLU-PRO Raw": 0.2829953457446808,
    "MMLU-PRO": 20.33281619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-21",
    "Submission Date": "2024-07-21",
    "Generation": 0,
    "Base Model": "Pretergeek/openchat-3.5-0106_Rebased_Mistral-7B-v0.2"
  },
  {
    "eval_name": "PygmalionAI_pygmalion-6b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTJForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/PygmalionAI/pygmalion-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">PygmalionAI/pygmalion-6b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/PygmalionAI__pygmalion-6b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "PygmalionAI/pygmalion-6b",
    "Model sha": "2a0d74449c8fbf0378194e95f64aa92e16297294",
    "Average ‚¨ÜÔ∏è": 5.392359658909203,
    "Hub License": "creativeml-openrail-m",
    "Hub ‚ù§Ô∏è": 732,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 31.923119280479877,
    "IFEval Raw": 0.20910406610016974,
    "IFEval": 20.910406610016974,
    "BBH Raw": 0.31988944643860034,
    "BBH": 5.089577143988909,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.3683541666666667,
    "MUSR": 3.7109374999999996,
    "MMLU-PRO Raw": 0.11835106382978723,
    "MMLU-PRO": 2.0390070921985806,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-01-07",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "PygmalionAI/pygmalion-6b"
  },
  {
    "eval_name": "Q-bert_MetaMath-1B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Q-bert/MetaMath-1B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Q-bert/MetaMath-1B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Q-bert__MetaMath-1B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Q-bert/MetaMath-1B",
    "Model sha": "da62756f069aba78d07d4c76108e246cb91dbc35",
    "Average ‚¨ÜÔ∏è": 11.32424791067333,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4650279939316764,
    "IFEval Raw": 0.5300391849182392,
    "IFEval": 53.00391849182392,
    "BBH Raw": 0.34506863677929517,
    "BBH": 8.434610644832558,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.3289166666666667,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.1495179521276596,
    "MMLU-PRO": 5.501994680851065,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Qwen_Qwen1.5-0.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-0.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-0.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-0.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-0.5B",
    "Model sha": "8f445e3628f3500ee69f24e1303c9f10f5342a39",
    "Average ‚¨ÜÔ∏è": 5.137017087672389,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 143,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9787373735507419,
    "IFEval Raw": 0.17056077873375977,
    "IFEval": 17.056077873375976,
    "BBH Raw": 0.3153538659142558,
    "BBH": 5.035475836799366,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.36162500000000003,
    "MUSR": 4.303125,
    "MMLU-PRO Raw": 0.1307347074468085,
    "MMLU-PRO": 3.4149674940898342,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-0.5B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-0.5B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-0.5B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-0.5B-Chat",
    "Model sha": "4d14e384a4b037942bb3f3016665157c8bcb70ea",
    "Average ‚¨ÜÔ∏è": 5.564869039793773,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 74,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5497443903172499,
    "IFEval Raw": 0.18072713732895385,
    "IFEval": 18.072713732895387,
    "BBH Raw": 0.3166662152036714,
    "BBH": 4.318032636938059,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.3837083333333333,
    "MUSR": 6.063541666666667,
    "MMLU-PRO Raw": 0.12125997340425532,
    "MMLU-PRO": 2.362219267139479,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-31",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-0.5B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-1.8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-1.8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-1.8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-1.8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-1.8B",
    "Model sha": "7846de7ed421727b318d6605a0bfab659da2c067",
    "Average ‚¨ÜÔ∏è": 9.181375704374858,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 43,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9488707865996205,
    "IFEval Raw": 0.2154239639711521,
    "IFEval": 21.542396397115212,
    "BBH Raw": 0.3476121558366305,
    "BBH": 9.759901587727937,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.36051041666666667,
    "MUSR": 3.963802083333334,
    "MMLU-PRO Raw": 0.18816489361702127,
    "MMLU-PRO": 9.79609929078014,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-1.8B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-1.8B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-1.8B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-1.8B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-1.8B-Chat",
    "Model sha": "e482ee3f73c375a627a16fdf66fd0c8279743ca6",
    "Average ‚¨ÜÔ∏è": 9.006021162921042,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 48,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5643288573973027,
    "IFEval Raw": 0.20190982149585324,
    "IFEval": 20.190982149585324,
    "BBH Raw": 0.3255912875735599,
    "BBH": 5.908662877770453,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.42596875,
    "MUSR": 12.179427083333335,
    "MMLU-PRO Raw": 0.18035239361702127,
    "MMLU-PRO": 8.928043735224584,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-1.8B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-110B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-110B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-110B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-110B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-110B",
    "Model sha": "16659038ecdcc771c1293cf47020fa7cc2750ee8",
    "Average ‚¨ÜÔ∏è": 29.846265621686655,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 91,
    "#Params (B)": 111,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 71.27088842602943,
    "IFEval Raw": 0.3421942667677318,
    "IFEval": 34.21942667677318,
    "BBH Raw": 0.6099964981780978,
    "BBH": 44.28047655387545,
    "MATH Lvl 5 Raw": 0.2477341389728097,
    "MATH Lvl 5": 24.773413897280967,
    "GPQA Raw": 0.3523489932885906,
    "GPQA": 13.646532438478745,
    "MUSR Raw": 0.44084375,
    "MUSR": 13.705468750000001,
    "MMLU-PRO Raw": 0.5360704787234043,
    "MMLU-PRO": 48.45227541371159,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-110B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-110B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-110B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-110B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-110B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-110B-Chat",
    "Model sha": "85f86cec25901f2dbd870a86e06756903c9a876a",
    "Average ‚¨ÜÔ∏è": 29.224836684325613,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 123,
    "#Params (B)": 111,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 72.5652930561157,
    "IFEval Raw": 0.5938864435254014,
    "IFEval": 59.388644352540155,
    "BBH Raw": 0.6183800385588633,
    "BBH": 44.98454525616634,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.45216666666666666,
    "MUSR": 16.287499999999994,
    "MMLU-PRO Raw": 0.48246343085106386,
    "MMLU-PRO": 42.495936761229316,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-110B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-14B",
    "Model sha": "dce4b190d34470818e5bec2a92cb8233aaa02ca2",
    "Average ‚¨ÜÔ∏è": 20.51420090838204,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.92549058647731,
    "IFEval Raw": 0.2905368865720732,
    "IFEval": 29.05368865720732,
    "BBH Raw": 0.5080327493808331,
    "BBH": 30.063103282917453,
    "MATH Lvl 5 Raw": 0.18202416918429004,
    "MATH Lvl 5": 18.202416918429005,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.41864583333333333,
    "MUSR": 10.464062500000002,
    "MMLU-PRO Raw": 0.36436170212765956,
    "MMLU-PRO": 29.373522458628837,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-14B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-14B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-14B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-14B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-14B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-14B-Chat",
    "Model sha": "9492b22871f43e975435455f5c616c77fe7a50ec",
    "Average ‚¨ÜÔ∏è": 21.02330687787111,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 110,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3384660100072798,
    "IFEval Raw": 0.47680820223673187,
    "IFEval": 47.68082022367319,
    "BBH Raw": 0.5228587510703555,
    "BBH": 32.75647930053065,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.43997916666666664,
    "MUSR": 13.930729166666667,
    "MMLU-PRO Raw": 0.36178523936170215,
    "MMLU-PRO": 29.08724881796691,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-14B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-32B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-32B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-32B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-32B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-32B",
    "Model sha": "cefef80dc06a65f89d1d71d0adbc56d335ca2490",
    "Average ‚¨ÜÔ∏è": 27.021817287170666,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 81,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 59.96715941585337,
    "IFEval Raw": 0.329729562006587,
    "IFEval": 32.97295620065869,
    "BBH Raw": 0.5715390555959325,
    "BBH": 38.980351633108974,
    "MATH Lvl 5 Raw": 0.2862537764350453,
    "MATH Lvl 5": 28.625377643504528,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.4277916666666666,
    "MUSR": 12.040625000000004,
    "MMLU-PRO Raw": 0.4499667553191489,
    "MMLU-PRO": 38.88519503546098,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-01",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-32B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-32B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-32B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-32B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-32B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-32B-Chat",
    "Model sha": "0997b012af6ddd5465d40465a8415535b2f06cfc",
    "Average ‚¨ÜÔ∏è": 27.19301707049657,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 106,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 46.0594497310076,
    "IFEval Raw": 0.5532199009738605,
    "IFEval": 55.32199009738605,
    "BBH Raw": 0.6066899757930234,
    "BBH": 44.55485402391639,
    "MATH Lvl 5 Raw": 0.0717522658610272,
    "MATH Lvl 5": 7.1752265861027205,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4159791666666666,
    "MUSR": 10.197395833333335,
    "MMLU-PRO Raw": 0.4457280585106383,
    "MMLU-PRO": 38.41422872340425,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-32B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-4B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-4B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-4B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-4B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-4B",
    "Model sha": "a66363a0c24e2155c561e4b53c658b1d3965474e",
    "Average ‚¨ÜÔ∏è": 11.327598669897498,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 33,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.638682166903881,
    "IFEval Raw": 0.24447466056729478,
    "IFEval": 24.447466056729475,
    "BBH Raw": 0.40538970296725463,
    "BBH": 16.249142581095292,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.3604479166666667,
    "MUSR": 4.8226562500000005,
    "MMLU-PRO Raw": 0.24601063829787234,
    "MMLU-PRO": 16.22340425531915,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-4B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-4B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-4B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-4B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-4B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-4B-Chat",
    "Model sha": "a7a4d4945d28bac955554c9abd2f74a71ebbf22f",
    "Average ‚¨ÜÔ∏è": 12.337753423984099,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 38,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8661506478387992,
    "IFEval Raw": 0.31566576683200576,
    "IFEval": 31.566576683200577,
    "BBH Raw": 0.40055485611486114,
    "BBH": 16.29707852890831,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.39778125,
    "MUSR": 7.355989583333333,
    "MMLU-PRO Raw": 0.23961103723404256,
    "MMLU-PRO": 15.512337470449172,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-4B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-7B",
    "Model sha": "831096e3a59a0789a541415da25ef195ceb802fe",
    "Average ‚¨ÜÔ∏è": 15.357503964067982,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 45,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8273540540952402,
    "IFEval Raw": 0.2684299879874289,
    "IFEval": 26.842998798742894,
    "BBH Raw": 0.4559896407693445,
    "BBH": 23.075768754340448,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4103333333333334,
    "MUSR": 9.158333333333333,
    "MMLU-PRO Raw": 0.29163896276595747,
    "MMLU-PRO": 21.293218085106382,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-7B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-7B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-7B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-7B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-7B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-7B-Chat",
    "Model sha": "5f4f5e69ac7f1d508f8369e977de208b4803444b",
    "Average ‚¨ÜÔ∏è": 16.57617293158245,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 162,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.07882659669136,
    "IFEval Raw": 0.43711574178734647,
    "IFEval": 43.711574178734644,
    "BBH Raw": 0.4510053116521351,
    "BBH": 22.379129599952787,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.37790624999999994,
    "MUSR": 4.6382812499999995,
    "MMLU-PRO Raw": 0.2951296542553192,
    "MMLU-PRO": 21.681072695035464,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-7B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen1.5-MoE-A2.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-MoE-A2.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-MoE-A2.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-MoE-A2.7B",
    "Model sha": "1a758c50ecb6350748b9ce0a99d2352fd9fc11c9",
    "Average ‚¨ÜÔ∏è": 12.42275797734545,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 195,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.54561289606972,
    "IFEval Raw": 0.265982038768246,
    "IFEval": 26.598203876824606,
    "BBH Raw": 0.4113515433010766,
    "BBH": 18.837858500547185,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.40134375000000005,
    "MUSR": 7.967968750000003,
    "MMLU-PRO Raw": 0.2777593085106383,
    "MMLU-PRO": 19.751034278959807,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-29",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-MoE-A2.7B"
  },
  {
    "eval_name": "Qwen_Qwen1.5-MoE-A2.7B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen1.5-MoE-A2.7B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen1.5-MoE-A2.7B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen1.5-MoE-A2.7B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen1.5-MoE-A2.7B-Chat",
    "Model sha": "ec052fda178e241c7c443468d2fa1db6618996be",
    "Average ‚¨ÜÔ∏è": 14.823498043433531,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 112,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.901971537535811,
    "IFEval Raw": 0.37953851336675576,
    "IFEval": 37.95385133667558,
    "BBH Raw": 0.4272088620635824,
    "BBH": 20.041818895540953,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.38987499999999997,
    "MUSR": 6.334375000000001,
    "MMLU-PRO Raw": 0.29230385638297873,
    "MMLU-PRO": 21.3670951536643,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-14",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen1.5-MoE-A2.7B-Chat"
  },
  {
    "eval_name": "Qwen_Qwen2-0.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-0.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-0.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-0.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-0.5B",
    "Model sha": "ff3a49fac17555b8dfc4db6709f480cc8f16a9fe",
    "Average ‚¨ÜÔ∏è": 7.1126352248636,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 112,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9675150826302632,
    "IFEval Raw": 0.1867223411658843,
    "IFEval": 18.672234116588427,
    "BBH Raw": 0.32533196207107246,
    "BBH": 7.994201896754274,
    "MATH Lvl 5 Raw": 0.028700906344410873,
    "MATH Lvl 5": 2.8700906344410875,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.37520833333333337,
    "MUSR": 4.601041666666668,
    "MMLU-PRO Raw": 0.1697972074468085,
    "MMLU-PRO": 7.7552452718676115,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen2-0.5B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-0.5B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-0.5B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-0.5B-Instruct",
    "Model sha": "c291d6fce4804a1d39305f388dd32897d1f7acc4",
    "Average ‚¨ÜÔ∏è": 6.41054699783957,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 163,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5578477231452632,
    "IFEval Raw": 0.22466610814860127,
    "IFEval": 22.466610814860125,
    "BBH Raw": 0.31725179384863494,
    "BBH": 5.876044259408482,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.33527083333333335,
    "MUSR": 2.408854166666666,
    "MMLU-PRO Raw": 0.15309175531914893,
    "MMLU-PRO": 5.89908392434988,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen2-1.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-1.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-1.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-1.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-1.5B",
    "Model sha": "8a16abf2848eda07cc5253dec660bf1ce007ad7a",
    "Average ‚¨ÜÔ∏è": 10.445452935561454,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 80,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1081948920047158,
    "IFEval Raw": 0.21132705665412216,
    "IFEval": 21.132705665412217,
    "BBH Raw": 0.35747931720577464,
    "BBH": 11.781833653483531,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.36581250000000004,
    "MUSR": 3.5932291666666667,
    "MMLU-PRO Raw": 0.2551529255319149,
    "MMLU-PRO": 17.239213947990542,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-1.5B"
  },
  {
    "eval_name": "Qwen_Qwen2-1.5B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-1.5B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-1.5B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-1.5B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-1.5B-Instruct",
    "Model sha": "ba1cf1846d7df0a0591d6c00649f57e798519da8",
    "Average ‚¨ÜÔ∏è": 13.990879413369,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 131,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6588239385562741,
    "IFEval Raw": 0.3371232773485463,
    "IFEval": 33.712327734854625,
    "BBH Raw": 0.3852232408376059,
    "BBH": 13.695346827502663,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.42928125,
    "MUSR": 12.026822916666667,
    "MMLU-PRO Raw": 0.25008311170212766,
    "MMLU-PRO": 16.675901300236408,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-1.5B-Instruct"
  },
  {
    "eval_name": "Qwen_Qwen2-57B-A14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-57B-A14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-57B-A14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-57B-A14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-57B-A14B",
    "Model sha": "973e466c39ba76372a2ae464dbca0af3f5a5a2a9",
    "Average ‚¨ÜÔ∏è": 25.0338731324107,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 48,
    "#Params (B)": 57,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 107.03147746473374,
    "IFEval Raw": 0.31126965340851165,
    "IFEval": 31.126965340851164,
    "BBH Raw": 0.5618204938684165,
    "BBH": 38.87598905034189,
    "MATH Lvl 5 Raw": 0.1865558912386707,
    "MATH Lvl 5": 18.65558912386707,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.417375,
    "MUSR": 10.538541666666669,
    "MMLU-PRO Raw": 0.4916057180851064,
    "MMLU-PRO": 43.511746453900706,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-57B-A14B"
  },
  {
    "eval_name": "Qwen_Qwen2-57B-A14B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-57B-A14B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-57B-A14B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-57B-A14B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-57B-A14B-Instruct",
    "Model sha": "5ea455a449e61a92a5b194ee06be807647d3e8b5",
    "Average ‚¨ÜÔ∏è": 29.780722801392006,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 77,
    "#Params (B)": 57,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 42.5062475032037,
    "IFEval Raw": 0.6337783747124297,
    "IFEval": 63.37783747124297,
    "BBH Raw": 0.5887606963532052,
    "BBH": 41.785917734842535,
    "MATH Lvl 5 Raw": 0.08761329305135951,
    "MATH Lvl 5": 8.76132930513595,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.43613541666666666,
    "MUSR": 14.183593749999995,
    "MMLU-PRO Raw": 0.45752992021276595,
    "MMLU-PRO": 39.725546690307326,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-08-14",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-57B-A14B"
  },
  {
    "eval_name": "Qwen_Qwen2-72B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-72B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-72B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-72B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-72B",
    "Model sha": "87993795c78576318087f70b43fbf530eb7789e7",
    "Average ‚¨ÜÔ∏è": 35.48184716610958,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 190,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 64.06227923986644,
    "IFEval Raw": 0.3823610243044012,
    "IFEval": 38.23610243044012,
    "BBH Raw": 0.661734029856643,
    "BBH": 51.85613118695519,
    "MATH Lvl 5 Raw": 0.3126888217522659,
    "MATH Lvl 5": 31.268882175226594,
    "GPQA Raw": 0.39429530201342283,
    "GPQA": 19.239373601789712,
    "MUSR Raw": 0.47036458333333336,
    "MUSR": 19.728906250000005,
    "MMLU-PRO Raw": 0.5730551861702128,
    "MMLU-PRO": 52.56168735224587,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "Qwen_Qwen2-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-72B-Instruct",
    "Model sha": "1af63c698f59c4235668ec9c1395468cb7cd7e79",
    "Average ‚¨ÜÔ∏è": 42.91430415552085,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 677,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 37.55397442269999,
    "IFEval Raw": 0.7989168738945996,
    "IFEval": 79.89168738945996,
    "BBH Raw": 0.697730968386067,
    "BBH": 57.48300911876294,
    "MATH Lvl 5 Raw": 0.3768882175226586,
    "MATH Lvl 5": 37.68882175226586,
    "GPQA Raw": 0.3724832214765101,
    "GPQA": 16.33109619686801,
    "MUSR Raw": 0.4560104166666667,
    "MUSR": 17.167968749999996,
    "MMLU-PRO Raw": 0.5403091755319149,
    "MMLU-PRO": 48.92324172576833,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "Qwen_Qwen2-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-7B",
    "Model sha": "453ed1575b739b5b03ce3758b23befdb0967f40e",
    "Average ‚¨ÜÔ∏è": 23.93775025730012,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 137,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.280582255575841,
    "IFEval Raw": 0.3148667757106699,
    "IFEval": 31.48667757106699,
    "BBH Raw": 0.531531595001889,
    "BBH": 34.711136202753416,
    "MATH Lvl 5 Raw": 0.20468277945619337,
    "MATH Lvl 5": 20.468277945619338,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.4439166666666667,
    "MUSR": 14.322916666666666,
    "MMLU-PRO Raw": 0.41830119680851063,
    "MMLU-PRO": 35.366799645390074,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "Qwen_Qwen2-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-7B-Instruct",
    "Model sha": "41c66b0be1c3081f13defc6bdf946c2ef240d6a6",
    "Average ‚¨ÜÔ∏è": 24.90295162911335,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 588,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0420389858574919,
    "IFEval Raw": 0.5679075962889577,
    "IFEval": 56.79075962889577,
    "BBH Raw": 0.5544781563793189,
    "BBH": 37.80839092310167,
    "MATH Lvl 5 Raw": 0.0944108761329305,
    "MATH Lvl 5": 9.44108761329305,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.39279166666666665,
    "MUSR": 7.3656250000000005,
    "MMLU-PRO Raw": 0.38472406914893614,
    "MMLU-PRO": 31.636007683215123,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "Qwen_Qwen2-Math-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-Math-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-Math-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-Math-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-Math-72B-Instruct",
    "Model sha": "5c267882f3377bcfc35882f8609098a894eeeaa8",
    "Average ‚¨ÜÔ∏è": 35.15086636785174,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 85,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.168247694827105,
    "IFEval Raw": 0.569381463405985,
    "IFEval": 56.93814634059851,
    "BBH Raw": 0.634337660025181,
    "BBH": 47.96019950734914,
    "MATH Lvl 5 Raw": 0.38141993957703924,
    "MATH Lvl 5": 38.141993957703924,
    "GPQA Raw": 0.36828859060402686,
    "GPQA": 15.771812080536915,
    "MUSR Raw": 0.45169791666666664,
    "MUSR": 15.728906249999994,
    "MMLU-PRO Raw": 0.42727726063829785,
    "MMLU-PRO": 36.364140070921984,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-08",
    "Submission Date": "2024-08-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-Math-72B-Instruct"
  },
  {
    "eval_name": "Qwen_Qwen2-Math-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-Math-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-Math-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-Math-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-Math-7B",
    "Model sha": "47a44ff4136da8960adbab02b2326787086bcf6c",
    "Average ‚¨ÜÔ∏è": 11.941392447110303,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.563036062905102,
    "IFEval Raw": 0.2687048143370701,
    "IFEval": 26.870481433707006,
    "BBH Raw": 0.386954741074792,
    "BBH": 14.064494488871304,
    "MATH Lvl 5 Raw": 0.24320241691842898,
    "MATH Lvl 5": 24.3202416918429,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.35933333333333334,
    "MUSR": 2.4166666666666683,
    "MMLU-PRO Raw": 0.1196808510638298,
    "MMLU-PRO": 2.186761229314421,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-08",
    "Submission Date": "2024-08-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-Math-7B"
  },
  {
    "eval_name": "Qwen_Qwen2-VL-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üå∏ multimodal",
    "T": "üå∏",
    "Weight type": "Original",
    "Architecture": "Qwen2VLForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-VL-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-VL-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-VL-72B-Instruct",
    "Model sha": "f400120e59a6196b024298b7d09fb517f742db7d",
    "Average ‚¨ÜÔ∏è": 37.91275288601192,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 167,
    "#Params (B)": 73,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 27.249716515600202,
    "IFEval Raw": 0.5982326892644849,
    "IFEval": 59.82326892644849,
    "BBH Raw": 0.6946287292338682,
    "BBH": 56.3112338791251,
    "MATH Lvl 5 Raw": 0.24697885196374622,
    "MATH Lvl 5": 24.69788519637462,
    "GPQA Raw": 0.3875838926174497,
    "GPQA": 18.34451901565996,
    "MUSR Raw": 0.44921875,
    "MUSR": 15.885677083333329,
    "MMLU-PRO Raw": 0.5717253989361702,
    "MMLU-PRO": 52.41393321513003,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-10-20",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-VL-72B-Instruct"
  },
  {
    "eval_name": "Qwen_Qwen2-VL-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üå∏ multimodal",
    "T": "üå∏",
    "Weight type": "Original",
    "Architecture": "Qwen2VLForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2-VL-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2-VL-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2-VL-7B-Instruct",
    "Model sha": "51c47430f97dd7c74aa1fa6825e68a813478097f",
    "Average ‚¨ÜÔ∏è": 24.214809619420123,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 834,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0543822847239583,
    "IFEval Raw": 0.4599218961245052,
    "IFEval": 45.99218961245052,
    "BBH Raw": 0.5464507159069989,
    "BBH": 35.87710314498947,
    "MATH Lvl 5 Raw": 0.06193353474320241,
    "MATH Lvl 5": 6.193353474320241,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.4375,
    "MUSR": 13.554166666666669,
    "MMLU-PRO Raw": 0.40949135638297873,
    "MMLU-PRO": 34.38792848699764,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-28",
    "Submission Date": "2024-10-20",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2-VL-7B-Instruct"
  },
  {
    "eval_name": "Qwen_Qwen2.5-0.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-0.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-0.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-0.5B",
    "Model sha": "2630d3d2321bc1f1878f702166d1b2af019a7310",
    "Average ‚¨ÜÔ∏è": 6.310893394760252,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 112,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1653426288183506,
    "IFEval Raw": 0.16271714606133947,
    "IFEval": 16.271714606133948,
    "BBH Raw": 0.32748148151196615,
    "BBH": 6.953961634882263,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.3433333333333333,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.19057513297872342,
    "MMLU-PRO": 10.0639036643026,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-0.5B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-0.5B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-0.5B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-0.5B-Instruct",
    "Model sha": "a8b602d9dafd3a75d382e62757d83d89fca3be54",
    "Average ‚¨ÜÔ∏è": 8.140647319276075,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 104,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6308244528197698,
    "IFEval Raw": 0.307122878407071,
    "IFEval": 30.712287840707102,
    "BBH Raw": 0.3340729214937266,
    "BBH": 8.434863610588833,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.33288541666666666,
    "MUSR": 0.9440104166666662,
    "MMLU-PRO Raw": 0.16971409574468085,
    "MMLU-PRO": 7.7460106382978715,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-0.5B-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-0.5B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-0.5B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-0.5B-Instruct",
    "Model sha": "7ae557604adf67be50417f59c2c2f167def9a775",
    "Average ‚¨ÜÔ∏è": 8.382971846691058,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 104,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6185759745833384,
    "IFEval Raw": 0.31529120511354314,
    "IFEval": 31.529120511354318,
    "BBH Raw": 0.3321916429549138,
    "BBH": 8.169502268182768,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3341875,
    "MUSR": 1.3734374999999996,
    "MMLU-PRO Raw": 0.17195811170212766,
    "MMLU-PRO": 7.99534574468085,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-0.5B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-1.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-1.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-1.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-1.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-1.5B",
    "Model sha": "e5dfabbcffd9b0c7b31d89b82c5a6b72e663f32c",
    "Average ‚¨ÜÔ∏è": 13.802348694049366,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.248501010003509,
    "IFEval Raw": 0.26743041795768563,
    "IFEval": 26.743041795768562,
    "BBH Raw": 0.40779509451366147,
    "BBH": 16.660465167691854,
    "MATH Lvl 5 Raw": 0.08836858006042295,
    "MATH Lvl 5": 8.836858006042295,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.35759375,
    "MUSR": 5.265885416666666,
    "MMLU-PRO Raw": 0.28548869680851063,
    "MMLU-PRO": 20.609855200945624,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-1.5B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-1.5B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-1.5B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-1.5B-Instruct",
    "Model sha": "5fee7c4ed634dc66c6e318c8ac2897b8b9154536",
    "Average ‚¨ÜÔ∏è": 15.031717600858885,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 170,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6871893311954247,
    "IFEval Raw": 0.4475569267321817,
    "IFEval": 44.75569267321818,
    "BBH Raw": 0.4288982740422907,
    "BBH": 19.809786497358974,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.3663125,
    "MUSR": 3.1890625000000004,
    "MMLU-PRO Raw": 0.27992021276595747,
    "MMLU-PRO": 19.99113475177305,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-14B",
    "Model sha": "83a1904df002b00bc8db6f877821cb77dbb363b0",
    "Average ‚¨ÜÔ∏è": 31.749652824065382,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 35,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.585921351724733,
    "IFEval Raw": 0.3694464022127954,
    "IFEval": 36.94464022127954,
    "BBH Raw": 0.616051493531774,
    "BBH": 45.078312404984935,
    "MATH Lvl 5 Raw": 0.27794561933534745,
    "MATH Lvl 5": 27.794561933534744,
    "GPQA Raw": 0.38171140939597314,
    "GPQA": 17.561521252796418,
    "MUSR Raw": 0.4502395833333333,
    "MUSR": 15.913281249999997,
    "MMLU-PRO Raw": 0.5248503989361702,
    "MMLU-PRO": 47.205599881796694,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-14B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-14B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-14B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-14B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-14B-Instruct",
    "Model sha": "f55224c616ca27d4bcf28969a156de12c98981cf",
    "Average ‚¨ÜÔ∏è": 32.18307278426168,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 117,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7736503203376015,
    "IFEval Raw": 0.8157776920792386,
    "IFEval": 81.57776920792386,
    "BBH Raw": 0.6390453705906222,
    "BBH": 48.36070661282705,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.4100625,
    "MUSR": 10.157812500000004,
    "MMLU-PRO Raw": 0.4904421542553192,
    "MMLU-PRO": 43.382461583924346,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-32B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-32B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-32B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-32B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-32B",
    "Model sha": "ff23665d01c3665be5fdb271d18a62090b65c06d",
    "Average ‚¨ÜÔ∏è": 37.98279107151089,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 37,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.874885542747245,
    "IFEval Raw": 0.40766499554515356,
    "IFEval": 40.766499554515356,
    "BBH Raw": 0.6770522448726507,
    "BBH": 53.954752851331996,
    "MATH Lvl 5 Raw": 0.3549848942598187,
    "MATH Lvl 5": 35.49848942598187,
    "GPQA Raw": 0.41191275167785235,
    "GPQA": 21.588366890380314,
    "MUSR Raw": 0.49783333333333335,
    "MUSR": 22.69583333333333,
    "MMLU-PRO Raw": 0.5805352393617021,
    "MMLU-PRO": 53.39280437352246,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-32B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-32B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-32B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-32B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-32B-Instruct",
    "Model sha": "70e8dfb9ad18a7d499f765fe206ff065ed8ca197",
    "Average ‚¨ÜÔ∏è": 36.17418497413896,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 126,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.752483170342869,
    "IFEval Raw": 0.8346121623957765,
    "IFEval": 83.46121623957765,
    "BBH Raw": 0.6912525080134339,
    "BBH": 56.48934826159387,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.42612500000000003,
    "MUSR": 13.498958333333329,
    "MMLU-PRO Raw": 0.566655585106383,
    "MMLU-PRO": 51.85062056737589,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-32B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-3B",
    "Model sha": "e4aa5ac50aa507415cda96cc99eb77ad0a3d2d34",
    "Average ‚¨ÜÔ∏è": 17.108308989083472,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 35,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.8105074740058917,
    "IFEval Raw": 0.2689541527591236,
    "IFEval": 26.895415275912356,
    "BBH Raw": 0.4612475341011634,
    "BBH": 24.304241726371686,
    "MATH Lvl 5 Raw": 0.08836858006042296,
    "MATH Lvl 5": 8.836858006042297,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.4303333333333333,
    "MUSR": 11.758333333333335,
    "MMLU-PRO Raw": 0.3203125,
    "MMLU-PRO": 24.479166666666664,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-3B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-3B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-3B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-3B-Instruct",
    "Model sha": "82f42baa094a9600e39ccd80d34058aeeb3abbc1",
    "Average ‚¨ÜÔ∏è": 21.031344318800695,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 93,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3884743055809619,
    "IFEval Raw": 0.6474919879253713,
    "IFEval": 64.74919879253714,
    "BBH Raw": 0.469276665604885,
    "BBH": 25.801393944088584,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.39679166666666665,
    "MUSR": 7.565625,
    "MMLU-PRO Raw": 0.3254654255319149,
    "MMLU-PRO": 25.05171394799054,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-72B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-72B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-72B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-72B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-72B",
    "Model sha": "587cc4061cf6a7cc0d429d05c109447e5cf063af",
    "Average ‚¨ÜÔ∏è": 38.36561487162947,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 18.09192658999383,
    "IFEval Raw": 0.4137100670664947,
    "IFEval": 41.37100670664947,
    "BBH Raw": 0.6797320670694852,
    "BBH": 54.61505780163693,
    "MATH Lvl 5 Raw": 0.3867069486404834,
    "MATH Lvl 5": 38.670694864048336,
    "GPQA Raw": 0.4052013422818792,
    "GPQA": 20.69351230425056,
    "MUSR Raw": 0.477125,
    "MUSR": 19.640624999999996,
    "MMLU-PRO Raw": 0.5968251329787234,
    "MMLU-PRO": 55.20279255319149,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-72B-Instruct",
    "Model sha": "a13fff9ad76700c7ecff2769f75943ba8395b4a7",
    "Average ‚¨ÜÔ∏è": 38.21208126161476,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 486,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 33.00676832439911,
    "IFEval Raw": 0.863837949972739,
    "IFEval": 86.3837949972739,
    "BBH Raw": 0.7272747321744824,
    "BBH": 61.873255668787884,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.42060416666666667,
    "MUSR": 11.742187500000005,
    "MMLU-PRO Raw": 0.5625831117021277,
    "MMLU-PRO": 51.39812352245864,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-7B",
    "Model sha": "57597c00770845ceba45271ba1b24c94bbcc7baf",
    "Average ‚¨ÜÔ∏è": 24.98693434504172,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 68,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.0279608791428823,
    "IFEval Raw": 0.3374479713825982,
    "IFEval": 33.74479713825982,
    "BBH Raw": 0.5416303767788616,
    "BBH": 35.81347328754777,
    "MATH Lvl 5 Raw": 0.18882175226586104,
    "MATH Lvl 5": 18.882175226586103,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.4424270833333333,
    "MUSR": 14.13671875,
    "MMLU-PRO Raw": 0.4365026595744681,
    "MMLU-PRO": 37.38918439716312,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-7B-Instruct",
    "Model sha": "52e20a6f5f475e5c8f6a8ebda4ae5fa6b1ea22ac",
    "Average ‚¨ÜÔ∏è": 26.86677532661463,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 290,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.168271483115276,
    "IFEval Raw": 0.7585251576926999,
    "IFEval": 75.85251576926998,
    "BBH Raw": 0.5394231968299095,
    "BBH": 34.89211675876548,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.40203125,
    "MUSR": 8.453906250000001,
    "MMLU-PRO Raw": 0.4286901595744681,
    "MMLU-PRO": 36.52112884160757,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Coder-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Coder-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Coder-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Coder-14B",
    "Model sha": "1db30eb5ec86a6e51d8981818ee2910370b3010d",
    "Average ‚¨ÜÔ∏è": 24.766111696299742,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.6326252007532585,
    "IFEval Raw": 0.3472652561869174,
    "IFEval": 34.72652561869174,
    "BBH Raw": 0.5864860091741232,
    "BBH": 40.52300211536303,
    "MATH Lvl 5 Raw": 0.2212990936555891,
    "MATH Lvl 5": 22.129909365558913,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3873645833333333,
    "MUSR": 6.3872395833333355,
    "MMLU-PRO Raw": 0.4521276595744681,
    "MMLU-PRO": 39.125295508274235,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-Coder-14B (Merge)"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Coder-14B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-14B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Coder-14B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Coder-14B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Coder-14B-Instruct",
    "Model sha": "1a62978099f9b19f72fdd191988ff958abb18561",
    "Average ‚¨ÜÔ∏è": 31.178725416796762,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 51,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3832140652589733,
    "IFEval Raw": 0.6907560827493273,
    "IFEval": 69.07560827493273,
    "BBH Raw": 0.6140296423661326,
    "BBH": 44.220018215668375,
    "MATH Lvl 5 Raw": 0.2681268882175227,
    "MATH Lvl 5": 26.812688821752268,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.3914583333333333,
    "MUSR": 7.032291666666666,
    "MMLU-PRO Raw": 0.3939494680851064,
    "MMLU-PRO": 32.661052009456256,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-Coder-14B-Instruct (Merge)"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Coder-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Coder-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Coder-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Coder-7B",
    "Model sha": "097b213c52760d22753af1aa5cbdba94b5c99506",
    "Average ‚¨ÜÔ∏è": 19.209490538962246,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 68,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.301767168007955,
    "IFEval Raw": 0.344592348302504,
    "IFEval": 34.4592348302504,
    "BBH Raw": 0.48556405534214747,
    "BBH": 28.438944115255534,
    "MATH Lvl 5 Raw": 0.1918429003021148,
    "MATH Lvl 5": 19.18429003021148,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3448541666666667,
    "MUSR": 2.1734375000000004,
    "MMLU-PRO Raw": 0.3679355053191489,
    "MMLU-PRO": 29.770611702127653,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-Coder-7B (Merge)"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Coder-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Coder-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Coder-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "Model sha": "3030861ab8e72c6155e1821631bf977ef40d3e5b",
    "Average ‚¨ÜÔ∏è": 22.425432965182296,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 321,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7798574812515715,
    "IFEval Raw": 0.6101477413263474,
    "IFEval": 61.01477413263474,
    "BBH Raw": 0.5007976986224548,
    "BBH": 28.938504045379137,
    "MATH Lvl 5 Raw": 0.033987915407854986,
    "MATH Lvl 5": 3.3987915407854987,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4072708333333333,
    "MUSR": 9.475520833333334,
    "MMLU-PRO Raw": 0.3351894946808511,
    "MMLU-PRO": 26.13216607565012,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-11-07",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-Coder-7B-Instruct (Merge)"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Coder-7B-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Coder-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Coder-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "Model sha": "f784f10a7b2aac91bd26e6dbe7dccce691cd4ac5",
    "Average ‚¨ÜÔ∏è": 22.52451581645211,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 321,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6976473478971377,
    "IFEval Raw": 0.6147189457306613,
    "IFEval": 61.47189457306614,
    "BBH Raw": 0.4999048550311305,
    "BBH": 28.72657796895031,
    "MATH Lvl 5 Raw": 0.030966767371601207,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.4099375,
    "MUSR": 9.875520833333335,
    "MMLU-PRO Raw": 0.33543882978723405,
    "MMLU-PRO": 26.15986997635934,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-11-07",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-Coder-7B-Instruct (Merge)"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Math-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Math-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Math-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Math-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Math-72B-Instruct",
    "Model sha": "3743c8fd46b002d105c1d28d180f1e531df1d40f",
    "Average ‚¨ÜÔ∏è": 29.647637276245174,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 28.676884975257288,
    "IFEval Raw": 0.4003466358151926,
    "IFEval": 40.034663581519254,
    "BBH Raw": 0.6452266637803764,
    "BBH": 48.966096029421145,
    "MATH Lvl 5 Raw": 0.1933534743202417,
    "MATH Lvl 5": 19.335347432024168,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.44727083333333334,
    "MUSR": 16.342187499999998,
    "MMLU-PRO Raw": 0.4812167553191489,
    "MMLU-PRO": 42.35741725768321,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-29",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Math-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Math-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Math-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Math-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Math-7B",
    "Model sha": "8daf1d676c3f24ddec5a99c5cff00a5c0e1c441c",
    "Average ‚¨ÜÔ∏è": 17.836657156289718,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3397297456276407,
    "IFEval Raw": 0.24599839536873275,
    "IFEval": 24.599839536873276,
    "BBH Raw": 0.4454639372840941,
    "BBH": 22.00876067958663,
    "MATH Lvl 5 Raw": 0.3051359516616314,
    "MATH Lvl 5": 30.51359516616314,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.37809374999999995,
    "MUSR": 4.995052083333334,
    "MMLU-PRO Raw": 0.27177526595744683,
    "MMLU-PRO": 19.086140661938536,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "Qwen_Qwen2.5-Math-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Qwen/Qwen2.5-Math-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Qwen/Qwen2.5-Math-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Qwen__Qwen2.5-Math-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Qwen/Qwen2.5-Math-7B-Instruct",
    "Model sha": "b3b4c5794bf4b68c1978bb3525afc5e0d0d6fcc4",
    "Average ‚¨ÜÔ∏è": 16.506312907803878,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 37,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1392665608863097,
    "IFEval Raw": 0.26358395723347383,
    "IFEval": 26.358395723347385,
    "BBH Raw": 0.438762734452786,
    "BBH": 21.489765755272032,
    "MATH Lvl 5 Raw": 0.2651057401812689,
    "MATH Lvl 5": 26.510574018126892,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3647291666666666,
    "MUSR": 2.891145833333333,
    "MMLU-PRO Raw": 0.2819980053191489,
    "MMLU-PRO": 20.222000591016545,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-19",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "RESMPDEV_Qwen2-Wukong-0.5B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/RESMPDEV/Qwen2-Wukong-0.5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RESMPDEV/Qwen2-Wukong-0.5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/RESMPDEV__Qwen2-Wukong-0.5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "RESMPDEV/Qwen2-Wukong-0.5B",
    "Model sha": "52c58a4aa3d0b44c363c5761fa658243f5c53943",
    "Average ‚¨ÜÔ∏è": 4.950363477111334,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9872690931251266,
    "IFEval Raw": 0.1854235650296768,
    "IFEval": 18.54235650296768,
    "BBH Raw": 0.308451428837168,
    "BBH": 4.19666315993673,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23657718120805368,
    "GPQA": 0.0,
    "MUSR Raw": 0.3524791666666667,
    "MUSR": 3.3265624999999996,
    "MMLU-PRO Raw": 0.13272938829787234,
    "MMLU-PRO": 3.6365986997635926,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "RESMPDEV/Qwen2-Wukong-0.5B"
  },
  {
    "eval_name": "RLHFlow_ArmoRM-Llama3-8B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForRewardModelWithGating",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/RLHFlow/ArmoRM-Llama3-8B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RLHFlow/ArmoRM-Llama3-8B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/RLHFlow__ArmoRM-Llama3-8B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "RLHFlow/ArmoRM-Llama3-8B-v0.1",
    "Model sha": "eb2676d20da2f2d41082289d23c59b9f7427f955",
    "Average ‚¨ÜÔ∏è": 4.705487409302649,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 153,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9235229122819598,
    "IFEval Raw": 0.18967007539993883,
    "IFEval": 18.967007539993883,
    "BBH Raw": 0.2876467446788138,
    "BBH": 1.7494478703137453,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.3948020833333333,
    "MUSR": 6.650260416666666,
    "MMLU-PRO Raw": 0.10779587765957446,
    "MMLU-PRO": 0.8662086288416063,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-23",
    "Submission Date": "2024-10-08",
    "Generation": 0,
    "Base Model": "RLHFlow/ArmoRM-Llama3-8B-v0.1"
  },
  {
    "eval_name": "RLHFlow_LLaMA3-iterative-DPO-final_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/RLHFlow/LLaMA3-iterative-DPO-final\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RLHFlow/LLaMA3-iterative-DPO-final</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/RLHFlow__LLaMA3-iterative-DPO-final-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "RLHFlow/LLaMA3-iterative-DPO-final",
    "Model sha": "40b73bd07a019795837f80579fe95470484ca82b",
    "Average ‚¨ÜÔ∏è": 19.63634312977761,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 40,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8927465269670676,
    "IFEval Raw": 0.53401086893886,
    "IFEval": 53.401086893886,
    "BBH Raw": 0.5058257182733729,
    "BBH": 29.787760272097795,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3672708333333334,
    "MUSR": 5.075520833333335,
    "MMLU-PRO Raw": 0.32571476063829785,
    "MMLU-PRO": 25.07941784869976,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "RLHFlow/LLaMA3-iterative-DPO-final"
  },
  {
    "eval_name": "RWKV_rwkv-raven-14b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "RwkvForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/RWKV/rwkv-raven-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RWKV/rwkv-raven-14b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/RWKV__rwkv-raven-14b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "RWKV/rwkv-raven-14b",
    "Model sha": "359c0649b4f1d10a26ebea32908035bc00d152ee",
    "Average ‚¨ÜÔ∏è": 3.885056601409273,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 55,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5906290699310597,
    "IFEval Raw": 0.07683723631076655,
    "IFEval": 7.683723631076655,
    "BBH Raw": 0.3307041176552897,
    "BBH": 6.763765061303336,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.22902684563758388,
    "GPQA": 0.0,
    "MUSR Raw": 0.3951458333333333,
    "MUSR": 7.193229166666666,
    "MMLU-PRO Raw": 0.11502659574468085,
    "MMLU-PRO": 1.6696217494089831,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-05-05",
    "Submission Date": "2024-07-08",
    "Generation": 0,
    "Base Model": "RWKV/rwkv-raven-14b"
  },
  {
    "eval_name": "Rakuten_RakutenAI-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Rakuten/RakutenAI-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Rakuten/RakutenAI-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Rakuten__RakutenAI-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Rakuten/RakutenAI-7B",
    "Model sha": "c687b10cbf1aa6c34868904b62ecfcef2e0946bf",
    "Average ‚¨ÜÔ∏è": 11.546978376746557,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 45,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.641848681233742,
    "IFEval Raw": 0.1555971488982566,
    "IFEval": 15.559714889825662,
    "BBH Raw": 0.43149052613615435,
    "BBH": 20.982052312914476,
    "MATH Lvl 5 Raw": 0.019637462235649546,
    "MATH Lvl 5": 1.9637462235649545,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.37381250000000005,
    "MUSR": 4.6598958333333345,
    "MMLU-PRO Raw": 0.28773271276595747,
    "MMLU-PRO": 20.859190307328607,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-18",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "Rakuten/RakutenAI-7B"
  },
  {
    "eval_name": "Rakuten_RakutenAI-7B-chat_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Rakuten/RakutenAI-7B-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Rakuten/RakutenAI-7B-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Rakuten__RakutenAI-7B-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Rakuten/RakutenAI-7B-chat",
    "Model sha": "1685492c5c40f8a7f57e2cc1c8fa65e5b0c94d31",
    "Average ‚¨ÜÔ∏è": 12.777919128499393,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 60,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6240693126713804,
    "IFEval Raw": 0.26855521128383797,
    "IFEval": 26.8555211283838,
    "BBH Raw": 0.4316204035758174,
    "BBH": 20.23755200474476,
    "MATH Lvl 5 Raw": 0.027945619335347432,
    "MATH Lvl 5": 2.794561933534743,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.37895833333333334,
    "MUSR": 5.903125,
    "MMLU-PRO Raw": 0.2798371010638298,
    "MMLU-PRO": 19.98190011820331,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-18",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "Rakuten/RakutenAI-7B-chat"
  },
  {
    "eval_name": "Replete-AI_L3-Pneuma-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/L3-Pneuma-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/L3-Pneuma-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__L3-Pneuma-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/L3-Pneuma-8B",
    "Model sha": "3e477fa150bf31b360891d3920ccbd57dac110ab",
    "Average ‚¨ÜÔ∏è": 16.653994219637084,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.5267999260414062,
    "IFEval Raw": 0.24132745559559746,
    "IFEval": 24.132745559559744,
    "BBH Raw": 0.4908680380935449,
    "BBH": 28.163142118962657,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.4105208333333333,
    "MUSR": 9.181770833333333,
    "MMLU-PRO Raw": 0.3175698138297872,
    "MMLU-PRO": 24.174423758865245,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-15",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Replete-AI_L3.1-Pneuma-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/L3.1-Pneuma-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/L3.1-Pneuma-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__L3.1-Pneuma-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/L3.1-Pneuma-8B",
    "Model sha": "843163ca811525c4f98f817aae8fb5da7c1fb7bb",
    "Average ‚¨ÜÔ∏è": 27.382649662720095,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.705851105075447,
    "IFEval Raw": 0.707642388861554,
    "IFEval": 70.7642388861554,
    "BBH Raw": 0.504990389092237,
    "BBH": 30.26262992696204,
    "MATH Lvl 5 Raw": 0.20166163141993956,
    "MATH Lvl 5": 20.166163141993955,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.3871145833333333,
    "MUSR": 6.1559895833333345,
    "MMLU-PRO Raw": 0.36909906914893614,
    "MMLU-PRO": 29.899896572104012,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-09",
    "Submission Date": "2024-11-13",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "Replete-AI_Llama3-8B-Instruct-Replete-Adapted_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Llama3-8B-Instruct-Replete-Adapted\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Llama3-8B-Instruct-Replete-Adapted</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Llama3-8B-Instruct-Replete-Adapted-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Llama3-8B-Instruct-Replete-Adapted",
    "Model sha": "d930f2111913da6fb7693187e1cdc817191c8e5e",
    "Average ‚¨ÜÔ∏è": 22.55193174606386,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7603778620962051,
    "IFEval Raw": 0.6915306941138402,
    "IFEval": 69.15306941138402,
    "BBH Raw": 0.48702618293318983,
    "BBH": 26.88896431517229,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.36339583333333336,
    "MUSR": 2.824479166666667,
    "MMLU-PRO Raw": 0.3390957446808511,
    "MMLU-PRO": 26.566193853427894,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-07-09",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-Coder-Instruct-8b-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-Coder-Instruct-8b-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-Coder-Instruct-8b-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-Coder-Instruct-8b-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-Coder-Instruct-8b-Merged",
    "Model sha": "0594615bf84f0803a078b59f14eb090cec2004f3",
    "Average ‚¨ÜÔ∏è": 16.427667565252104,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9145351611132494,
    "IFEval Raw": 0.5387571643239937,
    "IFEval": 53.87571643239937,
    "BBH Raw": 0.4461693860075828,
    "BBH": 21.937706578272657,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.36603125,
    "MUSR": 3.4539062499999993,
    "MMLU-PRO Raw": 0.18051861702127658,
    "MMLU-PRO": 8.946513002364064,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-Coder-Llama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-Coder-Llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-Coder-Llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-Coder-Llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-Coder-Llama3-8B",
    "Model sha": "2aca75c53e7eb2f523889ab1a279e349b8f1b0e8",
    "Average ‚¨ÜÔ∏è": 11.731388181541368,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4158069390079968,
    "IFEval Raw": 0.4729362535849324,
    "IFEval": 47.293625358493244,
    "BBH Raw": 0.3271277102526684,
    "BBH": 7.055475836799367,
    "MATH Lvl 5 Raw": 0.03398791540785499,
    "MATH Lvl 5": 3.398791540785499,
    "GPQA Raw": 0.26090604026845643,
    "GPQA": 1.4541387024608574,
    "MUSR Raw": 0.39530208333333333,
    "MUSR": 7.512760416666666,
    "MMLU-PRO Raw": 0.13306183510638298,
    "MMLU-PRO": 3.6735372340425525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-Coder-Qwen2-1.5b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-Coder-Qwen2-1.5b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-Coder-Qwen2-1.5b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-Coder-Qwen2-1.5b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-Coder-Qwen2-1.5b",
    "Model sha": "86fcccbf921b7eb8a4d348e4a3cde0beb63d6626",
    "Average ‚¨ÜÔ∏è": 11.095283585246312,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7118710517795366,
    "IFEval Raw": 0.30142798884736943,
    "IFEval": 30.14279888473694,
    "BBH Raw": 0.34747295666696026,
    "BBH": 10.4265158026681,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.4072708333333333,
    "MUSR": 9.742187500000002,
    "MMLU-PRO Raw": 0.21467752659574468,
    "MMLU-PRO": 12.741947399527188,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-LLM-Qwen2-7b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-LLM-Qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-LLM-Qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-LLM-Qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-LLM-Qwen2-7b",
    "Model sha": "e3569433b23fde853683ad61f342d2c1bd01d60a",
    "Average ‚¨ÜÔ∏è": 3.325393716070183,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.929691912924937,
    "IFEval Raw": 0.09047549391170981,
    "IFEval": 9.047549391170982,
    "BBH Raw": 0.29852574011260374,
    "BBH": 2.8429334106486,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.38476041666666666,
    "MUSR": 5.861718749999999,
    "MMLU-PRO Raw": 0.1157746010638298,
    "MMLU-PRO": 1.7527334515366433,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-13",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-LLM-Qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-LLM-Qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-LLM-Qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-LLM-Qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-LLM-Qwen2-7b",
    "Model sha": "5b75b6180b45d83124e04a00766dc19d2ad52622",
    "Average ‚¨ÜÔ∏è": 3.509166955357833,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8560068866327826,
    "IFEval Raw": 0.09324813716494457,
    "IFEval": 9.324813716494457,
    "BBH Raw": 0.2976924067792704,
    "BBH": 2.7249704476856373,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.39409374999999996,
    "MUSR": 7.26171875,
    "MMLU-PRO Raw": 0.11569148936170212,
    "MMLU-PRO": 1.7434988179669018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-13",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-LLM-Qwen2-7b_Beta-Preview_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-LLM-Qwen2-7b_Beta-Preview\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-LLM-Qwen2-7b_Beta-Preview</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-LLM-Qwen2-7b_Beta-Preview-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-LLM-Qwen2-7b_Beta-Preview",
    "Model sha": "fe4c3fc2314db69083527ddd0c9a658fcbc54f15",
    "Average ‚¨ÜÔ∏è": 3.5774317458290468,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.35706870492039533,
    "IFEval Raw": 0.08575468645416384,
    "IFEval": 8.575468645416384,
    "BBH Raw": 0.2929321328066677,
    "BBH": 1.9656769418510358,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.3980625,
    "MUSR": 7.7578125,
    "MMLU-PRO Raw": 0.1284906914893617,
    "MMLU-PRO": 3.1656323877068555,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-07-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Replete-AI_Replete-LLM-V2-Llama-3.1-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Replete-AI/Replete-LLM-V2-Llama-3.1-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Replete-AI/Replete-LLM-V2-Llama-3.1-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Replete-AI__Replete-LLM-V2-Llama-3.1-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Replete-AI/Replete-LLM-V2-Llama-3.1-8b",
    "Model sha": "5ff5224804dcc31f536e491e52310f2e3cdc0b57",
    "Average ‚¨ÜÔ∏è": 25.054691032959767,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9086809784711636,
    "IFEval Raw": 0.5514966954347797,
    "IFEval": 55.14966954347798,
    "BBH Raw": 0.5339203611594218,
    "BBH": 33.20757217219532,
    "MATH Lvl 5 Raw": 0.14501510574018128,
    "MATH Lvl 5": 14.501510574018129,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.4000729166666667,
    "MUSR": 8.375781250000001,
    "MMLU-PRO Raw": 0.37533244680851063,
    "MMLU-PRO": 30.592494089834517,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "RubielLabarta_LogoS-7Bx2-MoE-13B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/RubielLabarta/LogoS-7Bx2-MoE-13B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">RubielLabarta/LogoS-7Bx2-MoE-13B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/RubielLabarta__LogoS-7Bx2-MoE-13B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "RubielLabarta/LogoS-7Bx2-MoE-13B-v0.2",
    "Model sha": "fb0f72b9914a81892bfeea5a04fcd9676c883d64",
    "Average ‚¨ÜÔ∏è": 20.079624910826094,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.837531938197392,
    "IFEval Raw": 0.4378903531518593,
    "IFEval": 43.78903531518593,
    "BBH Raw": 0.5206958722481815,
    "BBH": 32.794801632016096,
    "MATH Lvl 5 Raw": 0.05513595166163141,
    "MATH Lvl 5": 5.5135951661631415,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4226145833333333,
    "MUSR": 11.493489583333336,
    "MMLU-PRO Raw": 0.3087599734042553,
    "MMLU-PRO": 23.195552600472812,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-21",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "RubielLabarta/LogoS-7Bx2-MoE-13B-v0.2 (Merge)"
  },
  {
    "eval_name": "SaisExperiments_Evil-Alpaca-3B-L3.2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SaisExperiments/Evil-Alpaca-3B-L3.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaisExperiments/Evil-Alpaca-3B-L3.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SaisExperiments__Evil-Alpaca-3B-L3.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SaisExperiments/Evil-Alpaca-3B-L3.2",
    "Model sha": "77d25b9182270a66ac60a91d646b447e1530f70e",
    "Average ‚¨ÜÔ∏è": 15.1125242410546,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7331495359585803,
    "IFEval Raw": 0.32510848991786234,
    "IFEval": 32.51084899178623,
    "BBH Raw": 0.4340757699220565,
    "BBH": 20.851948385581196,
    "MATH Lvl 5 Raw": 0.06570996978851965,
    "MATH Lvl 5": 6.570996978851965,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.4197604166666667,
    "MUSR": 10.936718750000004,
    "MMLU-PRO Raw": 0.2621343085106383,
    "MMLU-PRO": 18.0149231678487,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "SaisExperiments/Evil-Alpaca-3B-L3.2 (Merge)"
  },
  {
    "eval_name": "SaisExperiments_Gemma-2-2B-Opus-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SaisExperiments/Gemma-2-2B-Opus-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaisExperiments/Gemma-2-2B-Opus-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SaisExperiments__Gemma-2-2B-Opus-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SaisExperiments/Gemma-2-2B-Opus-Instruct",
    "Model sha": "7caa9e833d3f5713cf1b8ebd8beeb6ef02da99ea",
    "Average ‚¨ÜÔ∏è": 17.1956384886446,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1575938811598998,
    "IFEval Raw": 0.474959773401242,
    "IFEval": 47.4959773401242,
    "BBH Raw": 0.4292846281445681,
    "BBH": 19.529532994538695,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.4056875,
    "MUSR": 8.577604166666665,
    "MMLU-PRO Raw": 0.2650432180851064,
    "MMLU-PRO": 18.3381353427896,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-03",
    "Submission Date": "2024-10-07",
    "Generation": 2,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "SaisExperiments_Gemma-2-2B-Stheno-Filtered_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SaisExperiments/Gemma-2-2B-Stheno-Filtered\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SaisExperiments/Gemma-2-2B-Stheno-Filtered</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SaisExperiments__Gemma-2-2B-Stheno-Filtered-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SaisExperiments/Gemma-2-2B-Stheno-Filtered",
    "Model sha": "683443cfa90c7a06978d1c5e9ead0fb0a68b49ca",
    "Average ‚¨ÜÔ∏è": 15.428162178774246,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8883672035861858,
    "IFEval Raw": 0.4196554032190144,
    "IFEval": 41.96554032190144,
    "BBH Raw": 0.4149234152222183,
    "BBH": 17.47886723805338,
    "MATH Lvl 5 Raw": 0.04229607250755288,
    "MATH Lvl 5": 4.229607250755288,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.40029166666666666,
    "MUSR": 8.103125000000002,
    "MMLU-PRO Raw": 0.2629654255319149,
    "MMLU-PRO": 18.107269503546096,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-10-08",
    "Generation": 2,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "Salesforce_LLaMA-3-8B-SFR-Iterative-DPO-R_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Salesforce__LLaMA-3-8B-SFR-Iterative-DPO-R-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R",
    "Model sha": "ad7d1aed82eb6d8ca4b3aad627ff76f72ab34f70",
    "Average ‚¨ÜÔ∏è": 17.029765390241824,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 74,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8573924305183018,
    "IFEval Raw": 0.38156203318306536,
    "IFEval": 38.15620331830654,
    "BBH Raw": 0.5011950469666927,
    "BBH": 29.150289349765558,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.36333333333333334,
    "MUSR": 5.550000000000002,
    "MMLU-PRO Raw": 0.3172373670212766,
    "MMLU-PRO": 24.137485224586285,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-09",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "Salesforce/LLaMA-3-8B-SFR-Iterative-DPO-R"
  },
  {
    "eval_name": "SanjiWatsuki_Kunoichi-DPO-v2-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SanjiWatsuki/Kunoichi-DPO-v2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SanjiWatsuki/Kunoichi-DPO-v2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SanjiWatsuki__Kunoichi-DPO-v2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SanjiWatsuki/Kunoichi-DPO-v2-7B",
    "Model sha": "5278247beb482c4fceff2294570236d68b74d132",
    "Average ‚¨ÜÔ∏è": 20.55645488872975,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 82,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2082068431884143,
    "IFEval Raw": 0.5431034100630772,
    "IFEval": 54.31034100630771,
    "BBH Raw": 0.4415592450869275,
    "BBH": 20.903472484123803,
    "MATH Lvl 5 Raw": 0.07477341389728098,
    "MATH Lvl 5": 7.477341389728098,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.41883333333333334,
    "MUSR": 11.087500000000004,
    "MMLU-PRO Raw": 0.3106715425531915,
    "MMLU-PRO": 23.407949172576835,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-13",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "SanjiWatsuki/Kunoichi-DPO-v2-7B"
  },
  {
    "eval_name": "SanjiWatsuki_Silicon-Maid-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SanjiWatsuki/Silicon-Maid-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SanjiWatsuki/Silicon-Maid-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SanjiWatsuki__Silicon-Maid-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SanjiWatsuki/Silicon-Maid-7B",
    "Model sha": "4e43d81f3fff1091df7cb2d85e9e306d25235701",
    "Average ‚¨ÜÔ∏è": 19.449859883810536,
    "Hub License": "cc-by-4.0",
    "Hub ‚ù§Ô∏è": 104,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6061633022825488,
    "IFEval Raw": 0.5367835121920947,
    "IFEval": 53.678351219209475,
    "BBH Raw": 0.4127972831009074,
    "BBH": 16.692746753586437,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.41883333333333334,
    "MUSR": 11.087500000000004,
    "MMLU-PRO Raw": 0.308344414893617,
    "MMLU-PRO": 23.149379432624112,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-27",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "SanjiWatsuki/Silicon-Maid-7B"
  },
  {
    "eval_name": "Sao10K_Fimbulvetr-11B-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/Fimbulvetr-11B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/Fimbulvetr-11B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__Fimbulvetr-11B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/Fimbulvetr-11B-v2",
    "Model sha": "b2dcd534dc3a53ff84e60a53b87816185169be19",
    "Average ‚¨ÜÔ∏è": 20.069619001586815,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 162,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8212041510763431,
    "IFEval Raw": 0.5100056738343152,
    "IFEval": 51.000567383431516,
    "BBH Raw": 0.4544495065184342,
    "BBH": 22.65512081005865,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.43536458333333333,
    "MUSR": 14.920572916666666,
    "MMLU-PRO Raw": 0.33011968085106386,
    "MMLU-PRO": 25.56885342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-06",
    "Submission Date": "2024-07-01",
    "Generation": 0,
    "Base Model": "Sao10K/Fimbulvetr-11B-v2"
  },
  {
    "eval_name": "Sao10K_L3-70B-Euryale-v2.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/L3-70B-Euryale-v2.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/L3-70B-Euryale-v2.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__L3-70B-Euryale-v2.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/L3-70B-Euryale-v2.1",
    "Model sha": "36ad832b771cd783ea7ad00ed39e61f679b1a7c6",
    "Average ‚¨ÜÔ∏è": 35.34801470603125,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 116,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.618348546199883,
    "IFEval Raw": 0.7384417789243651,
    "IFEval": 73.84417789243652,
    "BBH Raw": 0.6471322811268715,
    "BBH": 48.70118672944805,
    "MATH Lvl 5 Raw": 0.2084592145015106,
    "MATH Lvl 5": 20.84592145015106,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.42091666666666666,
    "MUSR": 12.247916666666669,
    "MMLU-PRO Raw": 0.5103889627659575,
    "MMLU-PRO": 45.598773640661946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-11",
    "Submission Date": "2024-07-01",
    "Generation": 0,
    "Base Model": "Sao10K/L3-70B-Euryale-v2.1"
  },
  {
    "eval_name": "Sao10K_L3-70B-Euryale-v2.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/L3-70B-Euryale-v2.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/L3-70B-Euryale-v2.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__L3-70B-Euryale-v2.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/L3-70B-Euryale-v2.1",
    "Model sha": "36ad832b771cd783ea7ad00ed39e61f679b1a7c6",
    "Average ‚¨ÜÔ∏è": 35.473252668728755,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 116,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.616454319859539,
    "IFEval Raw": 0.7281003293483512,
    "IFEval": 72.81003293483514,
    "BBH Raw": 0.6502778992745041,
    "BBH": 49.193003079898574,
    "MATH Lvl 5 Raw": 0.22432024169184292,
    "MATH Lvl 5": 22.43202416918429,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.41958333333333336,
    "MUSR": 12.047916666666667,
    "MMLU-PRO Raw": 0.5095578457446809,
    "MMLU-PRO": 45.50642730496454,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-11",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Sao10K/L3-70B-Euryale-v2.1"
  },
  {
    "eval_name": "Sao10K_L3-8B-Lunaris-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/L3-8B-Lunaris-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/L3-8B-Lunaris-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__L3-8B-Lunaris-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/L3-8B-Lunaris-v1",
    "Model sha": "8479c2a7ee119c935b9a02c921cc2a85b698dfe8",
    "Average ‚¨ÜÔ∏è": 25.628336385704557,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 91,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6604986385208319,
    "IFEval Raw": 0.6894573066131198,
    "IFEval": 68.94573066131198,
    "BBH Raw": 0.5235299282515419,
    "BBH": 32.11434845509543,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.3726666666666667,
    "MUSR": 5.550000000000002,
    "MMLU-PRO Raw": 0.3787400265957447,
    "MMLU-PRO": 30.971114066193856,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-07-22",
    "Generation": 0,
    "Base Model": "Sao10K/L3-8B-Lunaris-v1"
  },
  {
    "eval_name": "Sao10K_L3-8B-Stheno-v3.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/L3-8B-Stheno-v3.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/L3-8B-Stheno-v3.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__L3-8B-Stheno-v3.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/L3-8B-Stheno-v3.2",
    "Model sha": "4bb828f6e1b1efd648c39b1ad682c44ff260f018",
    "Average ‚¨ÜÔ∏è": 25.909569821830583,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 242,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8916722336274605,
    "IFEval Raw": 0.6872841837435781,
    "IFEval": 68.72841837435782,
    "BBH Raw": 0.522778637171633,
    "BBH": 32.02159792407502,
    "MATH Lvl 5 Raw": 0.0944108761329305,
    "MATH Lvl 5": 9.44108761329305,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.3793645833333333,
    "MUSR": 6.453906249999996,
    "MMLU-PRO Raw": 0.3768284574468085,
    "MMLU-PRO": 30.758717494089833,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-05",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "Sao10K/L3-8B-Stheno-v3.2"
  },
  {
    "eval_name": "Sao10K_L3-8B-Stheno-v3.3-32K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/L3-8B-Stheno-v3.3-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/L3-8B-Stheno-v3.3-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__L3-8B-Stheno-v3.3-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/L3-8B-Stheno-v3.3-32K",
    "Model sha": "1a59d163e079c7e7f1542553d085853119960f0c",
    "Average ‚¨ÜÔ∏è": 12.587040520385296,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 51,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4686322460719525,
    "IFEval Raw": 0.46037181345496614,
    "IFEval": 46.03718134549661,
    "BBH Raw": 0.3844012923008206,
    "BBH": 13.512008983197541,
    "MATH Lvl 5 Raw": 0.01057401812688822,
    "MATH Lvl 5": 1.057401812688822,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.3725416666666667,
    "MUSR": 4.067708333333334,
    "MMLU-PRO Raw": 0.1895777925531915,
    "MMLU-PRO": 9.95308806146572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Sao10K/L3-8B-Stheno-v3.3-32K"
  },
  {
    "eval_name": "Sao10K_MN-12B-Lyra-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Sao10K/MN-12B-Lyra-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Sao10K/MN-12B-Lyra-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Sao10K__MN-12B-Lyra-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Sao10K/MN-12B-Lyra-v3",
    "Model sha": "da76fa39d128ca84065427189bb228f2dfc6b8a3",
    "Average ‚¨ÜÔ∏è": 19.39645689227663,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 33,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.084664771380936,
    "IFEval Raw": 0.4486063644463357,
    "IFEval": 44.86063644463357,
    "BBH Raw": 0.4803954360397243,
    "BBH": 25.870963383072453,
    "MATH Lvl 5 Raw": 0.07930513595166162,
    "MATH Lvl 5": 7.930513595166162,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.40190624999999996,
    "MUSR": 9.038281250000004,
    "MMLU-PRO Raw": 0.32488364361702127,
    "MMLU-PRO": 24.987071513002363,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-03",
    "Generation": 0,
    "Base Model": "Sao10K/MN-12B-Lyra-v3"
  },
  {
    "eval_name": "Saxo_Linkbricks-Horizon-AI-Korean-Superb-27B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Saxo/Linkbricks-Horizon-AI-Korean-Superb-27B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Saxo/Linkbricks-Horizon-AI-Korean-Superb-27B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Saxo__Linkbricks-Horizon-AI-Korean-Superb-27B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Saxo/Linkbricks-Horizon-AI-Korean-Superb-27B",
    "Model sha": "0b4cf265801f8ee050a54eea7ee51d3142e98c74",
    "Average ‚¨ÜÔ∏è": 38.32420717396262,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.8884690055220217,
    "IFEval Raw": 0.7767601076255447,
    "IFEval": 77.67601076255447,
    "BBH Raw": 0.6518345685119445,
    "BBH": 50.60725661580997,
    "MATH Lvl 5 Raw": 0.26963746223564955,
    "MATH Lvl 5": 26.963746223564954,
    "GPQA Raw": 0.3598993288590604,
    "GPQA": 14.65324384787472,
    "MUSR Raw": 0.47913541666666665,
    "MUSR": 19.52526041666666,
    "MMLU-PRO Raw": 0.4646775265957447,
    "MMLU-PRO": 40.51972517730496,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 2,
    "Base Model": "google/gemma-2-27b"
  },
  {
    "eval_name": "SeaLLMs_SeaLLM-7B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SeaLLMs/SeaLLM-7B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SeaLLMs/SeaLLM-7B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SeaLLMs__SeaLLM-7B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SeaLLMs/SeaLLM-7B-v2",
    "Model sha": "35c5464399144a14915733dc690c4a74e1f71b16",
    "Average ‚¨ÜÔ∏è": 18.116037314356188,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 65,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.63489301124129,
    "IFEval Raw": 0.36712367629002157,
    "IFEval": 36.71236762900216,
    "BBH Raw": 0.4902100795458318,
    "BBH": 27.438159401570932,
    "MATH Lvl 5 Raw": 0.0823262839879154,
    "MATH Lvl 5": 8.23262839879154,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.4069583333333333,
    "MUSR": 9.36979166666667,
    "MMLU-PRO Raw": 0.30826130319148937,
    "MMLU-PRO": 23.140144799054376,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-29",
    "Submission Date": "2024-09-17",
    "Generation": 0,
    "Base Model": "SeaLLMs/SeaLLM-7B-v2"
  },
  {
    "eval_name": "SeaLLMs_SeaLLM-7B-v2.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SeaLLMs/SeaLLM-7B-v2.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SeaLLMs/SeaLLM-7B-v2.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SeaLLMs__SeaLLM-7B-v2.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SeaLLMs/SeaLLM-7B-v2.5",
    "Model sha": "a961daf713dcb31e3253ebe40d43ea5fb7a84099",
    "Average ‚¨ÜÔ∏è": 18.930467329222825,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 49,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1009767218520359,
    "IFEval Raw": 0.4521536190640833,
    "IFEval": 45.21536190640833,
    "BBH Raw": 0.49802029594352754,
    "BBH": 28.738153930102815,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.42032291666666666,
    "MUSR": 11.60703125,
    "MMLU-PRO Raw": 0.3203125,
    "MMLU-PRO": 24.479166666666664,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "SeaLLMs/SeaLLM-7B-v2.5"
  },
  {
    "eval_name": "SeaLLMs_SeaLLMs-v3-7B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SeaLLMs/SeaLLMs-v3-7B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SeaLLMs/SeaLLMs-v3-7B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SeaLLMs__SeaLLMs-v3-7B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SeaLLMs/SeaLLMs-v3-7B-Chat",
    "Model sha": "67ef6dfd0a5df7af4be7a325786105a2ba4cbaf7",
    "Average ‚¨ÜÔ∏è": 23.884404819428013,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 44,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8428531246952065,
    "IFEval Raw": 0.43766539448662883,
    "IFEval": 43.76653944866288,
    "BBH Raw": 0.5266406284595359,
    "BBH": 33.801622722378404,
    "MATH Lvl 5 Raw": 0.1661631419939577,
    "MATH Lvl 5": 16.61631419939577,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.417375,
    "MUSR": 10.471875000000002,
    "MMLU-PRO Raw": 0.3894614361702128,
    "MMLU-PRO": 32.16238179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-03",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "SeaLLMs/SeaLLMs-v3-7B-Chat"
  },
  {
    "eval_name": "SenseLLM_ReflectionCoder-CL-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SenseLLM/ReflectionCoder-CL-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SenseLLM/ReflectionCoder-CL-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SenseLLM__ReflectionCoder-CL-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SenseLLM/ReflectionCoder-CL-34B",
    "Model sha": "e939100132251cf340ba88d9bdd342faa3c3b211",
    "Average ‚¨ÜÔ∏è": 11.933934311549068,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 33,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9700698270129788,
    "IFEval Raw": 0.4007710652180658,
    "IFEval": 40.077106521806584,
    "BBH Raw": 0.39529304297033296,
    "BBH": 14.264686822563535,
    "MATH Lvl 5 Raw": 0.020392749244712995,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.41548958333333336,
    "MUSR": 10.402864583333335,
    "MMLU-PRO Raw": 0.14237034574468085,
    "MMLU-PRO": 4.707816193853427,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "SenseLLM/ReflectionCoder-CL-34B"
  },
  {
    "eval_name": "SenseLLM_ReflectionCoder-DS-33B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SenseLLM/ReflectionCoder-DS-33B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SenseLLM/ReflectionCoder-DS-33B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SenseLLM__ReflectionCoder-DS-33B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SenseLLM/ReflectionCoder-DS-33B",
    "Model sha": "07ae97a21fbef0503294e1eb258ce0a308b8dc35",
    "Average ‚¨ÜÔ∏è": 9.056025736412328,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 33,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.3095281597481114,
    "IFEval Raw": 0.3786641666334215,
    "IFEval": 37.86641666334215,
    "BBH Raw": 0.3449447540164568,
    "BBH": 8.337659356727954,
    "MATH Lvl 5 Raw": 0.021903323262839884,
    "MATH Lvl 5": 2.1903323262839884,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3343125,
    "MUSR": 0.45572916666666624,
    "MMLU-PRO Raw": 0.12017952127659574,
    "MMLU-PRO": 2.24216903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "SenseLLM/ReflectionCoder-DS-33B"
  },
  {
    "eval_name": "SeppeV_SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SeppeV/SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SeppeV/SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SeppeV__SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SeppeV/SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo",
    "Model sha": "6ced77bb27efc0d6f33d447b9cc8fca35976e91c",
    "Average ‚¨ÜÔ∏è": 4.0981087422687,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.32719630615710266,
    "IFEval Raw": 0.09554648333089535,
    "IFEval": 9.554648333089535,
    "BBH Raw": 0.3072665948660797,
    "BBH": 3.612865412111988,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.40320833333333334,
    "MUSR": 8.401041666666666,
    "MMLU-PRO Raw": 0.11610704787234043,
    "MMLU-PRO": 1.7896719858156023,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "SeppeV/SmolLM_pretrained_with_sft_trained_with_1pc_data_on_a_preference_dpo (Merge)"
  },
  {
    "eval_name": "Shreyash2010_Uma-4x4B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Shreyash2010/Uma-4x4B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Shreyash2010/Uma-4x4B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Shreyash2010__Uma-4x4B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Shreyash2010/Uma-4x4B-Instruct-v0.1",
    "Model sha": "f78146bdd1632585b3520717885e0ca41ddbce69",
    "Average ‚¨ÜÔ∏è": 27.695518682317058,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1917650832983249,
    "IFEval Raw": 0.5516961661724225,
    "IFEval": 55.169616617242255,
    "BBH Raw": 0.5511602059856503,
    "BBH": 36.28453127383045,
    "MATH Lvl 5 Raw": 0.1638972809667674,
    "MATH Lvl 5": 16.38972809667674,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4441041666666667,
    "MUSR": 15.146354166666663,
    "MMLU-PRO Raw": 0.386968085106383,
    "MMLU-PRO": 31.885342789598102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-25",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "SicariusSicariiStuff_2B-ad_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/2B-ad\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/2B-ad</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__2B-ad-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/2B-ad",
    "Model sha": "fa0e405edfb1c6e454b7a25852b5bbf5049cf132",
    "Average ‚¨ÜÔ∏è": 15.818025859469854,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7392998398253874,
    "IFEval Raw": 0.4378903531518593,
    "IFEval": 43.78903531518593,
    "BBH Raw": 0.40922431523996955,
    "BBH": 16.007592932115813,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.40153124999999995,
    "MUSR": 8.124739583333335,
    "MMLU-PRO Raw": 0.2662067819148936,
    "MMLU-PRO": 18.467420212765955,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/2B-ad"
  },
  {
    "eval_name": "SicariusSicariiStuff_2B_or_not_2B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/2B_or_not_2B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/2B_or_not_2B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__2B_or_not_2B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/2B_or_not_2B",
    "Model sha": "abf87e8422284aa83a42efd7a91154f9af3c7ed3",
    "Average ‚¨ÜÔ∏è": 6.579424547799557,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.877135651473592,
    "IFEval Raw": 0.2062316874781136,
    "IFEval": 20.623168747811363,
    "BBH Raw": 0.3415917024092019,
    "BBH": 7.68230049623281,
    "MATH Lvl 5 Raw": 0.018882175226586105,
    "MATH Lvl 5": 1.8882175226586104,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.3790833333333334,
    "MUSR": 4.8520833333333355,
    "MMLU-PRO Raw": 0.13987699468085107,
    "MMLU-PRO": 4.43077718676123,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-11",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/2B_or_not_2B"
  },
  {
    "eval_name": "SicariusSicariiStuff_Dusk_Rainbow_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Dusk_Rainbow\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Dusk_Rainbow</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Dusk_Rainbow-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Dusk_Rainbow",
    "Model sha": "106058ac50593d65bc4b5ae75c8c010e87cd8487",
    "Average ‚¨ÜÔ∏è": 18.611302510505624,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 29,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.604654524294257,
    "IFEval Raw": 0.3588057465303173,
    "IFEval": 35.88057465303173,
    "BBH Raw": 0.47717504280736184,
    "BBH": 25.95903682422342,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.40252083333333327,
    "MUSR": 7.448437499999998,
    "MMLU-PRO Raw": 0.3443317819148936,
    "MMLU-PRO": 27.14797576832151,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-10-08",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/Dusk_Rainbow"
  },
  {
    "eval_name": "SicariusSicariiStuff_Impish_LLAMA_3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Impish_LLAMA_3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Impish_LLAMA_3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Impish_LLAMA_3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Impish_LLAMA_3B",
    "Model sha": "72703d3083d1a67849cbea0b7add3c1270a77cc7",
    "Average ‚¨ÜÔ∏è": 17.779359377064228,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7229523812767585,
    "IFEval Raw": 0.46299485365496884,
    "IFEval": 46.29948536549688,
    "BBH Raw": 0.40905101627873225,
    "BBH": 16.98575485690441,
    "MATH Lvl 5 Raw": 0.11178247734138971,
    "MATH Lvl 5": 11.17824773413897,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.3672708333333334,
    "MUSR": 5.60885416666667,
    "MMLU-PRO Raw": 0.2941323138297872,
    "MMLU-PRO": 21.570257092198577,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/Impish_LLAMA_3B"
  },
  {
    "eval_name": "SicariusSicariiStuff_LLAMA-3_8B_Unaligned_BETA_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__LLAMA-3_8B_Unaligned_BETA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA",
    "Model sha": "9bc5b68a7448a4e46eeaf27a4ac477d79578db95",
    "Average ‚¨ÜÔ∏è": 19.153340496601754,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7493732199734641,
    "IFEval Raw": 0.3713203189758729,
    "IFEval": 37.13203189758729,
    "BBH Raw": 0.4717234028484832,
    "BBH": 24.998013753807424,
    "MATH Lvl 5 Raw": 0.08459214501510574,
    "MATH Lvl 5": 8.459214501510575,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.41194791666666664,
    "MUSR": 9.56015625,
    "MMLU-PRO Raw": 0.3464926861702128,
    "MMLU-PRO": 27.38807624113475,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/LLAMA-3_8B_Unaligned_BETA"
  },
  {
    "eval_name": "SicariusSicariiStuff_Qwen2.5-14B_Uncencored_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Qwen2.5-14B_Uncencored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Qwen2.5-14B_Uncencored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Qwen2.5-14B_Uncencored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Qwen2.5-14B_Uncencored",
    "Model sha": "1daf648ac2f837c66bf6bb00459e034987d9486f",
    "Average ‚¨ÜÔ∏è": 31.674586251326968,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.7391953907317577,
    "IFEval Raw": 0.31579099012841483,
    "IFEval": 31.579099012841482,
    "BBH Raw": 0.6308941945507827,
    "BBH": 46.7202351109504,
    "MATH Lvl 5 Raw": 0.3149546827794562,
    "MATH Lvl 5": 31.49546827794562,
    "GPQA Raw": 0.38171140939597314,
    "GPQA": 17.561521252796418,
    "MUSR Raw": 0.45166666666666666,
    "MUSR": 15.291666666666666,
    "MMLU-PRO Raw": 0.526595744680851,
    "MMLU-PRO": 47.399527186761226,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-20",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "SicariusSicariiStuff_Qwen2.5-14B_Uncensored_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Qwen2.5-14B_Uncensored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Qwen2.5-14B_Uncensored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Qwen2.5-14B_Uncensored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Qwen2.5-14B_Uncensored",
    "Model sha": "0710a2341d269dcd56f9136fed442373d4dadc5d",
    "Average ‚¨ÜÔ∏è": 31.69998183135093,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4208867010472463,
    "IFEval Raw": 0.3173147249298528,
    "IFEval": 31.73147249298528,
    "BBH Raw": 0.6308941945507827,
    "BBH": 46.7202351109504,
    "MATH Lvl 5 Raw": 0.3149546827794562,
    "MATH Lvl 5": 31.49546827794562,
    "GPQA Raw": 0.38171140939597314,
    "GPQA": 17.561521252796418,
    "MUSR Raw": 0.45166666666666666,
    "MUSR": 15.291666666666666,
    "MMLU-PRO Raw": 0.526595744680851,
    "MMLU-PRO": 47.399527186761226,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "SicariusSicariiStuff_Qwen2.5-14B_Uncensored_Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Qwen2.5-14B_Uncensored_Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Qwen2.5-14B_Uncensored_Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Qwen2.5-14B_Uncensored_Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Qwen2.5-14B_Uncensored_Instruct",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 28.56856067039128,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.9051631212923,
    "IFEval Raw": 0.3789389929830627,
    "IFEval": 37.89389929830627,
    "BBH Raw": 0.5936792404117958,
    "BBH": 42.113096716972805,
    "MATH Lvl 5 Raw": 0.30513595166163143,
    "MATH Lvl 5": 30.513595166163142,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.36965625,
    "MUSR": 4.40703125,
    "MMLU-PRO Raw": 0.5127160904255319,
    "MMLU-PRO": 45.85734338061466,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "SicariusSicariiStuff_Zion_Alpha_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/Zion_Alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/Zion_Alpha</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__Zion_Alpha-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/Zion_Alpha",
    "Model sha": "e52e1b6e98dce3a54d82f87f83920c0a3f189457",
    "Average ‚¨ÜÔ∏è": 19.186491401477962,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5904773190551414,
    "IFEval Raw": 0.3324024698910003,
    "IFEval": 33.24024698910004,
    "BBH Raw": 0.49321099934509743,
    "BBH": 29.160501194115735,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4726875,
    "MUSR": 18.452604166666667,
    "MMLU-PRO Raw": 0.31316489361702127,
    "MMLU-PRO": 23.684988179669027,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-10-18",
    "Generation": 0,
    "Base Model": "SicariusSicariiStuff/Zion_Alpha"
  },
  {
    "eval_name": "SicariusSicariiStuff_dn_ep02_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/SicariusSicariiStuff/dn_ep02\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">SicariusSicariiStuff/dn_ep02</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/SicariusSicariiStuff__dn_ep02-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "SicariusSicariiStuff/dn_ep02",
    "Model sha": "ab9d5937cff45d0da251d6094cbf5a3cef4d42d8",
    "Average ‚¨ÜÔ∏è": 25.271852358738084,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6937297804663696,
    "IFEval Raw": 0.5064340394597445,
    "IFEval": 50.643403945974455,
    "BBH Raw": 0.5266008759836228,
    "BBH": 32.6437741461978,
    "MATH Lvl 5 Raw": 0.14123867069486404,
    "MATH Lvl 5": 14.123867069486403,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.43163541666666666,
    "MUSR": 12.187760416666668,
    "MMLU-PRO Raw": 0.39976728723404253,
    "MMLU-PRO": 33.30747635933806,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Solshine_Brimful-merged-replete_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Solshine/Brimful-merged-replete\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Solshine/Brimful-merged-replete</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Solshine__Brimful-merged-replete-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Solshine/Brimful-merged-replete",
    "Model sha": "01ce8c3df6edb87d31f0e9a9651cbcbc4d4823e8",
    "Average ‚¨ÜÔ∏è": 3.829474587630019,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.166723268658952,
    "IFEval Raw": 0.17605619755581856,
    "IFEval": 17.605619755581856,
    "BBH Raw": 0.28834447696551024,
    "BBH": 1.9921389967360958,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.342125,
    "MUSR": 1.432291666666666,
    "MMLU-PRO Raw": 0.10846077127659574,
    "MMLU-PRO": 0.9400856973995264,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "Solshine/Brimful-merged-replete (Merge)"
  },
  {
    "eval_name": "Solshine_Llama-3-1-big-thoughtful-passthrough-merge-2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Solshine/Llama-3-1-big-thoughtful-passthrough-merge-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Solshine/Llama-3-1-big-thoughtful-passthrough-merge-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Solshine__Llama-3-1-big-thoughtful-passthrough-merge-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Solshine/Llama-3-1-big-thoughtful-passthrough-merge-2",
    "Model sha": "d48047d6577e22fdda73a1be8e18971912db66d2",
    "Average ‚¨ÜÔ∏è": 6.777645791540958,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 18,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.381352696123503,
    "IFEval Raw": 0.25466650709007654,
    "IFEval": 25.466650709007656,
    "BBH Raw": 0.32093808427144627,
    "BBH": 5.008442306492267,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.38894791666666667,
    "MUSR": 6.751822916666666,
    "MMLU-PRO Raw": 0.11851728723404255,
    "MMLU-PRO": 2.05747635933806,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "Solshine/Llama-3-1-big-thoughtful-passthrough-merge-2 (Merge)"
  },
  {
    "eval_name": "Stark2008_GutenLaserPi_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Stark2008/GutenLaserPi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Stark2008/GutenLaserPi</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Stark2008__GutenLaserPi-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Stark2008/GutenLaserPi",
    "Model sha": "d5ab84c6f8f0c88c16380242c7e11e8cefc934b7",
    "Average ‚¨ÜÔ∏è": 21.287431571598006,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5699651837437177,
    "IFEval Raw": 0.42265300513747966,
    "IFEval": 42.26530051374797,
    "BBH Raw": 0.5212342482489518,
    "BBH": 32.97771006701662,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4620208333333333,
    "MUSR": 16.985937499999995,
    "MMLU-PRO Raw": 0.31058843085106386,
    "MMLU-PRO": 23.39871453900709,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-11",
    "Submission Date": "2024-07-11",
    "Generation": 1,
    "Base Model": "Stark2008/GutenLaserPi (Merge)"
  },
  {
    "eval_name": "Stark2008_LayleleFlamPi_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Stark2008/LayleleFlamPi\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Stark2008/LayleleFlamPi</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Stark2008__LayleleFlamPi-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Stark2008/LayleleFlamPi",
    "Model sha": "b2897d17a65dea17383f52711475c8b41567c5d0",
    "Average ‚¨ÜÔ∏è": 20.69486258112838,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6296579933124996,
    "IFEval Raw": 0.42842325030917966,
    "IFEval": 42.84232503091796,
    "BBH Raw": 0.5115654142581095,
    "BBH": 31.20740955947399,
    "MATH Lvl 5 Raw": 0.055891238670694864,
    "MATH Lvl 5": 5.589123867069486,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.46084375,
    "MUSR": 16.57213541666667,
    "MMLU-PRO Raw": 0.3093417553191489,
    "MMLU-PRO": 23.26019503546099,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-12",
    "Submission Date": "2024-07-12",
    "Generation": 1,
    "Base Model": "Stark2008/LayleleFlamPi (Merge)"
  },
  {
    "eval_name": "Stark2008_VisFlamCat_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Stark2008/VisFlamCat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Stark2008/VisFlamCat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Stark2008__VisFlamCat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Stark2008/VisFlamCat",
    "Model sha": "290efa41ac83b8408cab084d093bcd9ae9abb0c9",
    "Average ‚¨ÜÔ∏è": 21.164673480185755,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6102159266576387,
    "IFEval Raw": 0.43659157701565177,
    "IFEval": 43.65915770156518,
    "BBH Raw": 0.5216957865099948,
    "BBH": 32.881396834037055,
    "MATH Lvl 5 Raw": 0.06570996978851963,
    "MATH Lvl 5": 6.570996978851963,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.44627083333333334,
    "MUSR": 14.68385416666667,
    "MMLU-PRO Raw": 0.31441156914893614,
    "MMLU-PRO": 23.823507683215126,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-12",
    "Submission Date": "2024-07-12",
    "Generation": 1,
    "Base Model": "Stark2008/VisFlamCat (Merge)"
  },
  {
    "eval_name": "StelleX_Qwen2.5_Math_7B_Cot_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/StelleX/Qwen2.5_Math_7B_Cot\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">StelleX/Qwen2.5_Math_7B_Cot</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/StelleX__Qwen2.5_Math_7B_Cot-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "StelleX/Qwen2.5_Math_7B_Cot",
    "Model sha": "1549288a296c6e44cfcf4b9513769000bc768e36",
    "Average ‚¨ÜÔ∏è": 17.21021450760981,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0279919496579069,
    "IFEval Raw": 0.2142747908881767,
    "IFEval": 21.42747908881767,
    "BBH Raw": 0.4312922433417096,
    "BBH": 19.796911486609314,
    "MATH Lvl 5 Raw": 0.290785498489426,
    "MATH Lvl 5": 29.078549848942597,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.39241666666666664,
    "MUSR": 6.918750000000002,
    "MMLU-PRO Raw": 0.281000664893617,
    "MMLU-PRO": 20.111184988179666,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-29",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "StelleX_Vorisatex-7B-preview_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/StelleX/Vorisatex-7B-preview\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">StelleX/Vorisatex-7B-preview</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/StelleX__Vorisatex-7B-preview-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "StelleX/Vorisatex-7B-preview",
    "Model sha": "57612bb8af75e5e8d75b4df3dde993fdc48efbea",
    "Average ‚¨ÜÔ∏è": 5.627322193913661,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2490714881171332,
    "IFEval Raw": 0.1515013497519914,
    "IFEval": 15.150134975199137,
    "BBH Raw": 0.3111695757290421,
    "BBH": 4.133712426973548,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.41923958333333333,
    "MUSR": 11.504947916666666,
    "MMLU-PRO Raw": 0.11660571808510638,
    "MMLU-PRO": 1.8450797872340412,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-29",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Svak_MN-12B-Inferor-v0.0_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Svak/MN-12B-Inferor-v0.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Svak/MN-12B-Inferor-v0.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Svak__MN-12B-Inferor-v0.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Svak/MN-12B-Inferor-v0.0",
    "Model sha": "ab9efd0cc19b862ea1ab37a60dacac78aa022ad1",
    "Average ‚¨ÜÔ∏è": 25.38575801243763,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.247129792198927,
    "IFEval Raw": 0.5707555951541909,
    "IFEval": 57.07555951541909,
    "BBH Raw": 0.5195010930589931,
    "BBH": 30.846426792178836,
    "MATH Lvl 5 Raw": 0.10045317220543806,
    "MATH Lvl 5": 10.045317220543806,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.46388541666666666,
    "MUSR": 18.085677083333334,
    "MMLU-PRO Raw": 0.3558843085106383,
    "MMLU-PRO": 28.431589834515364,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Svak/MN-12B-Inferor-v0.0 (Merge)"
  },
  {
    "eval_name": "Svak_MN-12B-Inferor-v0.1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Svak/MN-12B-Inferor-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Svak/MN-12B-Inferor-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Svak__MN-12B-Inferor-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Svak/MN-12B-Inferor-v0.1",
    "Model sha": "2d8cfac16dac3151d5e8e5ecd62866ca83c5149a",
    "Average ‚¨ÜÔ∏è": 26.79906604691008,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.995382493039316,
    "IFEval Raw": 0.6346527214457639,
    "IFEval": 63.46527214457639,
    "BBH Raw": 0.5146762089838804,
    "BBH": 30.850764971038174,
    "MATH Lvl 5 Raw": 0.11782477341389729,
    "MATH Lvl 5": 11.782477341389729,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.4350833333333333,
    "MUSR": 15.052083333333336,
    "MMLU-PRO Raw": 0.3661901595744681,
    "MMLU-PRO": 29.57668439716312,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "Svak/MN-12B-Inferor-v0.1 (Merge)"
  },
  {
    "eval_name": "Syed-Hasan-8503_Phi-3-mini-4K-instruct-cpo-simpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Syed-Hasan-8503/Phi-3-mini-4K-instruct-cpo-simpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Syed-Hasan-8503/Phi-3-mini-4K-instruct-cpo-simpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Syed-Hasan-8503__Phi-3-mini-4K-instruct-cpo-simpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Syed-Hasan-8503/Phi-3-mini-4K-instruct-cpo-simpo",
    "Model sha": "2896ef357be81fd433c17801d76ce148e60a7032",
    "Average ‚¨ÜÔ∏è": 25.995327153379638,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1642383119217057,
    "IFEval Raw": 0.5714049832222946,
    "IFEval": 57.14049832222946,
    "BBH Raw": 0.5681534123661078,
    "BBH": 39.148157776889455,
    "MATH Lvl 5 Raw": 0.0838368580060423,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.3963541666666666,
    "MUSR": 8.777604166666663,
    "MMLU-PRO Raw": 0.38605385638297873,
    "MMLU-PRO": 31.783761820330973,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Syed-Hasan-8503/Phi-3-mini-4K-instruct-cpo-simpo"
  },
  {
    "eval_name": "T145_qwen-2.5-3B-merge-test_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/T145/qwen-2.5-3B-merge-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">T145/qwen-2.5-3B-merge-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/T145__qwen-2.5-3B-merge-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "T145/qwen-2.5-3B-merge-test",
    "Model sha": "0d5f82d841f811fbf1ee07bfbf7c6eb1de812840",
    "Average ‚¨ÜÔ∏è": 21.154150518099627,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7839565865232557,
    "IFEval Raw": 0.5751018408932742,
    "IFEval": 57.510184089327424,
    "BBH Raw": 0.4842488747720393,
    "BBH": 27.889341313676073,
    "MATH Lvl 5 Raw": 0.030966767371601207,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40072916666666664,
    "MUSR": 8.291145833333333,
    "MMLU-PRO Raw": 0.3289561170212766,
    "MMLU-PRO": 25.439568557919618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "T145/qwen-2.5-3B-merge-test (Merge)"
  },
  {
    "eval_name": "THUDM_glm-4-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "ChatGLMModelM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/THUDM/glm-4-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">THUDM/glm-4-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/THUDM__glm-4-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "THUDM/glm-4-9b",
    "Model sha": "99a140996f9d4f197842fb6b1aab217a42e27ef3",
    "Average ‚¨ÜÔ∏è": 18.006731731716215,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 111,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6724468375511796,
    "IFEval Raw": 0.1426082793654171,
    "IFEval": 14.260827936541709,
    "BBH Raw": 0.5528368141665274,
    "BBH": 35.811283581208905,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.4385833333333333,
    "MUSR": 14.189583333333331,
    "MMLU-PRO Raw": 0.4144780585106383,
    "MMLU-PRO": 34.94200650118203,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "THUDM/glm-4-9b"
  },
  {
    "eval_name": "THUDM_glm-4-9b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "ChatGLMModelM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/THUDM/glm-4-9b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">THUDM/glm-4-9b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/THUDM__glm-4-9b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "THUDM/glm-4-9b-chat",
    "Model sha": "04419001bc63e05e70991ade6da1f91c4aeec278",
    "Average ‚¨ÜÔ∏è": 10.973477297045166,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 630,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.2471345084927311,
    "IFEval Raw": 0.0,
    "IFEval": 0.0,
    "BBH Raw": 0.47363884291035735,
    "BBH": 25.205183674440235,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.3994270833333333,
    "MUSR": 8.061718749999999,
    "MMLU-PRO Raw": 0.316655585106383,
    "MMLU-PRO": 24.072842789598106,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-07-09",
    "Generation": 0,
    "Base Model": "THUDM/glm-4-9b-chat"
  },
  {
    "eval_name": "THUDM_glm-4-9b-chat-1m_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "ChatGLMModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/THUDM/glm-4-9b-chat-1m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">THUDM/glm-4-9b-chat-1m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/THUDM__glm-4-9b-chat-1m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "THUDM/glm-4-9b-chat-1m",
    "Model sha": "0aa722c7e0745dd21453427dd44c257dd253304f",
    "Average ‚¨ÜÔ∏è": 8.922510186531982,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 178,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.20566995498616636,
    "IFEval Raw": 0.0,
    "IFEval": 0.0,
    "BBH Raw": 0.41800578218330303,
    "BBH": 17.10802850816805,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.3794583333333333,
    "MUSR": 5.232291666666668,
    "MMLU-PRO Raw": 0.31632313829787234,
    "MMLU-PRO": 24.03590425531915,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-10-09",
    "Generation": 0,
    "Base Model": "THUDM/glm-4-9b-chat-1m"
  },
  {
    "eval_name": "TIGER-Lab_MAmmoTH2-7B-Plus_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TIGER-Lab/MAmmoTH2-7B-Plus\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TIGER-Lab/MAmmoTH2-7B-Plus</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TIGER-Lab__MAmmoTH2-7B-Plus-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TIGER-Lab/MAmmoTH2-7B-Plus",
    "Model sha": "3ed578d8dda09787137e363a0dc32e3a8ed908de",
    "Average ‚¨ÜÔ∏è": 21.46986225962917,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5526633505557356,
    "IFEval Raw": 0.5574664113441224,
    "IFEval": 55.74664113441224,
    "BBH Raw": 0.42346949888019064,
    "BBH": 18.925953227555734,
    "MATH Lvl 5 Raw": 0.17598187311178246,
    "MATH Lvl 5": 17.598187311178247,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.41235416666666663,
    "MUSR": 10.1109375,
    "MMLU-PRO Raw": 0.30169547872340424,
    "MMLU-PRO": 22.410608747044915,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-06",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "TIGER-Lab/MAmmoTH2-7B-Plus"
  },
  {
    "eval_name": "TTTXXX01_Mistral-7B-Base-SimPO2-5e-7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TTTXXX01/Mistral-7B-Base-SimPO2-5e-7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TTTXXX01/Mistral-7B-Base-SimPO2-5e-7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TTTXXX01__Mistral-7B-Base-SimPO2-5e-7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TTTXXX01/Mistral-7B-Base-SimPO2-5e-7",
    "Model sha": "7a271e3061165f4e1abfe26715c04e20c2ac935e",
    "Average ‚¨ÜÔ∏è": 16.37968840147641,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5229961322672869,
    "IFEval Raw": 0.43918912928806675,
    "IFEval": 43.91891292880668,
    "BBH Raw": 0.43195515014882774,
    "BBH": 20.692627382557507,
    "MATH Lvl 5 Raw": 0.02416918429003021,
    "MATH Lvl 5": 2.416918429003021,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.36041666666666666,
    "MUSR": 5.252083333333334,
    "MMLU-PRO Raw": 0.2765957446808511,
    "MMLU-PRO": 19.62174940898345,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-09-01",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "TeeZee_DoubleBagel-57B-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TeeZee/DoubleBagel-57B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TeeZee/DoubleBagel-57B-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TeeZee__DoubleBagel-57B-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TeeZee/DoubleBagel-57B-v1.0",
    "Model sha": "6e10dc1fb5223d1b045dc2a19c9c267a574e520f",
    "Average ‚¨ÜÔ∏è": 8.54410296272912,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 56,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.368647478172592,
    "IFEval Raw": 0.23363342597640924,
    "IFEval": 23.363342597640923,
    "BBH Raw": 0.325078559362514,
    "BBH": 5.5227816982611495,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.43148958333333337,
    "MUSR": 13.60286458333333,
    "MMLU-PRO Raw": 0.14777260638297873,
    "MMLU-PRO": 5.308067375886525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-05",
    "Submission Date": "2024-08-10",
    "Generation": 1,
    "Base Model": "TeeZee/DoubleBagel-57B-v1.0 (Merge)"
  },
  {
    "eval_name": "TencentARC_LLaMA-Pro-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TencentARC__LLaMA-Pro-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TencentARC/LLaMA-Pro-8B",
    "Model sha": "7115e7179060e0623d1ee9ff4476faed7e478d8c",
    "Average ‚¨ÜÔ∏è": 8.778934275693588,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 172,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 47.80773355108649,
    "IFEval Raw": 0.2277135777514772,
    "IFEval": 22.77135777514772,
    "BBH Raw": 0.3484197711435169,
    "BBH": 9.2939499758607,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.40181249999999996,
    "MUSR": 8.593229166666669,
    "MMLU-PRO Raw": 0.18110039893617022,
    "MMLU-PRO": 9.011155437352246,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-05",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "TencentARC/LLaMA-Pro-8B"
  },
  {
    "eval_name": "TencentARC_LLaMA-Pro-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/LLaMA-Pro-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/LLaMA-Pro-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TencentARC__LLaMA-Pro-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TencentARC/LLaMA-Pro-8B-Instruct",
    "Model sha": "9850c8afce19a69d8fc4a1603a82441157514016",
    "Average ‚¨ÜÔ∏è": 15.144990895303266,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 62,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1052033558226526,
    "IFEval Raw": 0.4486063644463357,
    "IFEval": 44.86063644463357,
    "BBH Raw": 0.4224205282459997,
    "BBH": 19.485726056875954,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.41902083333333334,
    "MUSR": 11.1109375,
    "MMLU-PRO Raw": 0.19456449468085107,
    "MMLU-PRO": 10.507166075650117,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-06",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "TencentARC/LLaMA-Pro-8B-Instruct"
  },
  {
    "eval_name": "TencentARC_MetaMath-Mistral-Pro_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/MetaMath-Mistral-Pro\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/MetaMath-Mistral-Pro</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TencentARC__MetaMath-Mistral-Pro-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TencentARC/MetaMath-Mistral-Pro",
    "Model sha": "3835d38de15ed2a04c32aca879b782fc50e390bf",
    "Average ‚¨ÜÔ∏è": 12.013002201474537,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6007522517906861,
    "IFEval Raw": 0.21187670935340452,
    "IFEval": 21.18767093534045,
    "BBH Raw": 0.44131618555883606,
    "BBH": 22.37227879113455,
    "MATH Lvl 5 Raw": 0.04607250755287009,
    "MATH Lvl 5": 4.607250755287009,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.35241666666666666,
    "MUSR": 4.9854166666666675,
    "MMLU-PRO Raw": 0.2471742021276596,
    "MMLU-PRO": 16.35268912529551,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-26",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "TencentARC/MetaMath-Mistral-Pro"
  },
  {
    "eval_name": "TencentARC_Mistral_Pro_8B_v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TencentARC/Mistral_Pro_8B_v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TencentARC/Mistral_Pro_8B_v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TencentARC__Mistral_Pro_8B_v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TencentARC/Mistral_Pro_8B_v0.1",
    "Model sha": "366f159fc5b314ba2a955209d2bca4600f84dac0",
    "Average ‚¨ÜÔ∏è": 14.195345928021323,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 66,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6324822820385008,
    "IFEval Raw": 0.21145227995053123,
    "IFEval": 21.145227995053123,
    "BBH Raw": 0.4525975968066435,
    "BBH": 22.894188758768042,
    "MATH Lvl 5 Raw": 0.05664652567975831,
    "MATH Lvl 5": 5.664652567975831,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.42422916666666666,
    "MUSR": 11.828645833333335,
    "MMLU-PRO Raw": 0.2765126329787234,
    "MMLU-PRO": 19.61251477541371,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "TencentARC/Mistral_Pro_8B_v0.1"
  },
  {
    "eval_name": "TheDrummer_Cydonia-22B-v1.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheDrummer/Cydonia-22B-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheDrummer/Cydonia-22B-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheDrummer__Cydonia-22B-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheDrummer/Cydonia-22B-v1.2",
    "Model sha": "acd8da5efadc7dc404bb4eeebef2b27b1554a2ca",
    "Average ‚¨ÜÔ∏è": 28.39985671417215,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6287042750308922,
    "IFEval Raw": 0.5635114828654637,
    "IFEval": 56.35114828654636,
    "BBH Raw": 0.580856074392761,
    "BBH": 39.93260406588619,
    "MATH Lvl 5 Raw": 0.1797583081570997,
    "MATH Lvl 5": 17.97583081570997,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.40217708333333335,
    "MUSR": 10.50546875,
    "MMLU-PRO Raw": 0.4140625,
    "MMLU-PRO": 34.895833333333336,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-26",
    "Generation": 0,
    "Base Model": "TheDrummer/Cydonia-22B-v1.2"
  },
  {
    "eval_name": "TheDrummer_Gemmasutra-9B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheDrummer/Gemmasutra-9B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheDrummer/Gemmasutra-9B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheDrummer__Gemmasutra-9B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheDrummer/Gemmasutra-9B-v1",
    "Model sha": "21591f6a0140e095f1c6668ac7a267f214547609",
    "Average ‚¨ÜÔ∏è": 22.736097081617697,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9038187422170423,
    "IFEval Raw": 0.24155130609006326,
    "IFEval": 24.155130609006328,
    "BBH Raw": 0.5886914248369671,
    "BBH": 41.20039631726062,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.48459375,
    "MUSR": 20.940885416666664,
    "MMLU-PRO Raw": 0.4045046542553192,
    "MMLU-PRO": 33.83385047281324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "TheDrummer/Gemmasutra-9B-v1 (Merge)"
  },
  {
    "eval_name": "TheDrummer_Gemmasutra-Mini-2B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheDrummer/Gemmasutra-Mini-2B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheDrummer/Gemmasutra-Mini-2B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheDrummer__Gemmasutra-Mini-2B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheDrummer/Gemmasutra-Mini-2B-v1",
    "Model sha": "c1db4c8f975d3848edbdaf851217039c8dfdaeb5",
    "Average ‚¨ÜÔ∏è": 9.028587586742827,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3979546742564943,
    "IFEval Raw": 0.25486597782771936,
    "IFEval": 25.48659778277193,
    "BBH Raw": 0.35750190791471836,
    "BBH": 9.81033614467704,
    "MATH Lvl 5 Raw": 0.03172205438066465,
    "MATH Lvl 5": 3.1722054380664653,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.3489791666666666,
    "MUSR": 1.1890624999999992,
    "MMLU-PRO Raw": 0.20545212765957446,
    "MMLU-PRO": 11.71690307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "TheDrummer/Gemmasutra-Mini-2B-v1"
  },
  {
    "eval_name": "TheDrummer_Ministrations-8B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheDrummer/Ministrations-8B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheDrummer/Ministrations-8B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheDrummer__Ministrations-8B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheDrummer/Ministrations-8B-v1",
    "Model sha": "39b892de64401ec7990ebb816c4455ba4532bafb",
    "Average ‚¨ÜÔ∏è": 21.151983196990248,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8625558908808669,
    "IFEval Raw": 0.28219346888478125,
    "IFEval": 28.219346888478125,
    "BBH Raw": 0.48766312602251366,
    "BBH": 26.98563733629608,
    "MATH Lvl 5 Raw": 0.17598187311178248,
    "MATH Lvl 5": 17.598187311178247,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.44490625,
    "MUSR": 14.779947916666664,
    "MMLU-PRO Raw": 0.36436170212765956,
    "MMLU-PRO": 29.373522458628837,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-14",
    "Generation": 0,
    "Base Model": "TheDrummer/Ministrations-8B-v1"
  },
  {
    "eval_name": "TheDrummer_Rocinante-12B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheDrummer/Rocinante-12B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheDrummer/Rocinante-12B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheDrummer__Rocinante-12B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheDrummer/Rocinante-12B-v1",
    "Model sha": "74a4ae2584d45655298995198d5ab3e660364a1a",
    "Average ‚¨ÜÔ∏è": 23.60845566468781,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8644415950803428,
    "IFEval Raw": 0.6076499244227538,
    "IFEval": 60.764992442275386,
    "BBH Raw": 0.5065452085797449,
    "BBH": 30.025654065607256,
    "MATH Lvl 5 Raw": 0.06570996978851965,
    "MATH Lvl 5": 6.570996978851965,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.40171874999999996,
    "MUSR": 11.281510416666665,
    "MMLU-PRO Raw": 0.34773936170212766,
    "MMLU-PRO": 27.526595744680847,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-14",
    "Submission Date": "2024-09-03",
    "Generation": 0,
    "Base Model": "TheDrummer/Rocinante-12B-v1"
  },
  {
    "eval_name": "TheTsar1209_nemo-carpmuscle-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/nemo-carpmuscle-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/nemo-carpmuscle-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__nemo-carpmuscle-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/nemo-carpmuscle-v0.1",
    "Model sha": "84d20db8220014958ff157047b2216910637ae39",
    "Average ‚¨ÜÔ∏è": 16.70637219965645,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8084403853529205,
    "IFEval Raw": 0.2275639746982451,
    "IFEval": 22.75639746982451,
    "BBH Raw": 0.5083529697101391,
    "BBH": 30.034995783434088,
    "MATH Lvl 5 Raw": 0.04229607250755288,
    "MATH Lvl 5": 4.229607250755288,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4135,
    "MUSR": 10.220833333333337,
    "MMLU-PRO Raw": 0.3405917553191489,
    "MMLU-PRO": 26.73241725768321,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-15",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "unsloth/Mistral-Nemo-Base-2407-bnb-4bit"
  },
  {
    "eval_name": "TheTsar1209_qwen-carpmuscle-r-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/qwen-carpmuscle-r-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/qwen-carpmuscle-r-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__qwen-carpmuscle-r-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/qwen-carpmuscle-r-v0.3",
    "Model sha": "30f8221d2f5f587343b1dbd65cf7d9bda4f5ef16",
    "Average ‚¨ÜÔ∏è": 31.93755648871615,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2569964031424745,
    "IFEval Raw": 0.44550902715904905,
    "IFEval": 44.55090271590491,
    "BBH Raw": 0.6227124007872,
    "BBH": 46.37591354449345,
    "MATH Lvl 5 Raw": 0.2968277945619336,
    "MATH Lvl 5": 29.68277945619336,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.42776041666666664,
    "MUSR": 12.003385416666669,
    "MMLU-PRO Raw": 0.5103058510638298,
    "MMLU-PRO": 45.58953900709219,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "TheTsar1209/qwen-carpmuscle-r-v0.3 (Merge)"
  },
  {
    "eval_name": "TheTsar1209_qwen-carpmuscle-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/qwen-carpmuscle-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/qwen-carpmuscle-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__qwen-carpmuscle-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/qwen-carpmuscle-v0.1",
    "Model sha": "7c7b06a1788aef48054c3c6d6ad90c6dc5264a81",
    "Average ‚¨ÜÔ∏è": 32.91632851687221,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1762175523031555,
    "IFEval Raw": 0.5621628390448454,
    "IFEval": 56.21628390448454,
    "BBH Raw": 0.643430074129922,
    "BBH": 48.82559521217237,
    "MATH Lvl 5 Raw": 0.23111782477341392,
    "MATH Lvl 5": 23.111782477341393,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.41610416666666666,
    "MUSR": 10.146354166666669,
    "MMLU-PRO Raw": 0.520029920212766,
    "MMLU-PRO": 46.669991134751776,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-05",
    "Submission Date": "2024-10-10",
    "Generation": 3,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "TheTsar1209_qwen-carpmuscle-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/qwen-carpmuscle-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/qwen-carpmuscle-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__qwen-carpmuscle-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/qwen-carpmuscle-v0.2",
    "Model sha": "081f6b067ebca9bc384af283f1d267880534b8e3",
    "Average ‚¨ÜÔ∏è": 33.47789095763978,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2481988911684136,
    "IFEval Raw": 0.5256929391791557,
    "IFEval": 52.56929391791557,
    "BBH Raw": 0.6386922464145662,
    "BBH": 48.18244143380709,
    "MATH Lvl 5 Raw": 0.2719033232628399,
    "MATH Lvl 5": 27.19033232628399,
    "GPQA Raw": 0.35570469798657717,
    "GPQA": 14.093959731543624,
    "MUSR Raw": 0.43455208333333334,
    "MUSR": 12.75234375,
    "MMLU-PRO Raw": 0.5147107712765957,
    "MMLU-PRO": 46.07897458628841,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-19",
    "Generation": 3,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "TheTsar1209_qwen-carpmuscle-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/qwen-carpmuscle-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/qwen-carpmuscle-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__qwen-carpmuscle-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/qwen-carpmuscle-v0.3",
    "Model sha": "ec92820e4ff36b6f21e1ef63546fe2ddcb34456a",
    "Average ‚¨ÜÔ∏è": 31.227939070725686,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.230978874966906,
    "IFEval Raw": 0.4476322823441801,
    "IFEval": 44.76322823441801,
    "BBH Raw": 0.6151533941210218,
    "BBH": 45.5433921378403,
    "MATH Lvl 5 Raw": 0.2794561933534743,
    "MATH Lvl 5": 27.945619335347434,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.4131875,
    "MUSR": 9.781770833333335,
    "MMLU-PRO Raw": 0.5061502659574468,
    "MMLU-PRO": 45.12780732860521,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "TheTsar1209_qwen-carpmuscle-v0.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TheTsar1209/qwen-carpmuscle-v0.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TheTsar1209/qwen-carpmuscle-v0.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TheTsar1209__qwen-carpmuscle-v0.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TheTsar1209/qwen-carpmuscle-v0.4",
    "Model sha": "3e11d5aad0f19bd652b8605620d0cf6af7a0ea00",
    "Average ‚¨ÜÔ∏è": 35.66938822001474,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.36962424840301,
    "IFEval Raw": 0.7202068289915202,
    "IFEval": 72.02068289915204,
    "BBH Raw": 0.6453667027727318,
    "BBH": 49.38495588865565,
    "MATH Lvl 5 Raw": 0.17371601208459214,
    "MATH Lvl 5": 17.371601208459214,
    "GPQA Raw": 0.3523489932885906,
    "GPQA": 13.646532438478745,
    "MUSR Raw": 0.45160416666666664,
    "MUSR": 15.550520833333335,
    "MMLU-PRO Raw": 0.5143783244680851,
    "MMLU-PRO": 46.04203605200946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 3,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "Tijmen2_cosmosage-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tijmen2/cosmosage-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tijmen2/cosmosage-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tijmen2__cosmosage-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tijmen2/cosmosage-v3",
    "Model sha": "e6d4b4e6868fcf113ab5261d71c7214a1f7fbb0c",
    "Average ‚¨ÜÔ∏è": 16.81345765604147,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8308588807599449,
    "IFEval Raw": 0.44823180272787316,
    "IFEval": 44.82318027278731,
    "BBH Raw": 0.4550637900339029,
    "BBH": 22.6871057550123,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4198854166666666,
    "MUSR": 10.685677083333333,
    "MMLU-PRO Raw": 0.24858710106382978,
    "MMLU-PRO": 16.50967789598109,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-20",
    "Submission Date": "2024-08-27",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "TinyLlama_TinyLlama-1.1B-Chat-v0.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-Chat-v0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TinyLlama__TinyLlama-1.1B-Chat-v0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TinyLlama/TinyLlama-1.1B-Chat-v0.5",
    "Model sha": "5c9e70dd07f5234bf6bf6a2425fffeecd5a6020b",
    "Average ‚¨ÜÔ∏è": 4.075811436466043,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.09496383175490669,
    "IFEval Raw": 0.1633665341294432,
    "IFEval": 16.336653412944322,
    "BBH Raw": 0.3105046915935697,
    "BBH": 3.4076909375697055,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.36612500000000003,
    "MUSR": 3.565625000000001,
    "MMLU-PRO Raw": 0.10962433510638298,
    "MMLU-PRO": 1.0693705673758855,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "TinyLlama/TinyLlama-1.1B-Chat-v0.5"
  },
  {
    "eval_name": "TinyLlama_TinyLlama-1.1B-Chat-v0.6_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-Chat-v0.6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TinyLlama__TinyLlama-1.1B-Chat-v0.6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "Model sha": "bf9ae1c8bf026667e6f810768de259bb4a7f4777",
    "Average ‚¨ÜÔ∏è": 4.09286641277805,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 89,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.43034726093831155,
    "IFEval Raw": 0.15742119797692344,
    "IFEval": 15.742119797692347,
    "BBH Raw": 0.3066976656166826,
    "BBH": 3.390370709512531,
    "MATH Lvl 5 Raw": 0.0037764350453172208,
    "MATH Lvl 5": 0.3776435045317221,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.34221875,
    "MUSR": 2.2773437500000004,
    "MMLU-PRO Raw": 0.11486037234042554,
    "MMLU-PRO": 1.6511524822695034,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "TinyLlama/TinyLlama-1.1B-Chat-v0.6"
  },
  {
    "eval_name": "TinyLlama_TinyLlama-1.1B-Chat-v1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama-1.1B-Chat-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TinyLlama__TinyLlama-1.1B-Chat-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    "Model sha": "fe8a4ea1ffedaf415f4da2f062534de366a451e6",
    "Average ‚¨ÜÔ∏è": 2.7181545515830545,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1091,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2684414548124125,
    "IFEval Raw": 0.0595763684800773,
    "IFEval": 5.957636848007731,
    "BBH Raw": 0.3103562867491015,
    "BBH": 4.013396848486799,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.35152083333333334,
    "MUSR": 4.306770833333336,
    "MMLU-PRO Raw": 0.11012300531914894,
    "MMLU-PRO": 1.124778368794326,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
  },
  {
    "eval_name": "TinyLlama_TinyLlama_v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/TinyLlama/TinyLlama_v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">TinyLlama/TinyLlama_v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/TinyLlama__TinyLlama_v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "TinyLlama/TinyLlama_v1.1",
    "Model sha": "ff3c701f2424c7625fdefb9dd470f45ef18b02d6",
    "Average ‚¨ÜÔ∏è": 4.723848910038992,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 74,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.24892861860236287,
    "IFEval Raw": 0.20006139266036338,
    "IFEval": 20.00613926603634,
    "BBH Raw": 0.30237018045076064,
    "BBH": 3.2103010497128146,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.36996874999999996,
    "MUSR": 3.979427083333333,
    "MMLU-PRO Raw": 0.10488696808510638,
    "MMLU-PRO": 0.542996453900708,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-09",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "TinyLlama/TinyLlama_v1.1"
  },
  {
    "eval_name": "Trappu_Magnum-Picaro-0.7-v2-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Trappu/Magnum-Picaro-0.7-v2-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Trappu/Magnum-Picaro-0.7-v2-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Trappu__Magnum-Picaro-0.7-v2-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Trappu/Magnum-Picaro-0.7-v2-12b",
    "Model sha": "2ffc46cde49eb823f5588990bd6b848cd505271e",
    "Average ‚¨ÜÔ∏è": 21.4783017974636,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6749588551373191,
    "IFEval Raw": 0.300278815764394,
    "IFEval": 30.027881576439405,
    "BBH Raw": 0.5506661918828847,
    "BBH": 35.74623319855443,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.32298657718120805,
    "GPQA": 9.731543624161072,
    "MUSR Raw": 0.47271875,
    "MUSR": 19.556510416666665,
    "MMLU-PRO Raw": 0.35804521276595747,
    "MMLU-PRO": 28.671690307328607,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "Trappu/Magnum-Picaro-0.7-v2-12b (Merge)"
  },
  {
    "eval_name": "Trappu_Nemo-Picaro-12B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Trappu/Nemo-Picaro-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Trappu/Nemo-Picaro-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Trappu__Nemo-Picaro-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Trappu/Nemo-Picaro-12B",
    "Model sha": "d65bf383d744998ae93a5589ec886532bb7e18eb",
    "Average ‚¨ÜÔ∏è": 21.324728198424097,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8410278639878006,
    "IFEval Raw": 0.2577139766929525,
    "IFEval": 25.771397669295247,
    "BBH Raw": 0.5489586125997546,
    "BBH": 35.9731352844479,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.47259375,
    "MUSR": 18.74088541666666,
    "MMLU-PRO Raw": 0.36045545212765956,
    "MMLU-PRO": 28.93949468085106,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-22",
    "Generation": 2,
    "Base Model": "royallab/MN-LooseCannon-12B-v2 (Merge)"
  },
  {
    "eval_name": "Tremontaine_L3-12B-Lunaris-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tremontaine/L3-12B-Lunaris-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tremontaine/L3-12B-Lunaris-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tremontaine__L3-12B-Lunaris-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tremontaine/L3-12B-Lunaris-v1",
    "Model sha": "7be236530a835416ebca712d51d661c4488a45de",
    "Average ‚¨ÜÔ∏è": 25.502430831782842,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 11,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1409639487172696,
    "IFEval Raw": 0.6909311737301471,
    "IFEval": 69.0931173730147,
    "BBH Raw": 0.5230217237244009,
    "BBH": 32.1807456461844,
    "MATH Lvl 5 Raw": 0.08912386706948641,
    "MATH Lvl 5": 8.912386706948642,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.3673645833333334,
    "MUSR": 4.053906250000002,
    "MMLU-PRO Raw": 0.3774933510638298,
    "MMLU-PRO": 30.832594562647753,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-15",
    "Generation": 1,
    "Base Model": "Tremontaine/L3-12B-Lunaris-v1 (Merge)"
  },
  {
    "eval_name": "Tsunami-th_Tsunami-0.5-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tsunami-th/Tsunami-0.5-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tsunami-th/Tsunami-0.5-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tsunami-th__Tsunami-0.5-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tsunami-th/Tsunami-0.5-7B-Instruct",
    "Model sha": "10706336513d54c4e8962f54653f25941c4031f4",
    "Average ‚¨ÜÔ∏è": 28.04341070878014,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0900527971611962,
    "IFEval Raw": 0.7400153814102137,
    "IFEval": 74.00153814102137,
    "BBH Raw": 0.552369427738073,
    "BBH": 36.13825418700338,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.42571875,
    "MUSR": 12.214843750000002,
    "MMLU-PRO Raw": 0.44132313829787234,
    "MMLU-PRO": 37.92479314420804,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "Tsunami-th/Tsunami-0.5-7B-Instruct (Merge)"
  },
  {
    "eval_name": "Tsunami-th_Tsunami-0.5x-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tsunami-th/Tsunami-0.5x-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tsunami-th/Tsunami-0.5x-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tsunami-th__Tsunami-0.5x-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tsunami-th/Tsunami-0.5x-7B-Instruct",
    "Model sha": "83d048ab565893a660fa7eaeb4a749d360c76b53",
    "Average ‚¨ÜÔ∏è": 29.823981177705836,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0585633993553958,
    "IFEval Raw": 0.709915247099917,
    "IFEval": 70.99152470999172,
    "BBH Raw": 0.5592865858560252,
    "BBH": 37.36306059795168,
    "MATH Lvl 5 Raw": 0.04984894259818732,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.46667708333333335,
    "MUSR": 18.567968749999995,
    "MMLU-PRO Raw": 0.44581117021276595,
    "MMLU-PRO": 38.423463356974,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "Tsunami-th/Tsunami-0.5x-7B-Instruct (Merge)"
  },
  {
    "eval_name": "Tsunami-th_Tsunami-1.0-14B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tsunami-th/Tsunami-1.0-14B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tsunami-th/Tsunami-1.0-14B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tsunami-th__Tsunami-1.0-14B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tsunami-th/Tsunami-1.0-14B-Instruct",
    "Model sha": "b468814b5242acbe6294226db71bc19dead6c8b6",
    "Average ‚¨ÜÔ∏è": 34.19905841687916,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6536313087233494,
    "IFEval Raw": 0.7829049145157072,
    "IFEval": 78.29049145157072,
    "BBH Raw": 0.6438763263011559,
    "BBH": 49.150255113097735,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.44593750000000004,
    "MUSR": 16.342187499999998,
    "MMLU-PRO Raw": 0.5248503989361702,
    "MMLU-PRO": 47.205599881796694,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "Tsunami-th/Tsunami-1.0-14B-Instruct (Merge)"
  },
  {
    "eval_name": "Tsunami-th_Tsunami-1.0-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Tsunami-th/Tsunami-1.0-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Tsunami-th/Tsunami-1.0-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Tsunami-th__Tsunami-1.0-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Tsunami-th/Tsunami-1.0-7B-Instruct",
    "Model sha": "34d0f8da8ce6b0de50a269eef622ff2e93e5c059",
    "Average ‚¨ÜÔ∏è": 28.762307783788902,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.981399506116264,
    "IFEval Raw": 0.730872972601586,
    "IFEval": 73.0872972601586,
    "BBH Raw": 0.549071195618326,
    "BBH": 35.85724274977317,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.44928125,
    "MUSR": 15.760156249999996,
    "MMLU-PRO Raw": 0.4424035904255319,
    "MMLU-PRO": 38.04484338061466,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "Tsunami-th/Tsunami-1.0-7B-Instruct (Merge)"
  },
  {
    "eval_name": "UCLA-AGI_Gemma-2-9B-It-SPPO-Iter1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Gemma-2-9B-It-SPPO-Iter1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Gemma-2-9B-It-SPPO-Iter1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Gemma-2-9B-It-SPPO-Iter1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter1",
    "Model sha": "33cfd6919f22efc38f71e9d21a7e697afb418e6b",
    "Average ‚¨ÜÔ∏è": 21.08816868559016,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9426582385169677,
    "IFEval Raw": 0.308221075634871,
    "IFEval": 30.8221075634871,
    "BBH Raw": 0.5968934762705508,
    "BBH": 41.80922962006354,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.4099375,
    "MUSR": 10.075520833333334,
    "MMLU-PRO Raw": 0.39070811170212766,
    "MMLU-PRO": 32.300901300236404,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter1"
  },
  {
    "eval_name": "UCLA-AGI_Gemma-2-9B-It-SPPO-Iter2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Gemma-2-9B-It-SPPO-Iter2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Gemma-2-9B-It-SPPO-Iter2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Gemma-2-9B-It-SPPO-Iter2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter2",
    "Model sha": "b7590721d92bf6e0606e3dbc1ca2c229b7c534b4",
    "Average ‚¨ÜÔ∏è": 21.216144486637848,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.716461798426298,
    "IFEval Raw": 0.3100196367859502,
    "IFEval": 31.00196367859502,
    "BBH Raw": 0.5989880877421281,
    "BBH": 42.16983380174582,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4139375,
    "MUSR": 10.942187500000003,
    "MMLU-PRO Raw": 0.386968085106383,
    "MMLU-PRO": 31.885342789598102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter2"
  },
  {
    "eval_name": "UCLA-AGI_Gemma-2-9B-It-SPPO-Iter3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Gemma-2-9B-It-SPPO-Iter3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3",
    "Model sha": "2261f2a03b2e15de13a18da52590c237ecf5f188",
    "Average ‚¨ÜÔ∏è": 21.467179975872483,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 117,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.815150149488856,
    "IFEval Raw": 0.31671409637539505,
    "IFEval": 31.671409637539504,
    "BBH Raw": 0.6007080229268026,
    "BBH": 42.53675224107426,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.41660416666666666,
    "MUSR": 11.342187500000001,
    "MMLU-PRO Raw": 0.382563164893617,
    "MMLU-PRO": 31.395907210401887,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-29",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Gemma-2-9B-It-SPPO-Iter3"
  },
  {
    "eval_name": "UCLA-AGI_Llama-3-Instruct-8B-SPPO-Iter1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Llama-3-Instruct-8B-SPPO-Iter1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter1",
    "Model sha": "2076437f65776aeb9686c95f1f41515f70c4db27",
    "Average ‚¨ÜÔ∏è": 24.640077305514044,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7012289124730338,
    "IFEval Raw": 0.7298988904994304,
    "IFEval": 72.98988904994303,
    "BBH Raw": 0.5057890691082708,
    "BBH": 29.489353188071963,
    "MATH Lvl 5 Raw": 0.10725075528700906,
    "MATH Lvl 5": 10.725075528700906,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3567916666666666,
    "MUSR": 2.165624999999999,
    "MMLU-PRO Raw": 0.37109375,
    "MMLU-PRO": 30.12152777777778,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter1"
  },
  {
    "eval_name": "UCLA-AGI_Llama-3-Instruct-8B-SPPO-Iter2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Llama-3-Instruct-8B-SPPO-Iter2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter2",
    "Model sha": "730c7207d4b538feeb3c2e6d6f6a6ba8615a9be3",
    "Average ‚¨ÜÔ∏è": 23.927649495929,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6567667237627417,
    "IFEval Raw": 0.6988745417713889,
    "IFEval": 69.88745417713888,
    "BBH Raw": 0.5088696278852957,
    "BBH": 29.86944932809155,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.35942708333333334,
    "MUSR": 1.995052083333335,
    "MMLU-PRO Raw": 0.36918218085106386,
    "MMLU-PRO": 29.909131205673756,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter2"
  },
  {
    "eval_name": "UCLA-AGI_Llama-3-Instruct-8B-SPPO-Iter3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Llama-3-Instruct-8B-SPPO-Iter3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3",
    "Model sha": "f73dafc2923acd56f115f21f76e9d14f8d19a63e",
    "Average ‚¨ÜÔ∏è": 23.479398322273383,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 77,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.20181186822828,
    "IFEval Raw": 0.6834122350917787,
    "IFEval": 68.34122350917787,
    "BBH Raw": 0.50795799761689,
    "BBH": 29.739683580440694,
    "MATH Lvl 5 Raw": 0.08308157099697885,
    "MATH Lvl 5": 8.308157099697885,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.36606249999999996,
    "MUSR": 3.0911458333333326,
    "MMLU-PRO Raw": 0.3644448138297872,
    "MMLU-PRO": 29.38275709219858,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3"
  },
  {
    "eval_name": "UCLA-AGI_Llama-3-Instruct-8B-SPPO-Iter3_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Llama-3-Instruct-8B-SPPO-Iter3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3",
    "Model sha": "f73dafc2923acd56f115f21f76e9d14f8d19a63e",
    "Average ‚¨ÜÔ∏è": 23.05947024187678,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 77,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9104745048401183,
    "IFEval Raw": 0.67029814226253,
    "IFEval": 67.02981422625301,
    "BBH Raw": 0.5076407742830437,
    "BBH": 29.71670075746525,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3647291666666667,
    "MUSR": 2.891145833333335,
    "MMLU-PRO Raw": 0.3657746010638298,
    "MMLU-PRO": 29.53051122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Llama-3-Instruct-8B-SPPO-Iter3"
  },
  {
    "eval_name": "UCLA-AGI_Mistral7B-PairRM-SPPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Mistral7B-PairRM-SPPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Mistral7B-PairRM-SPPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Mistral7B-PairRM-SPPO",
    "Model sha": "abdc173603690fcf6b333b351c291a321d2631c3",
    "Average ‚¨ÜÔ∏è": 16.356579938813663,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5041588991194149,
    "IFEval Raw": 0.43549227161708715,
    "IFEval": 43.549227161708714,
    "BBH Raw": 0.4438979817093698,
    "BBH": 22.08465647856181,
    "MATH Lvl 5 Raw": 0.025679758308157104,
    "MATH Lvl 5": 2.5679758308157106,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.39647916666666666,
    "MUSR": 7.793229166666667,
    "MMLU-PRO Raw": 0.26205119680851063,
    "MMLU-PRO": 18.005688534278956,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Mistral7B-PairRM-SPPO"
  },
  {
    "eval_name": "UCLA-AGI_Mistral7B-PairRM-SPPO-Iter1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO-Iter1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Mistral7B-PairRM-SPPO-Iter1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Mistral7B-PairRM-SPPO-Iter1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter1",
    "Model sha": "97252e2d868725b2fa5055adc241c5182610fb6a",
    "Average ‚¨ÜÔ∏è": 17.879981444801057,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5266735410929787,
    "IFEval Raw": 0.5047352136774869,
    "IFEval": 50.47352136774869,
    "BBH Raw": 0.4468056921650662,
    "BBH": 22.93229237099634,
    "MATH Lvl 5 Raw": 0.022658610271903325,
    "MATH Lvl 5": 2.2658610271903323,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3991770833333333,
    "MUSR": 8.29713541666667,
    "MMLU-PRO Raw": 0.26953125,
    "MMLU-PRO": 18.836805555555554,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter1"
  },
  {
    "eval_name": "UCLA-AGI_Mistral7B-PairRM-SPPO-Iter2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Mistral7B-PairRM-SPPO-Iter2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2",
    "Model sha": "8201064df67b5762ff9f361ff1b98aae3747855c",
    "Average ‚¨ÜÔ∏è": 17.030022683033646,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5154844442317217,
    "IFEval Raw": 0.4445848127413041,
    "IFEval": 44.45848127413042,
    "BBH Raw": 0.4465719945610438,
    "BBH": 22.479924250197836,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.40854166666666664,
    "MUSR": 9.801041666666668,
    "MMLU-PRO Raw": 0.2677027925531915,
    "MMLU-PRO": 18.633643617021278,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter2"
  },
  {
    "eval_name": "UCLA-AGI_Mistral7B-PairRM-SPPO-Iter3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UCLA-AGI__Mistral7B-PairRM-SPPO-Iter3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3",
    "Model sha": "72cd8e5435ae679249ddad7ac4cdb64c5b4590c3",
    "Average ‚¨ÜÔ∏è": 16.41312873159652,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5184075762952913,
    "IFEval Raw": 0.4350678422142138,
    "IFEval": 43.506784221421384,
    "BBH Raw": 0.4396587862984616,
    "BBH": 21.817495985928634,
    "MATH Lvl 5 Raw": 0.018882175226586105,
    "MATH Lvl 5": 1.8882175226586104,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.40711458333333334,
    "MUSR": 9.489322916666671,
    "MMLU-PRO Raw": 0.2657912234042553,
    "MMLU-PRO": 18.42124704491726,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "UCLA-AGI/Mistral7B-PairRM-SPPO-Iter3"
  },
  {
    "eval_name": "UKzExecution_LlamaExecutor-8B-3.0.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/UKzExecution/LlamaExecutor-8B-3.0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">UKzExecution/LlamaExecutor-8B-3.0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/UKzExecution__LlamaExecutor-8B-3.0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "UKzExecution/LlamaExecutor-8B-3.0.5",
    "Model sha": "2047978e8ab1146b8881cde3d998856594f437a4",
    "Average ‚¨ÜÔ∏è": 24.49072672808805,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.807001750300754,
    "IFEval Raw": 0.740290207759855,
    "IFEval": 74.0290207759855,
    "BBH Raw": 0.5006000507021341,
    "BBH": 28.41381524085358,
    "MATH Lvl 5 Raw": 0.09894259818731119,
    "MATH Lvl 5": 9.894259818731118,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.3753645833333333,
    "MUSR": 4.653906250000001,
    "MMLU-PRO Raw": 0.3625332446808511,
    "MMLU-PRO": 29.170360520094558,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-07-30",
    "Generation": 1,
    "Base Model": "UKzExecution/LlamaExecutor-8B-3.0.5 (Merge)"
  },
  {
    "eval_name": "Unbabel_TowerInstruct-Mistral-7B-v0.2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Unbabel/TowerInstruct-Mistral-7B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Unbabel/TowerInstruct-Mistral-7B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Unbabel__TowerInstruct-Mistral-7B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Unbabel/TowerInstruct-Mistral-7B-v0.2",
    "Model sha": "454bdfedc8b51f292a402aba2c560df145a0817d",
    "Average ‚¨ÜÔ∏è": 11.827188448409279,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6038895611140432,
    "IFEval Raw": 0.2843422119975,
    "IFEval": 28.43422119975,
    "BBH Raw": 0.388195180992626,
    "BBH": 14.224326422972673,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.4522291666666667,
    "MUSR": 15.961979166666667,
    "MMLU-PRO Raw": 0.19680851063829788,
    "MMLU-PRO": 10.756501182033098,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-26",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "Unbabel/TowerInstruct-Mistral-7B-v0.2"
  },
  {
    "eval_name": "Undi95_MG-FinalMix-72B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Undi95/MG-FinalMix-72B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Undi95/MG-FinalMix-72B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Undi95__MG-FinalMix-72B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Undi95/MG-FinalMix-72B",
    "Model sha": "6c9c2f5d052495dcd49f44bf5623d21210653c65",
    "Average ‚¨ÜÔ∏è": 43.693132532674156,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.222023526680106,
    "IFEval Raw": 0.8013648231137825,
    "IFEval": 80.13648231137824,
    "BBH Raw": 0.6973017446417747,
    "BBH": 57.502411706281976,
    "MATH Lvl 5 Raw": 0.36102719033232633,
    "MATH Lvl 5": 36.10271903323263,
    "GPQA Raw": 0.3850671140939597,
    "GPQA": 18.008948545861294,
    "MUSR Raw": 0.48227083333333337,
    "MUSR": 21.217187499999998,
    "MMLU-PRO Raw": 0.542719414893617,
    "MMLU-PRO": 49.19104609929077,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-07-13",
    "Generation": 1,
    "Base Model": "Undi95/MG-FinalMix-72B (Merge)"
  },
  {
    "eval_name": "VAGOsolutions_Llama-3-SauerkrautLM-70b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-70b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/Llama-3-SauerkrautLM-70b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__Llama-3-SauerkrautLM-70b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/Llama-3-SauerkrautLM-70b-Instruct",
    "Model sha": "707cfd1a93875247c0223e0c7e3d86d58c432318",
    "Average ‚¨ÜÔ∏è": 38.16923335630724,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.626190816152077,
    "IFEval Raw": 0.8044621604010691,
    "IFEval": 80.44621604010692,
    "BBH Raw": 0.6663247245334951,
    "BBH": 52.029579759117915,
    "MATH Lvl 5 Raw": 0.2379154078549849,
    "MATH Lvl 5": 23.79154078549849,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.43393750000000003,
    "MUSR": 13.542187500000002,
    "MMLU-PRO Raw": 0.5392287234042553,
    "MMLU-PRO": 48.8031914893617,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-24",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/Llama-3-SauerkrautLM-70b-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_Llama-3-SauerkrautLM-8b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__Llama-3-SauerkrautLM-8b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct",
    "Model sha": "37127c44d7c0fb56cef817270c4b1a6802d8793a",
    "Average ‚¨ÜÔ∏è": 26.66765472658618,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 51,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7956935282067676,
    "IFEval Raw": 0.744536718130117,
    "IFEval": 74.45367181301171,
    "BBH Raw": 0.494337579362695,
    "BBH": 28.0492424520597,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.42410416666666667,
    "MUSR": 11.2796875,
    "MMLU-PRO Raw": 0.3857214095744681,
    "MMLU-PRO": 31.746823286052013,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-19",
    "Submission Date": "2024-07-22",
    "Generation": 0,
    "Base Model": "VAGOsolutions/Llama-3-SauerkrautLM-8b-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_Llama-3.1-SauerkrautLM-70b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__Llama-3.1-SauerkrautLM-70b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct",
    "Model sha": "e8e74aa789243c25a3a8f7565780a402f5050bbb",
    "Average ‚¨ÜÔ∏è": 42.671070947754295,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 15.09161729420377,
    "IFEval Raw": 0.8656365111238181,
    "IFEval": 86.56365111238182,
    "BBH Raw": 0.7006249194404001,
    "BBH": 57.24162100868165,
    "MATH Lvl 5 Raw": 0.324773413897281,
    "MATH Lvl 5": 32.477341389728096,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.4710833333333333,
    "MUSR": 19.38541666666666,
    "MMLU-PRO Raw": 0.5334940159574468,
    "MMLU-PRO": 48.16600177304965,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-08-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/Llama-3.1-SauerkrautLM-70b-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_Llama-3.1-SauerkrautLM-8b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/Llama-3.1-SauerkrautLM-8b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/Llama-3.1-SauerkrautLM-8b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__Llama-3.1-SauerkrautLM-8b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/Llama-3.1-SauerkrautLM-8b-Instruct",
    "Model sha": "23ca79966a4ab0a61f7ccc7a0454ffef553b66eb",
    "Average ‚¨ÜÔ∏è": 28.684849520122796,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.656874446221014,
    "IFEval Raw": 0.8017393848322452,
    "IFEval": 80.1739384832245,
    "BBH Raw": 0.5114932190011187,
    "BBH": 30.999360665356374,
    "MATH Lvl 5 Raw": 0.11933534743202417,
    "MATH Lvl 5": 11.933534743202417,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4148020833333333,
    "MUSR": 11.51692708333333,
    "MMLU-PRO Raw": 0.3890458776595745,
    "MMLU-PRO": 32.116208628841605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "VAGOsolutions/Llama-3.1-SauerkrautLM-8b-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-1.5b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-1.5b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-1.5b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-1.5b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-1.5b",
    "Model sha": "8f5170f03e6b0355dd920adc3a7e65d0417ee14e",
    "Average ‚¨ÜÔ∏è": 10.122505251282272,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7762368684901493,
    "IFEval Raw": 0.24040324117785256,
    "IFEval": 24.040324117785254,
    "BBH Raw": 0.3703912164863146,
    "BBH": 13.419518424915282,
    "MATH Lvl 5 Raw": 0.027190332326283994,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.37390625000000005,
    "MUSR": 4.971614583333335,
    "MMLU-PRO Raw": 0.21509308510638298,
    "MMLU-PRO": 12.788120567375886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-12",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-1.5b"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-7b-HerO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-HerO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-7b-HerO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-7b-HerO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-7b-HerO",
    "Model sha": "3a14b437e2f375b74de3b6923e171662133347bb",
    "Average ‚¨ÜÔ∏è": 19.69448798081142,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5697686188205925,
    "IFEval Raw": 0.534610389322553,
    "IFEval": 53.461038932255306,
    "BBH Raw": 0.49044349935812964,
    "BBH": 27.991873536830212,
    "MATH Lvl 5 Raw": 0.040785498489425975,
    "MATH Lvl 5": 4.078549848942598,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.39238541666666665,
    "MUSR": 6.881510416666668,
    "MMLU-PRO Raw": 0.30460438829787234,
    "MMLU-PRO": 22.733820921985814,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-11-24",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-7b-HerO"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-7b-LaserChat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-7b-LaserChat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-7b-LaserChat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-7b-LaserChat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-7b-LaserChat",
    "Model sha": "cb759636a3d5b0768df2f43a3d3da9b17e10e7b9",
    "Average ‚¨ÜÔ∏è": 22.134728374463325,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6046788613286371,
    "IFEval Raw": 0.5987823419637672,
    "IFEval": 59.87823419637673,
    "BBH Raw": 0.45432707993295685,
    "BBH": 22.99208011647468,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4148020833333333,
    "MUSR": 9.916927083333333,
    "MMLU-PRO Raw": 0.3304521276595745,
    "MMLU-PRO": 25.60579196217494,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-05",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-7b-LaserChat"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-Gemma-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-Gemma-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-Gemma-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-Gemma-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-Gemma-2b",
    "Model sha": "f9d5575c23da96f33ce77dea3b0776746b9469bc",
    "Average ‚¨ÜÔ∏è": 7.615390021547905,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9162352547124495,
    "IFEval Raw": 0.24752213017017072,
    "IFEval": 24.75221301701707,
    "BBH Raw": 0.3416315376053174,
    "BBH": 9.133870459903902,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.3675833333333333,
    "MUSR": 3.514583333333334,
    "MMLU-PRO Raw": 0.14685837765957446,
    "MMLU-PRO": 5.2064864066193834,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-06",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-Gemma-2b"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-Gemma-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-Gemma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-Gemma-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-Gemma-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-Gemma-7b",
    "Model sha": "4296bdabf82e900235b094e5348be03ebb0ec891",
    "Average ‚¨ÜÔ∏è": 14.600569546301648,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.513249432287414,
    "IFEval Raw": 0.3406705319662939,
    "IFEval": 34.06705319662939,
    "BBH Raw": 0.41879127895858687,
    "BBH": 18.492651800030927,
    "MATH Lvl 5 Raw": 0.05513595166163143,
    "MATH Lvl 5": 5.513595166163143,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.35942708333333334,
    "MUSR": 2.9283854166666674,
    "MMLU-PRO Raw": 0.2961269946808511,
    "MMLU-PRO": 21.79188829787234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-27",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-Gemma-7b"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-Mixtral-8x7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-Mixtral-8x7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct",
    "Model sha": "30ed549de7d84f68b4c6cb619f73275c99af23cc",
    "Average ‚¨ÜÔ∏è": 24.47487868357288,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.7743901940337143,
    "IFEval Raw": 0.5601891869129465,
    "IFEval": 56.01891869129465,
    "BBH Raw": 0.5277342269858817,
    "BBH": 33.94516253986293,
    "MATH Lvl 5 Raw": 0.0974320241691843,
    "MATH Lvl 5": 9.74320241691843,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.42041666666666666,
    "MUSR": 11.318750000000001,
    "MMLU-PRO Raw": 0.3650265957446808,
    "MMLU-PRO": 29.44739952718675,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-15",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-Mixtral-8x7B-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-Nemo-12b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-Nemo-12b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-Nemo-12b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-Nemo-12b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-Nemo-12b-Instruct",
    "Model sha": "fcb056465084ab2c71503a0760f46e4be79c985c",
    "Average ‚¨ÜÔ∏è": 25.765909310772553,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3805272693035775,
    "IFEval Raw": 0.6112969144093228,
    "IFEval": 61.129691440932284,
    "BBH Raw": 0.5214128647611115,
    "BBH": 32.34378307249567,
    "MATH Lvl 5 Raw": 0.09516616314199397,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4468958333333333,
    "MUSR": 17.161979166666665,
    "MMLU-PRO Raw": 0.33851396276595747,
    "MMLU-PRO": 26.501551418439718,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-22",
    "Submission Date": "2024-07-22",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-Nemo-12b-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-Phi-3-medium_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-Phi-3-medium\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-Phi-3-medium</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-Phi-3-medium-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-Phi-3-medium",
    "Model sha": "ebfed26a2b35ede15fe526f57029e0ad866ac66d",
    "Average ‚¨ÜÔ∏è": 30.31979818726856,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7827475105429801,
    "IFEval Raw": 0.4408879550703245,
    "IFEval": 44.08879550703245,
    "BBH Raw": 0.6432931765847228,
    "BBH": 49.630350331717615,
    "MATH Lvl 5 Raw": 0.15483383685800606,
    "MATH Lvl 5": 15.483383685800606,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4845,
    "MUSR": 20.69583333333333,
    "MMLU-PRO Raw": 0.46650598404255317,
    "MMLU-PRO": 40.72288711583924,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-09",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-Phi-3-medium"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-SOLAR-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-SOLAR-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-SOLAR-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-SOLAR-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-SOLAR-Instruct",
    "Model sha": "2665d7600ccd253728453433d2434844e6f702bd",
    "Average ‚¨ÜÔ∏è": 20.16424435950889,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8157302969106476,
    "IFEval Raw": 0.49172085621705963,
    "IFEval": 49.17208562170596,
    "BBH Raw": 0.5169447300097646,
    "BBH": 31.838919738784018,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.3965416666666666,
    "MUSR": 8.334374999999996,
    "MMLU-PRO Raw": 0.31831781914893614,
    "MMLU-PRO": 24.257535460992905,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-20",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-SOLAR-Instruct"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-gemma-2-2b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-gemma-2-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-gemma-2-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-gemma-2-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-gemma-2-2b-it",
    "Model sha": "7fd35fcb32aebfc422e535739161d7528fc562d5",
    "Average ‚¨ÜÔ∏è": 10.46520167420392,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.159145214396478,
    "IFEval Raw": 0.13206625088099574,
    "IFEval": 13.206625088099575,
    "BBH Raw": 0.42408371860644856,
    "BBH": 18.914195373183343,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3994583333333333,
    "MUSR": 8.765624999999998,
    "MMLU-PRO Raw": 0.269281914893617,
    "MMLU-PRO": 18.809101654846337,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-gemma-2-2b-it"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-gemma-2-9b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-gemma-2-9b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-gemma-2-9b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-gemma-2-9b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-gemma-2-9b-it",
    "Model sha": "8e02fc1c24e0499c74ee1186ddc46b989fe497f1",
    "Average ‚¨ÜÔ∏è": 21.832650076743665,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9060682022407844,
    "IFEval Raw": 0.3024009627787604,
    "IFEval": 30.240096277876034,
    "BBH Raw": 0.6072645787154746,
    "BBH": 43.249988966600455,
    "MATH Lvl 5 Raw": 0.00528700906344411,
    "MATH Lvl 5": 0.528700906344411,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.43182291666666667,
    "MUSR": 12.344531250000001,
    "MMLU-PRO Raw": 0.40907579787234044,
    "MMLU-PRO": 34.34175531914893,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-08-26",
    "Generation": 0,
    "Base Model": "VAGOsolutions/SauerkrautLM-gemma-2-9b-it"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-v2-14b-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-v2-14b-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-v2-14b-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-v2-14b-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-v2-14b-DPO",
    "Model sha": "1fbe5364bc443255a06df7fa0debbcc3d38ab866",
    "Average ‚¨ÜÔ∏è": 36.86636909806251,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4772324665341148,
    "IFEval Raw": 0.7411645544931892,
    "IFEval": 74.11645544931892,
    "BBH Raw": 0.6560374350756156,
    "BBH": 50.92613155208554,
    "MATH Lvl 5 Raw": 0.27341389728096677,
    "MATH Lvl 5": 27.341389728096676,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.43746875,
    "MUSR": 13.783593750000003,
    "MMLU-PRO Raw": 0.51171875,
    "MMLU-PRO": 45.74652777777778,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "VAGOsolutions/SauerkrautLM-v2-14b-DPO (Merge)"
  },
  {
    "eval_name": "VAGOsolutions_SauerkrautLM-v2-14b-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VAGOsolutions/SauerkrautLM-v2-14b-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VAGOsolutions/SauerkrautLM-v2-14b-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VAGOsolutions__SauerkrautLM-v2-14b-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VAGOsolutions/SauerkrautLM-v2-14b-SFT",
    "Model sha": "606ddc7819d4a5d9cd8618d5ede57e2bdd99a1ed",
    "Average ‚¨ÜÔ∏è": 35.649022384402066,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5163104758956842,
    "IFEval Raw": 0.6963767248677952,
    "IFEval": 69.63767248677954,
    "BBH Raw": 0.6210355880693049,
    "BBH": 45.824351326219606,
    "MATH Lvl 5 Raw": 0.29229607250755285,
    "MATH Lvl 5": 29.229607250755286,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.417875,
    "MUSR": 11.067708333333336,
    "MMLU-PRO Raw": 0.5205285904255319,
    "MMLU-PRO": 46.725398936170215,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "VAGOsolutions/SauerkrautLM-v2-14b-SFT (Merge)"
  },
  {
    "eval_name": "VIRNECT_llama-3-Korean-8B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VIRNECT/llama-3-Korean-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VIRNECT/llama-3-Korean-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VIRNECT__llama-3-Korean-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VIRNECT/llama-3-Korean-8B",
    "Model sha": "c658409e094ff04eeb6ab6cee2d4bc56716e45f1",
    "Average ‚¨ÜÔ∏è": 20.245300698827084,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8126872040589411,
    "IFEval Raw": 0.5058345190760515,
    "IFEval": 50.58345190760515,
    "BBH Raw": 0.49082453083378397,
    "BBH": 27.322411613379888,
    "MATH Lvl 5 Raw": 0.09290030211480363,
    "MATH Lvl 5": 9.290030211480364,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.36615624999999996,
    "MUSR": 3.26953125,
    "MMLU-PRO Raw": 0.3538896276595745,
    "MMLU-PRO": 28.20995862884161,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-07-17",
    "Generation": 0,
    "Base Model": "VIRNECT/llama-3-Korean-8B"
  },
  {
    "eval_name": "VIRNECT_llama-3-Korean-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VIRNECT/llama-3-Korean-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VIRNECT/llama-3-Korean-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VIRNECT__llama-3-Korean-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VIRNECT/llama-3-Korean-8B",
    "Model sha": "c658409e094ff04eeb6ab6cee2d4bc56716e45f1",
    "Average ‚¨ÜÔ∏è": 20.406433108256554,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8203066313793821,
    "IFEval Raw": 0.5021376614050719,
    "IFEval": 50.21376614050719,
    "BBH Raw": 0.491837579362695,
    "BBH": 27.564318704783016,
    "MATH Lvl 5 Raw": 0.10649546827794562,
    "MATH Lvl 5": 10.649546827794563,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.3647916666666666,
    "MUSR": 3.032291666666666,
    "MMLU-PRO Raw": 0.3536402925531915,
    "MMLU-PRO": 28.182254728132396,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-07-17",
    "Generation": 0,
    "Base Model": "VIRNECT/llama-3-Korean-8B"
  },
  {
    "eval_name": "VIRNECT_llama-3-Korean-8B-r-v-0.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/VIRNECT/llama-3-Korean-8B-r-v-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">VIRNECT/llama-3-Korean-8B-r-v-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/VIRNECT__llama-3-Korean-8B-r-v-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "VIRNECT/llama-3-Korean-8B-r-v-0.1",
    "Model sha": "10acb1aa4f341f2d3c899d78c520b0822a909b95",
    "Average ‚¨ÜÔ∏è": 18.673749835243385,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1994908807701423,
    "IFEval Raw": 0.49157125316382755,
    "IFEval": 49.15712531638276,
    "BBH Raw": 0.48061568139086264,
    "BBH": 25.8849543311167,
    "MATH Lvl 5 Raw": 0.08157099697885196,
    "MATH Lvl 5": 8.157099697885197,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.36748958333333337,
    "MUSR": 3.73619791666667,
    "MMLU-PRO Raw": 0.3259640957446808,
    "MMLU-PRO": 25.10712174940898,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-18",
    "Submission Date": "2024-07-18",
    "Generation": 2,
    "Base Model": "MLP-KTLim/llama-3-Korean-Bllossom-8B (Merge)"
  },
  {
    "eval_name": "ValiantLabs_Llama3-70B-Fireplace_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3-70B-Fireplace\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3-70B-Fireplace</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3-70B-Fireplace-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3-70B-Fireplace",
    "Model sha": "220079e4115733991eb19c30d5480db9696a665e",
    "Average ‚¨ÜÔ∏è": 37.150403066457734,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.692171995228287,
    "IFEval Raw": 0.7773596280092377,
    "IFEval": 77.73596280092377,
    "BBH Raw": 0.648899361888402,
    "BBH": 49.55653001638277,
    "MATH Lvl 5 Raw": 0.21601208459214502,
    "MATH Lvl 5": 21.6012084592145,
    "GPQA Raw": 0.3548657718120805,
    "GPQA": 13.982102908277403,
    "MUSR Raw": 0.4448541666666667,
    "MUSR": 16.7734375,
    "MMLU-PRO Raw": 0.4892785904255319,
    "MMLU-PRO": 43.25317671394799,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-09",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "ValiantLabs/Llama3-70B-Fireplace"
  },
  {
    "eval_name": "ValiantLabs_Llama3-70B-ShiningValiant2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3-70B-ShiningValiant2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3-70B-ShiningValiant2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3-70B-ShiningValiant2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3-70B-ShiningValiant2",
    "Model sha": "bd6cce8da08ccefe9ec58cae3df4bf75c97d8950",
    "Average ‚¨ÜÔ∏è": 30.603091708338482,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.217593739279344,
    "IFEval Raw": 0.6121712611426571,
    "IFEval": 61.217126114265696,
    "BBH Raw": 0.6338341405069171,
    "BBH": 46.71026104076922,
    "MATH Lvl 5 Raw": 0.08006042296072508,
    "MATH Lvl 5": 8.006042296072508,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.4325729166666667,
    "MUSR": 13.63828125,
    "MMLU-PRO Raw": 0.48977726063829785,
    "MMLU-PRO": 43.30858451536643,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-20",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "ValiantLabs/Llama3-70B-ShiningValiant2"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-70B-ShiningValiant2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-70B-ShiningValiant2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-70B-ShiningValiant2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-70B-ShiningValiant2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-70B-ShiningValiant2",
    "Model sha": "55436621ed65f0b79e7c6324b780bd6a18e06c79",
    "Average ‚¨ÜÔ∏è": 36.165892816840746,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.997285025092191,
    "IFEval Raw": 0.5355346037402979,
    "IFEval": 53.5534603740298,
    "BBH Raw": 0.6738408402945882,
    "BBH": 52.390968518523096,
    "MATH Lvl 5 Raw": 0.2719033232628399,
    "MATH Lvl 5": 27.19033232628399,
    "GPQA Raw": 0.3926174496644295,
    "GPQA": 19.01565995525727,
    "MUSR Raw": 0.4681041666666667,
    "MUSR": 18.4796875,
    "MMLU-PRO Raw": 0.5172872340425532,
    "MMLU-PRO": 46.36524822695035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-10-30",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Cobalt_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Cobalt\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Cobalt</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Cobalt-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Cobalt",
    "Model sha": "3a69145a2acc1f7f51735aa3ae5d81c090249c65",
    "Average ‚¨ÜÔ∏è": 20.214217508763127,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7792774160356644,
    "IFEval Raw": 0.3496134700372789,
    "IFEval": 34.96134700372789,
    "BBH Raw": 0.4946769968149292,
    "BBH": 27.41777700049443,
    "MATH Lvl 5 Raw": 0.1253776435045317,
    "MATH Lvl 5": 12.53776435045317,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.3959479166666667,
    "MUSR": 9.82682291666667,
    "MMLU-PRO Raw": 0.3644448138297872,
    "MMLU-PRO": 29.38275709219858,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-10-02",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Cobalt_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Cobalt\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Cobalt</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Cobalt-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Cobalt",
    "Model sha": "3a69145a2acc1f7f51735aa3ae5d81c090249c65",
    "Average ‚¨ÜÔ∏è": 25.558664369322077,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9381708246723047,
    "IFEval Raw": 0.7168346653545925,
    "IFEval": 71.68346653545925,
    "BBH Raw": 0.4910700749859321,
    "BBH": 27.235483048638354,
    "MATH Lvl 5 Raw": 0.15332326283987915,
    "MATH Lvl 5": 15.332326283987916,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3512395833333333,
    "MUSR": 4.704947916666668,
    "MMLU-PRO Raw": 0.36627327127659576,
    "MMLU-PRO": 29.58591903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-09-20",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Enigma_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Enigma\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Enigma</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Enigma-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Enigma",
    "Model sha": "332c99d80f378c77b090745a5aac10f8ab339519",
    "Average ‚¨ÜÔ∏è": 16.59998122465048,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.415623111815019,
    "IFEval Raw": 0.26805542626896633,
    "IFEval": 26.80554262689663,
    "BBH Raw": 0.44776000880153927,
    "BBH": 22.012915076928266,
    "MATH Lvl 5 Raw": 0.08761329305135952,
    "MATH Lvl 5": 8.761329305135952,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.4196041666666666,
    "MUSR": 10.2171875,
    "MMLU-PRO Raw": 0.34092420212765956,
    "MMLU-PRO": 26.769355791962173,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-11",
    "Submission Date": "2024-10-02",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Esper2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Esper2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Esper2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Esper2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Esper2",
    "Model sha": "38f24f2fe90f839acbc57e7530221acf1232e9dc",
    "Average ‚¨ÜÔ∏è": 13.84010516392186,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8767651162121209,
    "IFEval Raw": 0.2567398945907968,
    "IFEval": 25.673989459079685,
    "BBH Raw": 0.4469866863000255,
    "BBH": 22.195685067925833,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3560729166666667,
    "MUSR": 5.709114583333332,
    "MMLU-PRO Raw": 0.29039228723404253,
    "MMLU-PRO": 21.15469858156028,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-09",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Fireplace2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Fireplace2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Fireplace2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Fireplace2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Fireplace2",
    "Model sha": "be3a5c18b5e8e86a3703df1a8227f784ad2c713c",
    "Average ‚¨ÜÔ∏è": 18.312602016344886,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9162338472706587,
    "IFEval Raw": 0.5483240025354947,
    "IFEval": 54.83240025354947,
    "BBH Raw": 0.4609817052543379,
    "BBH": 24.07027321429611,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.34330208333333334,
    "MUSR": 4.379427083333335,
    "MMLU-PRO Raw": 0.24069148936170212,
    "MMLU-PRO": 15.632387706855791,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-23",
    "Submission Date": "2024-07-25",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-Fireplace2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-Fireplace2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-Fireplace2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-Fireplace2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-Fireplace2",
    "Model sha": "ef129903bbdcc59efdbe10fe9061bff473334a99",
    "Average ‚¨ÜÔ∏è": 18.218113337558123,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9010976896871584,
    "IFEval Raw": 0.5328118281714739,
    "IFEval": 53.28118281714739,
    "BBH Raw": 0.4613311485871581,
    "BBH": 24.089953790013478,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.33666666666666667,
    "MUSR": 4.216666666666668,
    "MMLU-PRO Raw": 0.24235372340425532,
    "MMLU-PRO": 15.817080378250589,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-23",
    "Submission Date": "2024-08-10",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-ShiningValiant2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-ShiningValiant2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-ShiningValiant2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-ShiningValiant2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-ShiningValiant2",
    "Model sha": "6b2b5694a192cb29ad0e4314138affa25b630c0e",
    "Average ‚¨ÜÔ∏è": 24.365740185609393,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6750307549925971,
    "IFEval Raw": 0.6495653754260917,
    "IFEval": 64.95653754260917,
    "BBH Raw": 0.477390600131639,
    "BBH": 26.346118640067065,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.39086458333333335,
    "MUSR": 7.458072916666668,
    "MMLU-PRO Raw": 0.33818151595744683,
    "MMLU-PRO": 26.46461288416076,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-10",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.1-8B-ShiningValiant2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.1-8B-ShiningValiant2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.1-8B-ShiningValiant2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.1-8B-ShiningValiant2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.1-8B-ShiningValiant2",
    "Model sha": "6b2b5694a192cb29ad0e4314138affa25b630c0e",
    "Average ‚¨ÜÔ∏è": 15.45803558114332,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.528982131474145,
    "IFEval Raw": 0.26780608784691284,
    "IFEval": 26.780608784691285,
    "BBH Raw": 0.4429290017852748,
    "BBH": 21.618149642278947,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.39591666666666664,
    "MUSR": 10.789583333333331,
    "MMLU-PRO Raw": 0.292719414893617,
    "MMLU-PRO": 21.413268321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-11-05",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "ValiantLabs_Llama3.2-3B-Enigma_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.2-3B-Enigma\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.2-3B-Enigma</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.2-3B-Enigma-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.2-3B-Enigma",
    "Model sha": "ca6adf3a289ce47c7598139e7a312e2b4b3708ce",
    "Average ‚¨ÜÔ∏è": 11.64237812643446,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5053553740624162,
    "IFEval Raw": 0.2786218345102107,
    "IFEval": 27.86218345102107,
    "BBH Raw": 0.3722590772046992,
    "BBH": 12.434025970150065,
    "MATH Lvl 5 Raw": 0.04078549848942599,
    "MATH Lvl 5": 4.078549848942599,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3921354166666667,
    "MUSR": 8.050260416666669,
    "MMLU-PRO Raw": 0.2427692819148936,
    "MMLU-PRO": 15.86325354609929,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-02",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "ValiantLabs_Llama3.2-3B-Esper2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.2-3B-Esper2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.2-3B-Esper2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.2-3B-Esper2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.2-3B-Esper2",
    "Model sha": "64a2c619a2e1680ab42945fcf5b75a5242cab3a1",
    "Average ‚¨ÜÔ∏è": 10.730297140733242,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7388843500166301,
    "IFEval Raw": 0.27497484452364174,
    "IFEval": 27.49748445236417,
    "BBH Raw": 0.38082611390366106,
    "BBH": 13.851732907913407,
    "MATH Lvl 5 Raw": 0.02341389728096677,
    "MATH Lvl 5": 2.341389728096677,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.3549583333333333,
    "MUSR": 4.036458333333333,
    "MMLU-PRO Raw": 0.22573138297872342,
    "MMLU-PRO": 13.9701536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "ValiantLabs_Llama3.2-3B-ShiningValiant2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ValiantLabs/Llama3.2-3B-ShiningValiant2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ValiantLabs/Llama3.2-3B-ShiningValiant2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ValiantLabs__Llama3.2-3B-ShiningValiant2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ValiantLabs/Llama3.2-3B-ShiningValiant2",
    "Model sha": "1336e200485675c9b92baae17831eab17c601803",
    "Average ‚¨ÜÔ∏è": 14.214462062759141,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.8562988781823337,
    "IFEval Raw": 0.2625101397624968,
    "IFEval": 26.251013976249684,
    "BBH Raw": 0.42259325337870185,
    "BBH": 18.912708783001538,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.38664583333333336,
    "MUSR": 8.597395833333335,
    "MMLU-PRO Raw": 0.28291223404255317,
    "MMLU-PRO": 20.323581560283685,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-11-05",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "Vikhrmodels_Vikhr-Llama3.1-8B-Instruct-R-21-09-24_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Vikhrmodels__Vikhr-Llama3.1-8B-Instruct-R-21-09-24-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24",
    "Model sha": "c0b57cf6d4444b35fc5cec0525ff5eef32af22c9",
    "Average ‚¨ÜÔ∏è": 24.83883857243592,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 24,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.856611700315968,
    "IFEval Raw": 0.643145742186288,
    "IFEval": 64.31457421862879,
    "BBH Raw": 0.527224269970207,
    "BBH": 32.66941729424733,
    "MATH Lvl 5 Raw": 0.1865558912386707,
    "MATH Lvl 5": 18.65558912386707,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.3753958333333334,
    "MUSR": 5.0911458333333375,
    "MMLU-PRO Raw": 0.3547207446808511,
    "MMLU-PRO": 28.302304964539005,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-21-09-24 (Merge)"
  },
  {
    "eval_name": "Vikhrmodels_Vikhr-Nemo-12B-Instruct-R-21-09-24_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Vikhrmodels__Vikhr-Nemo-12B-Instruct-R-21-09-24-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24",
    "Model sha": "6abd887cb631f705042c9e8085615fe4d76e9779",
    "Average ‚¨ÜÔ∏è": 24.403136398568407,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 85,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7207918119114691,
    "IFEval Raw": 0.5999315150467426,
    "IFEval": 59.993151504674266,
    "BBH Raw": 0.5212309052827618,
    "BBH": 31.41440911337631,
    "MATH Lvl 5 Raw": 0.13444108761329307,
    "MATH Lvl 5": 13.444108761329307,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.40730208333333334,
    "MUSR": 9.446093750000001,
    "MMLU-PRO Raw": 0.33976063829787234,
    "MMLU-PRO": 26.640070921985814,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Vikhrmodels/Vikhr-Nemo-12B-Instruct-R-21-09-24 (Merge)"
  },
  {
    "eval_name": "Weyaxi_Bagel-Hermes-2x34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Bagel-Hermes-2x34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Bagel-Hermes-2x34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Bagel-Hermes-2x34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Bagel-Hermes-2x34B",
    "Model sha": "44fddd32d7dcafc0fa670fd87a2e129310640aac",
    "Average ‚¨ÜÔ∏è": 25.47280416231614,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 60,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.815167910304776,
    "IFEval Raw": 0.5431532777474878,
    "IFEval": 54.31532777474878,
    "BBH Raw": 0.49166555632285514,
    "BBH": 27.409031445428763,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.45166666666666666,
    "MUSR": 15.625000000000002,
    "MMLU-PRO Raw": 0.4588597074468085,
    "MMLU-PRO": 39.873300827423165,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-12",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "Weyaxi/Bagel-Hermes-2x34B"
  },
  {
    "eval_name": "Weyaxi_Bagel-Hermes-34B-Slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Bagel-Hermes-34B-Slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Bagel-Hermes-34B-Slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Bagel-Hermes-34B-Slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Bagel-Hermes-34B-Slerp",
    "Model sha": "dcdcc17a2c650a95bc27129a3ddbf261dffed37f",
    "Average ‚¨ÜÔ∏è": 27.20909327814938,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.0213754984860577,
    "IFEval Raw": 0.4602720780861448,
    "IFEval": 46.02720780861448,
    "BBH Raw": 0.5921903605860047,
    "BBH": 41.957047480557854,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.46220833333333333,
    "MUSR": 17.009375000000002,
    "MMLU-PRO Raw": 0.4703291223404255,
    "MMLU-PRO": 41.147680260047274,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-12",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "Weyaxi/Bagel-Hermes-34B-Slerp"
  },
  {
    "eval_name": "Weyaxi_Einstein-v4-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Einstein-v4-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Einstein-v4-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Einstein-v4-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Einstein-v4-7B",
    "Model sha": "7eecd9833b8a012e23ac1df789884888b047baa0",
    "Average ‚¨ÜÔ∏è": 16.76825217160735,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 47,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6677540108548141,
    "IFEval Raw": 0.47081299839980145,
    "IFEval": 47.08129983998015,
    "BBH Raw": 0.38494699692741774,
    "BBH": 14.30445141720726,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.4681666666666667,
    "MUSR": 19.020833333333332,
    "MMLU-PRO Raw": 0.22589760638297873,
    "MMLU-PRO": 13.98862293144208,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-22",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "Weyaxi_Einstein-v6.1-Llama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Einstein-v6.1-Llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Einstein-v6.1-Llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Einstein-v6.1-Llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Einstein-v6.1-Llama3-8B",
    "Model sha": "5cab6d54666b6024d0f745d61abf1842edb934e0",
    "Average ‚¨ÜÔ∏è": 20.119138899189185,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 66,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.859597759313963,
    "IFEval Raw": 0.4568245588372186,
    "IFEval": 45.68245588372186,
    "BBH Raw": 0.5008295581095018,
    "BBH": 29.38377348658535,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.42128125,
    "MUSR": 11.22682291666667,
    "MMLU-PRO Raw": 0.3130817819148936,
    "MMLU-PRO": 23.675753546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-19",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "Weyaxi_Einstein-v6.1-developed-by-Weyaxi-Llama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Einstein-v6.1-developed-by-Weyaxi-Llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Einstein-v6.1-developed-by-Weyaxi-Llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Einstein-v6.1-developed-by-Weyaxi-Llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Einstein-v6.1-developed-by-Weyaxi-Llama3-8B",
    "Model sha": "b7507e94146c0832c26609e9ab8115934d3e25b3",
    "Average ‚¨ÜÔ∏è": 19.205449654941816,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8717282290731772,
    "IFEval Raw": 0.39270247388041507,
    "IFEval": 39.27024738804151,
    "BBH Raw": 0.5043837450549643,
    "BBH": 29.69444747698505,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.43324999999999997,
    "MUSR": 13.389583333333329,
    "MMLU-PRO Raw": 0.30925864361702127,
    "MMLU-PRO": 23.250960401891252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-23",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Weyaxi/Einstein-v6.1-developed-by-Weyaxi-Llama3-8B"
  },
  {
    "eval_name": "Weyaxi_Einstein-v7-Qwen2-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Einstein-v7-Qwen2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Einstein-v7-Qwen2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Einstein-v7-Qwen2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Einstein-v7-Qwen2-7B",
    "Model sha": "d5a2f245bf98a40d196821bc378e10f35b4da81a",
    "Average ‚¨ÜÔ∏è": 24.23995279329782,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 35,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3139713765266914,
    "IFEval Raw": 0.4099633417111043,
    "IFEval": 40.99633417111043,
    "BBH Raw": 0.5161472249498397,
    "BBH": 32.84181889691276,
    "MATH Lvl 5 Raw": 0.1654078549848943,
    "MATH Lvl 5": 16.54078549848943,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.43997916666666664,
    "MUSR": 14.0640625,
    "MMLU-PRO Raw": 0.4095744680851064,
    "MMLU-PRO": 34.39716312056737,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "Weyaxi_Einstein-v8-Llama3.2-1B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/Einstein-v8-Llama3.2-1B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/Einstein-v8-Llama3.2-1B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__Einstein-v8-Llama3.2-1B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/Einstein-v8-Llama3.2-1B",
    "Model sha": "1edc6abcb8eedd047bc40b79d2d36c3723ff28e2",
    "Average ‚¨ÜÔ∏è": 4.6278210436436655,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.38792460181225513,
    "IFEval Raw": 0.18622255615101263,
    "IFEval": 18.622255615101263,
    "BBH Raw": 0.30184334823943154,
    "BBH": 3.0137741782829335,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.36178125,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.11610704787234043,
    "MMLU-PRO": 1.7896719858156023,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-1B"
  },
  {
    "eval_name": "Weyaxi_SauerkrautLM-UNA-SOLAR-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Weyaxi__SauerkrautLM-UNA-SOLAR-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct",
    "Model sha": "9678b9ca952abe0083dbfc772a56b849866bfa1a",
    "Average ‚¨ÜÔ∏è": 19.70813326438918,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7460111016054243,
    "IFEval Raw": 0.4573243438520902,
    "IFEval": 45.73243438520902,
    "BBH Raw": 0.5166357112030591,
    "BBH": 31.824686783543132,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.397875,
    "MUSR": 8.601041666666662,
    "MMLU-PRO Raw": 0.31532579787234044,
    "MMLU-PRO": 23.92508865248227,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-21",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Weyaxi/SauerkrautLM-UNA-SOLAR-Instruct"
  },
  {
    "eval_name": "WizardLMTeam_WizardLM-13B-V1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/WizardLMTeam/WizardLM-13B-V1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">WizardLMTeam/WizardLM-13B-V1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/WizardLMTeam__WizardLM-13B-V1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "WizardLMTeam/WizardLM-13B-V1.0",
    "Model sha": "964a93aa2e78da377115bb856075a69ebe8aefa4",
    "Average ‚¨ÜÔ∏è": 4.546091523510591,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 73,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 70.97758708474622,
    "IFEval Raw": 0.18504900331121424,
    "IFEval": 18.504900331121426,
    "BBH Raw": 0.29134447696551025,
    "BBH": 2.147966883446335,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.34971875,
    "MUSR": 3.5481770833333326,
    "MMLU-PRO Raw": 0.11660571808510638,
    "MMLU-PRO": 1.8450797872340412,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-13",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "WizardLMTeam/WizardLM-13B-V1.0"
  },
  {
    "eval_name": "WizardLMTeam_WizardLM-13B-V1.2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/WizardLMTeam/WizardLM-13B-V1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">WizardLMTeam/WizardLM-13B-V1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/WizardLMTeam__WizardLM-13B-V1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "WizardLMTeam/WizardLM-13B-V1.2",
    "Model sha": "cf5f40382559f19e13874e45b39575171ca46ef8",
    "Average ‚¨ÜÔ∏è": 15.164944624066,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 225,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.5194581300833336,
    "IFEval Raw": 0.3392465325336773,
    "IFEval": 33.92465325336773,
    "BBH Raw": 0.44619994364600474,
    "BBH": 22.88865497804447,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.43784375000000003,
    "MUSR": 14.030468750000002,
    "MMLU-PRO Raw": 0.25191156914893614,
    "MMLU-PRO": 16.87906323877068,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-25",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "WizardLMTeam/WizardLM-13B-V1.2"
  },
  {
    "eval_name": "WizardLMTeam_WizardLM-70B-V1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/WizardLMTeam/WizardLM-70B-V1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">WizardLMTeam/WizardLM-70B-V1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/WizardLMTeam__WizardLM-70B-V1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "WizardLMTeam/WizardLM-70B-V1.0",
    "Model sha": "54aaecaff7d0790eb9f0ecea1cc267a94cc66949",
    "Average ‚¨ÜÔ∏è": 22.359677748876226,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 235,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 29.09606329195423,
    "IFEval Raw": 0.49514288753839814,
    "IFEval": 49.514288753839814,
    "BBH Raw": 0.5590366047184262,
    "BBH": 37.54335453368136,
    "MATH Lvl 5 Raw": 0.037009063444108765,
    "MATH Lvl 5": 3.7009063444108765,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.43911458333333336,
    "MUSR": 14.089322916666669,
    "MMLU-PRO Raw": 0.34466422872340424,
    "MMLU-PRO": 27.184914302600472,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-08-09",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "WizardLMTeam/WizardLM-70B-V1.0"
  },
  {
    "eval_name": "Xclbr7_Arcanum-12b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Xclbr7/Arcanum-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Xclbr7/Arcanum-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Xclbr7__Arcanum-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Xclbr7/Arcanum-12b",
    "Model sha": "845ac67d2b527296ae8c06da4453bf8a60f2e59b",
    "Average ‚¨ÜÔ∏è": 20.694285318437824,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6905107056350748,
    "IFEval Raw": 0.2906864896253053,
    "IFEval": 29.068648962530528,
    "BBH Raw": 0.5265359354118465,
    "BBH": 31.879959562746524,
    "MATH Lvl 5 Raw": 0.11555891238670694,
    "MATH Lvl 5": 11.555891238670695,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.41703124999999996,
    "MUSR": 13.528906249999997,
    "MMLU-PRO Raw": 0.3586269946808511,
    "MMLU-PRO": 28.736332742316783,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-17",
    "Generation": 0,
    "Base Model": "Xclbr7/Arcanum-12b"
  },
  {
    "eval_name": "Xclbr7_Hyena-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Xclbr7/Hyena-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Xclbr7/Hyena-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Xclbr7__Hyena-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Xclbr7/Hyena-12b",
    "Model sha": "9dd5eb77ce8e0e05e260ae4d812631fb980527fa",
    "Average ‚¨ÜÔ∏è": 20.437243081688283,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8598933964531719,
    "IFEval Raw": 0.3404455733010634,
    "IFEval": 34.04455733010634,
    "BBH Raw": 0.5457182415468321,
    "BBH": 34.665648637656034,
    "MATH Lvl 5 Raw": 0.09365558912386708,
    "MATH Lvl 5": 9.365558912386708,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.39842708333333327,
    "MUSR": 11.070052083333328,
    "MMLU-PRO Raw": 0.3439162234042553,
    "MMLU-PRO": 27.101802600472812,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "Xclbr7/Arcanum-12b"
  },
  {
    "eval_name": "Xclbr7_caliburn-12b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Xclbr7/caliburn-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Xclbr7/caliburn-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Xclbr7__caliburn-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Xclbr7/caliburn-12b",
    "Model sha": "f76fa67c7ca8bf7e75540baf55972ba52a46630b",
    "Average ‚¨ÜÔ∏è": 22.808395334774527,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.856220455380966,
    "IFEval Raw": 0.35763108551975425,
    "IFEval": 35.76310855197542,
    "BBH Raw": 0.5518630300231809,
    "BBH": 35.63684056756332,
    "MATH Lvl 5 Raw": 0.10422960725075528,
    "MATH Lvl 5": 10.422960725075528,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.4291875,
    "MUSR": 13.781770833333331,
    "MMLU-PRO Raw": 0.36751994680851063,
    "MMLU-PRO": 29.724438534278963,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Xclbr7/caliburn-12b"
  },
  {
    "eval_name": "Xclbr7_caliburn-v2-12b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Xclbr7/caliburn-v2-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Xclbr7/caliburn-v2-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Xclbr7__caliburn-v2-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Xclbr7/caliburn-v2-12b",
    "Model sha": "fa736b3b852298dd8c047ac6dcc620161df4a79b",
    "Average ‚¨ÜÔ∏è": 20.953524952806195,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6321937481713287,
    "IFEval Raw": 0.2966816934622358,
    "IFEval": 29.668169346223575,
    "BBH Raw": 0.5141426125097639,
    "BBH": 30.387966946397217,
    "MATH Lvl 5 Raw": 0.10422960725075528,
    "MATH Lvl 5": 10.422960725075528,
    "GPQA Raw": 0.3263422818791946,
    "GPQA": 10.17897091722595,
    "MUSR Raw": 0.43703125,
    "MUSR": 14.12890625,
    "MMLU-PRO Raw": 0.37840757978723405,
    "MMLU-PRO": 30.9341755319149,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "Xclbr7/caliburn-v2-12b"
  },
  {
    "eval_name": "Yash21_TinyYi-7B-Test_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Yash21/TinyYi-7B-Test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Yash21/TinyYi-7B-Test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Yash21__TinyYi-7B-Test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Yash21/TinyYi-7B-Test",
    "Model sha": "7750e5de73fbcf1dcc0832b4cdabaa9713c20475",
    "Average ‚¨ÜÔ∏è": 4.495167294967694,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7631090945772556,
    "IFEval Raw": 0.18564852369490728,
    "IFEval": 18.56485236949073,
    "BBH Raw": 0.29098007801214715,
    "BBH": 2.267966388832264,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3364479166666667,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.10912566489361702,
    "MMLU-PRO": 1.0139627659574466,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-06",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "Yash21/TinyYi-7B-Test"
  },
  {
    "eval_name": "Youlln_1PARAMMYL-8B-ModelStock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/1PARAMMYL-8B-ModelStock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/1PARAMMYL-8B-ModelStock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__1PARAMMYL-8B-ModelStock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/1PARAMMYL-8B-ModelStock",
    "Model sha": "4ce556da5ccd1ecac8d0f3e1e94d1982f11b910d",
    "Average ‚¨ÜÔ∏è": 26.283975359929922,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.892521845193913,
    "IFEval Raw": 0.5371336941537344,
    "IFEval": 53.71336941537343,
    "BBH Raw": 0.5215839663555125,
    "BBH": 31.799951193327704,
    "MATH Lvl 5 Raw": 0.14728096676737162,
    "MATH Lvl 5": 14.728096676737163,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4409375,
    "MUSR": 14.283854166666663,
    "MMLU-PRO Raw": 0.4000166223404255,
    "MMLU-PRO": 33.335180260047274,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-20",
    "Generation": 1,
    "Base Model": "Youlln/1PARAMMYL-8B-ModelStock (Merge)"
  },
  {
    "eval_name": "Youlln_2PRYMMAL-Yi1.5-6B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/2PRYMMAL-Yi1.5-6B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/2PRYMMAL-Yi1.5-6B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__2PRYMMAL-Yi1.5-6B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/2PRYMMAL-Yi1.5-6B-SLERP",
    "Model sha": "b776bd3ce6784b96ff928b1d5ad51b2991909f2c",
    "Average ‚¨ÜÔ∏è": 18.979223141358474,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.383177078812052,
    "IFEval Raw": 0.28259351853083153,
    "IFEval": 28.259351853083153,
    "BBH Raw": 0.46647504291710673,
    "BBH": 24.495644420709066,
    "MATH Lvl 5 Raw": 0.11253776435045318,
    "MATH Lvl 5": 11.253776435045317,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.47560416666666666,
    "MUSR": 18.150520833333335,
    "MMLU-PRO Raw": 0.3169880319148936,
    "MMLU-PRO": 24.109781323877066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "Youlln/2PRYMMAL-Yi1.5-6B-SLERP (Merge)"
  },
  {
    "eval_name": "Youlln_3PRYMMAL-PHI3-3B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/3PRYMMAL-PHI3-3B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/3PRYMMAL-PHI3-3B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__3PRYMMAL-PHI3-3B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/3PRYMMAL-PHI3-3B-SLERP",
    "Model sha": "9396bcf1709ac8360a95a746482520fab4295706",
    "Average ‚¨ÜÔ∏è": 24.849214307970467,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0784390552843282,
    "IFEval Raw": 0.3655500738041729,
    "IFEval": 36.555007380417294,
    "BBH Raw": 0.5421833887682153,
    "BBH": 35.82766762143187,
    "MATH Lvl 5 Raw": 0.1540785498489426,
    "MATH Lvl 5": 15.407854984894259,
    "GPQA Raw": 0.3263422818791946,
    "GPQA": 10.17897091722595,
    "MUSR Raw": 0.46484375,
    "MUSR": 17.77213541666666,
    "MMLU-PRO Raw": 0.4001828457446808,
    "MMLU-PRO": 33.353649527186754,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "Youlln/3PRYMMAL-PHI3-3B-SLERP (Merge)"
  },
  {
    "eval_name": "Youlln_4PRYMMAL-GEMMA2-9B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/4PRYMMAL-GEMMA2-9B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/4PRYMMAL-GEMMA2-9B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__4PRYMMAL-GEMMA2-9B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/4PRYMMAL-GEMMA2-9B-SLERP",
    "Model sha": "7dac3b4ab4298113ae3103d63bb284e1ac8bf4d4",
    "Average ‚¨ÜÔ∏è": 23.550238486572983,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.8526600862704843,
    "IFEval Raw": 0.2713766140507188,
    "IFEval": 27.137661405071878,
    "BBH Raw": 0.5922529923998928,
    "BBH": 42.064171912395665,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.46719791666666666,
    "MUSR": 17.46640625,
    "MMLU-PRO Raw": 0.42096077127659576,
    "MMLU-PRO": 35.662307919621746,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "Youlln/4PRYMMAL-GEMMA2-9B-SLERP (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-0.5B-FT-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-0.5B-FT-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-0.5B-FT-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-0.5B-FT-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-0.5B-FT-V3",
    "Model sha": "d542b4d53888fcc8e96c32892d47ec51afc9edc9",
    "Average ‚¨ÜÔ∏è": 4.342503912314145,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5731352075149264,
    "IFEval Raw": 0.16419101317836673,
    "IFEval": 16.419101317836674,
    "BBH Raw": 0.30931341134548046,
    "BBH": 3.6168825108366214,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.3644479166666667,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.11610704787234043,
    "MMLU-PRO": 1.7896719858156023,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL-0.5B-FT-V3 (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-0.5B-FT-V3-MUSR_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-0.5B-FT-V3-MUSR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-0.5B-FT-V3-MUSR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-0.5B-FT-V3-MUSR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-0.5B-FT-V3-MUSR",
    "Model sha": "221dc80a1acd6f7dda0644699e6d61b90a5a0a05",
    "Average ‚¨ÜÔ∏è": 5.551290974765486,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0299846272113329,
    "IFEval Raw": 0.15334977858748122,
    "IFEval": 15.334977858748122,
    "BBH Raw": 0.3041148294962408,
    "BBH": 5.062185886531173,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.36603125000000003,
    "MUSR": 3.2539062500000004,
    "MMLU-PRO Raw": 0.1644780585106383,
    "MMLU-PRO": 7.164228723404253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL-0.5B-FT-V3-MUSR (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-0.5B-FT-V4-MUSR_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-0.5B-FT-V4-MUSR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-0.5B-FT-V4-MUSR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-0.5B-FT-V4-MUSR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-0.5B-FT-V4-MUSR",
    "Model sha": "f5b268d63bb10f05a229da4f2ee9cb0882c93971",
    "Average ‚¨ÜÔ∏è": 4.198598563472031,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9486913674692107,
    "IFEval Raw": 0.1137570535069172,
    "IFEval": 11.375705350691721,
    "BBH Raw": 0.3038362724383693,
    "BBH": 4.949091743380625,
    "MATH Lvl 5 Raw": 0.011329305135951661,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.3528854166666667,
    "MUSR": 1.47734375,
    "MMLU-PRO Raw": 0.13214760638297873,
    "MMLU-PRO": 3.5719562647754137,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL-0.5B-FT-V4-MUSR (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-0.5B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-0.5B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-0.5B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-0.5B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-0.5B-SLERP-V2",
    "Model sha": "5e87669abcdc042774a63b94a13880f1acd6e15d",
    "Average ‚¨ÜÔ∏è": 4.614606797904897,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6549974035702691,
    "IFEval Raw": 0.1611934112599015,
    "IFEval": 16.119341125990154,
    "BBH Raw": 0.2934774313772131,
    "BBH": 1.9175609031491385,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3831145833333333,
    "MUSR": 5.355989583333333,
    "MMLU-PRO Raw": 0.10945811170212766,
    "MMLU-PRO": 1.0509013002364058,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL-0.5B-SLERP-V2 (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-0.5B-SLERP-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-0.5B-SLERP-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-0.5B-SLERP-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-0.5B-SLERP-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-0.5B-SLERP-V3",
    "Model sha": "94bfab3b1f41458427e5f8598ceb3ec731ba1bd6",
    "Average ‚¨ÜÔ∏è": 3.6630142582547953,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.637698456571065,
    "IFEval Raw": 0.16701352411601217,
    "IFEval": 16.701352411601217,
    "BBH Raw": 0.29383772587210827,
    "BBH": 2.319604893286366,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.354125,
    "MUSR": 1.7656250000000016,
    "MMLU-PRO Raw": 0.10871010638297872,
    "MMLU-PRO": 0.9677895981087459,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 0,
    "Base Model": "Youlln/ECE-PRYMMAL-0.5B-SLERP-V3"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-YL-1B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-YL-1B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-YL-1B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-YL-1B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-YL-1B-SLERP-V1",
    "Model sha": "b5cd268edb0cc5c2c6ab2c49c950e611b2b8138c",
    "Average ‚¨ÜÔ∏è": 16.404997812890667,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5953407879970395,
    "IFEval Raw": 0.32510848991786234,
    "IFEval": 32.51084899178623,
    "BBH Raw": 0.4208506248736219,
    "BBH": 18.27951144620795,
    "MATH Lvl 5 Raw": 0.09063444108761329,
    "MATH Lvl 5": 9.06344410876133,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.4265833333333333,
    "MUSR": 11.589583333333332,
    "MMLU-PRO Raw": 0.2935505319148936,
    "MMLU-PRO": 21.5056146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "Youlln/ECE-PRYMMAL-YL-1B-SLERP-V1"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-YL-1B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-YL-1B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-YL-1B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-YL-1B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-YL-1B-SLERP-V2",
    "Model sha": "3559f643c8d5774135a1cd8daea78fef31035679",
    "Average ‚¨ÜÔ∏è": 16.404997812890667,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.604627823543865,
    "IFEval Raw": 0.32510848991786234,
    "IFEval": 32.51084899178623,
    "BBH Raw": 0.4208506248736219,
    "BBH": 18.27951144620795,
    "MATH Lvl 5 Raw": 0.09063444108761329,
    "MATH Lvl 5": 9.06344410876133,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.4265833333333333,
    "MUSR": 11.589583333333332,
    "MMLU-PRO Raw": 0.2935505319148936,
    "MMLU-PRO": 21.5056146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "Youlln/ECE-PRYMMAL-YL-1B-SLERP-V2"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL-YL-7B-SLERP-V4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL-YL-7B-SLERP-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL-YL-7B-SLERP-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL-YL-7B-SLERP-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL-YL-7B-SLERP-V4",
    "Model sha": "4939b9e24be6f03d5df1e9bb7dc1b4fd5d59404a",
    "Average ‚¨ÜÔ∏è": 10.416375256367889,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7701569632706955,
    "IFEval Raw": 0.2509696494190969,
    "IFEval": 25.09696494190969,
    "BBH Raw": 0.37697272812325017,
    "BBH": 13.157437333845115,
    "MATH Lvl 5 Raw": 0.026435045317220542,
    "MATH Lvl 5": 2.643504531722054,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3744895833333333,
    "MUSR": 7.011197916666666,
    "MMLU-PRO Raw": 0.2131815159574468,
    "MMLU-PRO": 12.575723995271867,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "Youlln/ECE-PRYMMAL-YL-7B-SLERP-V4"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL0.5-FT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL0.5-FT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL0.5-FT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL0.5-FT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL0.5-FT",
    "Model sha": "56b9fd5f26e5b6379fe4aa62e0f66b87b5c6f8e8",
    "Average ‚¨ÜÔ∏è": 5.195510054664371,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5033914882173928,
    "IFEval Raw": 0.18507338306803725,
    "IFEval": 18.507338306803724,
    "BBH Raw": 0.31320911187036277,
    "BBH": 5.151599849335524,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.330125,
    "MUSR": 1.432291666666666,
    "MMLU-PRO Raw": 0.14768949468085107,
    "MMLU-PRO": 5.298832742316785,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-02",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL0.5-FT (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL0.5B-Youri_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL0.5B-Youri\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL0.5B-Youri</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL0.5B-Youri-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL0.5B-Youri",
    "Model sha": "1477d3deff98f35f523aa222bc0442278d464566",
    "Average ‚¨ÜÔ∏è": 3.505273892929676,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6558458858952403,
    "IFEval Raw": 0.1446317991817267,
    "IFEval": 14.46317991817267,
    "BBH Raw": 0.28173574256265815,
    "BBH": 1.5012962555992377,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.36965625,
    "MUSR": 4.007031250000002,
    "MMLU-PRO Raw": 0.10954122340425532,
    "MMLU-PRO": 1.0601359338061456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL0.5B-Youri (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-PRYMMAL1B-FT-V1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-PRYMMAL1B-FT-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-PRYMMAL1B-FT-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-PRYMMAL1B-FT-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-PRYMMAL1B-FT-V1",
    "Model sha": "d0fc3a6e93f91c8d586eb25c9f2a4ea4ca99e9f4",
    "Average ‚¨ÜÔ∏è": 11.923326667094797,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.737595569866024,
    "IFEval Raw": 0.2143745262569981,
    "IFEval": 21.43745262569981,
    "BBH Raw": 0.4032647427840684,
    "BBH": 16.18938601764277,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.34165625,
    "MUSR": 3.873697916666666,
    "MMLU-PRO Raw": 0.2742686170212766,
    "MMLU-PRO": 19.363179669030732,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "Youlln/ECE-PRYMMAL1B-FT-V1 (Merge)"
  },
  {
    "eval_name": "Youlln_ECE-Qwen0.5B-FT-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE-Qwen0.5B-FT-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE-Qwen0.5B-FT-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE-Qwen0.5B-FT-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE-Qwen0.5B-FT-V2",
    "Model sha": "c87da3f19ab74854fca30f9ca71ce5c4884ef629",
    "Average ‚¨ÜÔ∏è": 7.486570144330268,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5232995127945267,
    "IFEval Raw": 0.25259311958935626,
    "IFEval": 25.259311958935623,
    "BBH Raw": 0.328970813623839,
    "BBH": 7.632147610946966,
    "MATH Lvl 5 Raw": 0.015105740181268883,
    "MATH Lvl 5": 1.5105740181268883,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.30628125,
    "MUSR": 0.8851562499999998,
    "MMLU-PRO Raw": 0.16655585106382978,
    "MMLU-PRO": 7.395094562647753,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "Youlln/ECE-Qwen0.5B-FT-V2 (Merge)"
  },
  {
    "eval_name": "Youlln_ECE.EIFFEIL.ia-0.5B-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Youlln/ECE.EIFFEIL.ia-0.5B-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Youlln/ECE.EIFFEIL.ia-0.5B-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Youlln__ECE.EIFFEIL.ia-0.5B-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Youlln/ECE.EIFFEIL.ia-0.5B-SLERP",
    "Model sha": "e376ce416af881eefa778d2566d15d9a6d29e7d9",
    "Average ‚¨ÜÔ∏è": 8.716672600470547,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6040593594452638,
    "IFEval Raw": 0.2561403742071038,
    "IFEval": 25.614037420710382,
    "BBH Raw": 0.33056720460862643,
    "BBH": 8.405356119616796,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.31021875,
    "MUSR": 0.9440104166666662,
    "MMLU-PRO Raw": 0.1903257978723404,
    "MMLU-PRO": 10.036199763593379,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "Youlln/ECE.EIFFEIL.ia-0.5B-SLERP (Merge)"
  },
  {
    "eval_name": "YoungPanda_qwenqwen_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2MoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/YoungPanda/qwenqwen\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">YoungPanda/qwenqwen</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/YoungPanda__qwenqwen-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "YoungPanda/qwenqwen",
    "Model sha": "3b5d9b63076acc8988b8f7e9734cf1d78bb39c25",
    "Average ‚¨ÜÔ∏è": 4.443749032036031,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 7.122668890483286,
    "IFEval Raw": 0.12639684924888184,
    "IFEval": 12.639684924888185,
    "BBH Raw": 0.337898518087465,
    "BBH": 8.194779944827593,
    "MATH Lvl 5 Raw": 0.015105740181268883,
    "MATH Lvl 5": 1.5105740181268883,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.34336458333333336,
    "MUSR": 2.4539062499999997,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-12",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "Yuma42_KangalKhan-RawRuby-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/Yuma42/KangalKhan-RawRuby-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">Yuma42/KangalKhan-RawRuby-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/Yuma42__KangalKhan-RawRuby-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "Yuma42/KangalKhan-RawRuby-7B",
    "Model sha": "54f56d4c6889eaf43fdd5f7d6dcef3c2ebe51929",
    "Average ‚¨ÜÔ∏è": 20.44073707899282,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6537194445601759,
    "IFEval Raw": 0.547674614467391,
    "IFEval": 54.76746144673909,
    "BBH Raw": 0.47547278683676025,
    "BBH": 26.387283588738626,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.39495833333333336,
    "MUSR": 7.636458333333337,
    "MMLU-PRO Raw": 0.30227726063829785,
    "MMLU-PRO": 22.475251182033094,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-17",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "Yuma42/KangalKhan-RawRuby-7B (Merge)"
  },
  {
    "eval_name": "ZeroXClem_Qwen-2.5-Aether-SlerpFusion-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ZeroXClem/Qwen-2.5-Aether-SlerpFusion-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ZeroXClem/Qwen-2.5-Aether-SlerpFusion-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ZeroXClem__Qwen-2.5-Aether-SlerpFusion-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ZeroXClem/Qwen-2.5-Aether-SlerpFusion-7B",
    "Model sha": "23992e1be9f77d767181dc7bcb42176395f42c30",
    "Average ‚¨ÜÔ∏è": 29.58962889646757,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6763537155464325,
    "IFEval Raw": 0.6261597007052399,
    "IFEval": 62.615970070523986,
    "BBH Raw": 0.5462236205548866,
    "BBH": 36.01120909918873,
    "MATH Lvl 5 Raw": 0.24169184290030213,
    "MATH Lvl 5": 24.169184290030213,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.41778125,
    "MUSR": 11.28932291666667,
    "MMLU-PRO Raw": 0.43267952127659576,
    "MMLU-PRO": 36.96439125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-13",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "ZeroXClem/Qwen-2.5-Aether-SlerpFusion-7B (Merge)"
  },
  {
    "eval_name": "ZeroXClem_Qwen2.5-7B-Qandora-CySec_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ZeroXClem/Qwen2.5-7B-Qandora-CySec\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ZeroXClem/Qwen2.5-7B-Qandora-CySec</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ZeroXClem__Qwen2.5-7B-Qandora-CySec-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ZeroXClem/Qwen2.5-7B-Qandora-CySec",
    "Model sha": "6c8b513dbc61a9f704210d26124244f19f3bc4cc",
    "Average ‚¨ÜÔ∏è": 30.9535098871614,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6820802918316433,
    "IFEval Raw": 0.6773172958860268,
    "IFEval": 67.73172958860269,
    "BBH Raw": 0.5490022663689288,
    "BBH": 36.264898165897854,
    "MATH Lvl 5 Raw": 0.22885196374622357,
    "MATH Lvl 5": 22.885196374622357,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4286041666666667,
    "MUSR": 13.408854166666663,
    "MMLU-PRO Raw": 0.4484707446808511,
    "MMLU-PRO": 38.71897163120567,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "ZeroXClem/Qwen2.5-7B-Qandora-CySec (Merge)"
  },
  {
    "eval_name": "ZeusLabs_L3-Aethora-15B-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ZeusLabs/L3-Aethora-15B-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ZeusLabs/L3-Aethora-15B-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ZeusLabs__L3-Aethora-15B-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ZeusLabs/L3-Aethora-15B-V2",
    "Model sha": "2c601f116c37dd912c89357dbdbef879a637997e",
    "Average ‚¨ÜÔ∏è": 24.673537409896202,
    "Hub License": "cc-by-sa-4.0",
    "Hub ‚ù§Ô∏è": 39,
    "#Params (B)": 15,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.377668733891995,
    "IFEval Raw": 0.7208063493752133,
    "IFEval": 72.08063493752132,
    "BBH Raw": 0.5010910465463698,
    "BBH": 28.968504695312703,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.3870833333333333,
    "MUSR": 6.252083333333335,
    "MMLU-PRO Raw": 0.3499833776595745,
    "MMLU-PRO": 27.77593085106383,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "ZeusLabs/L3-Aethora-15B-V2 (Merge)"
  },
  {
    "eval_name": "ZhangShenao_SELM-Llama-3-8B-Instruct-iter-3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ZhangShenao/SELM-Llama-3-8B-Instruct-iter-3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ZhangShenao/SELM-Llama-3-8B-Instruct-iter-3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ZhangShenao__SELM-Llama-3-8B-Instruct-iter-3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ZhangShenao/SELM-Llama-3-8B-Instruct-iter-3",
    "Model sha": "9c95ccdeceed14a3c2881bc495101a1acca1385f",
    "Average ‚¨ÜÔ∏è": 23.65270602885461,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.655590932177198,
    "IFEval Raw": 0.6902817856620433,
    "IFEval": 69.02817856620433,
    "BBH Raw": 0.5046089390770511,
    "BBH": 29.07853088402274,
    "MATH Lvl 5 Raw": 0.06268882175226585,
    "MATH Lvl 5": 6.268882175226585,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.38451041666666663,
    "MUSR": 5.497135416666667,
    "MMLU-PRO Raw": 0.3783244680851064,
    "MMLU-PRO": 30.92494089834515,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-07-02",
    "Generation": 3,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "aaditya_Llama3-OpenBioLLM-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">aaditya/Llama3-OpenBioLLM-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/aaditya__Llama3-OpenBioLLM-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "aaditya/Llama3-OpenBioLLM-70B",
    "Model sha": "5f79deaf38bc5f662943d304d59cb30357e8e5bd",
    "Average ‚¨ÜÔ∏è": 35.004196645647404,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 351,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.657022322955198,
    "IFEval Raw": 0.7596743307756753,
    "IFEval": 75.96743307756752,
    "BBH Raw": 0.6398872375485518,
    "BBH": 47.147074677167915,
    "MATH Lvl 5 Raw": 0.1986404833836858,
    "MATH Lvl 5": 19.86404833836858,
    "GPQA Raw": 0.32298657718120805,
    "GPQA": 9.731543624161072,
    "MUSR Raw": 0.44171875,
    "MUSR": 14.348177083333335,
    "MMLU-PRO Raw": 0.4867021276595745,
    "MMLU-PRO": 42.96690307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-24",
    "Submission Date": "2024-08-30",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "abacusai_Dracarys-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Dracarys-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Dracarys-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Dracarys-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Dracarys-72B-Instruct",
    "Model sha": "10cabc4beb57a69df51533f65e39a7ad22821370",
    "Average ‚¨ÜÔ∏è": 42.710041944577235,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.383464072727529,
    "IFEval Raw": 0.7855778224001206,
    "IFEval": 78.55778224001206,
    "BBH Raw": 0.6944066392084981,
    "BBH": 56.93552010003367,
    "MATH Lvl 5 Raw": 0.35649546827794565,
    "MATH Lvl 5": 35.649546827794566,
    "GPQA Raw": 0.39093959731543626,
    "GPQA": 18.791946308724835,
    "MUSR Raw": 0.4558229166666667,
    "MUSR": 16.81119791666666,
    "MMLU-PRO Raw": 0.5456283244680851,
    "MMLU-PRO": 49.51425827423168,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-14",
    "Submission Date": "2024-08-16",
    "Generation": 0,
    "Base Model": "abacusai/Dracarys-72B-Instruct"
  },
  {
    "eval_name": "abacusai_Liberated-Qwen1.5-14B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Liberated-Qwen1.5-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Liberated-Qwen1.5-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Liberated-Qwen1.5-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Liberated-Qwen1.5-14B",
    "Model sha": "cc0fa5102bfee821bb5e49f082731ccb9d1fedf1",
    "Average ‚¨ÜÔ∏è": 19.86614827878299,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.077311900079353,
    "IFEval Raw": 0.36310212458499,
    "IFEval": 36.310212458499,
    "BBH Raw": 0.49480009174671863,
    "BBH": 28.020905999685464,
    "MATH Lvl 5 Raw": 0.12160120845921452,
    "MATH Lvl 5": 12.160120845921451,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.41746875,
    "MUSR": 10.316927083333333,
    "MMLU-PRO Raw": 0.35123005319148937,
    "MMLU-PRO": 27.914450354609933,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-05",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "abacusai/Liberated-Qwen1.5-14B"
  },
  {
    "eval_name": "abacusai_Llama-3-Smaug-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Llama-3-Smaug-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Llama-3-Smaug-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Llama-3-Smaug-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Llama-3-Smaug-8B",
    "Model sha": "fe54a7d42160d3d8fcc3289c8c411fd9dd5e8357",
    "Average ‚¨ÜÔ∏è": 18.96705744198789,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 86,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.910218264650151,
    "IFEval Raw": 0.48667535472546175,
    "IFEval": 48.66753547254618,
    "BBH Raw": 0.4930712769667174,
    "BBH": 27.880374189415942,
    "MATH Lvl 5 Raw": 0.07930513595166162,
    "MATH Lvl 5": 7.930513595166162,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.36224999999999996,
    "MUSR": 5.047916666666663,
    "MMLU-PRO Raw": 0.3184840425531915,
    "MMLU-PRO": 24.27600472813239,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-19",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "abacusai/Llama-3-Smaug-8B"
  },
  {
    "eval_name": "abacusai_Smaug-34B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Smaug-34B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Smaug-34B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Smaug-34B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Smaug-34B-v0.1",
    "Model sha": "34d54c65a0247d5eb694973106c816d9c0ad3fc2",
    "Average ‚¨ÜÔ∏è": 23.757346989413076,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 60,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.785940686355906,
    "IFEval Raw": 0.5015625207782018,
    "IFEval": 50.156252077820184,
    "BBH Raw": 0.5357785983493821,
    "BBH": 34.261660667279955,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.397875,
    "MUSR": 8.134375000000004,
    "MMLU-PRO Raw": 0.4542885638297872,
    "MMLU-PRO": 39.36539598108747,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-25",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "jondurbin/bagel-34b-v0.2"
  },
  {
    "eval_name": "abacusai_Smaug-72B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Smaug-72B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Smaug-72B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Smaug-72B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Smaug-72B-v0.1",
    "Model sha": "a1d657156f82c24b670158406378648233487011",
    "Average ‚¨ÜÔ∏è": 29.57365374322679,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 467,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 38.824997398915215,
    "IFEval Raw": 0.5167001334237601,
    "IFEval": 51.67001334237601,
    "BBH Raw": 0.5995632330786429,
    "BBH": 43.12510043134919,
    "MATH Lvl 5 Raw": 0.1812688821752266,
    "MATH Lvl 5": 18.12688821752266,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4473229166666666,
    "MUSR": 14.415364583333334,
    "MMLU-PRO Raw": 0.4623503989361702,
    "MMLU-PRO": 40.26115543735224,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-02",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "moreh/MoMo-72B-lora-1.8.7-DPO"
  },
  {
    "eval_name": "abacusai_Smaug-Llama-3-70B-Instruct-32K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Smaug-Llama-3-70B-Instruct-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Smaug-Llama-3-70B-Instruct-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Smaug-Llama-3-70B-Instruct-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Smaug-Llama-3-70B-Instruct-32K",
    "Model sha": "33840982dc253968f32ef3a534ee0e025eb97482",
    "Average ‚¨ÜÔ∏è": 35.02219270952673,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 21,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.303413234131712,
    "IFEval Raw": 0.7761107195574409,
    "IFEval": 77.6110719557441,
    "BBH Raw": 0.6493108088828602,
    "BBH": 49.07037043446443,
    "MATH Lvl 5 Raw": 0.23036253776435045,
    "MATH Lvl 5": 23.036253776435046,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.4207916666666667,
    "MUSR": 12.432291666666663,
    "MMLU-PRO Raw": 0.47647938829787234,
    "MMLU-PRO": 41.83104314420804,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-11",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "abacusai/Smaug-Llama-3-70B-Instruct-32K"
  },
  {
    "eval_name": "abacusai_Smaug-Mixtral-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Smaug-Mixtral-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Smaug-Mixtral-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Smaug-Mixtral-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Smaug-Mixtral-v0.1",
    "Model sha": "98fdc8315906b0a8b9e7f24bad89914869fcfc20",
    "Average ‚¨ÜÔ∏è": 22.235368526294035,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.941415502307858,
    "IFEval Raw": 0.5554428915278129,
    "IFEval": 55.54428915278129,
    "BBH Raw": 0.5162245602454115,
    "BBH": 31.919260543426763,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4298125,
    "MUSR": 12.993229166666671,
    "MMLU-PRO Raw": 0.3351894946808511,
    "MMLU-PRO": 26.13216607565012,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-18",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "abacusai/Smaug-Mixtral-v0.1"
  },
  {
    "eval_name": "abacusai_Smaug-Qwen2-72B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/Smaug-Qwen2-72B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/Smaug-Qwen2-72B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__Smaug-Qwen2-72B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/Smaug-Qwen2-72B-Instruct",
    "Model sha": "af015925946d0c60ef69f512c3b35f421cf8063d",
    "Average ‚¨ÜÔ∏è": 41.43222618381143,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.257334778136919,
    "IFEval Raw": 0.7825303527972447,
    "IFEval": 78.25303527972446,
    "BBH Raw": 0.6909789934583822,
    "BBH": 56.26617189727526,
    "MATH Lvl 5 Raw": 0.37462235649546827,
    "MATH Lvl 5": 37.46223564954683,
    "GPQA Raw": 0.3615771812080537,
    "GPQA": 14.876957494407161,
    "MUSR Raw": 0.44007291666666665,
    "MUSR": 15.175781249999998,
    "MMLU-PRO Raw": 0.519032579787234,
    "MMLU-PRO": 46.559175531914896,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "abacusai/Smaug-Qwen2-72B-Instruct"
  },
  {
    "eval_name": "abacusai_bigstral-12b-32k_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/bigstral-12b-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/bigstral-12b-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__bigstral-12b-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/bigstral-12b-32k",
    "Model sha": "b78a5385ec1b04d6c97f25e9ba1dff18dc98305f",
    "Average ‚¨ÜÔ∏è": 18.072200919117407,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 43,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9652794290367237,
    "IFEval Raw": 0.41938057686937324,
    "IFEval": 41.93805768693733,
    "BBH Raw": 0.4700122314782882,
    "BBH": 25.5569024540723,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.45597916666666666,
    "MUSR": 15.864062500000001,
    "MMLU-PRO Raw": 0.26412898936170215,
    "MMLU-PRO": 18.23655437352246,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-06",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "abacusai/bigstral-12b-32k (Merge)"
  },
  {
    "eval_name": "abacusai_bigyi-15b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abacusai/bigyi-15b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abacusai/bigyi-15b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abacusai__bigyi-15b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abacusai/bigyi-15b",
    "Model sha": "b878c15531f7aaf6cf287530f1117b1308b96dc4",
    "Average ‚¨ÜÔ∏è": 12.976295791626066,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 15,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8711219678164464,
    "IFEval Raw": 0.20940327220663396,
    "IFEval": 20.940327220663395,
    "BBH Raw": 0.4345298820215116,
    "BBH": 19.94022305425607,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.35378125,
    "MUSR": 4.289322916666667,
    "MMLU-PRO Raw": 0.30028257978723405,
    "MMLU-PRO": 22.25361997635934,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-06",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "abacusai/bigyi-15b (Merge)"
  },
  {
    "eval_name": "abhishek_autotrain-llama3-70b-orpo-v1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abhishek/autotrain-llama3-70b-orpo-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhishek/autotrain-llama3-70b-orpo-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abhishek__autotrain-llama3-70b-orpo-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abhishek/autotrain-llama3-70b-orpo-v1",
    "Model sha": "053236c6846cc561c1503ba05e2b28c94855a432",
    "Average ‚¨ÜÔ∏è": 14.71267209846267,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.761027987938647,
    "IFEval Raw": 0.4233023932055834,
    "IFEval": 42.33023932055834,
    "BBH Raw": 0.5997985900252331,
    "BBH": 41.565362273408454,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.24412751677852348,
    "GPQA": 0.0,
    "MUSR Raw": 0.35790625000000004,
    "MUSR": 2.5716145833333326,
    "MMLU-PRO Raw": 0.11220079787234043,
    "MMLU-PRO": 1.3556442080378246,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-02",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "abhishek/autotrain-llama3-70b-orpo-v1"
  },
  {
    "eval_name": "abhishek_autotrain-llama3-70b-orpo-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abhishek/autotrain-llama3-70b-orpo-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhishek/autotrain-llama3-70b-orpo-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abhishek__autotrain-llama3-70b-orpo-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abhishek/autotrain-llama3-70b-orpo-v2",
    "Model sha": "a2c16a8a7fa48792eb8a1f0c50e13309c2021a63",
    "Average ‚¨ÜÔ∏è": 28.804372782766325,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.527046588839074,
    "IFEval Raw": 0.5406055931594835,
    "IFEval": 54.06055931594835,
    "BBH Raw": 0.5899473641612185,
    "BBH": 39.88219882979646,
    "MATH Lvl 5 Raw": 0.20694864048338368,
    "MATH Lvl 5": 20.694864048338367,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.41133333333333333,
    "MUSR": 9.950000000000003,
    "MMLU-PRO Raw": 0.48179853723404253,
    "MMLU-PRO": 42.4220596926714,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-08-21",
    "Generation": 0,
    "Base Model": "abhishek/autotrain-llama3-70b-orpo-v2"
  },
  {
    "eval_name": "abhishek_autotrain-llama3-orpo-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abhishek/autotrain-llama3-orpo-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhishek/autotrain-llama3-orpo-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abhishek__autotrain-llama3-orpo-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abhishek/autotrain-llama3-orpo-v2",
    "Model sha": "1655d0683696a5de2eb9a59c339ee469297beb9c",
    "Average ‚¨ÜÔ∏è": 12.263692661007687,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9059400973404499,
    "IFEval Raw": 0.4371656094717572,
    "IFEval": 43.71656094717572,
    "BBH Raw": 0.31593828880846425,
    "BBH": 4.380133995067516,
    "MATH Lvl 5 Raw": 0.046072507552870096,
    "MATH Lvl 5": 4.6072507552870094,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.3792395833333333,
    "MUSR": 5.104947916666667,
    "MMLU-PRO Raw": 0.22182513297872342,
    "MMLU-PRO": 13.536125886524822,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "abhishek/autotrain-llama3-orpo-v2"
  },
  {
    "eval_name": "abhishek_autotrain-vr4a1-e5mms_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/abhishek/autotrain-vr4a1-e5mms\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">abhishek/autotrain-vr4a1-e5mms</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/abhishek__autotrain-vr4a1-e5mms-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "abhishek/autotrain-vr4a1-e5mms",
    "Model sha": "5206a32e0bd3067aef1ce90f5528ade7d866253f",
    "Average ‚¨ÜÔ∏è": 18.609615894077187,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8728780478601406,
    "IFEval Raw": 0.21422492320376602,
    "IFEval": 21.4224923203766,
    "BBH Raw": 0.5000624442873264,
    "BBH": 28.45661724854773,
    "MATH Lvl 5 Raw": 0.13821752265861026,
    "MATH Lvl 5": 13.821752265861026,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.389125,
    "MUSR": 9.040625,
    "MMLU-PRO Raw": 0.36668882978723405,
    "MMLU-PRO": 29.63209219858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-06",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "adamo1139_Yi-34B-200K-AEZAKMI-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/adamo1139/Yi-34B-200K-AEZAKMI-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">adamo1139/Yi-34B-200K-AEZAKMI-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/adamo1139__Yi-34B-200K-AEZAKMI-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "adamo1139/Yi-34B-200K-AEZAKMI-v2",
    "Model sha": "189b42b0dae6352fbe7165255aae851961c8e678",
    "Average ‚¨ÜÔ∏è": 23.789584729523124,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.0215744731066168,
    "IFEval Raw": 0.4555257827010111,
    "IFEval": 45.55257827010111,
    "BBH Raw": 0.5383819237015192,
    "BBH": 35.2764249557812,
    "MATH Lvl 5 Raw": 0.05438066465256798,
    "MATH Lvl 5": 5.4380664652567985,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.38860416666666664,
    "MUSR": 6.475520833333334,
    "MMLU-PRO Raw": 0.4512965425531915,
    "MMLU-PRO": 39.032949172576835,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-13",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "adamo1139/Yi-34B-200K-AEZAKMI-v2"
  },
  {
    "eval_name": "adriszmar_QAIMath-Qwen2.5-7B-TIES_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/adriszmar/QAIMath-Qwen2.5-7B-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">adriszmar/QAIMath-Qwen2.5-7B-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/adriszmar__QAIMath-Qwen2.5-7B-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "adriszmar/QAIMath-Qwen2.5-7B-TIES",
    "Model sha": "c89bc166dbe2a31c1fceb40ea7acdd96c5620ff5",
    "Average ‚¨ÜÔ∏è": 5.469542016632626,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2834082223865078,
    "IFEval Raw": 0.174632198123202,
    "IFEval": 17.463219812320197,
    "BBH Raw": 0.3126379538396578,
    "BBH": 5.253690606033476,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.40959375,
    "MUSR": 9.132552083333332,
    "MMLU-PRO Raw": 0.10871010638297872,
    "MMLU-PRO": 0.9677895981087459,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-27",
    "Generation": 0,
    "Base Model": "adriszmar/QAIMath-Qwen2.5-7B-TIES"
  },
  {
    "eval_name": "adriszmar_QAIMath-Qwen2.5-7B-TIES_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/adriszmar/QAIMath-Qwen2.5-7B-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">adriszmar/QAIMath-Qwen2.5-7B-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/adriszmar__QAIMath-Qwen2.5-7B-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "adriszmar/QAIMath-Qwen2.5-7B-TIES",
    "Model sha": "c89bc166dbe2a31c1fceb40ea7acdd96c5620ff5",
    "Average ‚¨ÜÔ∏è": 4.963265433447307,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3063158611399417,
    "IFEval Raw": 0.16853725891745014,
    "IFEval": 16.853725891745015,
    "BBH Raw": 0.31242688274884584,
    "BBH": 5.019151283406914,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.39629166666666665,
    "MUSR": 7.169791666666668,
    "MMLU-PRO Raw": 0.10663231382978723,
    "MMLU-PRO": 0.7369237588652473,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-27",
    "Generation": 0,
    "Base Model": "adriszmar/QAIMath-Qwen2.5-7B-TIES"
  },
  {
    "eval_name": "ai21labs_Jamba-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "JambaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ai21labs/Jamba-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ai21labs/Jamba-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ai21labs__Jamba-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ai21labs/Jamba-v0.1",
    "Model sha": "ce13f3fe99555a2606d1892665bb67649032ff2d",
    "Average ‚¨ÜÔ∏è": 9.142836388614539,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1170,
    "#Params (B)": 51,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.11214260696736,
    "IFEval Raw": 0.20255920956395698,
    "IFEval": 20.2559209563957,
    "BBH Raw": 0.36022602451645724,
    "BBH": 10.722058918870276,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.35902083333333334,
    "MUSR": 3.7109374999999996,
    "MMLU-PRO Raw": 0.24916888297872342,
    "MMLU-PRO": 16.574320330969268,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-28",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "ai21labs/Jamba-v0.1"
  },
  {
    "eval_name": "aixonlab_Aether-12b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/aixonlab/Aether-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">aixonlab/Aether-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/aixonlab__Aether-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "aixonlab/Aether-12b",
    "Model sha": "c55d08a69c74f87c18ab5afb05d46359f389c91a",
    "Average ‚¨ÜÔ∏è": 17.882297351655794,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8664320508033132,
    "IFEval Raw": 0.23468286369056326,
    "IFEval": 23.468286369056326,
    "BBH Raw": 0.5179400750435481,
    "BBH": 30.551138311303117,
    "MATH Lvl 5 Raw": 0.09667673716012086,
    "MATH Lvl 5": 9.667673716012086,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.38286458333333334,
    "MUSR": 7.991406250000002,
    "MMLU-PRO Raw": 0.3410073138297872,
    "MMLU-PRO": 26.778590425531913,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "Xclbr7/Arcanum-12b"
  },
  {
    "eval_name": "aixonlab_Grey-12b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/aixonlab/Grey-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">aixonlab/Grey-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/aixonlab__Grey-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "aixonlab/Grey-12b",
    "Model sha": "50f56572870c49186c3679f9949a602d2d97c046",
    "Average ‚¨ÜÔ∏è": 23.60602411614195,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4686936675538083,
    "IFEval Raw": 0.39679938119744496,
    "IFEval": 39.6799381197445,
    "BBH Raw": 0.5698957505959833,
    "BBH": 38.746043454917555,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4516354166666667,
    "MUSR": 16.25442708333333,
    "MMLU-PRO Raw": 0.3779089095744681,
    "MMLU-PRO": 30.87876773049646,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-09",
    "Generation": 2,
    "Base Model": "Xclbr7/Arcanum-12b"
  },
  {
    "eval_name": "akjindal53244_Llama-3.1-Storm-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/akjindal53244/Llama-3.1-Storm-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">akjindal53244/Llama-3.1-Storm-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/akjindal53244__Llama-3.1-Storm-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "akjindal53244/Llama-3.1-Storm-8B",
    "Model sha": "df21b06dcf534b026dd301a44a521d7253c8b94b",
    "Average ‚¨ÜÔ∏è": 29.365249771767235,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 164,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7943914887773879,
    "IFEval Raw": 0.803263119633683,
    "IFEval": 80.32631196336831,
    "BBH Raw": 0.5196330402870707,
    "BBH": 31.615695113850098,
    "MATH Lvl 5 Raw": 0.1623867069486405,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4028333333333334,
    "MUSR": 8.820833333333338,
    "MMLU-PRO Raw": 0.3812333776595745,
    "MMLU-PRO": 31.24815307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-10-27",
    "Generation": 0,
    "Base Model": "akjindal53244/Llama-3.1-Storm-8B"
  },
  {
    "eval_name": "alcholjung_llama3_medical_tuned_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/alcholjung/llama3_medical_tuned\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">alcholjung/llama3_medical_tuned</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/alcholjung__llama3_medical_tuned-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "alcholjung/llama3_medical_tuned",
    "Model sha": "62bd457b6fe961a42a631306577e622c83876cb6",
    "Average ‚¨ÜÔ∏è": 11.306071402090678,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9107210973468606,
    "IFEval Raw": 0.010566408241244343,
    "IFEval": 1.0566408241244343,
    "BBH Raw": 0.4512943191660926,
    "BBH": 23.265089024969495,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.46602083333333333,
    "MUSR": 16.852604166666666,
    "MMLU-PRO Raw": 0.29463098404255317,
    "MMLU-PRO": 21.625664893617017,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-14",
    "Submission Date": "2024-08-14",
    "Generation": 0,
    "Base Model": "alcholjung/llama3_medical_tuned"
  },
  {
    "eval_name": "allenai_OLMo-1B-hf_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OlmoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allenai/OLMo-1B-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allenai/OLMo-1B-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allenai__OLMo-1B-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allenai/OLMo-1B-hf",
    "Model sha": "8e995430edd24416ccfa98b5b283fa07b0c9f1a9",
    "Average ‚¨ÜÔ∏è": 6.470278440392426,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.24887374995174436,
    "IFEval Raw": 0.21819660722438686,
    "IFEval": 21.819660722438684,
    "BBH Raw": 0.30519468988429327,
    "BBH": 3.1965463124303173,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.40978125,
    "MUSR": 9.555989583333334,
    "MMLU-PRO Raw": 0.11735372340425532,
    "MMLU-PRO": 1.9281914893617011,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-12",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "allenai/OLMo-1B-hf"
  },
  {
    "eval_name": "allenai_OLMo-7B-Instruct-hf_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "OlmoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allenai/OLMo-7B-Instruct-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allenai/OLMo-7B-Instruct-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allenai__OLMo-7B-Instruct-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allenai/OLMo-7B-Instruct-hf",
    "Model sha": "2ea947518df93433aa71219f29b36c72ac63be95",
    "Average ‚¨ÜÔ∏è": 10.76085660371239,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.199950729120621,
    "IFEval Raw": 0.3472652561869174,
    "IFEval": 34.72652561869174,
    "BBH Raw": 0.3706469866662716,
    "BBH": 13.159933415267032,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.37647916666666664,
    "MUSR": 4.3265625,
    "MMLU-PRO Raw": 0.17852393617021275,
    "MMLU-PRO": 8.724881796690305,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-04",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "allenai/OLMo-7B-Instruct-hf"
  },
  {
    "eval_name": "allenai_OLMo-7B-hf_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OlmoForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allenai/OLMo-7B-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allenai/OLMo-7B-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allenai__OLMo-7B-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allenai/OLMo-7B-hf",
    "Model sha": "687d934d36a05417048d0fe7482f24f389fef6aa",
    "Average ‚¨ÜÔ∏è": 6.776151209771288,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5905641584250613,
    "IFEval Raw": 0.2719273749207658,
    "IFEval": 27.19273749207658,
    "BBH Raw": 0.32791316587362274,
    "BBH": 5.761987041080832,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3486666666666667,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.11727061170212766,
    "MMLU-PRO": 1.9189568557919614,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-12",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "allenai/OLMo-7B-hf"
  },
  {
    "eval_name": "allenai_OLMoE-1B-7B-0924_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OlmoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allenai/OLMoE-1B-7B-0924\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allenai/OLMoE-1B-7B-0924</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allenai__OLMoE-1B-7B-0924-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allenai/OLMoE-1B-7B-0924",
    "Model sha": "4fa3a6e09ed0e41639962f38bfba0fc532b90075",
    "Average ‚¨ÜÔ∏è": 7.178463786041392,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 107,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.0764071169246754,
    "IFEval Raw": 0.21847143357402804,
    "IFEval": 21.847143357402803,
    "BBH Raw": 0.3393437931177341,
    "BBH": 8.308106894895777,
    "MATH Lvl 5 Raw": 0.011329305135951661,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.34879166666666667,
    "MUSR": 3.565625000000001,
    "MMLU-PRO Raw": 0.1739527925531915,
    "MMLU-PRO": 8.216976950354608,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-20",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "allenai/OLMoE-1B-7B-0924"
  },
  {
    "eval_name": "allenai_OLMoE-1B-7B-0924-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "OlmoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allenai/OLMoE-1B-7B-0924-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allenai/OLMoE-1B-7B-0924-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allenai__OLMoE-1B-7B-0924-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allenai/OLMoE-1B-7B-0924-Instruct",
    "Model sha": "7f1c97f440f06ce36705e4f2b843edb5925f4498",
    "Average ‚¨ÜÔ∏è": 13.207221439435314,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 83,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.487959267855377,
    "IFEval Raw": 0.4652178442089212,
    "IFEval": 46.521784420892125,
    "BBH Raw": 0.3901610626816106,
    "BBH": 14.571562821337196,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3848229166666666,
    "MUSR": 6.06953125,
    "MMLU-PRO Raw": 0.18758311170212766,
    "MMLU-PRO": 9.731456855791961,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-30",
    "Generation": 2,
    "Base Model": "allenai/OLMoE-1B-7B-0924"
  },
  {
    "eval_name": "allknowingroger_Chocolatine-24B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Chocolatine-24B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Chocolatine-24B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Chocolatine-24B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Chocolatine-24B",
    "Model sha": "6245b82885ca4930575dbed2932ec1d32d901c0e",
    "Average ‚¨ÜÔ∏è": 21.333145473777037,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.184960078892751,
    "IFEval Raw": 0.19581488229010136,
    "IFEval": 19.581488229010137,
    "BBH Raw": 0.6191260063262436,
    "BBH": 45.78594021531884,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.43232291666666667,
    "MUSR": 12.940364583333334,
    "MMLU-PRO Raw": 0.4566156914893617,
    "MMLU-PRO": 39.623965721040186,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-02",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Chocolatine-24B (Merge)"
  },
  {
    "eval_name": "allknowingroger_LimyQstar-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/LimyQstar-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/LimyQstar-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__LimyQstar-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/LimyQstar-7B-slerp",
    "Model sha": "6dc557c7bfd6a6f9bc8190bc8a31c3b732deca40",
    "Average ‚¨ÜÔ∏è": 18.67252497281834,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6303240903275028,
    "IFEval Raw": 0.34911368502240725,
    "IFEval": 34.91136850224072,
    "BBH Raw": 0.5023559424245442,
    "BBH": 30.194567331120084,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4146458333333333,
    "MUSR": 10.197395833333335,
    "MMLU-PRO Raw": 0.3103390957446808,
    "MMLU-PRO": 23.371010638297868,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-23",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/LimyQstar-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Llama3.1-60B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Llama3.1-60B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Llama3.1-60B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Llama3.1-60B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Llama3.1-60B",
    "Model sha": "5fb1ddcce0bddc60949a9d0c2fc9f8326be5bc4e",
    "Average ‚¨ÜÔ∏è": 9.951594488947727,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 61,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.491858717706602,
    "IFEval Raw": 0.18145188100905596,
    "IFEval": 18.1451881009056,
    "BBH Raw": 0.32417552719382076,
    "BBH": 7.784282802508024,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3595833333333333,
    "MUSR": 2.18125,
    "MMLU-PRO Raw": 0.3310339095744681,
    "MMLU-PRO": 25.67043439716312,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Llama3.1-60B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Meme-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Meme-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Meme-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Meme-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Meme-7B-slerp",
    "Model sha": "7836c0f4fce70286382e61003e9a05d7559365d9",
    "Average ‚¨ÜÔ∏è": 19.326433024980343,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.48245045857568675,
    "IFEval Raw": 0.5163754393897082,
    "IFEval": 51.63754393897082,
    "BBH Raw": 0.4660944195552204,
    "BBH": 24.52948594942413,
    "MATH Lvl 5 Raw": 0.04682779456193353,
    "MATH Lvl 5": 4.682779456193353,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4223020833333333,
    "MUSR": 10.187760416666666,
    "MMLU-PRO Raw": 0.281000664893617,
    "MMLU-PRO": 20.111184988179666,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Meme-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ministral-8B-slerp_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ministral-8B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ministral-8B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ministral-8B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ministral-8B-slerp",
    "Model sha": "51c40046c0f9fead83485ae83b6c0d03f4ae47f2",
    "Average ‚¨ÜÔ∏è": 14.838024782746507,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1250982811283448,
    "IFEval Raw": 0.19608970863974257,
    "IFEval": 19.608970863974257,
    "BBH Raw": 0.4686018544963986,
    "BBH": 25.195564651348292,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.42853125000000003,
    "MUSR": 12.399739583333334,
    "MMLU-PRO Raw": 0.3119182180851064,
    "MMLU-PRO": 23.54646867612293,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Ministral-8B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MistralPhi3-11B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MistralPhi3-11B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MistralPhi3-11B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MistralPhi3-11B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MistralPhi3-11B",
    "Model sha": "3afeaf24c6306c4752c320c4fd32fa2e7694e12e",
    "Average ‚¨ÜÔ∏è": 21.627095011873774,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7070377341449438,
    "IFEval Raw": 0.1942911474886634,
    "IFEval": 19.42911474886634,
    "BBH Raw": 0.6234314600705605,
    "BBH": 46.16462900339792,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.4266770833333333,
    "MUSR": 12.23463541666667,
    "MMLU-PRO Raw": 0.46875,
    "MMLU-PRO": 40.97222222222222,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/MistralPhi3-11B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Mistralmash1-7B-s_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Mistralmash1-7B-s\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Mistralmash1-7B-s</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Mistralmash1-7B-s-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Mistralmash1-7B-s",
    "Model sha": "730b7b2867deef63961f002b6e1e70e7d416c599",
    "Average ‚¨ÜÔ∏è": 20.901277692856755,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6611884983106348,
    "IFEval Raw": 0.39610012544493056,
    "IFEval": 39.61001254449306,
    "BBH Raw": 0.5277485757172445,
    "BBH": 33.44855374433827,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.4267083333333333,
    "MUSR": 11.805208333333333,
    "MMLU-PRO Raw": 0.3292885638297872,
    "MMLU-PRO": 25.47650709219858,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Mistralmash1-7B-s (Merge)"
  },
  {
    "eval_name": "allknowingroger_Mistralmash2-7B-s_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Mistralmash2-7B-s\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Mistralmash2-7B-s</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Mistralmash2-7B-s-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Mistralmash2-7B-s",
    "Model sha": "3b2aafa0f931f3d3103fbc96a6da4ac36f376d78",
    "Average ‚¨ÜÔ∏è": 21.402269092143058,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6655368720012832,
    "IFEval Raw": 0.4101883003763348,
    "IFEval": 41.01883003763348,
    "BBH Raw": 0.530485814102601,
    "BBH": 33.29836428588566,
    "MATH Lvl 5 Raw": 0.08006042296072508,
    "MATH Lvl 5": 8.006042296072508,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.43724999999999997,
    "MUSR": 13.65625,
    "MMLU-PRO Raw": 0.3345246010638298,
    "MMLU-PRO": 26.0582890070922,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Mistralmash2-7B-s (Merge)"
  },
  {
    "eval_name": "allknowingroger_MixTAO-19B-pass_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MixTAO-19B-pass\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MixTAO-19B-pass</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MixTAO-19B-pass-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MixTAO-19B-pass",
    "Model sha": "a41369cfcfbada9d5387051ba616bf1432b31d31",
    "Average ‚¨ÜÔ∏è": 20.615004277462628,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.255132304319672,
    "IFEval Raw": 0.3814368098866563,
    "IFEval": 38.14368098866563,
    "BBH Raw": 0.5128248798224987,
    "BBH": 31.577918110916897,
    "MATH Lvl 5 Raw": 0.06042296072507554,
    "MATH Lvl 5": 6.042296072507554,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.47827083333333337,
    "MUSR": 19.950520833333332,
    "MMLU-PRO Raw": 0.31050531914893614,
    "MMLU-PRO": 23.389479905437348,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MixTAO-19B-pass (Merge)"
  },
  {
    "eval_name": "allknowingroger_MixTaoTruthful-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MixTaoTruthful-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MixTaoTruthful-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MixTaoTruthful-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MixTaoTruthful-13B-slerp",
    "Model sha": "3324d37e138c6bf0d6891e54b6dd839c8d2f35ec",
    "Average ‚¨ÜÔ∏è": 20.26556408489774,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8081085595214712,
    "IFEval Raw": 0.41388515804731446,
    "IFEval": 41.38851580473145,
    "BBH Raw": 0.5207335343585151,
    "BBH": 32.70636246605644,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.42924999999999996,
    "MUSR": 12.856249999999996,
    "MMLU-PRO Raw": 0.3100066489361702,
    "MMLU-PRO": 23.33407210401891,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MixTaoTruthful-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiCalm-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiCalm-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiCalm-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiCalm-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiCalm-7B-slerp",
    "Model sha": "1c23540e907fab4dfe0ef66edd0003e764bfe568",
    "Average ‚¨ÜÔ∏è": 19.459701309543572,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6165734263900131,
    "IFEval Raw": 0.3926526061960044,
    "IFEval": 39.26526061960044,
    "BBH Raw": 0.5121891599770304,
    "BBH": 31.46648332199455,
    "MATH Lvl 5 Raw": 0.06117824773413898,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.43194791666666665,
    "MUSR": 12.960156250000004,
    "MMLU-PRO Raw": 0.3032746010638298,
    "MMLU-PRO": 22.586066784869978,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiCalm-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash-12B-slerp",
    "Model sha": "91a6d0fe6b9271000ca713ee9ab414c782ba4c50",
    "Average ‚¨ÜÔ∏è": 20.19249196725997,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8419798184824282,
    "IFEval Raw": 0.39744876926554873,
    "IFEval": 39.74487692655487,
    "BBH Raw": 0.5141827379810838,
    "BBH": 31.925677106468356,
    "MATH Lvl 5 Raw": 0.08157099697885198,
    "MATH Lvl 5": 8.157099697885197,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.44379166666666664,
    "MUSR": 14.773958333333333,
    "MMLU-PRO Raw": 0.3067652925531915,
    "MMLU-PRO": 22.973921394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-20",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash10-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash10-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash10-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash10-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash10-13B-slerp",
    "Model sha": "6def2fd1a11d4c380a19b7a3bdf263a6b80cd8f3",
    "Average ‚¨ÜÔ∏è": 20.376084015154504,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8793591508664917,
    "IFEval Raw": 0.41628323958208663,
    "IFEval": 41.62832395820867,
    "BBH Raw": 0.5186335995744094,
    "BBH": 32.45250184104656,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.43179166666666663,
    "MUSR": 12.97395833333333,
    "MMLU-PRO Raw": 0.3116688829787234,
    "MMLU-PRO": 23.51876477541371,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash10-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash11-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash11-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash11-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash11-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash11-13B-slerp",
    "Model sha": "1134a0adabef4a26e1d49c302baff74c4a7e9f46",
    "Average ‚¨ÜÔ∏è": 20.614675908435512,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9857385654766669,
    "IFEval Raw": 0.4251009543566625,
    "IFEval": 42.51009543566625,
    "BBH Raw": 0.5193864686484946,
    "BBH": 32.59670310684399,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.43728125,
    "MUSR": 14.026822916666665,
    "MMLU-PRO Raw": 0.30851063829787234,
    "MMLU-PRO": 23.167848699763592,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash11-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash2-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash2-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash2-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash2-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash2-12B-slerp",
    "Model sha": "e44e9563368699f753a4474b068c059d233ddee3",
    "Average ‚¨ÜÔ∏è": 19.827554947936594,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8201396697282731,
    "IFEval Raw": 0.42607503645881817,
    "IFEval": 42.60750364588182,
    "BBH Raw": 0.5133973498532299,
    "BBH": 31.617950213580304,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.4228020833333333,
    "MUSR": 11.783593750000003,
    "MMLU-PRO Raw": 0.3042719414893617,
    "MMLU-PRO": 22.696882387706854,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-20",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash2-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash5-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash5-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash5-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash5-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash5-12B-slerp",
    "Model sha": "15ef0301c7ce939208d55ad13fa840662f92bce6",
    "Average ‚¨ÜÔ∏è": 19.60289357065609,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.804166644221118,
    "IFEval Raw": 0.41415998439695567,
    "IFEval": 41.415998439695564,
    "BBH Raw": 0.5144534995858502,
    "BBH": 31.856364255964934,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4202916666666667,
    "MUSR": 11.703124999999998,
    "MMLU-PRO Raw": 0.30277593085106386,
    "MMLU-PRO": 22.530658983451538,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-21",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash5-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash6-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash6-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash6-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash6-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash6-12B-slerp",
    "Model sha": "a04856a12b85e986e1b540cf0c7510e9ce2df09b",
    "Average ‚¨ÜÔ∏è": 20.263838988227736,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8245726526160646,
    "IFEval Raw": 0.43004672047943904,
    "IFEval": 43.0046720479439,
    "BBH Raw": 0.5195916915718951,
    "BBH": 32.403879619181005,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.4305833333333333,
    "MUSR": 12.522916666666669,
    "MMLU-PRO Raw": 0.30909242021276595,
    "MMLU-PRO": 23.232491134751772,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash6-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash7-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash7-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash7-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash7-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash7-12B-slerp",
    "Model sha": "5f91dd41fb4b58e76c52b03ed15477a046b079df",
    "Average ‚¨ÜÔ∏è": 19.779705395006918,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8217713466378345,
    "IFEval Raw": 0.42127887338927383,
    "IFEval": 42.12788733892738,
    "BBH Raw": 0.5111135397195524,
    "BBH": 31.298150090327656,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.42794791666666665,
    "MUSR": 12.026822916666669,
    "MMLU-PRO Raw": 0.3029421542553192,
    "MMLU-PRO": 22.549128250591018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash7-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash8-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash8-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash8-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash8-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash8-13B-slerp",
    "Model sha": "5590ccd99f74301951f450f9d0271a99e97728c8",
    "Average ‚¨ÜÔ∏è": 21.02451201921762,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7259072319685973,
    "IFEval Raw": 0.4320702402957486,
    "IFEval": 43.20702402957486,
    "BBH Raw": 0.5178483059643324,
    "BBH": 32.27299661531551,
    "MATH Lvl 5 Raw": 0.07401812688821752,
    "MATH Lvl 5": 7.401812688821751,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.4423958333333333,
    "MUSR": 14.499479166666665,
    "MMLU-PRO Raw": 0.31258311170212766,
    "MMLU-PRO": 23.620345744680847,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash8-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMash9-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMash9-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMash9-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMash9-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMash9-13B-slerp",
    "Model sha": "56dac45f387669baa04a8997ebb9ea63c65fbbd1",
    "Average ‚¨ÜÔ∏è": 20.642969652890788,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8657309831093002,
    "IFEval Raw": 0.4187810564856802,
    "IFEval": 41.878105648568024,
    "BBH Raw": 0.5193579939678727,
    "BBH": 32.55261171624742,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.4398229166666667,
    "MUSR": 14.211197916666665,
    "MMLU-PRO Raw": 0.3100066489361702,
    "MMLU-PRO": 23.33407210401891,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMash9-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiMerge-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiMerge-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiMerge-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiMerge-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiMerge-7B-slerp",
    "Model sha": "a026bbea09f0b1880deed62b9081e3708be0dec2",
    "Average ‚¨ÜÔ∏è": 19.554834844590108,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6131722434053442,
    "IFEval Raw": 0.3947758613811354,
    "IFEval": 39.47758613811354,
    "BBH Raw": 0.5140224933103638,
    "BBH": 31.803983321994554,
    "MATH Lvl 5 Raw": 0.06722054380664652,
    "MATH Lvl 5": 6.722054380664652,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.42797916666666663,
    "MUSR": 12.330729166666671,
    "MMLU-PRO Raw": 0.3036901595744681,
    "MMLU-PRO": 22.632239952718678,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-11",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiMerge-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Multimash3-12B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Multimash3-12B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Multimash3-12B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Multimash3-12B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Multimash3-12B-slerp",
    "Model sha": "0b90bf0b5230d02b4ba63879fc3bf0b85d46c3ce",
    "Average ‚¨ÜÔ∏è": 20.483321358079127,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8446579510699425,
    "IFEval Raw": 0.44371046600796993,
    "IFEval": 44.371046600796994,
    "BBH Raw": 0.5176624678276028,
    "BBH": 32.1508911391619,
    "MATH Lvl 5 Raw": 0.06344410876132932,
    "MATH Lvl 5": 6.344410876132931,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.4343958333333333,
    "MUSR": 13.0328125,
    "MMLU-PRO Raw": 0.3067652925531915,
    "MMLU-PRO": 22.973921394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-21",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Multimash3-12B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Multimerge-19B-pass_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Multimerge-19B-pass\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Multimerge-19B-pass</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Multimerge-19B-pass-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Multimerge-19B-pass",
    "Model sha": "e75918ed5601f400f62601cf6c0887aa936e8a52",
    "Average ‚¨ÜÔ∏è": 4.536203105914491,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9653789157476356,
    "IFEval Raw": 0.17730510600761534,
    "IFEval": 17.730510600761534,
    "BBH Raw": 0.2891778102988436,
    "BBH": 2.0803742908537424,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3429583333333333,
    "MUSR": 4.303125,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Multimerge-19B-pass (Merge)"
  },
  {
    "eval_name": "allknowingroger_MultiverseEx26-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/MultiverseEx26-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/MultiverseEx26-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__MultiverseEx26-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/MultiverseEx26-7B-slerp",
    "Model sha": "43f18d84e025693f00e9be335bf12fce96089b2f",
    "Average ‚¨ÜÔ∏è": 19.683310670804666,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6054927144671498,
    "IFEval Raw": 0.3938516469633905,
    "IFEval": 39.38516469633905,
    "BBH Raw": 0.5133591871690678,
    "BBH": 31.66377531246577,
    "MATH Lvl 5 Raw": 0.07477341389728095,
    "MATH Lvl 5": 7.477341389728095,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4293125,
    "MUSR": 12.597395833333337,
    "MMLU-PRO Raw": 0.3035239361702128,
    "MMLU-PRO": 22.613770685579198,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-30",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/MultiverseEx26-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_NeuralWestSeverus-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/NeuralWestSeverus-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/NeuralWestSeverus-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__NeuralWestSeverus-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/NeuralWestSeverus-7B-slerp",
    "Model sha": "5ee5d6a11ffc4f9733e78994169a2e1614d5e16e",
    "Average ‚¨ÜÔ∏è": 20.68795868384743,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5840462829985735,
    "IFEval Raw": 0.41356046401326263,
    "IFEval": 41.35604640132626,
    "BBH Raw": 0.5244283854305991,
    "BBH": 33.41446681662389,
    "MATH Lvl 5 Raw": 0.07401812688821752,
    "MATH Lvl 5": 7.401812688821751,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.45287499999999997,
    "MUSR": 15.409375000000002,
    "MMLU-PRO Raw": 0.3137466755319149,
    "MMLU-PRO": 23.749630614657207,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-16",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/NeuralWestSeverus-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Neuralcoven-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Neuralcoven-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Neuralcoven-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Neuralcoven-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Neuralcoven-7B-slerp",
    "Model sha": "129b40a7fd816f679ef5d4ab29fc77345f33a7b1",
    "Average ‚¨ÜÔ∏è": 20.37625856831286,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6319112155621772,
    "IFEval Raw": 0.3858584112377381,
    "IFEval": 38.58584112377381,
    "BBH Raw": 0.530287217712165,
    "BBH": 33.79913505465432,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.429,
    "MUSR": 11.758333333333335,
    "MMLU-PRO Raw": 0.3293716755319149,
    "MMLU-PRO": 25.485741725768317,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Neuralcoven-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Neuralmultiverse-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Neuralmultiverse-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Neuralmultiverse-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Neuralmultiverse-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Neuralmultiverse-7B-slerp",
    "Model sha": "a65fe05e26e10a488b08264ac8ed73a49c3f263a",
    "Average ‚¨ÜÔ∏è": 19.3862065518818,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6171595141435741,
    "IFEval Raw": 0.3769154731667531,
    "IFEval": 37.69154731667531,
    "BBH Raw": 0.5165722210470375,
    "BBH": 32.10018047347172,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.42804166666666665,
    "MUSR": 12.60520833333333,
    "MMLU-PRO Raw": 0.30418882978723405,
    "MMLU-PRO": 22.687647754137117,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Neuralmultiverse-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3della5-14B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3della5-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3della5-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3della5-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3della5-14B",
    "Model sha": "9c6819a910d4da414dd67c10da3bff3f986fefa5",
    "Average ‚¨ÜÔ∏è": 30.117271485598906,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0459248462033603,
    "IFEval Raw": 0.47985567183960776,
    "IFEval": 47.98556718396078,
    "BBH Raw": 0.6331746353794991,
    "BBH": 48.41436428305058,
    "MATH Lvl 5 Raw": 0.1555891238670695,
    "MATH Lvl 5": 15.55891238670695,
    "GPQA Raw": 0.3422818791946309,
    "GPQA": 12.304250559284117,
    "MUSR Raw": 0.4386145833333333,
    "MUSR": 14.360156249999998,
    "MMLU-PRO Raw": 0.4787234042553192,
    "MMLU-PRO": 42.08037825059102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3della5-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3merge-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3merge-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3merge-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3merge-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3merge-14B",
    "Model sha": "6d0ddaa4e0cf4c82d7149cc726b08be5753a760a",
    "Average ‚¨ÜÔ∏è": 23.532275395566582,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.012332433635963,
    "IFEval Raw": 0.27012881376968667,
    "IFEval": 27.012881376968664,
    "BBH Raw": 0.638087568868341,
    "BBH": 48.88242371785896,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.4334375,
    "MUSR": 13.279687500000001,
    "MMLU-PRO Raw": 0.4611037234042553,
    "MMLU-PRO": 40.12263593380615,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3merge-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3merge2-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3merge2-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3merge2-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3merge2-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3merge2-14B",
    "Model sha": "2256ab821e286a1d8a4f0d42e00a50013e119671",
    "Average ‚¨ÜÔ∏è": 7.962730746600417,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.0526599028620556,
    "IFEval Raw": 0.17061064641817045,
    "IFEval": 17.061064641817044,
    "BBH Raw": 0.3606937444321621,
    "BBH": 10.549967885447563,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.3910833333333333,
    "MUSR": 6.6520833333333345,
    "MMLU-PRO Raw": 0.1722905585106383,
    "MMLU-PRO": 8.03228427895981,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3merge2-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3merge3-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3merge3-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3merge3-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3merge3-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3merge3-14B",
    "Model sha": "90a036f7f136932ea525b5fd26cf2f54a66141af",
    "Average ‚¨ÜÔ∏è": 7.931823747573229,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9892565878484632,
    "IFEval Raw": 0.1645157072124186,
    "IFEval": 16.45157072124186,
    "BBH Raw": 0.3597431731140411,
    "BBH": 10.391379646236162,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40819791666666666,
    "MUSR": 8.858072916666668,
    "MMLU-PRO Raw": 0.16472739361702127,
    "MMLU-PRO": 7.191932624113473,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3merge3-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3task1-14B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3task1-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3task1-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3task1-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3task1-14B",
    "Model sha": "c9a5bab157dbdd281c651a5b7ea82a8bc64aa420",
    "Average ‚¨ÜÔ∏è": 30.284047573273323,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0088484968373466,
    "IFEval Raw": 0.46946435457918323,
    "IFEval": 46.94643545791833,
    "BBH Raw": 0.63178060736657,
    "BBH": 47.92690847304542,
    "MATH Lvl 5 Raw": 0.1510574018126888,
    "MATH Lvl 5": 15.105740181268882,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.45077083333333334,
    "MUSR": 16.81302083333333,
    "MMLU-PRO Raw": 0.4734042553191489,
    "MMLU-PRO": 41.48936170212765,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-07",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3task1-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3task2-14B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3task2-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3task2-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3task2-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3task2-14B",
    "Model sha": "2193bfec75bc90e87bc57863e02deefbdd195f9f",
    "Average ‚¨ÜÔ∏è": 28.42228883396186,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9350658025424768,
    "IFEval Raw": 0.4713127834146731,
    "IFEval": 47.13127834146731,
    "BBH Raw": 0.6098412220695854,
    "BBH": 44.08179620906436,
    "MATH Lvl 5 Raw": 0.1351963746223565,
    "MATH Lvl 5": 13.51963746223565,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.4535,
    "MUSR": 16.620833333333326,
    "MMLU-PRO Raw": 0.44597739361702127,
    "MMLU-PRO": 38.44193262411347,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3task2-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3task3-14B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3task3-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3task3-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3task3-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3task3-14B",
    "Model sha": "359de5c4969057206f846a41c72073b3429317fd",
    "Average ‚¨ÜÔ∏è": 30.395518587168695,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0376150598017824,
    "IFEval Raw": 0.4962421929369628,
    "IFEval": 49.62421929369628,
    "BBH Raw": 0.6297915743094921,
    "BBH": 47.99849937558212,
    "MATH Lvl 5 Raw": 0.15709969788519637,
    "MATH Lvl 5": 15.709969788519636,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.44255208333333335,
    "MUSR": 14.952343749999999,
    "MMLU-PRO Raw": 0.47706117021276595,
    "MMLU-PRO": 41.89568557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3task3-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Ph3unsloth-3B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Ph3unsloth-3B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Ph3unsloth-3B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Ph3unsloth-3B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Ph3unsloth-3B-slerp",
    "Model sha": "465444b3cdd43876717f7386ea2f3357c5fe8e53",
    "Average ‚¨ÜÔ∏è": 19.763283266410607,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5290418828356667,
    "IFEval Raw": 0.18944511673470835,
    "IFEval": 18.944511673470835,
    "BBH Raw": 0.5468077356147099,
    "BBH": 36.45877270267158,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.45278124999999997,
    "MUSR": 15.430989583333337,
    "MMLU-PRO Raw": 0.3700964095744681,
    "MMLU-PRO": 30.010712174940902,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Ph3unsloth-3B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Phi3mash1-17B-pass_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Phi3mash1-17B-pass\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Phi3mash1-17B-pass</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Phi3mash1-17B-pass-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Phi3mash1-17B-pass",
    "Model sha": "fcd265996f026475c15fa44833e0481dc610e469",
    "Average ‚¨ÜÔ∏è": 21.34996880563698,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4581264514682195,
    "IFEval Raw": 0.18842116694814204,
    "IFEval": 18.8421166948142,
    "BBH Raw": 0.6128878795560929,
    "BBH": 45.25041934691859,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.445125,
    "MUSR": 14.840624999999994,
    "MMLU-PRO Raw": 0.45894281914893614,
    "MMLU-PRO": 39.882535460992905,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-28",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "allknowingroger/Phi3mash1-17B-pass (Merge)"
  },
  {
    "eval_name": "allknowingroger_Quen2-65B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Quen2-65B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Quen2-65B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Quen2-65B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Quen2-65B",
    "Model sha": "2259cd8ea037d0e590920e7106b0fd1641a96c1d",
    "Average ‚¨ÜÔ∏è": 3.5313443657802126,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 63,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.31742355978756,
    "IFEval Raw": 0.17578137120617737,
    "IFEval": 17.57813712061774,
    "BBH Raw": 0.27565161872324456,
    "BBH": 1.23986036838978,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23573825503355705,
    "GPQA": 0.0,
    "MUSR Raw": 0.32085416666666666,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.11136968085106383,
    "MMLU-PRO": 1.2632978723404247,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-42B-AGI_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-42B-AGI\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-42B-AGI</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-42B-AGI-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-42B-AGI",
    "Model sha": "8939b021a9d84bc2e4ae0ea4f351d807f35b91d7",
    "Average ‚¨ÜÔ∏è": 4.47082956616707,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 42,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 8.85698112781925,
    "IFEval Raw": 0.19129354557019818,
    "IFEval": 19.129354557019816,
    "BBH Raw": 0.2942104150907988,
    "BBH": 2.2358856564144527,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.36203125,
    "MUSR": 2.2539062500000013,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-42B-AGI (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-7B-task2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-7B-task2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-7B-task2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-7B-task2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-7B-task2",
    "Model sha": "6f3ae972b2bbde0383c3a774e0e788a1af0dabc5",
    "Average ‚¨ÜÔ∏è": 29.11005901648579,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7299207114572962,
    "IFEval Raw": 0.45270327176336567,
    "IFEval": 45.270327176336565,
    "BBH Raw": 0.5625940266685543,
    "BBH": 37.52854979009311,
    "MATH Lvl 5 Raw": 0.30891238670694865,
    "MATH Lvl 5": 30.891238670694865,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.43696874999999996,
    "MUSR": 13.054427083333337,
    "MMLU-PRO Raw": 0.4517121010638298,
    "MMLU-PRO": 39.079122340425535,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-7B-task2 (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-7B-task3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-7B-task3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-7B-task3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-7B-task3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-7B-task3",
    "Model sha": "b1e524004242cdeec838ba21bce44ebb8598c12f",
    "Average ‚¨ÜÔ∏è": 27.89535697183935,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6196075029913799,
    "IFEval Raw": 0.512903540383959,
    "IFEval": 51.2903540383959,
    "BBH Raw": 0.5397623813486384,
    "BBH": 34.385984193445104,
    "MATH Lvl 5 Raw": 0.20996978851963746,
    "MATH Lvl 5": 20.996978851963746,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.43557291666666664,
    "MUSR": 12.846614583333334,
    "MMLU-PRO Raw": 0.45013297872340424,
    "MMLU-PRO": 38.90366430260047,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-7B-task3 (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-7B-task4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-7B-task4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-7B-task4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-7B-task4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-7B-task4",
    "Model sha": "ef4fe9331a0b9c34d829fcd5b1a09a7056e9300f",
    "Average ‚¨ÜÔ∏è": 29.331698653047734,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6620992956265225,
    "IFEval Raw": 0.5005385709916355,
    "IFEval": 50.05385709916355,
    "BBH Raw": 0.5583446038580263,
    "BBH": 37.02526862429277,
    "MATH Lvl 5 Raw": 0.2673716012084592,
    "MATH Lvl 5": 26.73716012084592,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.43954166666666666,
    "MUSR": 13.209375000000001,
    "MMLU-PRO Raw": 0.45611702127659576,
    "MMLU-PRO": 39.568557919621746,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-7B-task4 (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-7B-task7_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-7B-task7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-7B-task7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-7B-task7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-7B-task7",
    "Model sha": "090a873c77ed291867ddaf20249ed7f479ba4ba9",
    "Average ‚¨ÜÔ∏è": 23.274021320612203,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6716544484134207,
    "IFEval Raw": 0.42689951550774163,
    "IFEval": 42.68995155077417,
    "BBH Raw": 0.555243179835915,
    "BBH": 37.51817009438029,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.4325625,
    "MUSR": 13.036979166666669,
    "MMLU-PRO Raw": 0.4133144946808511,
    "MMLU-PRO": 34.812721631205676,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-7B-task7 (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-7B-task8_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-7B-task8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-7B-task8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-7B-task8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-7B-task8",
    "Model sha": "489a9a6fc98001026d9b96563d715cad43aabc8c",
    "Average ‚¨ÜÔ∏è": 29.240845401162915,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7037515624042076,
    "IFEval Raw": 0.4645185884564068,
    "IFEval": 46.45185884564068,
    "BBH Raw": 0.5524895381578828,
    "BBH": 36.09273684636751,
    "MATH Lvl 5 Raw": 0.30060422960725075,
    "MATH Lvl 5": 30.060422960725074,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.45144791666666667,
    "MUSR": 15.297656250000003,
    "MMLU-PRO Raw": 0.44331781914893614,
    "MMLU-PRO": 38.14642434988179,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-7B-task8 (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwen2.5-slerp-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwen2.5-slerp-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwen2.5-slerp-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwen2.5-slerp-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwen2.5-slerp-14B",
    "Model sha": "a44b0ea8291b62785152c2fe6ab336f5da672d1e",
    "Average ‚¨ÜÔ∏è": 34.109402524224485,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3668818724075553,
    "IFEval Raw": 0.49282016161562425,
    "IFEval": 49.282016161562424,
    "BBH Raw": 0.65124197415124,
    "BBH": 49.78953727809471,
    "MATH Lvl 5 Raw": 0.21903323262839883,
    "MATH Lvl 5": 21.903323262839884,
    "GPQA Raw": 0.3674496644295302,
    "GPQA": 15.659955257270694,
    "MUSR Raw": 0.47439583333333335,
    "MUSR": 19.36614583333333,
    "MMLU-PRO Raw": 0.5378989361702128,
    "MMLU-PRO": 48.65543735224587,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwen2.5-slerp-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwenslerp2-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwenslerp2-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwenslerp2-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwenslerp2-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwenslerp2-14B",
    "Model sha": "38e902c114b5640509a8615fc2a2546e07a5fb3f",
    "Average ‚¨ÜÔ∏è": 35.69424107412545,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.264443423976733,
    "IFEval Raw": 0.5007136619724553,
    "IFEval": 50.07136619724553,
    "BBH Raw": 0.6554876216007552,
    "BBH": 50.30369191199753,
    "MATH Lvl 5 Raw": 0.3021148036253777,
    "MATH Lvl 5": 30.211480362537767,
    "GPQA Raw": 0.36828859060402686,
    "GPQA": 15.771812080536915,
    "MUSR Raw": 0.4729375,
    "MUSR": 18.883854166666662,
    "MMLU-PRO Raw": 0.5403091755319149,
    "MMLU-PRO": 48.92324172576833,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-19",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwenslerp2-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwenslerp2-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwenslerp2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwenslerp2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwenslerp2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwenslerp2-7B",
    "Model sha": "46fe65fc2567b2430fa421478d47134ffe55c8f8",
    "Average ‚¨ÜÔ∏è": 30.42023764916729,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6365799320368153,
    "IFEval Raw": 0.5294396645345462,
    "IFEval": 52.943966453454614,
    "BBH Raw": 0.5609127334788001,
    "BBH": 37.437245340819274,
    "MATH Lvl 5 Raw": 0.3187311178247734,
    "MATH Lvl 5": 31.87311178247734,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.4356041666666666,
    "MUSR": 12.817187500000003,
    "MMLU-PRO Raw": 0.4515458776595745,
    "MMLU-PRO": 39.06065307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwenslerp2-7B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwenslerp3-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwenslerp3-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwenslerp3-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwenslerp3-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwenslerp3-14B",
    "Model sha": "ac60a6c4e224e5b52c42bebfd0cf81f920befdef",
    "Average ‚¨ÜÔ∏è": 35.550711886602095,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2196310822343723,
    "IFEval Raw": 0.5052349986923584,
    "IFEval": 50.52349986923585,
    "BBH Raw": 0.6520835120117142,
    "BBH": 49.80982854016478,
    "MATH Lvl 5 Raw": 0.29456193353474325,
    "MATH Lvl 5": 29.456193353474326,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.46760416666666665,
    "MUSR": 18.017187499999995,
    "MMLU-PRO Raw": 0.5394780585106383,
    "MMLU-PRO": 48.83089539007093,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-19",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwenslerp3-14B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Qwenslerp3-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Qwenslerp3-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Qwenslerp3-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Qwenslerp3-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Qwenslerp3-7B",
    "Model sha": "0351c5f6207cafd15e10e6d8dfe61b50d1b2378b",
    "Average ‚¨ÜÔ∏è": 29.97816783731106,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6094453465021652,
    "IFEval Raw": 0.501837347127843,
    "IFEval": 50.1837347127843,
    "BBH Raw": 0.5580160200086862,
    "BBH": 37.15398413723134,
    "MATH Lvl 5 Raw": 0.2824773413897281,
    "MATH Lvl 5": 28.247734138972806,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.45151041666666664,
    "MUSR": 14.972135416666667,
    "MMLU-PRO Raw": 0.45420545212765956,
    "MMLU-PRO": 39.35616134751773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "allknowingroger/Qwenslerp3-7B (Merge)"
  },
  {
    "eval_name": "allknowingroger_ROGERphi-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/ROGERphi-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/ROGERphi-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__ROGERphi-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/ROGERphi-7B-slerp",
    "Model sha": "a92f90ae5e4286daa2399df4951a3347aaf414e1",
    "Average ‚¨ÜÔ∏è": 20.70747082131683,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6163494379872385,
    "IFEval Raw": 0.3861332375873793,
    "IFEval": 38.61332375873792,
    "BBH Raw": 0.5195583428468424,
    "BBH": 32.81903240379116,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.46853125,
    "MUSR": 17.53307291666667,
    "MMLU-PRO Raw": 0.3052692819148936,
    "MMLU-PRO": 22.807697990543733,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-20",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/ROGERphi-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_RogerMerge-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/RogerMerge-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/RogerMerge-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__RogerMerge-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/RogerMerge-7B-slerp",
    "Model sha": "397f5c0b52a536c130982ca2a7c3056358bbdf92",
    "Average ‚¨ÜÔ∏è": 19.605147694000024,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6178581127072122,
    "IFEval Raw": 0.39330199426410817,
    "IFEval": 39.330199426410815,
    "BBH Raw": 0.5160176493085935,
    "BBH": 31.98716596760703,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.43197916666666664,
    "MUSR": 12.930729166666671,
    "MMLU-PRO Raw": 0.30302526595744683,
    "MMLU-PRO": 22.558362884160758,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-11",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/RogerMerge-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Rombos-LLM-V2.5-Qwen-42b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Rombos-LLM-V2.5-Qwen-42b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Rombos-LLM-V2.5-Qwen-42b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Rombos-LLM-V2.5-Qwen-42b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Rombos-LLM-V2.5-Qwen-42b",
    "Model sha": "977192ef80c5c904697f1d85d2eeab5db3947c65",
    "Average ‚¨ÜÔ∏è": 4.559640996200532,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 42,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 8.44788269726343,
    "IFEval Raw": 0.1879213819332704,
    "IFEval": 18.79213819332704,
    "BBH Raw": 0.2969164076001621,
    "BBH": 2.607639713842668,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.36333333333333334,
    "MUSR": 2.4166666666666683,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-21",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "allknowingroger_Strangecoven-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Strangecoven-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Strangecoven-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Strangecoven-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Strangecoven-7B-slerp",
    "Model sha": "8bc9d8f972d15fdd3e02c602ef4f549493bf2208",
    "Average ‚¨ÜÔ∏è": 20.299388938322068,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6324689688837714,
    "IFEval Raw": 0.37464261492839,
    "IFEval": 37.464261492839,
    "BBH Raw": 0.5368022290282338,
    "BBH": 34.832235357083775,
    "MATH Lvl 5 Raw": 0.0755287009063444,
    "MATH Lvl 5": 7.552870090634441,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.4198854166666666,
    "MUSR": 10.419010416666667,
    "MMLU-PRO Raw": 0.33643617021276595,
    "MMLU-PRO": 26.27068557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-16",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Strangecoven-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Weirdslerp2-25B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Weirdslerp2-25B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Weirdslerp2-25B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Weirdslerp2-25B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Weirdslerp2-25B",
    "Model sha": "4221341fe45e3ee6eaab27830b27d46bbbd5ea23",
    "Average ‚¨ÜÔ∏è": 4.039649477594208,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 25,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7077122940725133,
    "IFEval Raw": 0.1754068094877148,
    "IFEval": 17.54068094877148,
    "BBH Raw": 0.2873695911207614,
    "BBH": 1.5659917737677607,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.3523541666666667,
    "MUSR": 3.7109374999999996,
    "MMLU-PRO Raw": 0.11278257978723404,
    "MMLU-PRO": 1.4202866430260035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "allknowingroger/Weirdslerp2-25B (Merge)"
  },
  {
    "eval_name": "allknowingroger_WestlakeMaziyar-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/WestlakeMaziyar-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/WestlakeMaziyar-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__WestlakeMaziyar-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/WestlakeMaziyar-7B-slerp",
    "Model sha": "751534a844b0d439fe62f98bf8882fe9ab9872e0",
    "Average ‚¨ÜÔ∏è": 22.208593463266496,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6316705585239104,
    "IFEval Raw": 0.48377748817581795,
    "IFEval": 48.3777488175818,
    "BBH Raw": 0.5245479952765804,
    "BBH": 33.34281144377223,
    "MATH Lvl 5 Raw": 0.06797583081570996,
    "MATH Lvl 5": 6.797583081570996,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.44738541666666665,
    "MUSR": 14.48984375,
    "MMLU-PRO Raw": 0.3077626329787234,
    "MMLU-PRO": 23.084736997635936,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-16",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/WestlakeMaziyar-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_YamMaths-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/YamMaths-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/YamMaths-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__YamMaths-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/YamMaths-7B-slerp",
    "Model sha": "bd4ac9d63ca88c80d34fa60ef5cbb56d60a39077",
    "Average ‚¨ÜÔ∏è": 20.52713067321613,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6126235610180045,
    "IFEval Raw": 0.4148093724650594,
    "IFEval": 41.48093724650594,
    "BBH Raw": 0.5155845857281723,
    "BBH": 32.1333222251701,
    "MATH Lvl 5 Raw": 0.0838368580060423,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.43836458333333334,
    "MUSR": 13.462239583333336,
    "MMLU-PRO Raw": 0.3130817819148936,
    "MMLU-PRO": 23.675753546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/YamMaths-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yi-1.5-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yi-1.5-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yi-1.5-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yi-1.5-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yi-1.5-34B",
    "Model sha": "fef96e380cb3aeecac8e2e53ad2c73a1187beb68",
    "Average ‚¨ÜÔ∏è": 4.25273291237777,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.022691649268411,
    "IFEval Raw": 0.16391618682872555,
    "IFEval": 16.391618682872554,
    "BBH Raw": 0.28272506287695653,
    "BBH": 1.3390433749257278,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.38565625,
    "MUSR": 5.6070312499999995,
    "MMLU-PRO Raw": 0.10954122340425532,
    "MMLU-PRO": 1.0601359338061456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-21",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "allknowingroger_Yi-blossom-40B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yi-blossom-40B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yi-blossom-40B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yi-blossom-40B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yi-blossom-40B",
    "Model sha": "d1bf1cf9339808193c5a56ef23fecdfd1012acfb",
    "Average ‚¨ÜÔ∏è": 5.827458303350088,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 18,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9884573159686586,
    "IFEval Raw": 0.20088587170928693,
    "IFEval": 20.08858717092869,
    "BBH Raw": 0.32150442258143547,
    "BBH": 5.539183494900659,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3842604166666666,
    "MUSR": 5.199218749999999,
    "MMLU-PRO Raw": 0.10804521276595745,
    "MMLU-PRO": 0.8939125295508273,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "allknowingroger/Yi-blossom-40B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yibuddy-35B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yibuddy-35B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yibuddy-35B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yibuddy-35B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yibuddy-35B",
    "Model sha": "592e1e52b97ec88a80ba3b496c19f2498ada4ea3",
    "Average ‚¨ÜÔ∏è": 27.930703297359486,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.184353448065919,
    "IFEval Raw": 0.4234774841864032,
    "IFEval": 42.34774841864032,
    "BBH Raw": 0.5916185369526096,
    "BBH": 42.80824233844326,
    "MATH Lvl 5 Raw": 0.13595166163141995,
    "MATH Lvl 5": 13.595166163141995,
    "GPQA Raw": 0.35570469798657717,
    "GPQA": 14.093959731543624,
    "MUSR Raw": 0.45045833333333335,
    "MUSR": 15.973958333333329,
    "MMLU-PRO Raw": 0.44888630319148937,
    "MMLU-PRO": 38.765144799054376,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Yibuddy-35B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yillama-40B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yillama-40B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yillama-40B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yillama-40B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yillama-40B",
    "Model sha": "65db687755e716481a218cac99d20619d78e41f7",
    "Average ‚¨ÜÔ∏è": 8.31148742313468,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.5240661174641805,
    "IFEval Raw": 0.16968643200042555,
    "IFEval": 16.968643200042553,
    "BBH Raw": 0.40628855371888356,
    "BBH": 15.875797412234048,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3500625,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.1981382978723404,
    "MMLU-PRO": 10.904255319148934,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "allknowingroger/Yillama-40B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yislerp-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yislerp-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yislerp-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yislerp-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yislerp-34B",
    "Model sha": "131ad918edd652271510ee8dba63d3e7319df133",
    "Average ‚¨ÜÔ∏è": 29.373749562244843,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9887778728817245,
    "IFEval Raw": 0.3691970637907419,
    "IFEval": 36.91970637907419,
    "BBH Raw": 0.6158722731484186,
    "BBH": 45.9816957285586,
    "MATH Lvl 5 Raw": 0.21450151057401814,
    "MATH Lvl 5": 21.450151057401815,
    "GPQA Raw": 0.35822147651006714,
    "GPQA": 14.429530201342287,
    "MUSR Raw": 0.456625,
    "MUSR": 15.778125000000003,
    "MMLU-PRO Raw": 0.4751496010638298,
    "MMLU-PRO": 41.68328900709219,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "allknowingroger/Yislerp-34B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yislerp2-34B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yislerp2-34B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yislerp2-34B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yislerp2-34B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yislerp2-34B",
    "Model sha": "3147cf866736b786347928b655c887e8b9c07bfc",
    "Average ‚¨ÜÔ∏è": 30.421276990470318,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.035957880414699,
    "IFEval Raw": 0.39994658616914236,
    "IFEval": 39.994658616914236,
    "BBH Raw": 0.6245771970170245,
    "BBH": 47.20230580445542,
    "MATH Lvl 5 Raw": 0.22885196374622357,
    "MATH Lvl 5": 22.885196374622357,
    "GPQA Raw": 0.3640939597315436,
    "GPQA": 15.212527964205815,
    "MUSR Raw": 0.45296875,
    "MUSR": 15.854427083333327,
    "MMLU-PRO Raw": 0.472406914893617,
    "MMLU-PRO": 41.37854609929077,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "allknowingroger/Yislerp2-34B (Merge)"
  },
  {
    "eval_name": "allknowingroger_Yunconglong-13B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/Yunconglong-13B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/Yunconglong-13B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__Yunconglong-13B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/Yunconglong-13B-slerp",
    "Model sha": "dead687b7342d875bd8ac73bfcd34b88a2e5564c",
    "Average ‚¨ÜÔ∏è": 19.612692297466157,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7996314840749801,
    "IFEval Raw": 0.42417673993891764,
    "IFEval": 42.41767399389176,
    "BBH Raw": 0.5165807158493828,
    "BBH": 32.14072892807635,
    "MATH Lvl 5 Raw": 0.055135951661631426,
    "MATH Lvl 5": 5.513595166163142,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.4160729166666666,
    "MUSR": 10.842447916666663,
    "MMLU-PRO Raw": 0.30360704787234044,
    "MMLU-PRO": 22.623005319148938,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/Yunconglong-13B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_limyClown-7B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/limyClown-7B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/limyClown-7B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__limyClown-7B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/limyClown-7B-slerp",
    "Model sha": "732a1ed0c2c7007297ad9d9797793073825f65ca",
    "Average ‚¨ÜÔ∏è": 19.70388869479609,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.610050396647597,
    "IFEval Raw": 0.4017451473202215,
    "IFEval": 40.174514732022146,
    "BBH Raw": 0.5147517317055973,
    "BBH": 31.9314661071385,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.4293125,
    "MUSR": 12.464062500000004,
    "MMLU-PRO Raw": 0.30377327127659576,
    "MMLU-PRO": 22.641474586288414,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-23",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "allknowingroger/limyClown-7B-slerp (Merge)"
  },
  {
    "eval_name": "allknowingroger_llama3-Jallabi-40B-s_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/llama3-Jallabi-40B-s\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/llama3-Jallabi-40B-s</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__llama3-Jallabi-40B-s-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/llama3-Jallabi-40B-s",
    "Model sha": "a86d8cc3530fb466245b2cac55f25c28d0bd8c22",
    "Average ‚¨ÜÔ∏è": 5.029701636906855,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 18,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9795747397987066,
    "IFEval Raw": 0.19206815693471102,
    "IFEval": 19.2068156934711,
    "BBH Raw": 0.32522424198526295,
    "BBH": 5.957911562958214,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23741610738255034,
    "GPQA": 0.0,
    "MUSR Raw": 0.37495833333333334,
    "MUSR": 4.036458333333333,
    "MMLU-PRO Raw": 0.10879321808510638,
    "MMLU-PRO": 0.9770242316784857,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "allknowingroger/llama3-Jallabi-40B-s (Merge)"
  },
  {
    "eval_name": "allknowingroger_llama3AnFeng-40B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allknowingroger/llama3AnFeng-40B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allknowingroger/llama3AnFeng-40B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allknowingroger__llama3AnFeng-40B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allknowingroger/llama3AnFeng-40B",
    "Model sha": "5995441962287970ffc98ad9b292e14420bf49ca",
    "Average ‚¨ÜÔ∏è": 9.237994378100716,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 39,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 4.063468101255612,
    "IFEval Raw": 0.17420776872032873,
    "IFEval": 17.42077687203287,
    "BBH Raw": 0.3794080447660335,
    "BBH": 12.476996185725282,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.39399999999999996,
    "MUSR": 7.149999999999999,
    "MMLU-PRO Raw": 0.1979720744680851,
    "MMLU-PRO": 10.885786052009454,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-13",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "allknowingroger/llama3AnFeng-40B (Merge)"
  },
  {
    "eval_name": "allura-org_MS-Meadowlark-22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allura-org/MS-Meadowlark-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allura-org/MS-Meadowlark-22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allura-org__MS-Meadowlark-22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allura-org/MS-Meadowlark-22B",
    "Model sha": "6eb2f6bee66dbffa1b17397e75a7380ed4f9d0ac",
    "Average ‚¨ÜÔ∏è": 26.393029865170902,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.8959015837971753,
    "IFEval Raw": 0.669698621878837,
    "IFEval": 66.9698621878837,
    "BBH Raw": 0.5162576933217772,
    "BBH": 30.29658044666958,
    "MATH Lvl 5 Raw": 0.14123867069486404,
    "MATH Lvl 5": 14.123867069486403,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.3842604166666667,
    "MUSR": 5.532552083333333,
    "MMLU-PRO Raw": 0.38231382978723405,
    "MMLU-PRO": 31.36820330969267,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "allura-org/MS-Meadowlark-22B (Merge)"
  },
  {
    "eval_name": "allura-org_MoE-Girl-1BA-7BT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "OlmoeForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/allura-org/MoE-Girl-1BA-7BT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">allura-org/MoE-Girl-1BA-7BT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/allura-org__MoE-Girl-1BA-7BT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "allura-org/MoE-Girl-1BA-7BT",
    "Model sha": "ecfac73ab9e7f2ee006d6a2ad9c8e86a85deab2b",
    "Average ‚¨ÜÔ∏è": 6.39021099096268,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.2011546471020527,
    "IFEval Raw": 0.27050337548814923,
    "IFEval": 27.050337548814923,
    "BBH Raw": 0.3139175363262408,
    "BBH": 4.8423440285204995,
    "MATH Lvl 5 Raw": 0.014350453172205437,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.34355208333333337,
    "MUSR": 1.47734375,
    "MMLU-PRO Raw": 0.12175864361702128,
    "MMLU-PRO": 2.4176270685579193,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "allenai/OLMoE-1B-7B-0924"
  },
  {
    "eval_name": "aloobun_Meta-Llama-3-7B-28Layers_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/aloobun/Meta-Llama-3-7B-28Layers\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">aloobun/Meta-Llama-3-7B-28Layers</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/aloobun__Meta-Llama-3-7B-28Layers-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "aloobun/Meta-Llama-3-7B-28Layers",
    "Model sha": "9822e6b8d4de0c0f2964d299f6fcef72385a0341",
    "Average ‚¨ÜÔ∏è": 13.136516078457651,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.809358294835876,
    "IFEval Raw": 0.19636453498938372,
    "IFEval": 19.636453498938373,
    "BBH Raw": 0.4437497014253391,
    "BBH": 22.096530251343513,
    "MATH Lvl 5 Raw": 0.013595166163141994,
    "MATH Lvl 5": 1.3595166163141994,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.35892708333333334,
    "MUSR": 5.7992187500000005,
    "MMLU-PRO Raw": 0.3159906914893617,
    "MMLU-PRO": 23.99896572104019,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-10",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "aloobun/Meta-Llama-3-7B-28Layers (Merge)"
  },
  {
    "eval_name": "alpindale_WizardLM-2-8x22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/alpindale/WizardLM-2-8x22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">alpindale/WizardLM-2-8x22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/alpindale__WizardLM-2-8x22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "alpindale/WizardLM-2-8x22B",
    "Model sha": "087834da175523cffd66a7e19583725e798c1b4f",
    "Average ‚¨ÜÔ∏è": 32.98352313683298,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 388,
    "#Params (B)": 140,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 93.30522166371864,
    "IFEval Raw": 0.5272166739805937,
    "IFEval": 52.72166739805937,
    "BBH Raw": 0.6377307938917097,
    "BBH": 48.57616817936264,
    "MATH Lvl 5 Raw": 0.24546827794561935,
    "MATH Lvl 5": 24.546827794561935,
    "GPQA Raw": 0.38171140939597314,
    "GPQA": 17.561521252796418,
    "MUSR Raw": 0.4387083333333333,
    "MUSR": 14.538541666666667,
    "MMLU-PRO Raw": 0.45960771276595747,
    "MMLU-PRO": 39.95641252955083,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-16",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "alpindale/WizardLM-2-8x22B"
  },
  {
    "eval_name": "alpindale_magnum-72b-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/alpindale/magnum-72b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">alpindale/magnum-72b-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/alpindale__magnum-72b-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "alpindale/magnum-72b-v1",
    "Model sha": "fef27e0f235ae8858b84b765db773a2a954110dd",
    "Average ‚¨ÜÔ∏è": 42.57658747468742,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 160,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.515122705028329,
    "IFEval Raw": 0.7606484128778308,
    "IFEval": 76.06484128778308,
    "BBH Raw": 0.6982215794373214,
    "BBH": 57.653184855142705,
    "MATH Lvl 5 Raw": 0.37688821752265866,
    "MATH Lvl 5": 37.688821752265866,
    "GPQA Raw": 0.39093959731543626,
    "GPQA": 18.791946308724835,
    "MUSR Raw": 0.4489375,
    "MUSR": 15.617187499999998,
    "MMLU-PRO Raw": 0.5467918882978723,
    "MMLU-PRO": 49.64354314420804,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-17",
    "Submission Date": "2024-07-25",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "altomek_YiSM-34B-0rn_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/altomek/YiSM-34B-0rn\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">altomek/YiSM-34B-0rn</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/altomek__YiSM-34B-0rn-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "altomek/YiSM-34B-0rn",
    "Model sha": "7a481c67cbdd5c846d6aaab5ef9f1eebfad812c2",
    "Average ‚¨ÜÔ∏è": 30.474248057889294,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9606241441622823,
    "IFEval Raw": 0.428373382624769,
    "IFEval": 42.83733826247689,
    "BBH Raw": 0.6140009573868866,
    "BBH": 45.38292724900714,
    "MATH Lvl 5 Raw": 0.2258308157099698,
    "MATH Lvl 5": 22.58308157099698,
    "GPQA Raw": 0.3716442953020134,
    "GPQA": 16.21923937360179,
    "MUSR Raw": 0.445,
    "MUSR": 14.758333333333331,
    "MMLU-PRO Raw": 0.4695811170212766,
    "MMLU-PRO": 41.06456855791962,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "altomek/YiSM-34B-0rn (Merge)"
  },
  {
    "eval_name": "amazon_MegaBeam-Mistral-7B-300k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/amazon/MegaBeam-Mistral-7B-300k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">amazon/MegaBeam-Mistral-7B-300k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/amazon__MegaBeam-Mistral-7B-300k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "amazon/MegaBeam-Mistral-7B-300k",
    "Model sha": "42572e5c9a0747b19af5c5c9962d122622f32295",
    "Average ‚¨ÜÔ∏è": 17.072822971393897,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6496099383682525,
    "IFEval Raw": 0.520347123410329,
    "IFEval": 52.0347123410329,
    "BBH Raw": 0.4227731731112974,
    "BBH": 19.291805959591997,
    "MATH Lvl 5 Raw": 0.02416918429003021,
    "MATH Lvl 5": 2.416918429003021,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.39799999999999996,
    "MUSR": 8.350000000000003,
    "MMLU-PRO Raw": 0.2549035904255319,
    "MMLU-PRO": 17.211510047281322,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-13",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "amazon/MegaBeam-Mistral-7B-300k"
  },
  {
    "eval_name": "amd_AMD-Llama-135m_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/amd/AMD-Llama-135m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">amd/AMD-Llama-135m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/amd__AMD-Llama-135m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "amd/AMD-Llama-135m",
    "Model sha": "8f9c39b5ed86d422ab332ed1ecf042fdaeb57903",
    "Average ‚¨ÜÔ∏è": 4.759627159992882,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 110,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.12871917576638336,
    "IFEval Raw": 0.18422452426229072,
    "IFEval": 18.42245242622907,
    "BBH Raw": 0.2973931917569524,
    "BBH": 2.4854950529752244,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.37796874999999996,
    "MUSR": 4.912760416666667,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-19",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "amd/AMD-Llama-135m"
  },
  {
    "eval_name": "amd_AMD-Llama-135m_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/amd/AMD-Llama-135m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">amd/AMD-Llama-135m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/amd__AMD-Llama-135m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "amd/AMD-Llama-135m",
    "Model sha": "8f9c39b5ed86d422ab332ed1ecf042fdaeb57903",
    "Average ‚¨ÜÔ∏è": 5.191212208507016,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 110,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5257072833160255,
    "IFEval Raw": 0.19184319826948054,
    "IFEval": 19.18431982694805,
    "BBH Raw": 0.29694449748780255,
    "BBH": 2.537952680477511,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.38457291666666665,
    "MUSR": 5.904947916666668,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-19",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "amd/AMD-Llama-135m"
  },
  {
    "eval_name": "anakin87_gemma-2b-orpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anakin87/gemma-2b-orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anakin87/gemma-2b-orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anakin87__gemma-2b-orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anakin87/gemma-2b-orpo",
    "Model sha": "bf6bfe30c31c18620767ad60d0bff89343804230",
    "Average ‚¨ÜÔ∏è": 7.184001294083682,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7899265655204333,
    "IFEval Raw": 0.24779695651981187,
    "IFEval": 24.779695651981186,
    "BBH Raw": 0.34261709435617754,
    "BBH": 7.94944502776896,
    "MATH Lvl 5 Raw": 0.012839879154078549,
    "MATH Lvl 5": 1.2839879154078548,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.37276041666666665,
    "MUSR": 4.128385416666667,
    "MMLU-PRO Raw": 0.1305684840425532,
    "MMLU-PRO": 3.3964982269503543,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-24",
    "Submission Date": "2024-07-06",
    "Generation": 1,
    "Base Model": "google/gemma-2b"
  },
  {
    "eval_name": "anthracite-org_magnum-v1-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v1-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v1-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v1-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v1-72b",
    "Model sha": "f8f85021bace7e8250ed8559c5b78b8b34f0c4cc",
    "Average ‚¨ÜÔ∏è": 42.61044779777647,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 160,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.891056208981025,
    "IFEval Raw": 0.7606484128778308,
    "IFEval": 76.06484128778308,
    "BBH Raw": 0.6982215794373214,
    "BBH": 57.653184855142705,
    "MATH Lvl 5 Raw": 0.37688821752265866,
    "MATH Lvl 5": 37.688821752265866,
    "GPQA Raw": 0.39093959731543626,
    "GPQA": 18.791946308724835,
    "MUSR Raw": 0.4489375,
    "MUSR": 15.617187499999998,
    "MMLU-PRO Raw": 0.5486203457446809,
    "MMLU-PRO": 49.84670508274232,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-17",
    "Submission Date": "2024-09-21",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "anthracite-org_magnum-v2-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v2-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v2-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v2-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v2-12b",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 18.695116628816773,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 77,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6488353878888227,
    "IFEval Raw": 0.376166349729828,
    "IFEval": 37.6166349729828,
    "BBH Raw": 0.5020864013200114,
    "BBH": 28.785551595365874,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.41790625,
    "MUSR": 11.371614583333335,
    "MMLU-PRO Raw": 0.31673869680851063,
    "MMLU-PRO": 24.082077423167846,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-09-05",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "anthracite-org_magnum-v2-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v2-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v2-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v2-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v2-72b",
    "Model sha": "c9c5826ef42b9fcc8a8e1079be574481cf0b6cc6",
    "Average ‚¨ÜÔ∏è": 41.556286164202575,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.134216855124471,
    "IFEval Raw": 0.7560273407891063,
    "IFEval": 75.60273407891063,
    "BBH Raw": 0.7005076514129516,
    "BBH": 57.85470432085098,
    "MATH Lvl 5 Raw": 0.34063444108761337,
    "MATH Lvl 5": 34.06344410876134,
    "GPQA Raw": 0.3859060402684564,
    "GPQA": 18.120805369127517,
    "MUSR Raw": 0.4371875,
    "MUSR": 14.181770833333331,
    "MMLU-PRO Raw": 0.5456283244680851,
    "MMLU-PRO": 49.51425827423168,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-09-05",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "anthracite-org_magnum-v2.5-12b-kto_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v2.5-12b-kto\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v2.5-12b-kto</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v2.5-12b-kto-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v2.5-12b-kto",
    "Model sha": "aee0374e5a43e950c9977b0004dede1c57be2999",
    "Average ‚¨ÜÔ∏è": 18.882085055018837,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 40,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6090630508881307,
    "IFEval Raw": 0.3865576669902525,
    "IFEval": 38.65576669902525,
    "BBH Raw": 0.5076961186254344,
    "BBH": 29.625059445981027,
    "MATH Lvl 5 Raw": 0.04607250755287009,
    "MATH Lvl 5": 4.607250755287009,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.40863541666666664,
    "MUSR": 9.979427083333336,
    "MMLU-PRO Raw": 0.3214760638297872,
    "MMLU-PRO": 24.608451536643024,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-08-29",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "anthracite-org_magnum-v3-27b-kto_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v3-27b-kto\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v3-27b-kto</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v3-27b-kto-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v3-27b-kto",
    "Model sha": "96fbb750b3150e5fe9d6d2fcf757f49310d99a43",
    "Average ‚¨ÜÔ∏è": 29.097905798270787,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.93753396148653,
    "IFEval Raw": 0.5674831668860845,
    "IFEval": 56.74831668860845,
    "BBH Raw": 0.586040577894583,
    "BBH": 41.1601029248443,
    "MATH Lvl 5 Raw": 0.16691842900302117,
    "MATH Lvl 5": 16.691842900302117,
    "GPQA Raw": 0.35570469798657717,
    "GPQA": 14.093959731543624,
    "MUSR Raw": 0.38546874999999997,
    "MUSR": 9.916927083333329,
    "MMLU-PRO Raw": 0.42378656914893614,
    "MMLU-PRO": 35.976285460992905,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-06",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "anthracite-org/magnum-v3-27b-kto (Merge)"
  },
  {
    "eval_name": "anthracite-org_magnum-v3-34b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v3-34b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v3-34b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v3-34b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v3-34b",
    "Model sha": "3bcd8c3dbb93021a5ce22203c690a1a084cafb73",
    "Average ‚¨ÜÔ∏è": 29.66608133452966,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.1893095570763865,
    "IFEval Raw": 0.5115294086357531,
    "IFEval": 51.15294086357531,
    "BBH Raw": 0.6087828692085228,
    "BBH": 44.32790341462959,
    "MATH Lvl 5 Raw": 0.19486404833836857,
    "MATH Lvl 5": 19.486404833836858,
    "GPQA Raw": 0.36073825503355705,
    "GPQA": 14.76510067114094,
    "MUSR Raw": 0.3872395833333333,
    "MUSR": 6.571614583333336,
    "MMLU-PRO Raw": 0.47523271276595747,
    "MMLU-PRO": 41.692523640661946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-09-18",
    "Generation": 0,
    "Base Model": "anthracite-org/magnum-v3-34b"
  },
  {
    "eval_name": "anthracite-org_magnum-v3-9b-chatml_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v3-9b-chatml\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v3-9b-chatml</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v3-9b-chatml-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v3-9b-chatml",
    "Model sha": "96c2d023c56ef73be095ffbae8cedd7243ebca84",
    "Average ‚¨ÜÔ∏è": 19.415999551544644,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.889965304987409,
    "IFEval Raw": 0.12747066671985885,
    "IFEval": 12.747066671985886,
    "BBH Raw": 0.5427688488887096,
    "BBH": 35.31787541238543,
    "MATH Lvl 5 Raw": 0.06419939577039274,
    "MATH Lvl 5": 6.419939577039274,
    "GPQA Raw": 0.34563758389261745,
    "GPQA": 12.751677852348994,
    "MUSR Raw": 0.4432291666666666,
    "MUSR": 13.236979166666666,
    "MMLU-PRO Raw": 0.4242021276595745,
    "MMLU-PRO": 36.022458628841605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "IntervitensInc/gemma-2-9b-chatml"
  },
  {
    "eval_name": "anthracite-org_magnum-v3-9b-customgemma2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v3-9b-customgemma2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v3-9b-customgemma2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v3-9b-customgemma2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v3-9b-customgemma2",
    "Model sha": "9a7cd3d47434bed2bd80e34e45c74e413f8baaa8",
    "Average ‚¨ÜÔ∏è": 19.13732654194111,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.901825017061758,
    "IFEval Raw": 0.1272955757390391,
    "IFEval": 12.729557573903909,
    "BBH Raw": 0.5340136936916174,
    "BBH": 34.11678334094384,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.45646875,
    "MUSR": 15.058593749999998,
    "MMLU-PRO Raw": 0.4204621010638298,
    "MMLU-PRO": 35.606900118203306,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "anthracite-org_magnum-v4-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v4-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v4-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v4-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v4-12b",
    "Model sha": "704f2ccfe662052e415499e56789dd88ec01a113",
    "Average ‚¨ÜÔ∏è": 19.949135830506954,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.699015728978048,
    "IFEval Raw": 0.33929640021808805,
    "IFEval": 33.9296400218088,
    "BBH Raw": 0.5176693046591915,
    "BBH": 30.503902266484772,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40928125,
    "MUSR": 10.360156249999996,
    "MMLU-PRO Raw": 0.3603723404255319,
    "MMLU-PRO": 28.930260047281326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "anthracite-org/magnum-v4-12b"
  },
  {
    "eval_name": "anthracite-org_magnum-v4-22b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v4-22b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v4-22b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v4-22b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v4-22b",
    "Model sha": "e5239e71d2628269b453a832de98c1ecb79d2557",
    "Average ‚¨ÜÔ∏è": 27.715900428010542,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.650290035734293,
    "IFEval Raw": 0.5628620947973599,
    "IFEval": 56.28620947973599,
    "BBH Raw": 0.548612004937422,
    "BBH": 35.549148532773465,
    "MATH Lvl 5 Raw": 0.19184290030211482,
    "MATH Lvl 5": 19.184290030211482,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.44078124999999996,
    "MUSR": 13.430989583333334,
    "MMLU-PRO Raw": 0.3829787234042553,
    "MMLU-PRO": 31.44208037825059,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "anthracite-org/magnum-v4-22b"
  },
  {
    "eval_name": "anthracite-org_magnum-v4-27b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v4-27b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v4-27b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v4-27b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v4-27b",
    "Model sha": "50a14716bdeb6a9376b9377df31ab1497864f3f9",
    "Average ‚¨ÜÔ∏è": 26.33088900109563,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.736353893566187,
    "IFEval Raw": 0.34541682735142754,
    "IFEval": 34.54168273514276,
    "BBH Raw": 0.5867298109891389,
    "BBH": 40.96038433350091,
    "MATH Lvl 5 Raw": 0.16163141993957703,
    "MATH Lvl 5": 16.1631419939577,
    "GPQA Raw": 0.3699664429530201,
    "GPQA": 15.99552572706935,
    "MUSR Raw": 0.4379895833333333,
    "MUSR": 12.815364583333334,
    "MMLU-PRO Raw": 0.43758311170212766,
    "MMLU-PRO": 37.50923463356973,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "anthracite-org/magnum-v4-27b"
  },
  {
    "eval_name": "anthracite-org_magnum-v4-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/anthracite-org/magnum-v4-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">anthracite-org/magnum-v4-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/anthracite-org__magnum-v4-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "anthracite-org/magnum-v4-9b",
    "Model sha": "e9db6cb80f02ca2e2db4538ef59f7a30f69a849d",
    "Average ‚¨ÜÔ∏è": 23.773818388464395,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.556326151061109,
    "IFEval Raw": 0.3502628581053826,
    "IFEval": 35.02628581053826,
    "BBH Raw": 0.5336423991931557,
    "BBH": 33.27040443647636,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.34731543624161076,
    "GPQA": 12.975391498881436,
    "MUSR Raw": 0.45157291666666666,
    "MUSR": 15.646614583333331,
    "MMLU-PRO Raw": 0.3952792553191489,
    "MMLU-PRO": 32.8088061465721,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-23",
    "Generation": 0,
    "Base Model": "anthracite-org/magnum-v4-9b"
  },
  {
    "eval_name": "apple_DCLM-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OpenLMModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/apple/DCLM-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">apple/DCLM-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/apple__DCLM-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "apple/DCLM-7B",
    "Model sha": "c85bfa168f999ce27e954808bc005a2748fda5c5",
    "Average ‚¨ÜÔ∏è": 13.986977121551305,
    "Hub License": "apple-ascl",
    "Hub ‚ù§Ô∏è": 824,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6299555515092122,
    "IFEval Raw": 0.21727239280664196,
    "IFEval": 21.727239280664193,
    "BBH Raw": 0.42321423668184166,
    "BBH": 19.760934974772244,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.3920729166666667,
    "MUSR": 7.309114583333334,
    "MMLU-PRO Raw": 0.3110871010638298,
    "MMLU-PRO": 23.45412234042553,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-11",
    "Submission Date": "2024-08-16",
    "Generation": 0,
    "Base Model": "apple/DCLM-7B"
  },
  {
    "eval_name": "arcee-ai_Arcee-Nova_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/Arcee-Nova\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/Arcee-Nova</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__Arcee-Nova-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/Arcee-Nova",
    "Model sha": "ec3bfe88b83f81481daa04b6789c1e0d32827dc5",
    "Average ‚¨ÜÔ∏è": 43.90233522645246,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 40,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 11.493293564750758,
    "IFEval Raw": 0.7907485471881275,
    "IFEval": 79.07485471881274,
    "BBH Raw": 0.694196965855899,
    "BBH": 56.74098753952074,
    "MATH Lvl 5 Raw": 0.42900302114803623,
    "MATH Lvl 5": 42.90030211480362,
    "GPQA Raw": 0.3850671140939597,
    "GPQA": 18.008948545861294,
    "MUSR Raw": 0.45616666666666666,
    "MUSR": 17.220833333333328,
    "MMLU-PRO Raw": 0.5452127659574468,
    "MMLU-PRO": 49.46808510638298,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "arcee-ai/Arcee-Nova"
  },
  {
    "eval_name": "arcee-ai_Arcee-Spark_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/Arcee-Spark\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/Arcee-Spark</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__Arcee-Spark-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/Arcee-Spark",
    "Model sha": "3fe368ea5fd32bc4a8d1bcf42510416f7fa28668",
    "Average ‚¨ÜÔ∏è": 25.53645563140378,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 86,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.098532827772252,
    "IFEval Raw": 0.5620874834328471,
    "IFEval": 56.208748343284704,
    "BBH Raw": 0.5489474198567446,
    "BBH": 37.13852245584468,
    "MATH Lvl 5 Raw": 0.12311178247734139,
    "MATH Lvl 5": 12.311178247734139,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.40209374999999997,
    "MUSR": 8.595052083333334,
    "MMLU-PRO Raw": 0.3822307180851064,
    "MMLU-PRO": 31.358968676122927,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "arcee-ai/Arcee-Spark"
  },
  {
    "eval_name": "arcee-ai_Arcee-Spark_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/Arcee-Spark\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/Arcee-Spark</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__Arcee-Spark-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/Arcee-Spark",
    "Model sha": "3fe368ea5fd32bc4a8d1bcf42510416f7fa28668",
    "Average ‚¨ÜÔ∏è": 25.443168747377587,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 86,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1360401936497302,
    "IFEval Raw": 0.571829412625168,
    "IFEval": 57.18294126251679,
    "BBH Raw": 0.5480864114714127,
    "BBH": 36.92439043586489,
    "MATH Lvl 5 Raw": 0.11404833836858004,
    "MATH Lvl 5": 11.404833836858003,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4007604166666667,
    "MUSR": 8.395052083333335,
    "MMLU-PRO Raw": 0.38131648936170215,
    "MMLU-PRO": 31.257387706855795,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "arcee-ai/Arcee-Spark"
  },
  {
    "eval_name": "arcee-ai_Llama-3.1-SuperNova-Lite_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/Llama-3.1-SuperNova-Lite\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/Llama-3.1-SuperNova-Lite</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__Llama-3.1-SuperNova-Lite-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/Llama-3.1-SuperNova-Lite",
    "Model sha": "76246ca4448c1a11787daee0958b60ab27f17774",
    "Average ‚¨ÜÔ∏è": 30.04240657864892,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 173,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8559933850118808,
    "IFEval Raw": 0.8017393848322452,
    "IFEval": 80.1739384832245,
    "BBH Raw": 0.5151992115104819,
    "BBH": 31.572340212980667,
    "MATH Lvl 5 Raw": 0.17371601208459214,
    "MATH Lvl 5": 17.371601208459214,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.41632291666666665,
    "MUSR": 11.673697916666663,
    "MMLU-PRO Raw": 0.3877160904255319,
    "MMLU-PRO": 31.96845449172577,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-17",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "arcee-ai_Llama-Spark_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/Llama-Spark\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/Llama-Spark</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__Llama-Spark-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/Llama-Spark",
    "Model sha": "6d74a617fbb17a1ada08528f2673c89f84fb062e",
    "Average ‚¨ÜÔ∏è": 24.922433276152727,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 24,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8307141017457323,
    "IFEval Raw": 0.7910732412221794,
    "IFEval": 79.10732412221793,
    "BBH Raw": 0.5053504145749979,
    "BBH": 29.770253700208638,
    "MATH Lvl 5 Raw": 0.012084592145015107,
    "MATH Lvl 5": 1.2084592145015107,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.35933333333333334,
    "MUSR": 2.6166666666666667,
    "MMLU-PRO Raw": 0.3720910904255319,
    "MMLU-PRO": 30.232343380614658,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-26",
    "Submission Date": "2024-08-08",
    "Generation": 0,
    "Base Model": "arcee-ai/Llama-Spark"
  },
  {
    "eval_name": "arcee-ai_SuperNova-Medius_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/SuperNova-Medius\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/SuperNova-Medius</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__SuperNova-Medius-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/SuperNova-Medius",
    "Model sha": "e34fafcac2801be1ae5c7eb744e191a08119f2af",
    "Average ‚¨ÜÔ∏è": 33.89247105296946,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 179,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.867811667451773,
    "IFEval Raw": 0.7183584001560305,
    "IFEval": 71.83584001560305,
    "BBH Raw": 0.6377284463115707,
    "BBH": 48.00501462716327,
    "MATH Lvl 5 Raw": 0.15332326283987915,
    "MATH Lvl 5": 15.332326283987916,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.4232708333333333,
    "MUSR": 12.275520833333337,
    "MMLU-PRO Raw": 0.5034906914893617,
    "MMLU-PRO": 44.83229905437352,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "arcee-ai/SuperNova-Medius (Merge)"
  },
  {
    "eval_name": "arcee-ai_raspberry-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/arcee-ai/raspberry-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">arcee-ai/raspberry-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/arcee-ai__raspberry-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "arcee-ai/raspberry-3B",
    "Model sha": "66bf1346c060bbfe1f1b98cd22e7a26ada69cf70",
    "Average ‚¨ÜÔ∏è": 15.538003121443232,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0365265177350849,
    "IFEval Raw": 0.31541642840995227,
    "IFEval": 31.541642840995227,
    "BBH Raw": 0.42689280188827033,
    "BBH": 19.528234400992485,
    "MATH Lvl 5 Raw": 0.08459214501510574,
    "MATH Lvl 5": 8.459214501510575,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.41232291666666665,
    "MUSR": 9.407031250000001,
    "MMLU-PRO Raw": 0.285405585106383,
    "MMLU-PRO": 20.600620567375884,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-05",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "argilla_notus-7b-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/argilla/notus-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">argilla/notus-7b-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/argilla__notus-7b-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "argilla/notus-7b-v1",
    "Model sha": "30172203a2d41cb487bf7e2b92a821080783b2c9",
    "Average ‚¨ÜÔ∏è": 18.411321188854632,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 122,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.667908131155709,
    "IFEval Raw": 0.508207112683236,
    "IFEval": 50.820711268323606,
    "BBH Raw": 0.4511857407381495,
    "BBH": 22.74711196116141,
    "MATH Lvl 5 Raw": 0.027945619335347435,
    "MATH Lvl 5": 2.7945619335347436,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.33641666666666664,
    "MUSR": 6.58541666666667,
    "MMLU-PRO Raw": 0.3003656914893617,
    "MMLU-PRO": 22.26285460992908,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-16",
    "Submission Date": "2024-06-27",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "argilla_notux-8x7b-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/argilla/notux-8x7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">argilla/notux-8x7b-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/argilla__notux-8x7b-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "argilla/notux-8x7b-v1",
    "Model sha": "0b29f9afcbae2ab4c5085638d8f5a7f6d44c6b17",
    "Average ‚¨ÜÔ∏è": 24.42823109564674,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 165,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 21.390844571977862,
    "IFEval Raw": 0.5422290633297429,
    "IFEval": 54.22290633297429,
    "BBH Raw": 0.5363304164516353,
    "BBH": 34.75806168290175,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.41759375,
    "MUSR": 10.532552083333334,
    "MMLU-PRO Raw": 0.3660239361702128,
    "MMLU-PRO": 29.558215130023648,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-12",
    "Submission Date": "2024-06-12",
    "Generation": 2,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "argilla-warehouse_Llama-3.1-8B-MagPie-Ultra_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/argilla-warehouse/Llama-3.1-8B-MagPie-Ultra\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">argilla-warehouse/Llama-3.1-8B-MagPie-Ultra</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/argilla-warehouse__Llama-3.1-8B-MagPie-Ultra-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "argilla-warehouse/Llama-3.1-8B-MagPie-Ultra",
    "Model sha": "1e12f20ca5db84f65a6db793a65100433aac0ac6",
    "Average ‚¨ÜÔ∏è": 19.45875971727332,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9656770519916641,
    "IFEval Raw": 0.5756514935925566,
    "IFEval": 57.565149359255656,
    "BBH Raw": 0.46196134634468616,
    "BBH": 23.516310364827646,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.35425,
    "MUSR": 4.2479166666666694,
    "MMLU-PRO Raw": 0.31441156914893614,
    "MMLU-PRO": 23.823507683215126,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.1-8B"
  },
  {
    "eval_name": "athirdpath_Llama-3.1-Instruct_NSFW-pretrained_e1-plus_reddit_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/athirdpath/Llama-3.1-Instruct_NSFW-pretrained_e1-plus_reddit\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">athirdpath/Llama-3.1-Instruct_NSFW-pretrained_e1-plus_reddit</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/athirdpath__Llama-3.1-Instruct_NSFW-pretrained_e1-plus_reddit-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "athirdpath/Llama-3.1-Instruct_NSFW-pretrained_e1-plus_reddit",
    "Model sha": "42eaee4de10302fec7c0c20ad96f527cfb0b10a3",
    "Average ‚¨ÜÔ∏è": 20.905594645477773,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8865273057959672,
    "IFEval Raw": 0.4521037513796726,
    "IFEval": 45.21037513796726,
    "BBH Raw": 0.4939066588253951,
    "BBH": 28.015909017593412,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.3863958333333333,
    "MUSR": 8.299479166666666,
    "MMLU-PRO Raw": 0.3564660904255319,
    "MMLU-PRO": 28.496232269503547,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-08-01",
    "Generation": 1,
    "Base Model": "athirdpath/Llama-3.1-Instruct_NSFW-pretrained_e1"
  },
  {
    "eval_name": "automerger_YamshadowExperiment28-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/automerger/YamshadowExperiment28-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">automerger/YamshadowExperiment28-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/automerger__YamshadowExperiment28-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "automerger/YamshadowExperiment28-7B",
    "Model sha": "76972ed8aacba1fd14f78e6f8d347f087f8b6800",
    "Average ‚¨ÜÔ∏è": 19.896858146403503,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5844240709525,
    "IFEval Raw": 0.4070156074770498,
    "IFEval": 40.701560747704974,
    "BBH Raw": 0.5150030227855061,
    "BBH": 31.980235156677427,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4306145833333333,
    "MUSR": 12.693489583333337,
    "MMLU-PRO Raw": 0.30601728723404253,
    "MMLU-PRO": 22.89080969267139,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-18",
    "Submission Date": "2024-06-29",
    "Generation": 1,
    "Base Model": "automerger/YamshadowExperiment28-7B (Merge)"
  },
  {
    "eval_name": "awnr_Mistral-7B-v0.1-signtensors-1-over-2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/awnr/Mistral-7B-v0.1-signtensors-1-over-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">awnr/Mistral-7B-v0.1-signtensors-1-over-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/awnr__Mistral-7B-v0.1-signtensors-1-over-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "awnr/Mistral-7B-v0.1-signtensors-1-over-2",
    "Model sha": "9575327242f8539eac59b6d788beccf54a6f9414",
    "Average ‚¨ÜÔ∏è": 14.257193911777046,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.567058367715035,
    "IFEval Raw": 0.21792178087474567,
    "IFEval": 21.792178087474564,
    "BBH Raw": 0.4422884892437673,
    "BBH": 22.40015255970937,
    "MATH Lvl 5 Raw": 0.027190332326283987,
    "MATH Lvl 5": 2.719033232628399,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.40060416666666665,
    "MUSR": 8.80885416666667,
    "MMLU-PRO Raw": 0.2999501329787234,
    "MMLU-PRO": 22.21668144208038,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "awnr/Mistral-7B-v0.1-signtensors-1-over-2"
  },
  {
    "eval_name": "awnr_Mistral-7B-v0.1-signtensors-1-over-4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/awnr/Mistral-7B-v0.1-signtensors-1-over-4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">awnr/Mistral-7B-v0.1-signtensors-1-over-4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/awnr__Mistral-7B-v0.1-signtensors-1-over-4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "awnr/Mistral-7B-v0.1-signtensors-1-over-4",
    "Model sha": "b288ab9d8adfd2963a44a7935bb47649f55bcbee",
    "Average ‚¨ÜÔ∏è": 8.709433197510963,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2823301685029582,
    "IFEval Raw": 0.2133007087860211,
    "IFEval": 21.330070878602108,
    "BBH Raw": 0.35070947402846286,
    "BBH": 9.22769372478478,
    "MATH Lvl 5 Raw": 0.022658610271903322,
    "MATH Lvl 5": 2.2658610271903323,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.34603125,
    "MUSR": 2.187239583333333,
    "MMLU-PRO Raw": 0.2310505319148936,
    "MMLU-PRO": 14.561170212765957,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "awnr/Mistral-7B-v0.1-signtensors-1-over-4"
  },
  {
    "eval_name": "awnr_Mistral-7B-v0.1-signtensors-3-over-8_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/awnr/Mistral-7B-v0.1-signtensors-3-over-8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">awnr/Mistral-7B-v0.1-signtensors-3-over-8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/awnr__Mistral-7B-v0.1-signtensors-3-over-8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "awnr/Mistral-7B-v0.1-signtensors-3-over-8",
    "Model sha": "fa368f705ace05da2fef25c030fe740cf1fef176",
    "Average ‚¨ÜÔ∏è": 13.725352082619835,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2709913108512711,
    "IFEval Raw": 0.23942915907569692,
    "IFEval": 23.94291590756969,
    "BBH Raw": 0.4299940969601492,
    "BBH": 20.435230589690022,
    "MATH Lvl 5 Raw": 0.027945619335347432,
    "MATH Lvl 5": 2.794561933534743,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.38175000000000003,
    "MUSR": 5.785416666666667,
    "MMLU-PRO Raw": 0.30011635638297873,
    "MMLU-PRO": 22.23515070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "awnr/Mistral-7B-v0.1-signtensors-3-over-8"
  },
  {
    "eval_name": "awnr_Mistral-7B-v0.1-signtensors-5-over-16_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/awnr/Mistral-7B-v0.1-signtensors-5-over-16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">awnr/Mistral-7B-v0.1-signtensors-5-over-16</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/awnr__Mistral-7B-v0.1-signtensors-5-over-16-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "awnr/Mistral-7B-v0.1-signtensors-5-over-16",
    "Model sha": "5ea13b3d0723237889e1512bc70dae72f71884d1",
    "Average ‚¨ÜÔ∏è": 12.15864791851745,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6497345585240509,
    "IFEval Raw": 0.21182684166899385,
    "IFEval": 21.18268416689938,
    "BBH Raw": 0.4124151161773006,
    "BBH": 17.543031293477835,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.3686041666666667,
    "MUSR": 6.142187500000003,
    "MMLU-PRO Raw": 0.29579454787234044,
    "MMLU-PRO": 21.75494976359338,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "awnr/Mistral-7B-v0.1-signtensors-5-over-16"
  },
  {
    "eval_name": "awnr_Mistral-7B-v0.1-signtensors-7-over-16_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/awnr/Mistral-7B-v0.1-signtensors-7-over-16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">awnr/Mistral-7B-v0.1-signtensors-7-over-16</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/awnr__Mistral-7B-v0.1-signtensors-7-over-16-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "awnr/Mistral-7B-v0.1-signtensors-7-over-16",
    "Model sha": "0e1f2cb0a81c38fc6c567d9c007883ab62fae266",
    "Average ‚¨ÜÔ∏è": 14.14599980116308,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.296432659955362,
    "IFEval Raw": 0.22936253584932426,
    "IFEval": 22.936253584932423,
    "BBH Raw": 0.43158208189876196,
    "BBH": 21.040436509874464,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.39520833333333333,
    "MUSR": 7.934375000000003,
    "MMLU-PRO Raw": 0.30302526595744683,
    "MMLU-PRO": 22.558362884160758,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-29",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "awnr/Mistral-7B-v0.1-signtensors-7-over-16"
  },
  {
    "eval_name": "aws-prototyping_MegaBeam-Mistral-7B-512k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">aws-prototyping/MegaBeam-Mistral-7B-512k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/aws-prototyping__MegaBeam-Mistral-7B-512k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "aws-prototyping/MegaBeam-Mistral-7B-512k",
    "Model sha": "3e3b8c4b933650eed81ede7c4395df943d2a0796",
    "Average ‚¨ÜÔ∏è": 17.59506983930647,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 43,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6471879072064601,
    "IFEval Raw": 0.5972586071623293,
    "IFEval": 59.72586071623293,
    "BBH Raw": 0.3662336639946533,
    "BBH": 12.361177501580405,
    "MATH Lvl 5 Raw": 0.029456193353474325,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3993645833333333,
    "MUSR": 8.520572916666667,
    "MMLU-PRO Raw": 0.25889295212765956,
    "MMLU-PRO": 17.65477245862884,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "aws-prototyping/MegaBeam-Mistral-7B-512k"
  },
  {
    "eval_name": "axolotl-ai-co_romulus-mistral-nemo-12b-simpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/axolotl-ai-co/romulus-mistral-nemo-12b-simpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">axolotl-ai-co/romulus-mistral-nemo-12b-simpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/axolotl-ai-co__romulus-mistral-nemo-12b-simpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "axolotl-ai-co/romulus-mistral-nemo-12b-simpo",
    "Model sha": "15fd3ffa46c1ea51aa5d26a1da24214e324d7cf2",
    "Average ‚¨ÜÔ∏è": 23.426338156309612,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.715151652860276,
    "IFEval Raw": 0.607924750772395,
    "IFEval": 60.79247507723951,
    "BBH Raw": 0.5395057669562011,
    "BBH": 34.64240096637378,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.42330208333333336,
    "MUSR": 12.979427083333333,
    "MMLU-PRO Raw": 0.3469082446808511,
    "MMLU-PRO": 27.43424940898345,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-09-21",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "beomi_gemma-mling-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/beomi/gemma-mling-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beomi/gemma-mling-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/beomi__gemma-mling-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "beomi/gemma-mling-7b",
    "Model sha": "3f442e28bd50db6c438ce2a15b3a003532babba0",
    "Average ‚¨ÜÔ∏è": 11.25370440964923,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6435055986333496,
    "IFEval Raw": 0.20290939152559653,
    "IFEval": 20.290939152559652,
    "BBH Raw": 0.40675941947154004,
    "BBH": 17.631391012223656,
    "MATH Lvl 5 Raw": 0.046072507552870096,
    "MATH Lvl 5": 4.6072507552870094,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.37585416666666666,
    "MUSR": 6.848437499999999,
    "MMLU-PRO Raw": 0.2632978723404255,
    "MMLU-PRO": 18.144208037825056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-15",
    "Submission Date": "2024-07-17",
    "Generation": 0,
    "Base Model": "beomi/gemma-mling-7b"
  },
  {
    "eval_name": "beowolx_CodeNinja-1.0-OpenChat-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/beowolx/CodeNinja-1.0-OpenChat-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">beowolx/CodeNinja-1.0-OpenChat-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/beowolx__CodeNinja-1.0-OpenChat-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "beowolx/CodeNinja-1.0-OpenChat-7B",
    "Model sha": "9934c04c767e6ae0f792712a060f02915391d4ec",
    "Average ‚¨ÜÔ∏è": 20.34738938182539,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 105,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6360726008946632,
    "IFEval Raw": 0.5446770125489258,
    "IFEval": 54.467701254892575,
    "BBH Raw": 0.4441338669403703,
    "BBH": 21.713423267203808,
    "MATH Lvl 5 Raw": 0.06042296072507554,
    "MATH Lvl 5": 6.042296072507554,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.42432291666666666,
    "MUSR": 11.540364583333336,
    "MMLU-PRO Raw": 0.3015292553191489,
    "MMLU-PRO": 22.392139479905435,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-20",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "beowolx/CodeNinja-1.0-OpenChat-7B"
  },
  {
    "eval_name": "berkeley-nest_Starling-LM-7B-alpha_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">berkeley-nest/Starling-LM-7B-alpha</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/berkeley-nest__Starling-LM-7B-alpha-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "berkeley-nest/Starling-LM-7B-alpha",
    "Model sha": "1dddf3b95bc1391f6307299eb1c162c194bde9bd",
    "Average ‚¨ÜÔ∏è": 20.826772930450105,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 554,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5516288866920487,
    "IFEval Raw": 0.5480491761858536,
    "IFEval": 54.80491761858535,
    "BBH Raw": 0.4440065261164004,
    "BBH": 21.95402808715926,
    "MATH Lvl 5 Raw": 0.08308157099697885,
    "MATH Lvl 5": 8.308157099697885,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.41201041666666666,
    "MUSR": 9.501302083333334,
    "MMLU-PRO Raw": 0.3171542553191489,
    "MMLU-PRO": 24.128250591016545,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-25",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "berkeley-nest/Starling-LM-7B-alpha"
  },
  {
    "eval_name": "bigcode_starcoder2-15b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Starcoder2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/starcoder2-15b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/starcoder2-15b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigcode__starcoder2-15b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigcode/starcoder2-15b",
    "Model sha": "46d44742909c03ac8cee08eb03fdebce02e193ec",
    "Average ‚¨ÜÔ∏è": 12.551763538463561,
    "Hub License": "bigcode-openrail-m",
    "Hub ‚ù§Ô∏è": 570,
    "#Params (B)": 15,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 35.04454774970807,
    "IFEval Raw": 0.2780223141265177,
    "IFEval": 27.802231412651764,
    "BBH Raw": 0.4447957841230437,
    "BBH": 20.373540752678547,
    "MATH Lvl 5 Raw": 0.060422960725075525,
    "MATH Lvl 5": 6.042296072507552,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.35009375000000004,
    "MUSR": 2.9283854166666674,
    "MMLU-PRO Raw": 0.23528922872340424,
    "MMLU-PRO": 15.032136524822693,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-20",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "bigcode/starcoder2-15b"
  },
  {
    "eval_name": "bigcode_starcoder2-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Starcoder2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/starcoder2-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/starcoder2-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigcode__starcoder2-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigcode/starcoder2-3b",
    "Model sha": "733247c55e3f73af49ce8e9c7949bf14af205928",
    "Average ‚¨ÜÔ∏è": 6.536559509561811,
    "Hub License": "bigcode-openrail-m",
    "Hub ‚ù§Ô∏è": 149,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.446628918326529,
    "IFEval Raw": 0.20370838264693236,
    "IFEval": 20.370838264693234,
    "BBH Raw": 0.35087141384601755,
    "BBH": 8.909299421083569,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.24412751677852348,
    "GPQA": 0.0,
    "MUSR Raw": 0.34345833333333337,
    "MUSR": 1.432291666666666,
    "MMLU-PRO Raw": 0.1636469414893617,
    "MMLU-PRO": 7.0718823877068555,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-29",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "bigcode/starcoder2-3b"
  },
  {
    "eval_name": "bigcode_starcoder2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Starcoder2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigcode/starcoder2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigcode/starcoder2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigcode__starcoder2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigcode/starcoder2-7b",
    "Model sha": "a3d33687b51284b528abeb17830776ffd24892a9",
    "Average ‚¨ÜÔ∏è": 8.255674026026627,
    "Hub License": "bigcode-openrail-m",
    "Hub ‚ù§Ô∏è": 161,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5064013785275613,
    "IFEval Raw": 0.22091938279321088,
    "IFEval": 22.09193827932109,
    "BBH Raw": 0.36609857669123036,
    "BBH": 11.395110106503443,
    "MATH Lvl 5 Raw": 0.028700906344410873,
    "MATH Lvl 5": 2.8700906344410875,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.3793333333333333,
    "MUSR": 5.8166666666666655,
    "MMLU-PRO Raw": 0.16422872340425532,
    "MMLU-PRO": 7.1365248226950335,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-20",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "bigcode/starcoder2-7b"
  },
  {
    "eval_name": "bigscience_bloom-1b1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "BloomForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-1b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-1b1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigscience__bloom-1b1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigscience/bloom-1b1",
    "Model sha": "eb3dd7399312f5f94fd13f41d2f318117d3eb1e4",
    "Average ‚¨ÜÔ∏è": 3.962215291979836,
    "Hub License": "bigscience-bloom-rail-1.0",
    "Hub ‚ù§Ô∏è": 61,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7170213575437042,
    "IFEval Raw": 0.13733781920858879,
    "IFEval": 13.733781920858878,
    "BBH Raw": 0.31072762377370394,
    "BBH": 4.042705269260129,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.36999999999999994,
    "MUSR": 3.416666666666666,
    "MMLU-PRO Raw": 0.1107878989361702,
    "MMLU-PRO": 1.1986554373522447,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-19",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "bigscience/bloom-1b1"
  },
  {
    "eval_name": "bigscience_bloom-1b7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "BloomForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-1b7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-1b7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigscience__bloom-1b7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigscience/bloom-1b7",
    "Model sha": "cc72a88036c2fb937d65efeacc57a0c2ef5d6fe5",
    "Average ‚¨ÜÔ∏è": 3.9712257798358466,
    "Hub License": "bigscience-bloom-rail-1.0",
    "Hub ‚ù§Ô∏è": 117,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8183595987895594,
    "IFEval Raw": 0.10438968603305895,
    "IFEval": 10.438968603305895,
    "BBH Raw": 0.314054919904072,
    "BBH": 4.39745292760164,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.38857291666666666,
    "MUSR": 6.838281250000001,
    "MMLU-PRO Raw": 0.10862699468085106,
    "MMLU-PRO": 0.9585549645390061,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-19",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "bigscience/bloom-1b7"
  },
  {
    "eval_name": "bigscience_bloom-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "BloomForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigscience__bloom-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigscience/bloom-3b",
    "Model sha": "52bc5b43010b4844513826b8be3f78c7344c37d7",
    "Average ‚¨ÜÔ∏è": 4.262012960471914,
    "Hub License": "bigscience-bloom-rail-1.0",
    "Hub ‚ù§Ô∏è": 88,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.996056190661247,
    "IFEval Raw": 0.1270961050013963,
    "IFEval": 12.709610500139629,
    "BBH Raw": 0.3062918592346337,
    "BBH": 3.4200982840077354,
    "MATH Lvl 5 Raw": 0.000755287009063444,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.23993288590604026,
    "GPQA": 0.0,
    "MUSR Raw": 0.3980625,
    "MUSR": 7.891145833333333,
    "MMLU-PRO Raw": 0.11328125,
    "MMLU-PRO": 1.4756944444444438,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-19",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "bigscience/bloom-3b"
  },
  {
    "eval_name": "bigscience_bloom-560m_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "BloomForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-560m\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-560m</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigscience__bloom-560m-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigscience/bloom-560m",
    "Model sha": "ac2ae5fab2ce3f9f40dc79b5ca9f637430d24971",
    "Average ‚¨ÜÔ∏è": 3.4568911318914637,
    "Hub License": "bigscience-bloom-rail-1.0",
    "Hub ‚ù§Ô∏è": 347,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.762716140153453,
    "IFEval Raw": 0.06202431769926019,
    "IFEval": 6.202431769926019,
    "BBH Raw": 0.3025950541549823,
    "BBH": 2.885363608028119,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.4030833333333333,
    "MUSR": 8.185416666666667,
    "MMLU-PRO Raw": 0.11643949468085106,
    "MMLU-PRO": 1.8266105200945615,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-19",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "bigscience/bloom-560m"
  },
  {
    "eval_name": "bigscience_bloom-7b1_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "BloomForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bigscience/bloom-7b1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bigscience/bloom-7b1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bigscience__bloom-7b1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bigscience/bloom-7b1",
    "Model sha": "6232703e399354503377bf59dfbb8397fd569e4a",
    "Average ‚¨ÜÔ∏è": 3.7073934241241133,
    "Hub License": "bigscience-bloom-rail-1.0",
    "Hub ‚ù§Ô∏è": 199,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0057750185999519,
    "IFEval Raw": 0.13221696210499254,
    "IFEval": 13.221696210499253,
    "BBH Raw": 0.3113718529627139,
    "BBH": 4.038808518979752,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.34869791666666666,
    "MUSR": 1.920572916666666,
    "MMLU-PRO Raw": 0.11045545212765957,
    "MMLU-PRO": 1.1617169030732852,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-19",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "bigscience/bloom-7b1"
  },
  {
    "eval_name": "bosonai_Higgs-Llama-3-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bosonai/Higgs-Llama-3-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bosonai/Higgs-Llama-3-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bosonai__Higgs-Llama-3-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bosonai/Higgs-Llama-3-70B",
    "Model sha": "b2c7540768046dfdae7a0cb846a7da6c41d826b1",
    "Average ‚¨ÜÔ∏è": 32.21623382392481,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 214,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.726847026196765,
    "IFEval Raw": 0.5560678998390935,
    "IFEval": 55.60678998390936,
    "BBH Raw": 0.625765879603832,
    "BBH": 45.89740563396065,
    "MATH Lvl 5 Raw": 0.17371601208459217,
    "MATH Lvl 5": 17.371601208459218,
    "GPQA Raw": 0.36661073825503354,
    "GPQA": 15.548098434004473,
    "MUSR Raw": 0.44708333333333333,
    "MUSR": 15.518750000000002,
    "MMLU-PRO Raw": 0.49019281914893614,
    "MMLU-PRO": 43.35475768321512,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-05",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "brgx53_3Bgeneral-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/brgx53/3Bgeneral-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">brgx53/3Bgeneral-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/brgx53__3Bgeneral-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "brgx53/3Bgeneral-ECE-PRYMMAL-Martial",
    "Model sha": "78ee3bde02df349ee7161f9c2a5b36161c294009",
    "Average ‚¨ÜÔ∏è": 23.167894101884695,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6536518309645085,
    "IFEval Raw": 0.32893057088525113,
    "IFEval": 32.89305708852512,
    "BBH Raw": 0.5458008312900208,
    "BBH": 36.673582111407946,
    "MATH Lvl 5 Raw": 0.12462235649546828,
    "MATH Lvl 5": 12.462235649546828,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.43728125,
    "MUSR": 14.426822916666666,
    "MMLU-PRO Raw": 0.3933676861702128,
    "MMLU-PRO": 32.59640957446809,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "brgx53/3Bgeneral-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "brgx53_3Bgeneralv2-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/brgx53/3Bgeneralv2-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">brgx53/3Bgeneralv2-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/brgx53__3Bgeneralv2-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "brgx53/3Bgeneralv2-ECE-PRYMMAL-Martial",
    "Model sha": "8525f801c47b2bce2ca4dad360ce71b2cb6b370b",
    "Average ‚¨ÜÔ∏è": 30.777462062310445,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3411621176700346,
    "IFEval Raw": 0.567708125551315,
    "IFEval": 56.77081255513149,
    "BBH Raw": 0.5607195549186694,
    "BBH": 37.250632564299146,
    "MATH Lvl 5 Raw": 0.3074018126888218,
    "MATH Lvl 5": 30.74018126888218,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.43563541666666666,
    "MUSR": 12.78776041666667,
    "MMLU-PRO Raw": 0.45054853723404253,
    "MMLU-PRO": 38.949837470449175,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "brgx53/3Bgeneralv2-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "brgx53_3Blareneg-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/brgx53/3Blareneg-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">brgx53/3Blareneg-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/brgx53__3Blareneg-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "brgx53/3Blareneg-ECE-PRYMMAL-Martial",
    "Model sha": "abac4757125a66a427fb82751bf171dabaea3458",
    "Average ‚¨ÜÔ∏è": 21.33386000090471,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.807534457534839,
    "IFEval Raw": 0.28763902002242936,
    "IFEval": 28.763902002242936,
    "BBH Raw": 0.535846215598753,
    "BBH": 35.45258577949333,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4428958333333333,
    "MUSR": 15.428645833333329,
    "MMLU-PRO Raw": 0.4015957446808511,
    "MMLU-PRO": 33.51063829787233,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "brgx53/3Blareneg-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "brgx53_3Blarenegv2-ECE-PRYMMAL-Martial_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/brgx53/3Blarenegv2-ECE-PRYMMAL-Martial\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">brgx53/3Blarenegv2-ECE-PRYMMAL-Martial</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/brgx53__3Blarenegv2-ECE-PRYMMAL-Martial-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "brgx53/3Blarenegv2-ECE-PRYMMAL-Martial",
    "Model sha": "304038fc2b2527e31c738f9091206253a0d40f6c",
    "Average ‚¨ÜÔ∏è": 30.752066482286477,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6868959640429771,
    "IFEval Raw": 0.5661843907498769,
    "IFEval": 56.6184390749877,
    "BBH Raw": 0.5607195549186694,
    "BBH": 37.250632564299146,
    "MATH Lvl 5 Raw": 0.3074018126888218,
    "MATH Lvl 5": 30.74018126888218,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.43563541666666666,
    "MUSR": 12.78776041666667,
    "MMLU-PRO Raw": 0.45054853723404253,
    "MMLU-PRO": 38.949837470449175,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "brgx53/3Blarenegv2-ECE-PRYMMAL-Martial (Merge)"
  },
  {
    "eval_name": "bunnycore_Best-Mix-Llama-3.1-8B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Best-Mix-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Best-Mix-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Best-Mix-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Best-Mix-Llama-3.1-8B",
    "Model sha": "4bde0e60ac20d6944b1fbdfb3456efea8ba59ae9",
    "Average ‚¨ÜÔ∏è": 8.624958956811936,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9050025226144224,
    "IFEval Raw": 0.20670598456539757,
    "IFEval": 20.670598456539757,
    "BBH Raw": 0.343178100574048,
    "BBH": 7.255275858385576,
    "MATH Lvl 5 Raw": 0.14425981873111782,
    "MATH Lvl 5": 14.425981873111782,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.2928541666666667,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.15649933510638298,
    "MMLU-PRO": 6.277703900709218,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-10",
    "Generation": 0,
    "Base Model": "bunnycore/Best-Mix-Llama-3.1-8B"
  },
  {
    "eval_name": "bunnycore_HyperLlama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/HyperLlama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/HyperLlama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__HyperLlama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/HyperLlama-3.1-8B",
    "Model sha": "659b18ffaee2c1e8dbe8a9a56a44502325d71696",
    "Average ‚¨ÜÔ∏è": 28.398623238435054,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8945445185384541,
    "IFEval Raw": 0.7883005979689446,
    "IFEval": 78.83005979689446,
    "BBH Raw": 0.5103385292046213,
    "BBH": 29.80665561261375,
    "MATH Lvl 5 Raw": 0.1797583081570997,
    "MATH Lvl 5": 17.97583081570997,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.38292708333333336,
    "MUSR": 7.932552083333335,
    "MMLU-PRO Raw": 0.3783244680851064,
    "MMLU-PRO": 30.92494089834515,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "bunnycore/HyperLlama-3.1-8B"
  },
  {
    "eval_name": "bunnycore_Llama-3.1-8B-TitanFusion-Mix_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.1-8B-TitanFusion-Mix\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.1-8B-TitanFusion-Mix</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.1-8B-TitanFusion-Mix-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.1-8B-TitanFusion-Mix",
    "Model sha": "9eb89de7df048276ccbc4405ce4f005f9185f82e",
    "Average ‚¨ÜÔ∏è": 24.96189537051428,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9330848040028549,
    "IFEval Raw": 0.4924954675815725,
    "IFEval": 49.24954675815725,
    "BBH Raw": 0.5755964197928182,
    "BBH": 39.535483334813364,
    "MATH Lvl 5 Raw": 0.12537764350453173,
    "MATH Lvl 5": 12.537764350453173,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.4316979166666666,
    "MUSR": 12.462239583333336,
    "MMLU-PRO Raw": 0.3695146276595745,
    "MMLU-PRO": 29.94606973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.1-8B-TitanFusion-Mix (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.1-8B-TitanFusion-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.1-8B-TitanFusion-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.1-8B-TitanFusion-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.1-8B-TitanFusion-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.1-8B-TitanFusion-v3",
    "Model sha": "ea8269ac3b2e9c0dc855a9089251ebdb273ada16",
    "Average ‚¨ÜÔ∏è": 24.23172088440454,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8878242183166523,
    "IFEval Raw": 0.4809549772381725,
    "IFEval": 48.095497723817246,
    "BBH Raw": 0.5262113071794826,
    "BBH": 32.072941144614084,
    "MATH Lvl 5 Raw": 0.14274924471299094,
    "MATH Lvl 5": 14.274924471299094,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.4302083333333333,
    "MUSR": 11.942708333333334,
    "MMLU-PRO Raw": 0.38056848404255317,
    "MMLU-PRO": 31.174276004728128,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.1-8B-TitanFusion-v3 (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-All-Mix_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-All-Mix\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-All-Mix</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-All-Mix-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-All-Mix",
    "Model sha": "adacdd571c4073990ecf05a23277793e9e5f0410",
    "Average ‚¨ÜÔ∏è": 22.416477656771107,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7403094827455026,
    "IFEval Raw": 0.7226049105262924,
    "IFEval": 72.26049105262925,
    "BBH Raw": 0.45083384652782293,
    "BBH": 22.516311192334513,
    "MATH Lvl 5 Raw": 0.11858006042296072,
    "MATH Lvl 5": 11.858006042296072,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.32869791666666665,
    "MUSR": 2.187239583333333,
    "MMLU-PRO Raw": 0.3159906914893617,
    "MMLU-PRO": 23.99896572104019,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-All-Mix (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-Booval_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-Booval\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-Booval</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-Booval-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-Booval",
    "Model sha": "d7f3449f89fa86d8e2c411aa4ca10ad552a62803",
    "Average ‚¨ÜÔ∏è": 21.301100450927045,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6552407354088423,
    "IFEval Raw": 0.6669259786256023,
    "IFEval": 66.69259786256023,
    "BBH Raw": 0.45143904014934083,
    "BBH": 22.515991469149508,
    "MATH Lvl 5 Raw": 0.11102719033232629,
    "MATH Lvl 5": 11.10271903323263,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.3394270833333333,
    "MUSR": 2.3950520833333333,
    "MMLU-PRO Raw": 0.30576795212765956,
    "MMLU-PRO": 22.86310579196217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-Booval (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-Long-Think_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-Long-Think\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-Long-Think</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-Long-Think-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-Long-Think",
    "Model sha": "a8522bfc03657b41b0541b164a98ddff302a6fd2",
    "Average ‚¨ÜÔ∏è": 19.549051506484762,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3810287495461295,
    "IFEval Raw": 0.5473499204333391,
    "IFEval": 54.73499204333391,
    "BBH Raw": 0.4610394542442049,
    "BBH": 24.22680316567029,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.33955208333333337,
    "MUSR": 1.2106770833333331,
    "MMLU-PRO Raw": 0.30477061170212766,
    "MMLU-PRO": 22.752290189125294,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-Long-Think (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-Mix-Skill_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-Mix-Skill\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-Mix-Skill</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-Mix-Skill-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-Mix-Skill",
    "Model sha": "d07d6e733aaeaf48cb6616228d00104b05b52afd",
    "Average ‚¨ÜÔ∏è": 21.399686909553257,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.685066791807929,
    "IFEval Raw": 0.6404229666174639,
    "IFEval": 64.04229666174639,
    "BBH Raw": 0.45818358891543803,
    "BBH": 23.784246657651128,
    "MATH Lvl 5 Raw": 0.1268882175226586,
    "MATH Lvl 5": 12.688821752265861,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.33961458333333333,
    "MUSR": 2.751822916666668,
    "MMLU-PRO Raw": 0.3120844414893617,
    "MMLU-PRO": 23.56493794326241,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-Mix-Skill (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-ProdigyPlus_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-ProdigyPlus\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-ProdigyPlus</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-ProdigyPlus-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-ProdigyPlus",
    "Model sha": "799f7669701ecf27f4c3e29998dd839b4d54c408",
    "Average ‚¨ÜÔ∏è": 16.079069030590112,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7139397859248248,
    "IFEval Raw": 0.40152018865499095,
    "IFEval": 40.1520188654991,
    "BBH Raw": 0.4392279045834126,
    "BBH": 20.622988697146294,
    "MATH Lvl 5 Raw": 0.09894259818731117,
    "MATH Lvl 5": 9.894259818731117,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.35800000000000004,
    "MUSR": 3.1500000000000004,
    "MMLU-PRO Raw": 0.28174867021276595,
    "MMLU-PRO": 20.194296690307326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-ProdigyPlus (Merge)"
  },
  {
    "eval_name": "bunnycore_Llama-3.2-3B-ProdigyPlusPlus_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Llama-3.2-3B-ProdigyPlusPlus\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Llama-3.2-3B-ProdigyPlusPlus</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Llama-3.2-3B-ProdigyPlusPlus-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Llama-3.2-3B-ProdigyPlusPlus",
    "Model sha": "512865708a7ec9754997fb404b1ffc0752b099d7",
    "Average ‚¨ÜÔ∏è": 6.443825974411914,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6715757174970428,
    "IFEval Raw": 0.1645157072124186,
    "IFEval": 16.45157072124186,
    "BBH Raw": 0.3689926047041594,
    "BBH": 11.56197768121448,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.354125,
    "MUSR": 1.698958333333333,
    "MMLU-PRO Raw": 0.15001662234042554,
    "MMLU-PRO": 5.557402482269504,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "bunnycore/Llama-3.2-3B-ProdigyPlusPlus (Merge)"
  },
  {
    "eval_name": "bunnycore_Phi-3.5-mini-TitanFusion-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Phi-3.5-mini-TitanFusion-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Phi-3.5-mini-TitanFusion-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Phi-3.5-mini-TitanFusion-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Phi-3.5-mini-TitanFusion-0.1",
    "Model sha": "72939b8b75e23b22b1758bb05a842e5834f75d96",
    "Average ‚¨ÜÔ∏è": 25.392387958249845,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.796885466585362,
    "IFEval Raw": 0.5227950726295119,
    "IFEval": 52.279507262951185,
    "BBH Raw": 0.5373733988565133,
    "BBH": 35.446219076522446,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.4453125,
    "MUSR": 15.797395833333333,
    "MMLU-PRO Raw": 0.3806515957446808,
    "MMLU-PRO": 31.183510638297868,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "bunnycore/Phi-3.5-mini-TitanFusion-0.1 (Merge)"
  },
  {
    "eval_name": "bunnycore_Qandora-2.5-7B-Creative_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Qandora-2.5-7B-Creative\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Qandora-2.5-7B-Creative</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Qandora-2.5-7B-Creative-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Qandora-2.5-7B-Creative",
    "Model sha": "fdb174364d4a4f323ed1cb01219ac4d87708219d",
    "Average ‚¨ÜÔ∏è": 30.9311314754903,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7097950061613115,
    "IFEval Raw": 0.6803148978044922,
    "IFEval": 68.03148978044922,
    "BBH Raw": 0.5541763892398439,
    "BBH": 36.424651784758105,
    "MATH Lvl 5 Raw": 0.23564954682779457,
    "MATH Lvl 5": 23.564954682779458,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.4211875,
    "MUSR": 10.848437500000001,
    "MMLU-PRO Raw": 0.4479720744680851,
    "MMLU-PRO": 38.66356382978723,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "bunnycore/Qandora-2.5-7B-Creative (Merge)"
  },
  {
    "eval_name": "bunnycore_QandoraExp-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/QandoraExp-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/QandoraExp-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__QandoraExp-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/QandoraExp-7B",
    "Model sha": "74906d5518c7feb7df9b168763dabc1b0167942f",
    "Average ‚¨ÜÔ∏è": 28.51072041520625,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6705275793534974,
    "IFEval Raw": 0.7509064836855099,
    "IFEval": 75.090648368551,
    "BBH Raw": 0.5477959748047708,
    "BBH": 35.92474216004687,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.43120833333333336,
    "MUSR": 13.201041666666669,
    "MMLU-PRO Raw": 0.4409906914893617,
    "MMLU-PRO": 37.88785460992907,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "bunnycore/QandoraExp-7B (Merge)"
  },
  {
    "eval_name": "bunnycore_QandoraExp-7B-Persona_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/QandoraExp-7B-Persona\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/QandoraExp-7B-Persona</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__QandoraExp-7B-Persona-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/QandoraExp-7B-Persona",
    "Model sha": "21bd6c2e270358b70f9af98bcccd6ec9c2cfce88",
    "Average ‚¨ÜÔ∏è": 29.188505790830817,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6879707866416571,
    "IFEval Raw": 0.6246858335882126,
    "IFEval": 62.46858335882126,
    "BBH Raw": 0.5558337526959515,
    "BBH": 36.8327094432999,
    "MATH Lvl 5 Raw": 0.16012084592145015,
    "MATH Lvl 5": 16.012084592145015,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.43715624999999997,
    "MUSR": 13.344531250000001,
    "MMLU-PRO Raw": 0.44074135638297873,
    "MMLU-PRO": 37.86015070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "bunnycore/QandoraExp-7B-Persona (Merge)"
  },
  {
    "eval_name": "bunnycore_QandoraExp-7B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/QandoraExp-7B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/QandoraExp-7B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__QandoraExp-7B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/QandoraExp-7B-v2",
    "Model sha": "017594240f9b3c4262e23de6d550453a1a3d5540",
    "Average ‚¨ÜÔ∏è": 23.274653785329264,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6936494781177268,
    "IFEval Raw": 0.5606889719278182,
    "IFEval": 56.06889719278182,
    "BBH Raw": 0.5444864824489132,
    "BBH": 34.94496675271275,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.40454166666666663,
    "MUSR": 9.267708333333339,
    "MMLU-PRO Raw": 0.390874335106383,
    "MMLU-PRO": 32.319370567375884,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "bunnycore/QandoraExp-7B-v2 (Merge)"
  },
  {
    "eval_name": "bunnycore_Qwen2.5-3B-RP-Mix_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Qwen2.5-3B-RP-Mix\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Qwen2.5-3B-RP-Mix</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Qwen2.5-3B-RP-Mix-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Qwen2.5-3B-RP-Mix",
    "Model sha": "0e8f3b56f9270fdcdd4badfd7b925dc8fc4902c7",
    "Average ‚¨ÜÔ∏è": 23.377815068917105,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.91970101664535,
    "IFEval Raw": 0.5720543712903984,
    "IFEval": 57.20543712903984,
    "BBH Raw": 0.4894378989397821,
    "BBH": 28.305922895366823,
    "MATH Lvl 5 Raw": 0.08761329305135951,
    "MATH Lvl 5": 8.76132930513595,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.42844791666666665,
    "MUSR": 12.555989583333334,
    "MMLU-PRO Raw": 0.37275598404255317,
    "MMLU-PRO": 30.306220449172578,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "bunnycore/Qwen2.5-3B-RP-Mix (Merge)"
  },
  {
    "eval_name": "bunnycore_Qwen2.5-7B-CyberRombos_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Qwen2.5-7B-CyberRombos\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Qwen2.5-7B-CyberRombos</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Qwen2.5-7B-CyberRombos-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Qwen2.5-7B-CyberRombos",
    "Model sha": "dfd4d30fc6956ffecb9fb3c59fad51875552f7f9",
    "Average ‚¨ÜÔ∏è": 27.692747245388176,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7101054102494385,
    "IFEval Raw": 0.751830698103255,
    "IFEval": 75.18306981032549,
    "BBH Raw": 0.5464960546716063,
    "BBH": 35.8840250776346,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.41254166666666664,
    "MUSR": 10.067708333333336,
    "MMLU-PRO Raw": 0.4390791223404255,
    "MMLU-PRO": 37.67545803782505,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-05",
    "Generation": 1,
    "Base Model": "bunnycore/Qwen2.5-7B-CyberRombos (Merge)"
  },
  {
    "eval_name": "bunnycore_Qwen2.5-7B-Instruct-Fusion_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/Qwen2.5-7B-Instruct-Fusion\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/Qwen2.5-7B-Instruct-Fusion</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__Qwen2.5-7B-Instruct-Fusion-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/Qwen2.5-7B-Instruct-Fusion",
    "Model sha": "6313c0b3de799ab48720c3b828e322a77cf8d023",
    "Average ‚¨ÜÔ∏è": 30.747252421264204,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6629499050008137,
    "IFEval Raw": 0.6962016338869754,
    "IFEval": 69.62016338869753,
    "BBH Raw": 0.5491903018724945,
    "BBH": 36.17985917773406,
    "MATH Lvl 5 Raw": 0.19939577039274925,
    "MATH Lvl 5": 19.939577039274926,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42971875,
    "MUSR": 12.948177083333334,
    "MMLU-PRO Raw": 0.4467253989361702,
    "MMLU-PRO": 38.52504432624112,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-02",
    "Generation": 1,
    "Base Model": "bunnycore/Qwen2.5-7B-Instruct-Fusion (Merge)"
  },
  {
    "eval_name": "bunnycore_SmolLM2-1.7-Persona_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/SmolLM2-1.7-Persona\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/SmolLM2-1.7-Persona</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__SmolLM2-1.7-Persona-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/SmolLM2-1.7-Persona",
    "Model sha": "ebeaa6f284c044bd54e3e66cc5458d974d92523e",
    "Average ‚¨ÜÔ∏è": 14.250410441580543,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.33133240068871256,
    "IFEval Raw": 0.5465254413844156,
    "IFEval": 54.652544138441556,
    "BBH Raw": 0.3623213930905173,
    "BBH": 11.203752907058009,
    "MATH Lvl 5 Raw": 0.04003021148036254,
    "MATH Lvl 5": 4.003021148036254,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.334125,
    "MUSR": 3.0322916666666675,
    "MMLU-PRO Raw": 0.1973902925531915,
    "MMLU-PRO": 10.821143617021276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 0,
    "Base Model": "bunnycore/SmolLM2-1.7-Persona"
  },
  {
    "eval_name": "bunnycore_SmolLM2-1.7B-roleplay-lora_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/bunnycore/SmolLM2-1.7B-roleplay-lora\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">bunnycore/SmolLM2-1.7B-roleplay-lora</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/bunnycore__SmolLM2-1.7B-roleplay-lora-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "bunnycore/SmolLM2-1.7B-roleplay-lora",
    "Model sha": "bbab860a4ffdd8e48f600192947ad3504bb0a944",
    "Average ‚¨ÜÔ∏è": 14.252473873670247,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7000256974531178,
    "IFEval Raw": 0.5382075116247114,
    "IFEval": 53.82075116247113,
    "BBH Raw": 0.3610343412303005,
    "BBH": 10.907238019540259,
    "MATH Lvl 5 Raw": 0.03927492447129909,
    "MATH Lvl 5": 3.927492447129909,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.33945833333333336,
    "MUSR": 2.7656250000000004,
    "MMLU-PRO Raw": 0.19664228723404256,
    "MMLU-PRO": 10.738031914893616,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 3,
    "Base Model": "HuggingFaceTB/SmolLM2-1.7B-Instruct (Merge)"
  },
  {
    "eval_name": "byroneverson_Mistral-Small-Instruct-2409-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/byroneverson/Mistral-Small-Instruct-2409-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">byroneverson/Mistral-Small-Instruct-2409-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/byroneverson__Mistral-Small-Instruct-2409-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "byroneverson/Mistral-Small-Instruct-2409-abliterated",
    "Model sha": "5e24aaef2a37f9cb69f70ae9fe714f9d9599fd6e",
    "Average ‚¨ÜÔ∏è": 27.169390532337705,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4022864616962205,
    "IFEval Raw": 0.6970759806203096,
    "IFEval": 69.70759806203097,
    "BBH Raw": 0.5237864400325174,
    "BBH": 31.255700427788586,
    "MATH Lvl 5 Raw": 0.14954682779456194,
    "MATH Lvl 5": 14.954682779456194,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.36971875000000004,
    "MUSR": 3.5481770833333326,
    "MMLU-PRO Raw": 0.39228723404255317,
    "MMLU-PRO": 32.476359338061464,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Small-Instruct-2409"
  },
  {
    "eval_name": "byroneverson_Yi-1.5-9B-Chat-16K-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/byroneverson/Yi-1.5-9B-Chat-16K-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">byroneverson/Yi-1.5-9B-Chat-16K-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/byroneverson__Yi-1.5-9B-Chat-16K-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "byroneverson/Yi-1.5-9B-Chat-16K-abliterated",
    "Model sha": "84a6eaa723633bbefc7cfac9c64bf0e0a4d39065",
    "Average ‚¨ÜÔ∏è": 26.532727564463084,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0901032098513954,
    "IFEval Raw": 0.5528453392553979,
    "IFEval": 55.28453392553979,
    "BBH Raw": 0.5282050829986801,
    "BBH": 32.843258967002555,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.4734375,
    "MUSR": 19.679687499999996,
    "MMLU-PRO Raw": 0.38231382978723405,
    "MMLU-PRO": 31.36820330969267,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-03",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "01-ai/Yi-1.5-9B-Chat-16K"
  },
  {
    "eval_name": "byroneverson_Yi-1.5-9B-Chat-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/byroneverson/Yi-1.5-9B-Chat-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">byroneverson/Yi-1.5-9B-Chat-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/byroneverson__Yi-1.5-9B-Chat-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "byroneverson/Yi-1.5-9B-Chat-abliterated",
    "Model sha": "4e26c200cdf2dc50dd50cdd9fe5b74887e9fa94a",
    "Average ‚¨ÜÔ∏è": 25.30072104853323,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.844572338140038,
    "IFEval Raw": 0.5723291976400395,
    "IFEval": 57.23291976400395,
    "BBH Raw": 0.5401219363002313,
    "BBH": 34.35218727198406,
    "MATH Lvl 5 Raw": 0.10800604229607252,
    "MATH Lvl 5": 10.800604229607252,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.43886458333333334,
    "MUSR": 13.658072916666667,
    "MMLU-PRO Raw": 0.3715093085106383,
    "MMLU-PRO": 30.167700945626475,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "01-ai/Yi-1.5-9B-Chat"
  },
  {
    "eval_name": "c10x_Q-Pluse_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/c10x/Q-Pluse\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">c10x/Q-Pluse</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/c10x__Q-Pluse-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "c10x/Q-Pluse",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 3.634370763516103,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3116911900099903,
    "IFEval Raw": 0.11228318638988993,
    "IFEval": 11.228318638988993,
    "BBH Raw": 0.2875111436321769,
    "BBH": 1.9479450969539605,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.39381249999999995,
    "MUSR": 7.126562500000001,
    "MMLU-PRO Raw": 0.11353058510638298,
    "MMLU-PRO": 1.5033983451536632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-10",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "c10x_longthinker_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/c10x/longthinker\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">c10x/longthinker</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/c10x__longthinker-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "c10x/longthinker",
    "Model sha": "e1bb4a2c2782ab52be7a8fa2e5905f08b7cfd464",
    "Average ‚¨ÜÔ∏è": 19.68607400361903,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9447744382359007,
    "IFEval Raw": 0.36087913403103766,
    "IFEval": 36.08791340310377,
    "BBH Raw": 0.49274888053364546,
    "BBH": 28.4247368611983,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3909583333333333,
    "MUSR": 6.703125000000001,
    "MMLU-PRO Raw": 0.3527260638297872,
    "MMLU-PRO": 28.08067375886525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "c10x/longthinker (Merge)"
  },
  {
    "eval_name": "carsenk_flippa-v6_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/carsenk/flippa-v6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">carsenk/flippa-v6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/carsenk__flippa-v6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "carsenk/flippa-v6",
    "Model sha": "5206a32e0bd3067aef1ce90f5528ade7d866253f",
    "Average ‚¨ÜÔ∏è": 20.738602675687222,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.064800311611869,
    "IFEval Raw": 0.3439429602344003,
    "IFEval": 34.39429602344003,
    "BBH Raw": 0.5046972457053399,
    "BBH": 29.993501279427136,
    "MATH Lvl 5 Raw": 0.1382175226586103,
    "MATH Lvl 5": 13.82175226586103,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.40887500000000004,
    "MUSR": 10.876041666666667,
    "MMLU-PRO Raw": 0.3667719414893617,
    "MMLU-PRO": 29.6413268321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-08-24",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "carsenk_phi3.5_mini_exp_825_uncensored_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/carsenk/phi3.5_mini_exp_825_uncensored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">carsenk/phi3.5_mini_exp_825_uncensored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/carsenk__phi3.5_mini_exp_825_uncensored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "carsenk/phi3.5_mini_exp_825_uncensored",
    "Model sha": "6b208dc3df02e0d5ef0c3fe5899f9f31618f2e94",
    "Average ‚¨ÜÔ∏è": 3.4668751054130524,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4878184839567265,
    "IFEval Raw": 0.13641360479084386,
    "IFEval": 13.641360479084385,
    "BBH Raw": 0.29647345147918264,
    "BBH": 1.827812730226084,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.36441666666666667,
    "MUSR": 3.385416666666666,
    "MMLU-PRO Raw": 0.11751994680851063,
    "MMLU-PRO": 1.9466607565011809,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-29",
    "Submission Date": "2024-08-29",
    "Generation": 1,
    "Base Model": "unsloth/phi-3.5-mini-instruct-bnb-4bit"
  },
  {
    "eval_name": "cat-searcher_gemma-2-9b-it-sppo-iter-1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cat-searcher/gemma-2-9b-it-sppo-iter-1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cat-searcher/gemma-2-9b-it-sppo-iter-1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cat-searcher__gemma-2-9b-it-sppo-iter-1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cat-searcher/gemma-2-9b-it-sppo-iter-1",
    "Model sha": "b29a3a5cef93ee044e2297fcb40bd2976415e900",
    "Average ‚¨ÜÔ∏è": 20.553948052865767,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7680393522812317,
    "IFEval Raw": 0.30147674836101546,
    "IFEval": 30.147674836101544,
    "BBH Raw": 0.5971867698707507,
    "BBH": 41.67630770023723,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3447986577181208,
    "GPQA": 12.639821029082773,
    "MUSR Raw": 0.39266666666666666,
    "MUSR": 7.1499999999999995,
    "MMLU-PRO Raw": 0.38538896276595747,
    "MMLU-PRO": 31.709884751773053,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-09",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "cat-searcher_gemma-2-9b-it-sppo-iter-1-evol-1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cat-searcher/gemma-2-9b-it-sppo-iter-1-evol-1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cat-searcher/gemma-2-9b-it-sppo-iter-1-evol-1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cat-searcher__gemma-2-9b-it-sppo-iter-1-evol-1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cat-searcher/gemma-2-9b-it-sppo-iter-1-evol-1",
    "Model sha": "c2d7b76786151aecfa5972a2a3e937feb2d2c48b",
    "Average ‚¨ÜÔ∏è": 20.103005917547325,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7878124079972344,
    "IFEval Raw": 0.2941827683878775,
    "IFEval": 29.418276838787747,
    "BBH Raw": 0.5939369622672414,
    "BBH": 41.10464026733791,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.34060402684563756,
    "GPQA": 12.080536912751676,
    "MUSR Raw": 0.39257291666666666,
    "MUSR": 6.904947916666669,
    "MMLU-PRO Raw": 0.37998670212765956,
    "MMLU-PRO": 31.109633569739948,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-08-09",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "cgato_TheSalt-L3-8b-v0.3.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cgato/TheSalt-L3-8b-v0.3.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cgato/TheSalt-L3-8b-v0.3.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cgato__TheSalt-L3-8b-v0.3.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cgato/TheSalt-L3-8b-v0.3.2",
    "Model sha": "5cf08e2bf9590ebcd14ba021e113def28c65afa2",
    "Average ‚¨ÜÔ∏è": 7.248831987266861,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.940294085500808,
    "IFEval Raw": 0.27050337548814923,
    "IFEval": 27.050337548814923,
    "BBH Raw": 0.29679653176003074,
    "BBH": 2.612714473502145,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.38962499999999994,
    "MUSR": 6.3031250000000005,
    "MMLU-PRO Raw": 0.11394614361702128,
    "MMLU-PRO": 1.549571513002364,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-18",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "cgato/TheSalt-L3-8b-v0.3.2"
  },
  {
    "eval_name": "chargoddard_prometheus-2-llama-3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/chargoddard/prometheus-2-llama-3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chargoddard/prometheus-2-llama-3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/chargoddard__prometheus-2-llama-3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "chargoddard/prometheus-2-llama-3-8b",
    "Model sha": "90a728ac98e5b4169f88ae4945e357cf45477568",
    "Average ‚¨ÜÔ∏è": 19.281097477881172,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9451146326066104,
    "IFEval Raw": 0.5288900118352637,
    "IFEval": 52.889001183526375,
    "BBH Raw": 0.4931144581470071,
    "BBH": 27.80383919259717,
    "MATH Lvl 5 Raw": 0.08006042296072508,
    "MATH Lvl 5": 8.006042296072508,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.33958333333333335,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.30867686170212766,
    "MMLU-PRO": 23.186317966903072,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "chargoddard/prometheus-2-llama-3-8b (Merge)"
  },
  {
    "eval_name": "chujiezheng_Llama-3-Instruct-8B-SimPO-ExPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/chujiezheng/Llama-3-Instruct-8B-SimPO-ExPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chujiezheng/Llama-3-Instruct-8B-SimPO-ExPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/chujiezheng__Llama-3-Instruct-8B-SimPO-ExPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "chujiezheng/Llama-3-Instruct-8B-SimPO-ExPO",
    "Model sha": "3fcaa9fe99691659eb197487e9a343f601bf63f2",
    "Average ‚¨ÜÔ∏è": 21.972344167758195,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7204813829159327,
    "IFEval Raw": 0.6433707008515184,
    "IFEval": 64.33707008515184,
    "BBH Raw": 0.4764515968840137,
    "BBH": 25.86828225174116,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.3920104166666667,
    "MUSR": 9.501302083333336,
    "MMLU-PRO Raw": 0.340093085106383,
    "MMLU-PRO": 26.677009456264773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "chujiezheng/Llama-3-Instruct-8B-SimPO-ExPO"
  },
  {
    "eval_name": "chujiezheng_Mistral7B-PairRM-SPPO-ExPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/chujiezheng/Mistral7B-PairRM-SPPO-ExPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">chujiezheng/Mistral7B-PairRM-SPPO-ExPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/chujiezheng__Mistral7B-PairRM-SPPO-ExPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "chujiezheng/Mistral7B-PairRM-SPPO-ExPO",
    "Model sha": "d3e8342a63e5ae096f450f2467a92168db12768c",
    "Average ‚¨ÜÔ∏è": 13.49126772081828,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5090339851534275,
    "IFEval Raw": 0.36734863495525205,
    "IFEval": 36.734863495525204,
    "BBH Raw": 0.3882191262277366,
    "BBH": 13.678635807519433,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.40553124999999995,
    "MUSR": 8.658072916666667,
    "MMLU-PRO Raw": 0.2551529255319149,
    "MMLU-PRO": 17.239213947990542,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "chujiezheng/Mistral7B-PairRM-SPPO-ExPO"
  },
  {
    "eval_name": "cloudyu_Llama-3-70Bx2-MOE_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Llama-3-70Bx2-MOE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Llama-3-70Bx2-MOE</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cloudyu__Llama-3-70Bx2-MOE-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cloudyu/Llama-3-70Bx2-MOE",
    "Model sha": "b8bd85e8db8e4ec352b93441c92e0ae1334bf5a7",
    "Average ‚¨ÜÔ∏è": 35.66646489034437,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 126,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 21.539554801943073,
    "IFEval Raw": 0.5482486469234964,
    "IFEval": 54.824864692349635,
    "BBH Raw": 0.6636234572270707,
    "BBH": 51.42213772529595,
    "MATH Lvl 5 Raw": 0.21752265861027192,
    "MATH Lvl 5": 21.752265861027194,
    "GPQA Raw": 0.3934563758389262,
    "GPQA": 19.12751677852349,
    "MUSR Raw": 0.48118750000000005,
    "MUSR": 20.848437499999996,
    "MMLU-PRO Raw": 0.5142121010638298,
    "MMLU-PRO": 46.02356678486997,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-20",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "cloudyu/Llama-3-70Bx2-MOE"
  },
  {
    "eval_name": "cloudyu_Mixtral_34Bx2_MoE_60B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Mixtral_34Bx2_MoE_60B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Mixtral_34Bx2_MoE_60B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cloudyu__Mixtral_34Bx2_MoE_60B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cloudyu/Mixtral_34Bx2_MoE_60B",
    "Model sha": "d01642769ccc782e1db1fc26cb25097aecb98e23",
    "Average ‚¨ÜÔ∏è": 27.598581076033312,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 111,
    "#Params (B)": 60,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.332588480112929,
    "IFEval Raw": 0.4537770892343427,
    "IFEval": 45.37770892343427,
    "BBH Raw": 0.5869701263465353,
    "BBH": 41.209129053590964,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.4625208333333333,
    "MUSR": 17.78177083333333,
    "MMLU-PRO Raw": 0.47664561170212766,
    "MMLU-PRO": 41.84951241134751,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-05",
    "Submission Date": "2024-08-22",
    "Generation": 0,
    "Base Model": "cloudyu/Mixtral_34Bx2_MoE_60B"
  },
  {
    "eval_name": "cloudyu_Yi-34Bx2-MoE-60B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cloudyu/Yi-34Bx2-MoE-60B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cloudyu/Yi-34Bx2-MoE-60B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cloudyu__Yi-34Bx2-MoE-60B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cloudyu/Yi-34Bx2-MoE-60B-DPO",
    "Model sha": "5c2d31042229ee06246064100b781dd926cb0ffd",
    "Average ‚¨ÜÔ∏è": 26.04350240565636,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 60,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 7.339246639361052,
    "IFEval Raw": 0.531887613753729,
    "IFEval": 53.1887613753729,
    "BBH Raw": 0.516831447641953,
    "BBH": 31.259298004231464,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.43746875,
    "MUSR": 14.316927083333331,
    "MMLU-PRO Raw": 0.46766954787234044,
    "MMLU-PRO": 40.8521719858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-23",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "cloudyu/Yi-34Bx2-MoE-60B-DPO"
  },
  {
    "eval_name": "cluebbers_Llama-3.1-8B-paraphrase-type-generation-apty-ipo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-ipo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-ipo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cluebbers__Llama-3.1-8B-paraphrase-type-generation-apty-ipo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-ipo",
    "Model sha": "eb04613997875935cb667a517e518874bb716169",
    "Average ‚¨ÜÔ∏è": 9.749647631566758,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.719830889824629,
    "IFEval Raw": 0.1326668794354535,
    "IFEval": 13.26668794354535,
    "BBH Raw": 0.3800219303191354,
    "BBH": 12.669478223003603,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.43321875,
    "MUSR": 12.419010416666666,
    "MMLU-PRO Raw": 0.2590591755319149,
    "MMLU-PRO": 17.673241725768317,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-ipo (Merge)"
  },
  {
    "eval_name": "cluebbers_Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cluebbers__Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid",
    "Model sha": "2c8b52e8db11a6ff57cccf890ee26688e858f9fb",
    "Average ‚¨ÜÔ∏è": 9.743408044310225,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7235601647354057,
    "IFEval Raw": 0.13184240038652995,
    "IFEval": 13.184240038652996,
    "BBH Raw": 0.37889016032903705,
    "BBH": 12.757325206130604,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.43055208333333334,
    "MUSR": 12.019010416666665,
    "MMLU-PRO Raw": 0.2562333776595745,
    "MMLU-PRO": 17.359264184397162,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-apty-sigmoid (Merge)"
  },
  {
    "eval_name": "cluebbers_Llama-3.1-8B-paraphrase-type-generation-etpc_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cluebbers/Llama-3.1-8B-paraphrase-type-generation-etpc\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cluebbers/Llama-3.1-8B-paraphrase-type-generation-etpc</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cluebbers__Llama-3.1-8B-paraphrase-type-generation-etpc-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-etpc",
    "Model sha": "a003a227aed5c1ad67cd4a653b13a0dd7acb7ed5",
    "Average ‚¨ÜÔ∏è": 9.430025838684463,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7415271680676786,
    "IFEval Raw": 0.12085156274241235,
    "IFEval": 12.085156274241236,
    "BBH Raw": 0.3780811415223316,
    "BBH": 12.69457911779628,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.43185416666666665,
    "MUSR": 12.048437499999999,
    "MMLU-PRO Raw": 0.25556848404255317,
    "MMLU-PRO": 17.28538711583924,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "cluebbers/Llama-3.1-8B-paraphrase-type-generation-etpc (Merge)"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9-llama3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9-llama3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9-llama3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9-llama3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9-llama3-8b",
    "Model sha": "5aeb036f9215c558b483a654a8c6e1cc22e841bf",
    "Average ‚¨ÜÔ∏è": 18.390285005161935,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 419,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7391201399324697,
    "IFEval Raw": 0.38503393218881454,
    "IFEval": 38.503393218881456,
    "BBH Raw": 0.49499220166609187,
    "BBH": 27.858929260905125,
    "MATH Lvl 5 Raw": 0.055891238670694864,
    "MATH Lvl 5": 5.589123867069486,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.43753125,
    "MUSR": 13.791406250000003,
    "MMLU-PRO Raw": 0.277094414893617,
    "MMLU-PRO": 19.67715721040189,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-20",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.1-llama-3-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.1-llama-3-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.1-llama-3-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.1-llama-3-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.1-llama-3-70b",
    "Model sha": "31adf616c3c9176d147e0a62e9fedb7bf97678ac",
    "Average ‚¨ÜÔ∏è": 23.444758722294452,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 38,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.149088069604229,
    "IFEval Raw": 0.3760167466765959,
    "IFEval": 37.60167466765959,
    "BBH Raw": 0.5204919312821467,
    "BBH": 31.101151872569243,
    "MATH Lvl 5 Raw": 0.05664652567975831,
    "MATH Lvl 5": 5.664652567975831,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.49756249999999996,
    "MUSR": 23.695312499999996,
    "MMLU-PRO Raw": 0.41298204787234044,
    "MMLU-PRO": 34.77578309692671,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.1-yi-1.5-34b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.1-yi-1.5-34b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.1-yi-1.5-34b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.1-yi-1.5-34b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.1-yi-1.5-34b",
    "Model sha": "1ec522298a6935c881df6dc29d3669833bd8672d",
    "Average ‚¨ÜÔ∏è": 27.90438430774833,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 34,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.992653350003133,
    "IFEval Raw": 0.3852588908540451,
    "IFEval": 38.52588908540451,
    "BBH Raw": 0.6076225600626862,
    "BBH": 44.17408874277273,
    "MATH Lvl 5 Raw": 0.16238670694864046,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.45979166666666665,
    "MUSR": 16.97395833333334,
    "MMLU-PRO Raw": 0.4518783244680851,
    "MMLU-PRO": 39.09759160756501,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-18",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "01-ai/Yi-1.5-34B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.1-yi-1.5-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.1-yi-1.5-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.1-yi-1.5-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.1-yi-1.5-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.1-yi-1.5-9b",
    "Model sha": "91f0a521e3e2a0675a3549aa5d3f40717068de94",
    "Average ‚¨ÜÔ∏è": 24.934789582515688,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0508657143917215,
    "IFEval Raw": 0.44653297694561545,
    "IFEval": 44.653297694561545,
    "BBH Raw": 0.5484314644603556,
    "BBH": 35.7760897255686,
    "MATH Lvl 5 Raw": 0.1095166163141994,
    "MATH Lvl 5": 10.95166163141994,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.4348020833333333,
    "MUSR": 13.516927083333336,
    "MMLU-PRO Raw": 0.3966921542553192,
    "MMLU-PRO": 32.96579491725768,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-18",
    "Submission Date": "2024-08-02",
    "Generation": 1,
    "Base Model": "01-ai/Yi-1.5-9B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.2-Phi-3-Medium_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.2-Phi-3-Medium\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.2-Phi-3-Medium</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.2-Phi-3-Medium-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium",
    "Model sha": "0470c5b912b51fa6e27d87a8ea7feafacd8cb101",
    "Average ‚¨ÜÔ∏è": 25.668897153286427,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": -1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8404817926898199,
    "IFEval Raw": 0.4247762603226107,
    "IFEval": 42.47762603226107,
    "BBH Raw": 0.6456739302686527,
    "BBH": 49.72194030508101,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.4190520833333333,
    "MUSR": 11.414843750000003,
    "MMLU-PRO Raw": 0.45553523936170215,
    "MMLU-PRO": 39.50391548463357,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium (Merge)"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.2-Phi-3-Medium-abliterated_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.2-Phi-3-Medium-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated",
    "Model sha": "d50be5f22ca9745a2a3175996611d6a840318b7f",
    "Average ‚¨ÜÔ∏è": 25.590063720348784,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8439536564991926,
    "IFEval Raw": 0.36125369574950017,
    "IFEval": 36.12536957495002,
    "BBH Raw": 0.612322545411745,
    "BBH": 45.44126655093765,
    "MATH Lvl 5 Raw": 0.12386706948640483,
    "MATH Lvl 5": 12.386706948640484,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.4111770833333333,
    "MUSR": 10.363802083333335,
    "MMLU-PRO Raw": 0.4493849734042553,
    "MMLU-PRO": 38.820552600472816,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated (Merge)"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.2-Phi-3-Medium-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.2-Phi-3-Medium-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated",
    "Model sha": "d50be5f22ca9745a2a3175996611d6a840318b7f",
    "Average ‚¨ÜÔ∏è": 25.618429176393875,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8207965966186911,
    "IFEval Raw": 0.4123614232458765,
    "IFEval": 41.23614232458765,
    "BBH Raw": 0.638289226729353,
    "BBH": 48.38534691270737,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.43492708333333335,
    "MUSR": 13.732552083333337,
    "MMLU-PRO Raw": 0.45246010638297873,
    "MMLU-PRO": 39.162234042553195,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "cognitivecomputations/dolphin-2.9.2-Phi-3-Medium-abliterated (Merge)"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.2-qwen2-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.2-qwen2-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.2-qwen2-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.2-qwen2-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.2-qwen2-72b",
    "Model sha": "e79582577c2bf2af304221af0e8308b7e7d46ca1",
    "Average ‚¨ÜÔ∏è": 35.74529231792153,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 92,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 25.1155535276709,
    "IFEval Raw": 0.6343778950961227,
    "IFEval": 63.43778950961227,
    "BBH Raw": 0.6296364939584073,
    "BBH": 47.69617372826186,
    "MATH Lvl 5 Raw": 0.20619335347432027,
    "MATH Lvl 5": 20.619335347432028,
    "GPQA Raw": 0.3699664429530201,
    "GPQA": 15.99552572706935,
    "MUSR Raw": 0.45207291666666666,
    "MUSR": 17.042447916666664,
    "MMLU-PRO Raw": 0.547124335106383,
    "MMLU-PRO": 49.680481678487006,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-72B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.2-qwen2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.2-qwen2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.2-qwen2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.2-qwen2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.2-qwen2-7b",
    "Model sha": "c443c4eb5138ed746ac49ed98bf3c183dc5380ac",
    "Average ‚¨ÜÔ∏è": 21.183965357050457,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 61,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2791972698243055,
    "IFEval Raw": 0.3534599307614906,
    "IFEval": 35.34599307614906,
    "BBH Raw": 0.48938263759195594,
    "BBH": 27.914874953255538,
    "MATH Lvl 5 Raw": 0.12915407854984898,
    "MATH Lvl 5": 12.915407854984897,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.41914583333333333,
    "MUSR": 11.65989583333333,
    "MMLU-PRO Raw": 0.4050864361702128,
    "MMLU-PRO": 33.89849290780142,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-24",
    "Submission Date": "2024-07-10",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.3-Yi-1.5-34B-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.3-Yi-1.5-34B-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.3-Yi-1.5-34B-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.3-Yi-1.5-34B-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.3-Yi-1.5-34B-32k",
    "Model sha": "ff4eee6438194a670a95dff3118b5231eb568610",
    "Average ‚¨ÜÔ∏è": 27.07320641433773,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.245260527111792,
    "IFEval Raw": 0.3639266036339136,
    "IFEval": 36.39266036339136,
    "BBH Raw": 0.6046995537773227,
    "BBH": 43.40647565235176,
    "MATH Lvl 5 Raw": 0.16540785498489424,
    "MATH Lvl 5": 16.540785498489424,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.43105208333333334,
    "MUSR": 13.348177083333331,
    "MMLU-PRO Raw": 0.4630152925531915,
    "MMLU-PRO": 40.33503250591017,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-23",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "01-ai/Yi-1.5-34B-32k"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.3-mistral-7B-32k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-7B-32k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.3-mistral-7B-32k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.3-mistral-7B-32k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.3-mistral-7B-32k",
    "Model sha": "4f4273ee8e7930dd64e2c6121c79d12546b883e2",
    "Average ‚¨ÜÔ∏è": 19.37387218316181,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 44,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6000825804202548,
    "IFEval Raw": 0.4126362495955177,
    "IFEval": 41.26362495955176,
    "BBH Raw": 0.48125401481062013,
    "BBH": 26.906353891780515,
    "MATH Lvl 5 Raw": 0.052114803625377654,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.4642604166666667,
    "MUSR": 17.932552083333338,
    "MMLU-PRO Raw": 0.2820811170212766,
    "MMLU-PRO": 20.231235224586285,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-25",
    "Submission Date": "2024-07-04",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.3"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.3-mistral-nemo-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.3-mistral-nemo-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.3-mistral-nemo-12b",
    "Model sha": "7b535c900688fc836fbeebaeb7133910b09bafda",
    "Average ‚¨ÜÔ∏è": 24.68290415484145,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 84,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3751424816578615,
    "IFEval Raw": 0.5600894515441251,
    "IFEval": 56.008945154412515,
    "BBH Raw": 0.5480369183144175,
    "BBH": 36.08275865915292,
    "MATH Lvl 5 Raw": 0.05664652567975831,
    "MATH Lvl 5": 5.664652567975831,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.4429895833333333,
    "MUSR": 15.20703125,
    "MMLU-PRO Raw": 0.3376828457446808,
    "MMLU-PRO": 26.409205082742314,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-23",
    "Submission Date": "2024-07-26",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.4-gemma2-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.4-gemma2-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.4-gemma2-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.4-gemma2-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.4-gemma2-2b",
    "Model sha": "5c0854beb88a6711221771d1b13d51f733e6ca06",
    "Average ‚¨ÜÔ∏è": 9.797440973598183,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 31,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5112480112307456,
    "IFEval Raw": 0.08955127949396491,
    "IFEval": 8.955127949396491,
    "BBH Raw": 0.40813187411055213,
    "BBH": 17.3676325443774,
    "MATH Lvl 5 Raw": 0.04682779456193354,
    "MATH Lvl 5": 4.682779456193354,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.41796875,
    "MUSR": 10.912760416666666,
    "MMLU-PRO Raw": 0.2105219414893617,
    "MMLU-PRO": 12.28021572104019,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-08-25",
    "Generation": 1,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "cognitivecomputations_dolphin-2.9.4-llama3.1-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cognitivecomputations/dolphin-2.9.4-llama3.1-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cognitivecomputations/dolphin-2.9.4-llama3.1-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cognitivecomputations__dolphin-2.9.4-llama3.1-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cognitivecomputations/dolphin-2.9.4-llama3.1-8b",
    "Model sha": "7b73d1b7760bf9abac168de3d388b30d1ca1a138",
    "Average ‚¨ÜÔ∏è": 6.95562754717428,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 87,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7563130224417407,
    "IFEval Raw": 0.27572396796056686,
    "IFEval": 27.572396796056683,
    "BBH Raw": 0.35236263850832567,
    "BBH": 8.972088688921525,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.3236145833333333,
    "MUSR": 0.6184895833333329,
    "MMLU-PRO Raw": 0.12367021276595745,
    "MMLU-PRO": 2.6300236406619386,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-04",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "collaiborateorg_Collaiborator-MEDLLM-Llama-3-8B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/collaiborateorg/Collaiborator-MEDLLM-Llama-3-8B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">collaiborateorg/Collaiborator-MEDLLM-Llama-3-8B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/collaiborateorg__Collaiborator-MEDLLM-Llama-3-8B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "collaiborateorg/Collaiborator-MEDLLM-Llama-3-8B-v2",
    "Model sha": "2560556d655d0ecaefec10f579c92292d65fb28b",
    "Average ‚¨ÜÔ∏è": 17.951635338229444,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7057891347155406,
    "IFEval Raw": 0.380887157187374,
    "IFEval": 38.08871571873739,
    "BBH Raw": 0.46480279544898967,
    "BBH": 23.648503176108246,
    "MATH Lvl 5 Raw": 0.05740181268882175,
    "MATH Lvl 5": 5.740181268882175,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.3434270833333333,
    "MUSR": 1.5950520833333328,
    "MMLU-PRO Raw": 0.3480718085106383,
    "MMLU-PRO": 27.56353427895981,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "cstr_llama3.1-8b-spaetzle-v90_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cstr/llama3.1-8b-spaetzle-v90\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cstr/llama3.1-8b-spaetzle-v90</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cstr__llama3.1-8b-spaetzle-v90-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cstr/llama3.1-8b-spaetzle-v90",
    "Model sha": "717e5c3d31ed2465cd7cf927327adf677a9420b5",
    "Average ‚¨ÜÔ∏è": 27.830191204942878,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7789078930831386,
    "IFEval Raw": 0.7356192679867197,
    "IFEval": 73.56192679867198,
    "BBH Raw": 0.5302860633332208,
    "BBH": 32.76366579584106,
    "MATH Lvl 5 Raw": 0.14803625377643506,
    "MATH Lvl 5": 14.803625377643506,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.41343749999999996,
    "MUSR": 11.146354166666663,
    "MMLU-PRO Raw": 0.37308843085106386,
    "MMLU-PRO": 30.343158983451534,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "cstr/llama3.1-8b-spaetzle-v90 (Merge)"
  },
  {
    "eval_name": "cyberagent_calm3-22b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/cyberagent/calm3-22b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">cyberagent/calm3-22b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/cyberagent__calm3-22b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "cyberagent/calm3-22b-chat",
    "Model sha": "055922aa0f0fb1fbfbc97a2e31134532485ee99b",
    "Average ‚¨ÜÔ∏è": 21.375589663219504,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 67,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.774248118132194,
    "IFEval Raw": 0.509131327100981,
    "IFEval": 50.9131327100981,
    "BBH Raw": 0.4991683247746046,
    "BBH": 29.52088396885831,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.45532291666666663,
    "MUSR": 16.082031249999996,
    "MMLU-PRO Raw": 0.29496343085106386,
    "MMLU-PRO": 21.66260342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-01",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "cyberagent/calm3-22b-chat"
  },
  {
    "eval_name": "darkc0de_BuddyGlassNeverSleeps_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/darkc0de/BuddyGlassNeverSleeps\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">darkc0de/BuddyGlassNeverSleeps</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/darkc0de__BuddyGlassNeverSleeps-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "darkc0de/BuddyGlassNeverSleeps",
    "Model sha": "f8849498f02c94b68ef0308a7bf6637264949a7d",
    "Average ‚¨ÜÔ∏è": 19.845817827958424,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3541487741584415,
    "IFEval Raw": 0.4239019135892764,
    "IFEval": 42.390191358927645,
    "BBH Raw": 0.49772281653646816,
    "BBH": 28.477953494418696,
    "MATH Lvl 5 Raw": 0.06419939577039274,
    "MATH Lvl 5": 6.419939577039274,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3992708333333333,
    "MUSR": 8.608854166666669,
    "MMLU-PRO Raw": 0.34524601063829785,
    "MMLU-PRO": 27.249556737588648,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-16",
    "Generation": 1,
    "Base Model": "darkc0de/BuddyGlassNeverSleeps (Merge)"
  },
  {
    "eval_name": "darkc0de_BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/darkc0de/BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">darkc0de/BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/darkc0de__BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "darkc0de/BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp",
    "Model sha": "57367fefe01c7d9653c303b28449b416fc777d93",
    "Average ‚¨ÜÔ∏è": 22.265314856492438,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8981820462609266,
    "IFEval Raw": 0.43584245357872664,
    "IFEval": 43.58424535787267,
    "BBH Raw": 0.5243087998656722,
    "BBH": 31.869311081858005,
    "MATH Lvl 5 Raw": 0.12462235649546828,
    "MATH Lvl 5": 12.462235649546828,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4143333333333334,
    "MUSR": 9.491666666666669,
    "MMLU-PRO Raw": 0.36727061170212766,
    "MMLU-PRO": 29.696734633569736,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "darkc0de/BuddyGlass_v0.3_Xortron7MethedUpSwitchedUp (Merge)"
  },
  {
    "eval_name": "databricks_dbrx-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "DbrxForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/databricks/dbrx-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">databricks/dbrx-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/databricks__dbrx-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "databricks/dbrx-instruct",
    "Model sha": "c0a9245908c187da8f43a81e538e67ff360904ea",
    "Average ‚¨ÜÔ∏è": 25.19901027244322,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1103,
    "#Params (B)": 131,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 47.958027273119946,
    "IFEval Raw": 0.5415796752616391,
    "IFEval": 54.157967526163915,
    "BBH Raw": 0.5428960796934387,
    "BBH": 35.96381960359357,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.42692708333333335,
    "MUSR": 12.19921875,
    "MMLU-PRO Raw": 0.36826795212765956,
    "MMLU-PRO": 29.807550236406616,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-26",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "databricks/dbrx-instruct"
  },
  {
    "eval_name": "databricks_dolly-v1-6b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTJForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/databricks/dolly-v1-6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">databricks/dolly-v1-6b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/databricks__dolly-v1-6b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "databricks/dolly-v1-6b",
    "Model sha": "c9a85b3a322b402e20c839c702c725afe0cb454d",
    "Average ‚¨ÜÔ∏è": 6.9182911264755065,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 310,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6607799122762328,
    "IFEval Raw": 0.22244311759464885,
    "IFEval": 22.244311759464885,
    "BBH Raw": 0.3172089528774696,
    "BBH": 4.7813091701327,
    "MATH Lvl 5 Raw": 0.015105740181268885,
    "MATH Lvl 5": 1.5105740181268885,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.40041666666666664,
    "MUSR": 8.118750000000002,
    "MMLU-PRO Raw": 0.12657912234042554,
    "MMLU-PRO": 2.953235815602837,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-03-23",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "databricks/dolly-v1-6b"
  },
  {
    "eval_name": "databricks_dolly-v2-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/databricks/dolly-v2-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">databricks/dolly-v2-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/databricks__dolly-v2-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "databricks/dolly-v2-12b",
    "Model sha": "19308160448536e378e3db21a73a751579ee7fdd",
    "Average ‚¨ÜÔ∏è": 6.3830238203141,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1948,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3971194622796634,
    "IFEval Raw": 0.23550734273948679,
    "IFEval": 23.550734273948677,
    "BBH Raw": 0.33199731673771277,
    "BBH": 6.377894137452961,
    "MATH Lvl 5 Raw": 0.01435045317220544,
    "MATH Lvl 5": 1.435045317220544,
    "GPQA Raw": 0.2407718120805369,
    "GPQA": 0.0,
    "MUSR Raw": 0.37390625000000005,
    "MUSR": 5.504947916666668,
    "MMLU-PRO Raw": 0.11286569148936171,
    "MMLU-PRO": 1.4295212765957446,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-04-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "databricks/dolly-v2-12b"
  },
  {
    "eval_name": "databricks_dolly-v2-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/databricks/dolly-v2-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">databricks/dolly-v2-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/databricks__dolly-v2-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "databricks/dolly-v2-3b",
    "Model sha": "f6c9be08f16fe4d3a719bee0a4a7c7415b5c65df",
    "Average ‚¨ÜÔ∏è": 5.461188958075847,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 288,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7580844840777289,
    "IFEval Raw": 0.22471597583301195,
    "IFEval": 22.471597583301197,
    "BBH Raw": 0.30792785961544844,
    "BBH": 3.3247689565453875,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.33378125,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.11452792553191489,
    "MMLU-PRO": 1.6142139479905429,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-04-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "databricks/dolly-v2-3b"
  },
  {
    "eval_name": "databricks_dolly-v2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/databricks/dolly-v2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">databricks/dolly-v2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/databricks__dolly-v2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "databricks/dolly-v2-7b",
    "Model sha": "d632f0c8b75b1ae5b26b250d25bfba4e99cb7c6f",
    "Average ‚¨ÜÔ∏è": 5.571831773906653,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 148,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8302059935466812,
    "IFEval Raw": 0.2009856070781083,
    "IFEval": 20.098560707810833,
    "BBH Raw": 0.31730628122070326,
    "BBH": 5.449892512817211,
    "MATH Lvl 5 Raw": 0.009818731117824775,
    "MATH Lvl 5": 0.9818731117824775,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.35530208333333335,
    "MUSR": 2.779427083333333,
    "MMLU-PRO Raw": 0.1149434840425532,
    "MMLU-PRO": 1.6603871158392434,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-04-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "databricks/dolly-v2-7b"
  },
  {
    "eval_name": "davidkim205_Rhea-72b-v0.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/davidkim205/Rhea-72b-v0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">davidkim205/Rhea-72b-v0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/davidkim205__Rhea-72b-v0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "davidkim205/Rhea-72b-v0.5",
    "Model sha": "bc3806efb23d2713e6630a748d9747fd76b27169",
    "Average ‚¨ÜÔ∏è": 4.2240313745834674,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 134,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 8.688690924550217,
    "IFEval Raw": 0.014538092261865185,
    "IFEval": 1.4538092261865185,
    "BBH Raw": 0.30783395929068597,
    "BBH": 3.6707473002836024,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.42413541666666665,
    "MUSR": 11.316927083333333,
    "MMLU-PRO Raw": 0.11660571808510638,
    "MMLU-PRO": 1.8450797872340412,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-22",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "davidkim205/Rhea-72b-v0.5"
  },
  {
    "eval_name": "davidkim205_nox-solar-10.7b-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/davidkim205/nox-solar-10.7b-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">davidkim205/nox-solar-10.7b-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/davidkim205__nox-solar-10.7b-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "davidkim205/nox-solar-10.7b-v4",
    "Model sha": "5f4be6cb7d8398b84689148d15f3838f2e01e104",
    "Average ‚¨ÜÔ∏è": 18.489144848488248,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8489762559451299,
    "IFEval Raw": 0.3753418706809044,
    "IFEval": 37.534187068090446,
    "BBH Raw": 0.4814038018918371,
    "BBH": 26.631088145699618,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.42984375,
    "MUSR": 12.563802083333334,
    "MMLU-PRO Raw": 0.3332779255319149,
    "MMLU-PRO": 25.919769503546096,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-16",
    "Submission Date": "2024-10-04",
    "Generation": 0,
    "Base Model": "davidkim205/nox-solar-10.7b-v4"
  },
  {
    "eval_name": "deepseek-ai_deepseek-llm-67b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-67b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-67b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/deepseek-ai__deepseek-llm-67b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "deepseek-ai/deepseek-llm-67b-chat",
    "Model sha": "79648bef7658bb824e4630740f6e1484c1b0620b",
    "Average ‚¨ÜÔ∏è": 26.995928954293646,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 177,
    "#Params (B)": 67,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 59.82180866869628,
    "IFEval Raw": 0.5587153197959193,
    "IFEval": 55.87153197959192,
    "BBH Raw": 0.5243416179742358,
    "BBH": 33.22524192534525,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.5058645833333334,
    "MUSR": 23.93307291666666,
    "MMLU-PRO Raw": 0.3943650265957447,
    "MMLU-PRO": 32.70722517730496,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "deepseek-ai/deepseek-llm-67b-chat"
  },
  {
    "eval_name": "deepseek-ai_deepseek-llm-7b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-7b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/deepseek-ai__deepseek-llm-7b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "deepseek-ai/deepseek-llm-7b-base",
    "Model sha": "7683fea62db869066ddaff6a41d032262c490d4f",
    "Average ‚¨ÜÔ∏è": 8.138981617146595,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8225355894473253,
    "IFEval Raw": 0.217871913190335,
    "IFEval": 21.787191319033496,
    "BBH Raw": 0.35030315829299524,
    "BBH": 9.76792479590425,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.37378124999999995,
    "MUSR": 3.755989583333332,
    "MMLU-PRO Raw": 0.18060172872340424,
    "MMLU-PRO": 8.955747635933804,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "deepseek-ai/deepseek-llm-7b-base"
  },
  {
    "eval_name": "deepseek-ai_deepseek-llm-7b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-llm-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-llm-7b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/deepseek-ai__deepseek-llm-7b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "deepseek-ai/deepseek-llm-7b-chat",
    "Model sha": "afbda8b347ec881666061fa67447046fc5164ec8",
    "Average ‚¨ÜÔ∏è": 14.785392500233186,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 75,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7744825476156803,
    "IFEval Raw": 0.4170822307034225,
    "IFEval": 41.70822307034224,
    "BBH Raw": 0.3632079760108669,
    "BBH": 11.258949371501748,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.46677083333333336,
    "MUSR": 19.21302083333333,
    "MMLU-PRO Raw": 0.21334773936170212,
    "MMLU-PRO": 12.594193262411347,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "deepseek-ai/deepseek-llm-7b-chat"
  },
  {
    "eval_name": "deepseek-ai_deepseek-moe-16b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "DeepseekForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-moe-16b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/deepseek-ai__deepseek-moe-16b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "deepseek-ai/deepseek-moe-16b-base",
    "Model sha": "521d2bc4fb69a3f3ae565310fcc3b65f97af2580",
    "Average ‚¨ÜÔ∏è": 7.390805090753893,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 83,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.002465430903852,
    "IFEval Raw": 0.2449744455821664,
    "IFEval": 24.497444558216642,
    "BBH Raw": 0.3409461055246395,
    "BBH": 8.355555779389382,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.36578125,
    "MUSR": 3.3559895833333346,
    "MMLU-PRO Raw": 0.1505152925531915,
    "MMLU-PRO": 5.612810283687943,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "deepseek-ai/deepseek-moe-16b-base"
  },
  {
    "eval_name": "deepseek-ai_deepseek-moe-16b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "DeepseekForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">deepseek-ai/deepseek-moe-16b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/deepseek-ai__deepseek-moe-16b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "deepseek-ai/deepseek-moe-16b-chat",
    "Model sha": "eefd8ac7e8dc90e095129fe1a537d5e236b2e57c",
    "Average ‚¨ÜÔ∏è": 10.177322172973907,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 112,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.59347807294513,
    "IFEval Raw": 0.36629919724109805,
    "IFEval": 36.629919724109804,
    "BBH Raw": 0.3274953026448241,
    "BBH": 6.573749026890635,
    "MATH Lvl 5 Raw": 0.0188821752265861,
    "MATH Lvl 5": 1.8882175226586102,
    "GPQA Raw": 0.22483221476510068,
    "GPQA": 0.0,
    "MUSR Raw": 0.38076041666666666,
    "MUSR": 5.261718750000001,
    "MMLU-PRO Raw": 0.1963929521276596,
    "MMLU-PRO": 10.710328014184398,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-09",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "deepseek-ai/deepseek-moe-16b-chat"
  },
  {
    "eval_name": "dfurman_CalmeRys-78B-Orpo-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dfurman/CalmeRys-78B-Orpo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dfurman/CalmeRys-78B-Orpo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dfurman__CalmeRys-78B-Orpo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dfurman/CalmeRys-78B-Orpo-v0.1",
    "Model sha": "7988deb48419c3f56bb24c139c23e5c476ec03f8",
    "Average ‚¨ÜÔ∏è": 51.24391119284468,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 45,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.99676737803676,
    "IFEval Raw": 0.8163273447785211,
    "IFEval": 81.6327344778521,
    "BBH Raw": 0.7262282792249927,
    "BBH": 61.92476379259157,
    "MATH Lvl 5 Raw": 0.4070996978851964,
    "MATH Lvl 5": 40.70996978851964,
    "GPQA Raw": 0.4001677852348993,
    "GPQA": 20.022371364653242,
    "MUSR Raw": 0.5901770833333333,
    "MUSR": 36.37213541666666,
    "MMLU-PRO Raw": 0.7012134308510638,
    "MMLU-PRO": 66.80149231678487,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "dfurman/CalmeRys-78B-Orpo-v0.1 (Merge)"
  },
  {
    "eval_name": "dfurman_Llama-3-70B-Orpo-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dfurman/Llama-3-70B-Orpo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dfurman/Llama-3-70B-Orpo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dfurman__Llama-3-70B-Orpo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dfurman/Llama-3-70B-Orpo-v0.1",
    "Model sha": "6bf3be5f7f427164c879f7a4ec9ccb6b22aa6631",
    "Average ‚¨ÜÔ∏è": 18.174180301183593,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.440342717036888,
    "IFEval Raw": 0.20490742341431845,
    "IFEval": 20.490742341431847,
    "BBH Raw": 0.46552376347015506,
    "BBH": 24.09381654636037,
    "MATH Lvl 5 Raw": 0.15030211480362538,
    "MATH Lvl 5": 15.030211480362537,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.4534375,
    "MUSR": 16.2796875,
    "MMLU-PRO Raw": 0.38929521276595747,
    "MMLU-PRO": 32.14391252955083,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "dfurman/Llama-3-70B-Orpo-v0.1 (Merge)"
  },
  {
    "eval_name": "dfurman_Llama-3-8B-Orpo-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dfurman/Llama-3-8B-Orpo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dfurman/Llama-3-8B-Orpo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dfurman__Llama-3-8B-Orpo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dfurman/Llama-3-8B-Orpo-v0.1",
    "Model sha": "f02aef830e12a50892ac065826d5eb3dfc7675d1",
    "Average ‚¨ÜÔ∏è": 10.756011142762683,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9280793908584456,
    "IFEval Raw": 0.28351773294857646,
    "IFEval": 28.351773294857644,
    "BBH Raw": 0.3842420919898036,
    "BBH": 13.68074574746978,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3566354166666667,
    "MUSR": 2.2460937499999996,
    "MMLU-PRO Raw": 0.22980385638297873,
    "MMLU-PRO": 14.422650709219859,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "dfurman/Llama-3-8B-Orpo-v0.1 (Merge)"
  },
  {
    "eval_name": "dfurman_Llama-3-8B-Orpo-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dfurman/Llama-3-8B-Orpo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dfurman/Llama-3-8B-Orpo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dfurman__Llama-3-8B-Orpo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dfurman/Llama-3-8B-Orpo-v0.1",
    "Model sha": "f02aef830e12a50892ac065826d5eb3dfc7675d1",
    "Average ‚¨ÜÔ∏è": 11.076157946218345,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9498608388033825,
    "IFEval Raw": 0.3000039894147528,
    "IFEval": 30.000398941475282,
    "BBH Raw": 0.3852967582460245,
    "BBH": 13.773376256003464,
    "MATH Lvl 5 Raw": 0.041540785498489434,
    "MATH Lvl 5": 4.154078549848943,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.357875,
    "MUSR": 2.7343749999999996,
    "MMLU-PRO Raw": 0.22805851063829788,
    "MMLU-PRO": 14.22872340425532,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "dfurman/Llama-3-8B-Orpo-v0.1 (Merge)"
  },
  {
    "eval_name": "dfurman_Qwen2-72B-Orpo-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dfurman/Qwen2-72B-Orpo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dfurman/Qwen2-72B-Orpo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dfurman__Qwen2-72B-Orpo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dfurman/Qwen2-72B-Orpo-v0.1",
    "Model sha": "26c7bbaa728822c60bb47b2808972140653aae4c",
    "Average ‚¨ÜÔ∏è": 43.76948011240021,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 12.625331621172595,
    "IFEval Raw": 0.7879759039348928,
    "IFEval": 78.79759039348927,
    "BBH Raw": 0.6969024790545039,
    "BBH": 57.41436351018751,
    "MATH Lvl 5 Raw": 0.3814199395770393,
    "MATH Lvl 5": 38.14199395770393,
    "GPQA Raw": 0.38422818791946306,
    "GPQA": 17.897091722595075,
    "MUSR Raw": 0.47842708333333334,
    "MUSR": 20.87005208333333,
    "MMLU-PRO Raw": 0.5454621010638298,
    "MMLU-PRO": 49.49578900709219,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-05",
    "Submission Date": "2024-08-22",
    "Generation": 1,
    "Base Model": "dfurman/Qwen2-72B-Orpo-v0.1 (Merge)"
  },
  {
    "eval_name": "dicta-il_dictalm2.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dicta-il/dictalm2.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dicta-il/dictalm2.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dicta-il__dictalm2.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dicta-il/dictalm2.0",
    "Model sha": "f8ab3208e95a7b44a9a2fbb9bbbdd8ea11be509d",
    "Average ‚¨ÜÔ∏è": 11.88259722876987,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6740384443699406,
    "IFEval Raw": 0.24132745559559746,
    "IFEval": 24.132745559559744,
    "BBH Raw": 0.4017869112495909,
    "BBH": 16.48984561578202,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.38196874999999997,
    "MUSR": 5.512760416666668,
    "MMLU-PRO Raw": 0.2604720744680851,
    "MMLU-PRO": 17.830230496453904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-10",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "dicta-il/dictalm2.0"
  },
  {
    "eval_name": "dicta-il_dictalm2.0-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dicta-il/dictalm2.0-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dicta-il/dictalm2.0-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dicta-il__dictalm2.0-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dicta-il/dictalm2.0-instruct",
    "Model sha": "257c6023d6ac1bfa12110b7b17e7600da7da4e1e",
    "Average ‚¨ÜÔ∏è": 16.577811600535515,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6483948039518549,
    "IFEval Raw": 0.44121264910437635,
    "IFEval": 44.12126491043764,
    "BBH Raw": 0.42560784985912875,
    "BBH": 19.688075851194238,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.39458333333333334,
    "MUSR": 9.722916666666668,
    "MMLU-PRO Raw": 0.2604720744680851,
    "MMLU-PRO": 17.830230496453904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-14",
    "Submission Date": "2024-07-31",
    "Generation": 1,
    "Base Model": "dicta-il/dictalm2.0"
  },
  {
    "eval_name": "distilbert_distilgpt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/distilbert/distilgpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">distilbert/distilgpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/distilbert__distilgpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "distilbert/distilgpt2",
    "Model sha": "2290a62682d06624634c1f46a6ad5be0f47f38aa",
    "Average ‚¨ÜÔ∏è": 3.9015688926785734,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 444,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.12308154041412217,
    "IFEval Raw": 0.06110010328151527,
    "IFEval": 6.110010328151527,
    "BBH Raw": 0.3037988148650536,
    "BBH": 2.835219845513963,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.42072916666666665,
    "MUSR": 11.1578125,
    "MMLU-PRO Raw": 0.11868351063829788,
    "MMLU-PRO": 2.0759456264775418,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "distilbert/distilgpt2"
  },
  {
    "eval_name": "djuna_G2-BigGSHT-27B-2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/G2-BigGSHT-27B-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/G2-BigGSHT-27B-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__G2-BigGSHT-27B-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/G2-BigGSHT-27B-2",
    "Model sha": "b52e0c08d19232acebf85b68ee5989cc23c0d519",
    "Average ‚¨ÜÔ∏è": 32.132148579655784,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 27,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.025428975037113,
    "IFEval Raw": 0.7974430067775724,
    "IFEval": 79.74430067775725,
    "BBH Raw": 0.641474454273013,
    "BBH": 48.814372082405725,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.36325503355704697,
    "GPQA": 15.100671140939594,
    "MUSR Raw": 0.40720833333333334,
    "MUSR": 9.934374999999998,
    "MMLU-PRO Raw": 0.45279255319148937,
    "MMLU-PRO": 39.199172576832154,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "djuna/G2-BigGSHT-27B-2 (Merge)"
  },
  {
    "eval_name": "djuna_G2-GSHT_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/G2-GSHT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/G2-GSHT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__G2-GSHT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/G2-GSHT",
    "Model sha": "afa34f893a74af2a21b71f83d7bcc16aa818d157",
    "Average ‚¨ÜÔ∏è": 22.00131716911301,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.151691970002746,
    "IFEval Raw": 0.5630116978505919,
    "IFEval": 56.301169785059194,
    "BBH Raw": 0.5269730491270207,
    "BBH": 30.992059015125676,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.40057291666666667,
    "MUSR": 8.171614583333332,
    "MMLU-PRO Raw": 0.3070146276595745,
    "MMLU-PRO": 23.001625295508276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "djuna/G2-GSHT (Merge)"
  },
  {
    "eval_name": "djuna_Gemma-2-gemmama-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/Gemma-2-gemmama-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/Gemma-2-gemmama-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__Gemma-2-gemmama-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/Gemma-2-gemmama-9b",
    "Model sha": "1d6c53ad18970ac082e86bfa0159789b6a6e79c0",
    "Average ‚¨ÜÔ∏è": 25.54250714906874,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7640972150750645,
    "IFEval Raw": 0.7703404743857409,
    "IFEval": 77.0340474385741,
    "BBH Raw": 0.5420037856495951,
    "BBH": 32.916050576064585,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.4031458333333333,
    "MUSR": 8.459895833333333,
    "MMLU-PRO Raw": 0.3109208776595745,
    "MMLU-PRO": 23.43565307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-31",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "djuna/Gemma-2-gemmama-9b (Merge)"
  },
  {
    "eval_name": "djuna_L3.1-ForStHS_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/L3.1-ForStHS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/L3.1-ForStHS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__L3.1-ForStHS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/L3.1-ForStHS",
    "Model sha": "f5442e1f27e4a0c469504624ea85afdc6907c9cc",
    "Average ‚¨ÜÔ∏è": 28.272627712063684,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.84366432365315,
    "IFEval Raw": 0.7813313120298586,
    "IFEval": 78.13313120298585,
    "BBH Raw": 0.5202703381267152,
    "BBH": 31.3912168031268,
    "MATH Lvl 5 Raw": 0.14577039274924472,
    "MATH Lvl 5": 14.577039274924472,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.40264583333333337,
    "MUSR": 9.664062500000002,
    "MMLU-PRO Raw": 0.37350398936170215,
    "MMLU-PRO": 30.38933215130024,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "djuna/L3.1-ForStHS (Merge)"
  },
  {
    "eval_name": "djuna_L3.1-Promissum_Mane-8B-Della-1.5-calc_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/L3.1-Promissum_Mane-8B-Della-1.5-calc\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/L3.1-Promissum_Mane-8B-Della-1.5-calc</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__L3.1-Promissum_Mane-8B-Della-1.5-calc-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/L3.1-Promissum_Mane-8B-Della-1.5-calc",
    "Model sha": "67dc71cb877c1ebaeb634e116fc938b223338cf6",
    "Average ‚¨ÜÔ∏è": 29.184843283993796,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7441530391636442,
    "IFEval Raw": 0.7235291249440374,
    "IFEval": 72.35291249440374,
    "BBH Raw": 0.5432920704935255,
    "BBH": 34.87957646776231,
    "MATH Lvl 5 Raw": 0.13972809667673716,
    "MATH Lvl 5": 13.972809667673717,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.42528125,
    "MUSR": 13.026822916666665,
    "MMLU-PRO Raw": 0.390375664893617,
    "MMLU-PRO": 32.263962765957444,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "djuna/L3.1-Promissum_Mane-8B-Della-1.5-calc (Merge)"
  },
  {
    "eval_name": "djuna_L3.1-Promissum_Mane-8B-Della-calc_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/L3.1-Promissum_Mane-8B-Della-calc\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/L3.1-Promissum_Mane-8B-Della-calc</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__L3.1-Promissum_Mane-8B-Della-calc-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/L3.1-Promissum_Mane-8B-Della-calc",
    "Model sha": "42c6cd88b8394876cdbcf64e56633ad0a371b5f4",
    "Average ‚¨ÜÔ∏è": 23.41729956623631,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8227195310138873,
    "IFEval Raw": 0.544152847777231,
    "IFEval": 54.4152847777231,
    "BBH Raw": 0.548587625935678,
    "BBH": 35.553825960108405,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.4229895833333333,
    "MUSR": 12.80703125,
    "MMLU-PRO Raw": 0.3801529255319149,
    "MMLU-PRO": 31.128102836879428,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "djuna/L3.1-Promissum_Mane-8B-Della-calc (Merge)"
  },
  {
    "eval_name": "djuna_L3.1-Purosani-2-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/L3.1-Purosani-2-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/L3.1-Purosani-2-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__L3.1-Purosani-2-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/L3.1-Purosani-2-8B",
    "Model sha": "e5acd6277a1286c5e18fcb3e89a836ffc8a75b8f",
    "Average ‚¨ÜÔ∏è": 23.0504333980543,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8649517979081178,
    "IFEval Raw": 0.4988153654525548,
    "IFEval": 49.88153654525547,
    "BBH Raw": 0.5182122256069372,
    "BBH": 31.391342665184258,
    "MATH Lvl 5 Raw": 0.11329305135951663,
    "MATH Lvl 5": 11.329305135951664,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.38162499999999994,
    "MUSR": 8.303124999999996,
    "MMLU-PRO Raw": 0.3751662234042553,
    "MMLU-PRO": 30.57402482269504,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "djuna/L3.1-Purosani-2-8B (Merge)"
  },
  {
    "eval_name": "djuna_L3.1-Suze-Vume-calc_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/L3.1-Suze-Vume-calc\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/L3.1-Suze-Vume-calc</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__L3.1-Suze-Vume-calc-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/L3.1-Suze-Vume-calc",
    "Model sha": "830c07d136ecd8171805078606f00c4ee69f21c3",
    "Average ‚¨ÜÔ∏è": 25.975607726314845,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8045188499112532,
    "IFEval Raw": 0.7296739318341999,
    "IFEval": 72.96739318341999,
    "BBH Raw": 0.516421105092519,
    "BBH": 31.136638199988273,
    "MATH Lvl 5 Raw": 0.11253776435045318,
    "MATH Lvl 5": 11.253776435045317,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.38429166666666664,
    "MUSR": 8.303124999999996,
    "MMLU-PRO Raw": 0.35147938829787234,
    "MMLU-PRO": 27.942154255319146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "djuna/L3.1-Suze-Vume-calc (Merge)"
  },
  {
    "eval_name": "djuna_MN-Chinofun_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/MN-Chinofun\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/MN-Chinofun</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__MN-Chinofun-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/MN-Chinofun",
    "Model sha": "71b47c86f32e107b407fada44ec6b893c5eb8bb0",
    "Average ‚¨ÜÔ∏è": 24.36913109850897,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4464930842733161,
    "IFEval Raw": 0.6110220880596817,
    "IFEval": 61.102208805968175,
    "BBH Raw": 0.49527033812671534,
    "BBH": 28.48357519092637,
    "MATH Lvl 5 Raw": 0.11178247734138973,
    "MATH Lvl 5": 11.178247734138973,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40835416666666663,
    "MUSR": 10.377604166666663,
    "MMLU-PRO Raw": 0.36028922872340424,
    "MMLU-PRO": 28.921025413711575,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "djuna/MN-Chinofun (Merge)"
  },
  {
    "eval_name": "djuna_Q2.5-Partron-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna/Q2.5-Partron-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna/Q2.5-Partron-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna__Q2.5-Partron-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna/Q2.5-Partron-7B",
    "Model sha": "3a6d3cca23c0e1c6bcba38887fc819729d5d16cf",
    "Average ‚¨ÜÔ∏è": 27.077247509364152,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.334762639354905,
    "IFEval Raw": 0.7321218810533828,
    "IFEval": 73.21218810533829,
    "BBH Raw": 0.5418474850726388,
    "BBH": 35.25726531667357,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.41654166666666664,
    "MUSR": 11.067708333333336,
    "MMLU-PRO Raw": 0.4282746010638298,
    "MMLU-PRO": 36.47495567375886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "djuna/Q2.5-Partron-7B (Merge)"
  },
  {
    "eval_name": "djuna-test-lab_TEST-L3.2-ReWish-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna-test-lab/TEST-L3.2-ReWish-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna-test-lab/TEST-L3.2-ReWish-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna-test-lab__TEST-L3.2-ReWish-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna-test-lab/TEST-L3.2-ReWish-3B",
    "Model sha": "0cb7d434c4647faed475f17d74e9047007cd3782",
    "Average ‚¨ÜÔ∏è": 22.445512437637706,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6406309009620793,
    "IFEval Raw": 0.6367759766308949,
    "IFEval": 63.677597663089486,
    "BBH Raw": 0.449540552927623,
    "BBH": 22.066700432422255,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.37775,
    "MUSR": 7.918749999999996,
    "MMLU-PRO Raw": 0.31258311170212766,
    "MMLU-PRO": 23.620345744680847,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "djuna-test-lab/TEST-L3.2-ReWish-3B (Merge)"
  },
  {
    "eval_name": "djuna-test-lab_TEST-L3.2-ReWish-3B-ties-w-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/djuna-test-lab/TEST-L3.2-ReWish-3B-ties-w-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">djuna-test-lab/TEST-L3.2-ReWish-3B-ties-w-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/djuna-test-lab__TEST-L3.2-ReWish-3B-ties-w-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "djuna-test-lab/TEST-L3.2-ReWish-3B-ties-w-base",
    "Model sha": "ebab6c0266ae7846b2bb9a595a2651a23b031372",
    "Average ‚¨ÜÔ∏è": 22.42011685761374,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.28137393932878,
    "IFEval Raw": 0.635252241829457,
    "IFEval": 63.5252241829457,
    "BBH Raw": 0.449540552927623,
    "BBH": 22.066700432422255,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.37775,
    "MUSR": 7.918749999999996,
    "MMLU-PRO Raw": 0.31258311170212766,
    "MMLU-PRO": 23.620345744680847,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "djuna-test-lab/TEST-L3.2-ReWish-3B-ties-w-base (Merge)"
  },
  {
    "eval_name": "dnhkng_RYS-Medium_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Medium\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Medium</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Medium-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Medium",
    "Model sha": "de09a79e6b2efdcc97490a37b770764e62749fd0",
    "Average ‚¨ÜÔ∏è": 25.94422727612673,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 18,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.1363783500494002,
    "IFEval Raw": 0.4406131287206833,
    "IFEval": 44.06131287206833,
    "BBH Raw": 0.6284726872432828,
    "BBH": 47.734201324861516,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.40692708333333333,
    "MUSR": 8.732552083333331,
    "MMLU-PRO Raw": 0.4325964095744681,
    "MMLU-PRO": 36.95515661938535,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-07-17",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Medium"
  },
  {
    "eval_name": "dnhkng_RYS-Llama-3-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Llama-3-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Llama-3-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Llama-3-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Llama-3-8B-Instruct",
    "Model sha": "293ab00d1e2be2752f97d5568fde2b09f6a1caae",
    "Average ‚¨ÜÔ∏è": 21.91018661983311,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8051873277794882,
    "IFEval Raw": 0.6957772044841022,
    "IFEval": 69.57772044841022,
    "BBH Raw": 0.4808708123069005,
    "BBH": 25.373015462245586,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.33834375,
    "MUSR": 0.29296874999999956,
    "MMLU-PRO Raw": 0.355718085106383,
    "MMLU-PRO": 28.413120567375884,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "dnhkng_RYS-Llama-3-Huge-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Llama-3-Huge-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Llama-3-Huge-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Llama-3-Huge-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Llama-3-Huge-Instruct",
    "Model sha": "cfe14a5339e88a7a89f075d9d48215d45f64acaf",
    "Average ‚¨ÜÔ∏è": 34.68177025604325,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 99,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.736988145025224,
    "IFEval Raw": 0.7685917809190725,
    "IFEval": 76.85917809190724,
    "BBH Raw": 0.6480872171360044,
    "BBH": 49.07372077223325,
    "MATH Lvl 5 Raw": 0.23111782477341392,
    "MATH Lvl 5": 23.111782477341393,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.4207604166666667,
    "MUSR": 11.92838541666667,
    "MMLU-PRO Raw": 0.510970744680851,
    "MMLU-PRO": 45.66341607565011,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Llama-3-Huge-Instruct"
  },
  {
    "eval_name": "dnhkng_RYS-Llama-3-Large-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Llama-3-Large-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Llama-3-Large-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Llama-3-Large-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Llama-3-Large-Instruct",
    "Model sha": "01e3208aaf7bf6d2b09737960c701ec6628977fe",
    "Average ‚¨ÜÔ∏è": 36.094509186487265,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 73,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.811516941604658,
    "IFEval Raw": 0.8050616807847621,
    "IFEval": 80.50616807847622,
    "BBH Raw": 0.65252690724939,
    "BBH": 49.665539028891345,
    "MATH Lvl 5 Raw": 0.23716012084592145,
    "MATH Lvl 5": 23.716012084592144,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.41803125,
    "MUSR": 11.453906250000003,
    "MMLU-PRO Raw": 0.5137134308510638,
    "MMLU-PRO": 45.96815898345154,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Llama-3-Large-Instruct"
  },
  {
    "eval_name": "dnhkng_RYS-Llama-3.1-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Llama-3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Llama-3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Llama-3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Llama-3.1-8B-Instruct",
    "Model sha": "d4e2393403dcae19860da7c29519c8fe6fbf2fad",
    "Average ‚¨ÜÔ∏è": 26.65066216362906,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9716719482659708,
    "IFEval Raw": 0.7684920455502511,
    "IFEval": 76.84920455502511,
    "BBH Raw": 0.5163645317446665,
    "BBH": 31.085445296018978,
    "MATH Lvl 5 Raw": 0.12613293051359514,
    "MATH Lvl 5": 12.613293051359515,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3681041666666667,
    "MUSR": 7.679687500000001,
    "MMLU-PRO Raw": 0.36394614361702127,
    "MMLU-PRO": 29.32734929078014,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-08",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Llama-3.1-8B-Instruct"
  },
  {
    "eval_name": "dnhkng_RYS-Llama3.1-Large_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Llama3.1-Large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Llama3.1-Large</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Llama3.1-Large-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Llama3.1-Large",
    "Model sha": "52cc979de78155b33689efa48f52a8aab184bd86",
    "Average ‚¨ÜÔ∏è": 41.937416384364845,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 81,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 15.40632892363376,
    "IFEval Raw": 0.8492001223420524,
    "IFEval": 84.92001223420525,
    "BBH Raw": 0.6899112229777242,
    "BBH": 55.414864048196534,
    "MATH Lvl 5 Raw": 0.304380664652568,
    "MATH Lvl 5": 30.438066465256803,
    "GPQA Raw": 0.37416107382550334,
    "GPQA": 16.554809843400445,
    "MUSR Raw": 0.4553958333333334,
    "MUSR": 17.091145833333332,
    "MMLU-PRO Raw": 0.5248503989361702,
    "MMLU-PRO": 47.205599881796694,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-11",
    "Submission Date": "2024-08-22",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Llama3.1-Large"
  },
  {
    "eval_name": "dnhkng_RYS-Phi-3-medium-4k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-Phi-3-medium-4k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-Phi-3-medium-4k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-Phi-3-medium-4k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-Phi-3-medium-4k-instruct",
    "Model sha": "1009e916b1ff8c9a53bc9d8ff48bea2a15ccde26",
    "Average ‚¨ÜÔ∏è": 28.464284108683696,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 17,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3105465366832307,
    "IFEval Raw": 0.4391392616036561,
    "IFEval": 43.913926160365605,
    "BBH Raw": 0.6226313539198264,
    "BBH": 46.748970518349154,
    "MATH Lvl 5 Raw": 0.12311178247734139,
    "MATH Lvl 5": 12.311178247734139,
    "GPQA Raw": 0.3548657718120805,
    "GPQA": 13.982102908277403,
    "MUSR Raw": 0.42528125,
    "MUSR": 11.093489583333332,
    "MMLU-PRO Raw": 0.484624335106383,
    "MMLU-PRO": 42.73603723404255,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-06",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-Phi-3-medium-4k-instruct"
  },
  {
    "eval_name": "dnhkng_RYS-XLarge_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-XLarge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-XLarge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-XLarge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-XLarge",
    "Model sha": "0f84dd9dde60f383e1e2821496befb4ce9a11ef6",
    "Average ‚¨ÜÔ∏è": 45.13122176315489,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 78,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.57608287594545,
    "IFEval Raw": 0.7995662619627034,
    "IFEval": 79.95662619627035,
    "BBH Raw": 0.7050033079850099,
    "BBH": 58.77356748233938,
    "MATH Lvl 5 Raw": 0.4123867069486405,
    "MATH Lvl 5": 41.23867069486405,
    "GPQA Raw": 0.38422818791946306,
    "GPQA": 17.897091722595075,
    "MUSR Raw": 0.49696875,
    "MUSR": 23.721093749999998,
    "MMLU-PRO Raw": 0.5428025265957447,
    "MMLU-PRO": 49.20028073286053,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-XLarge"
  },
  {
    "eval_name": "dnhkng_RYS-XLarge-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-XLarge-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-XLarge-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-XLarge-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-XLarge-base",
    "Model sha": "c718b3d9e24916e3b0347d3fdaa5e5a097c2f603",
    "Average ‚¨ÜÔ∏è": 43.970954532140375,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 77,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.58752366167113,
    "IFEval Raw": 0.7910233735377686,
    "IFEval": 79.10233735377686,
    "BBH Raw": 0.7047291858548728,
    "BBH": 58.692146076576385,
    "MATH Lvl 5 Raw": 0.3716012084592145,
    "MATH Lvl 5": 37.160120845921455,
    "GPQA Raw": 0.37919463087248323,
    "GPQA": 17.225950782997764,
    "MUSR Raw": 0.4902708333333334,
    "MUSR": 22.4171875,
    "MMLU-PRO Raw": 0.5430518617021277,
    "MMLU-PRO": 49.22798463356975,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-02",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "dnhkng/RYS-XLarge-base"
  },
  {
    "eval_name": "dnhkng_RYS-XLarge2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dnhkng/RYS-XLarge2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dnhkng/RYS-XLarge2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dnhkng__RYS-XLarge2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dnhkng/RYS-XLarge2",
    "Model sha": "3ce16c9427e93e09ce10a28fa644469d49a51113",
    "Average ‚¨ÜÔ∏è": 35.00187554079472,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 77,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.375884555218812,
    "IFEval Raw": 0.49019712141562166,
    "IFEval": 49.01971214156217,
    "BBH Raw": 0.6573947106260754,
    "BBH": 51.54993579817892,
    "MATH Lvl 5 Raw": 0.2719033232628399,
    "MATH Lvl 5": 27.19033232628399,
    "GPQA Raw": 0.37416107382550334,
    "GPQA": 16.554809843400445,
    "MUSR Raw": 0.4508020833333333,
    "MUSR": 17.050260416666664,
    "MMLU-PRO Raw": 0.5378158244680851,
    "MMLU-PRO": 48.646202718676115,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dreamgen_WizardLM-2-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dreamgen/WizardLM-2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dreamgen/WizardLM-2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dreamgen__WizardLM-2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dreamgen/WizardLM-2-7B",
    "Model sha": "b5f2d7bff91445a47331dcce588aee009d11d255",
    "Average ‚¨ÜÔ∏è": 14.827190126716792,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 37,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5667250020717028,
    "IFEval Raw": 0.45829842595424586,
    "IFEval": 45.82984259542458,
    "BBH Raw": 0.34867856163972016,
    "BBH": 9.213113542615597,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.39409374999999996,
    "MUSR": 7.528385416666667,
    "MMLU-PRO Raw": 0.2660405585106383,
    "MMLU-PRO": 18.44895094562648,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-16",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "dreamgen/WizardLM-2-7B"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v1",
    "Model sha": "e96bd9694ae87a4f612825310eb7afaea5b0aa28",
    "Average ‚¨ÜÔ∏è": 17.34065066831643,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8911415755233946,
    "IFEval Raw": 0.3657750324694034,
    "IFEval": 36.57750324694034,
    "BBH Raw": 0.4663596290293861,
    "BBH": 24.109958157326172,
    "MATH Lvl 5 Raw": 0.1148036253776435,
    "MATH Lvl 5": 11.48036253776435,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.3753958333333333,
    "MUSR": 4.824479166666667,
    "MMLU-PRO Raw": 0.3384308510638298,
    "MMLU-PRO": 26.492316784869978,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v2",
    "Model sha": "817408ebfaa7ba0ea9433e1de4bfa120d38d2a0f",
    "Average ‚¨ÜÔ∏è": 18.364750624036642,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9403697250383827,
    "IFEval Raw": 0.3912042270065648,
    "IFEval": 39.120422700656476,
    "BBH Raw": 0.47238018945807153,
    "BBH": 24.892196306273934,
    "MATH Lvl 5 Raw": 0.12160120845921452,
    "MATH Lvl 5": 12.160120845921451,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.3526354166666667,
    "MUSR": 4.912760416666669,
    "MMLU-PRO Raw": 0.3377659574468085,
    "MMLU-PRO": 26.418439716312054,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v3",
    "Model sha": "dcfa1a6a9f94a099286891d732b17cbbe97a644e",
    "Average ‚¨ÜÔ∏è": 20.50026525415772,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8914670888045706,
    "IFEval Raw": 0.536733644507684,
    "IFEval": 53.6733644507684,
    "BBH Raw": 0.4658310598309874,
    "BBH": 24.168293247720744,
    "MATH Lvl 5 Raw": 0.12084592145015106,
    "MATH Lvl 5": 12.084592145015106,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.35117708333333336,
    "MUSR": 4.763802083333334,
    "MMLU-PRO Raw": 0.35480385638297873,
    "MMLU-PRO": 28.311539598108748,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v4",
    "Model sha": "81e20c2e40f2028818d5d6d27ec9e0d503ae8cc1",
    "Average ‚¨ÜÔ∏è": 18.53093853220701,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8852697548808296,
    "IFEval Raw": 0.4697890486132351,
    "IFEval": 46.978904861323514,
    "BBH Raw": 0.46860140660011185,
    "BBH": 24.331770038797558,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.23406040268456377,
    "GPQA": 0.0,
    "MUSR Raw": 0.33930208333333334,
    "MUSR": 3.0460937500000003,
    "MMLU-PRO Raw": 0.3390126329787234,
    "MMLU-PRO": 26.556959219858157,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v5",
    "Model sha": "12970eec99f458a3982eb502b71b6df0bc74bb52",
    "Average ‚¨ÜÔ∏è": 18.58662220539731,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9130963052727826,
    "IFEval Raw": 0.42375231053604434,
    "IFEval": 42.37523105360444,
    "BBH Raw": 0.4781685533183147,
    "BBH": 25.195784260224865,
    "MATH Lvl 5 Raw": 0.12462235649546827,
    "MATH Lvl 5": 12.462235649546827,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.33536458333333335,
    "MUSR": 4.053906250000002,
    "MMLU-PRO Raw": 0.3217253989361702,
    "MMLU-PRO": 24.63615543735224,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v6_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v6",
    "Model sha": "a0b30a21a8eea9a32a2767755dc2dbd44eeb383f",
    "Average ‚¨ÜÔ∏è": 20.445597345618093,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8992028739037912,
    "IFEval Raw": 0.4938939790866014,
    "IFEval": 49.38939790866013,
    "BBH Raw": 0.4809537068664902,
    "BBH": 26.116102641092812,
    "MATH Lvl 5 Raw": 0.13595166163141995,
    "MATH Lvl 5": 13.595166163141995,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3753333333333333,
    "MUSR": 4.3500000000000005,
    "MMLU-PRO Raw": 0.347905585106383,
    "MMLU-PRO": 27.54506501182033,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dustinwloring1988_Reflexis-8b-chat-v7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dustinwloring1988/Reflexis-8b-chat-v7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dustinwloring1988/Reflexis-8b-chat-v7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dustinwloring1988__Reflexis-8b-chat-v7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dustinwloring1988/Reflexis-8b-chat-v7",
    "Model sha": "e8d990012ccd855e65d51cb7cfd1762632a8f217",
    "Average ‚¨ÜÔ∏è": 18.843738739478216,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9021110151511103,
    "IFEval Raw": 0.39804828964924177,
    "IFEval": 39.804828964924184,
    "BBH Raw": 0.4809830787114964,
    "BBH": 25.98749682684877,
    "MATH Lvl 5 Raw": 0.14803625377643506,
    "MATH Lvl 5": 14.803625377643506,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.32215625,
    "MUSR": 1.5361979166666664,
    "MMLU-PRO Raw": 0.3642785904255319,
    "MMLU-PRO": 29.364287825059105,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-14",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dwikitheduck_gemma-2-2b-id_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dwikitheduck/gemma-2-2b-id\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dwikitheduck/gemma-2-2b-id</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dwikitheduck__gemma-2-2b-id-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dwikitheduck/gemma-2-2b-id",
    "Model sha": "6f191d4a7618664619adda1cd96d9d1bf72f33b2",
    "Average ‚¨ÜÔ∏è": 14.182478041881888,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.599594167111726,
    "IFEval Raw": 0.38785644312646006,
    "IFEval": 38.785644312646,
    "BBH Raw": 0.39621721241423097,
    "BBH": 15.415129369168449,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.41542708333333334,
    "MUSR": 10.728385416666667,
    "MMLU-PRO Raw": 0.21733710106382978,
    "MMLU-PRO": 13.037455673758863,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 0,
    "Base Model": "dwikitheduck/gemma-2-2b-id"
  },
  {
    "eval_name": "dwikitheduck_gemma-2-2b-id-instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dwikitheduck/gemma-2-2b-id-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dwikitheduck/gemma-2-2b-id-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dwikitheduck__gemma-2-2b-id-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dwikitheduck/gemma-2-2b-id-instruct",
    "Model sha": "1c046ade199128da926004e154698546d65e3084",
    "Average ‚¨ÜÔ∏è": 14.182478041881888,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4169074342609027,
    "IFEval Raw": 0.38785644312646006,
    "IFEval": 38.785644312646,
    "BBH Raw": 0.39621721241423097,
    "BBH": 15.415129369168449,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.41542708333333334,
    "MUSR": 10.728385416666667,
    "MMLU-PRO Raw": 0.21733710106382978,
    "MMLU-PRO": 13.037455673758863,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-11-15",
    "Generation": 0,
    "Base Model": "dwikitheduck/gemma-2-2b-id-instruct"
  },
  {
    "eval_name": "dwikitheduck_gen-try1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dwikitheduck/gen-try1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dwikitheduck/gen-try1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dwikitheduck__gen-try1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dwikitheduck/gen-try1",
    "Model sha": "9c2cab728518e179e5d8891f3f9775515f15cea2",
    "Average ‚¨ÜÔ∏è": 34.83005268129006,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5830805553767415,
    "IFEval Raw": 0.7522052598217175,
    "IFEval": 75.22052598217175,
    "BBH Raw": 0.6358510933470735,
    "BBH": 47.41312903142858,
    "MATH Lvl 5 Raw": 0.1351963746223565,
    "MATH Lvl 5": 13.51963746223565,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.4415625,
    "MUSR": 14.961979166666664,
    "MMLU-PRO Raw": 0.5110538563829787,
    "MMLU-PRO": 45.67265070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "dwikitheduck/gen-try1 (Merge)"
  },
  {
    "eval_name": "dwikitheduck_gen-try1-notemp_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dwikitheduck/gen-try1-notemp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dwikitheduck/gen-try1-notemp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dwikitheduck__gen-try1-notemp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dwikitheduck/gen-try1-notemp",
    "Model sha": "391925b02f6cd60e7c4ef1321fe89a92d6b9fdf0",
    "Average ‚¨ÜÔ∏è": 29.66918471740927,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8956058795792776,
    "IFEval Raw": 0.26270961050013963,
    "IFEval": 26.270961050013963,
    "BBH Raw": 0.626267088306491,
    "BBH": 45.749092669505465,
    "MATH Lvl 5 Raw": 0.27416918429003023,
    "MATH Lvl 5": 27.416918429003022,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.47141666666666665,
    "MUSR": 17.927083333333332,
    "MMLU-PRO Raw": 0.5210272606382979,
    "MMLU-PRO": 46.780806737588655,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "dzakwan_dzakwan-MoE-4x7b-Beta_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/dzakwan/dzakwan-MoE-4x7b-Beta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">dzakwan/dzakwan-MoE-4x7b-Beta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/dzakwan__dzakwan-MoE-4x7b-Beta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "dzakwan/dzakwan-MoE-4x7b-Beta",
    "Model sha": "e89f82f2afa1961335de5a6d6d05bd850d1d61d9",
    "Average ‚¨ÜÔ∏è": 20.75671493642371,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4560283065171231,
    "IFEval Raw": 0.44426011870725235,
    "IFEval": 44.42601187072523,
    "BBH Raw": 0.514044131159397,
    "BBH": 32.074208465442545,
    "MATH Lvl 5 Raw": 0.07703927492447131,
    "MATH Lvl 5": 7.703927492447131,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.42673958333333334,
    "MUSR": 12.109114583333335,
    "MMLU-PRO Raw": 0.3107546542553192,
    "MMLU-PRO": 23.417183806146575,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "dzakwan/dzakwan-MoE-4x7b-Beta (Merge)"
  },
  {
    "eval_name": "ehristoforu_Gemma2-9B-it-psy10k-mental_health_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ehristoforu/Gemma2-9B-it-psy10k-mental_health\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ehristoforu/Gemma2-9B-it-psy10k-mental_health</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ehristoforu__Gemma2-9B-it-psy10k-mental_health-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ehristoforu/Gemma2-9B-it-psy10k-mental_health",
    "Model sha": "4adc2d61d530d23026493d29e6191e06cf549fc6",
    "Average ‚¨ÜÔ∏è": 26.76449350554223,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.276830182919858,
    "IFEval Raw": 0.5886658510529839,
    "IFEval": 58.866585105298384,
    "BBH Raw": 0.5539376944027642,
    "BBH": 35.56600949863266,
    "MATH Lvl 5 Raw": 0.13746223564954685,
    "MATH Lvl 5": 13.746223564954684,
    "GPQA Raw": 0.337248322147651,
    "GPQA": 11.633109619686799,
    "MUSR Raw": 0.40860416666666666,
    "MUSR": 9.3421875,
    "MMLU-PRO Raw": 0.38289561170212766,
    "MMLU-PRO": 31.432845744680847,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-07-31",
    "Generation": 2,
    "Base Model": "unsloth/gemma-2-9b-it-bnb-4bit"
  },
  {
    "eval_name": "ehristoforu_Gemma2-9b-it-train6_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ehristoforu/Gemma2-9b-it-train6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ehristoforu/Gemma2-9b-it-train6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ehristoforu__Gemma2-9b-it-train6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ehristoforu/Gemma2-9b-it-train6",
    "Model sha": "e72bf00b427c22c48b468818cf75300a373a0c8a",
    "Average ‚¨ÜÔ∏è": 28.89753155323267,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9936834743962102,
    "IFEval Raw": 0.7025215317579578,
    "IFEval": 70.25215317579578,
    "BBH Raw": 0.5898092579133603,
    "BBH": 40.98762530159646,
    "MATH Lvl 5 Raw": 0.09290030211480363,
    "MATH Lvl 5": 9.290030211480364,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.40841666666666665,
    "MUSR": 9.652083333333337,
    "MMLU-PRO Raw": 0.39419880319148937,
    "MMLU-PRO": 32.68875591016548,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-22",
    "Submission Date": "2024-07-31",
    "Generation": 6,
    "Base Model": "unsloth/gemma-2-9b-it-bnb-4bit"
  },
  {
    "eval_name": "elinas_Chronos-Gold-12B-1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/elinas/Chronos-Gold-12B-1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">elinas/Chronos-Gold-12B-1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/elinas__Chronos-Gold-12B-1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "elinas/Chronos-Gold-12B-1.0",
    "Model sha": "cf76a4621b9dfc0c2e6d930756e6c7c9ce2b260b",
    "Average ‚¨ÜÔ∏è": 21.48828879438861,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 31,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.502531064915665,
    "IFEval Raw": 0.3165656014929277,
    "IFEval": 31.656560149292766,
    "BBH Raw": 0.5514664110708439,
    "BBH": 35.90894700063131,
    "MATH Lvl 5 Raw": 0.049093655589123875,
    "MATH Lvl 5": 4.909365558912388,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.47398958333333335,
    "MUSR": 19.415364583333332,
    "MMLU-PRO Raw": 0.351811835106383,
    "MMLU-PRO": 27.979092789598102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "euclaise_ReMask-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/euclaise/ReMask-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">euclaise/ReMask-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/euclaise__ReMask-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "euclaise/ReMask-3B",
    "Model sha": "e094dae96097c2bc6f758101ee269c089b65a2cf",
    "Average ‚¨ÜÔ∏è": 7.256640238874923,
    "Hub License": "cc-by-sa-4.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4468403034455189,
    "IFEval Raw": 0.2419269759792905,
    "IFEval": 24.192697597929048,
    "BBH Raw": 0.3516779692917367,
    "BBH": 8.742082990875964,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.33409375,
    "MUSR": 2.6617187500000004,
    "MMLU-PRO Raw": 0.13572140957446807,
    "MMLU-PRO": 3.9690455082742293,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-28",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "euclaise/ReMask-3B"
  },
  {
    "eval_name": "facebook_opt-1.3b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OPTForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/facebook/opt-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/opt-1.3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/facebook__opt-1.3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "facebook/opt-1.3b",
    "Model sha": "3f5c25d0bc631cb57ac65913f76e22c2dfb61d62",
    "Average ‚¨ÜÔ∏è": 5.251513100569197,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 156,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.40300477784522964,
    "IFEval Raw": 0.23832985367713222,
    "IFEval": 23.83298536771322,
    "BBH Raw": 0.3093947052760125,
    "BBH": 3.6480520895226785,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.342,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.11070478723404255,
    "MMLU-PRO": 1.1894208037825047,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "facebook/opt-1.3b"
  },
  {
    "eval_name": "facebook_opt-30b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "OPTForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/facebook/opt-30b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">facebook/opt-30b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/facebook__opt-30b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "facebook/opt-30b",
    "Model sha": "ceea0a90ac0f6fae7c2c34bcb40477438c152546",
    "Average ‚¨ÜÔ∏è": 6.201345407060512,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 132,
    "#Params (B)": 30,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9998447862678397,
    "IFEval Raw": 0.2452991396162183,
    "IFEval": 24.52991396162183,
    "BBH Raw": 0.30703447525623373,
    "BBH": 3.4984293851759607,
    "MATH Lvl 5 Raw": 0.006042296072507554,
    "MATH Lvl 5": 0.6042296072507554,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.36041666666666666,
    "MUSR": 4.185416666666667,
    "MMLU-PRO Raw": 0.1163563829787234,
    "MMLU-PRO": 1.8173758865248217,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-05-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "facebook/opt-30b"
  },
  {
    "eval_name": "failspy_Llama-3-8B-Instruct-MopeyMule_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/failspy/Llama-3-8B-Instruct-MopeyMule\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">failspy/Llama-3-8B-Instruct-MopeyMule</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/failspy__Llama-3-8B-Instruct-MopeyMule-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "failspy/Llama-3-8B-Instruct-MopeyMule",
    "Model sha": "d1cbf407efe727c6b9fc94f22d51ff4915e1856e",
    "Average ‚¨ÜÔ∏è": 15.612956358952992,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 70,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.823135911240806,
    "IFEval Raw": 0.6750444376476638,
    "IFEval": 67.50444376476638,
    "BBH Raw": 0.383874490132152,
    "BBH": 13.620495859752507,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.23909395973154363,
    "GPQA": 0.0,
    "MUSR Raw": 0.35130208333333335,
    "MUSR": 2.2460937499999996,
    "MMLU-PRO Raw": 0.17644614361702127,
    "MMLU-PRO": 8.494015957446807,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "failspy/Llama-3-8B-Instruct-MopeyMule"
  },
  {
    "eval_name": "failspy_Llama-3-8B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/failspy/Llama-3-8B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">failspy/Llama-3-8B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/failspy__Llama-3-8B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "failspy/Llama-3-8B-Instruct-abliterated",
    "Model sha": "dd67dd055661e4cbcedb0ed2431693d9cc3be6e0",
    "Average ‚¨ÜÔ∏è": 19.17766799085752,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7419059249815282,
    "IFEval Raw": 0.5908888416069362,
    "IFEval": 59.088884160693624,
    "BBH Raw": 0.4353752684977051,
    "BBH": 18.86459884908717,
    "MATH Lvl 5 Raw": 0.03776435045317221,
    "MATH Lvl 5": 3.776435045317221,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.41158333333333336,
    "MUSR": 10.514583333333334,
    "MMLU-PRO Raw": 0.2741855053191489,
    "MMLU-PRO": 19.35394503546099,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "failspy/Llama-3-8B-Instruct-abliterated"
  },
  {
    "eval_name": "failspy_Meta-Llama-3-70B-Instruct-abliterated-v3.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/failspy__Meta-Llama-3-70B-Instruct-abliterated-v3.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5",
    "Model sha": "fc951b03d92972ab52ad9392e620eba6173526b9",
    "Average ‚¨ÜÔ∏è": 30.204882837947697,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 37,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.204711110907782,
    "IFEval Raw": 0.7746867201248244,
    "IFEval": 77.46867201248244,
    "BBH Raw": 0.574710022890038,
    "BBH": 37.87133313079306,
    "MATH Lvl 5 Raw": 0.13293051359516617,
    "MATH Lvl 5": 13.293051359516618,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.39818749999999997,
    "MUSR": 7.9734375,
    "MMLU-PRO Raw": 0.44522938829787234,
    "MMLU-PRO": 38.35882092198581,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "failspy/Meta-Llama-3-70B-Instruct-abliterated-v3.5"
  },
  {
    "eval_name": "failspy_Phi-3-medium-4k-instruct-abliterated-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/failspy/Phi-3-medium-4k-instruct-abliterated-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">failspy/Phi-3-medium-4k-instruct-abliterated-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/failspy__Phi-3-medium-4k-instruct-abliterated-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "failspy/Phi-3-medium-4k-instruct-abliterated-v3",
    "Model sha": "959b09eacf6cae85a8eb21b25e998addc89a367b",
    "Average ‚¨ÜÔ∏è": 31.775592079603722,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.520981193227809,
    "IFEval Raw": 0.6319299458769398,
    "IFEval": 63.19299458769399,
    "BBH Raw": 0.6304799176474429,
    "BBH": 46.73283933573803,
    "MATH Lvl 5 Raw": 0.15483383685800606,
    "MATH Lvl 5": 15.483383685800606,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.4604166666666667,
    "MUSR": 18.518749999999997,
    "MMLU-PRO Raw": 0.4399933510638298,
    "MMLU-PRO": 37.77703900709219,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "failspy/Phi-3-medium-4k-instruct-abliterated-v3"
  },
  {
    "eval_name": "failspy_llama-3-70B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/failspy/llama-3-70B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">failspy/llama-3-70B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/failspy__llama-3-70B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "failspy/llama-3-70B-Instruct-abliterated",
    "Model sha": "53ae9dafe8b3d163e05d75387575f8e9f43253d0",
    "Average ‚¨ÜÔ∏è": 36.091428530322595,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 94,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.374128960268552,
    "IFEval Raw": 0.8023389052159382,
    "IFEval": 80.23389052159382,
    "BBH Raw": 0.6464853840398571,
    "BBH": 48.93981832466943,
    "MATH Lvl 5 Raw": 0.25528700906344415,
    "MATH Lvl 5": 25.528700906344415,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.4127604166666667,
    "MUSR": 10.528385416666671,
    "MMLU-PRO Raw": 0.5145445478723404,
    "MMLU-PRO": 46.06050531914893,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "failspy/llama-3-70B-Instruct-abliterated"
  },
  {
    "eval_name": "fblgit_TheBeagle-v2beta-32B-MGS_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/TheBeagle-v2beta-32B-MGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/TheBeagle-v2beta-32B-MGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__TheBeagle-v2beta-32B-MGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/TheBeagle-v2beta-32B-MGS",
    "Model sha": "56830f63e4a40378b7721ae966637b4678cc8784",
    "Average ‚¨ÜÔ∏è": 41.62240796434389,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 32.87910721516035,
    "IFEval Raw": 0.518074265171966,
    "IFEval": 51.807426517196596,
    "BBH Raw": 0.7032634749563558,
    "BBH": 58.027976201167604,
    "MATH Lvl 5 Raw": 0.4335347432024169,
    "MATH Lvl 5": 43.353474320241695,
    "GPQA Raw": 0.3825503355704698,
    "GPQA": 17.67337807606264,
    "MUSR Raw": 0.50075,
    "MUSR": 24.260416666666668,
    "MMLU-PRO Raw": 0.5915059840425532,
    "MMLU-PRO": 54.61177600472813,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-30",
    "Generation": 1,
    "Base Model": "fblgit/TheBeagle-v2beta-32B-MGS (Merge)"
  },
  {
    "eval_name": "fblgit_TheBeagle-v2beta-32B-MGS_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/TheBeagle-v2beta-32B-MGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/TheBeagle-v2beta-32B-MGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__TheBeagle-v2beta-32B-MGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/TheBeagle-v2beta-32B-MGS",
    "Model sha": "56830f63e4a40378b7721ae966637b4678cc8784",
    "Average ‚¨ÜÔ∏è": 40.286669657817164,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.366068492834712,
    "IFEval Raw": 0.4503051902285935,
    "IFEval": 45.030519022859345,
    "BBH Raw": 0.703542441088263,
    "BBH": 58.06602977613295,
    "MATH Lvl 5 Raw": 0.3942598187311178,
    "MATH Lvl 5": 39.42598187311178,
    "GPQA Raw": 0.401006711409396,
    "GPQA": 20.134228187919465,
    "MUSR Raw": 0.5021145833333334,
    "MUSR": 24.497656250000002,
    "MMLU-PRO Raw": 0.5910904255319149,
    "MMLU-PRO": 54.565602836879435,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "fblgit/TheBeagle-v2beta-32B-MGS (Merge)"
  },
  {
    "eval_name": "fblgit_UNA-SimpleSmaug-34b-v1beta_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/UNA-SimpleSmaug-34b-v1beta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/UNA-SimpleSmaug-34b-v1beta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__UNA-SimpleSmaug-34b-v1beta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/UNA-SimpleSmaug-34b-v1beta",
    "Model sha": "4b62fccfc7e44c0a02c11a5279d98fafa6b922ba",
    "Average ‚¨ÜÔ∏è": 23.12139676764886,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1644662473043965,
    "IFEval Raw": 0.45562551806983254,
    "IFEval": 45.56255180698325,
    "BBH Raw": 0.5286654104993475,
    "BBH": 32.775788922324494,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.4255625,
    "MUSR": 11.961979166666667,
    "MMLU-PRO Raw": 0.4539561170212766,
    "MMLU-PRO": 39.328457446808514,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-05",
    "Submission Date": "2024-06-30",
    "Generation": 2,
    "Base Model": "jondurbin/bagel-34b-v0.2"
  },
  {
    "eval_name": "fblgit_UNA-TheBeagle-7b-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/UNA-TheBeagle-7b-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/UNA-TheBeagle-7b-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__UNA-TheBeagle-7b-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/UNA-TheBeagle-7b-v1",
    "Model sha": "866d3ee19f983728e21a624f8a27574960073f27",
    "Average ‚¨ÜÔ∏è": 19.633582661140952,
    "Hub License": "cc-by-nc-nd-4.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5606388227223992,
    "IFEval Raw": 0.36887236975669,
    "IFEval": 36.887236975669,
    "BBH Raw": 0.5028691097522866,
    "BBH": 30.173396964633465,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4564375,
    "MUSR": 16.088020833333328,
    "MMLU-PRO Raw": 0.3019448138297872,
    "MMLU-PRO": 22.43831264775413,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-09",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "fblgit/UNA-TheBeagle-7b-v1"
  },
  {
    "eval_name": "fblgit_UNA-ThePitbull-21.4B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/UNA-ThePitbull-21.4B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/UNA-ThePitbull-21.4B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__UNA-ThePitbull-21.4B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/UNA-ThePitbull-21.4B-v2",
    "Model sha": "f12aac93ae9c852550a16816e16116c4f8e7dec0",
    "Average ‚¨ÜÔ∏è": 22.799982694927763,
    "Hub License": "afl-3.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 21,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2984137776017173,
    "IFEval Raw": 0.3790387283518841,
    "IFEval": 37.90387283518841,
    "BBH Raw": 0.635038821016254,
    "BBH": 46.78807384004312,
    "MATH Lvl 5 Raw": 0.10800604229607252,
    "MATH Lvl 5": 10.800604229607252,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.3921666666666666,
    "MUSR": 6.420833333333333,
    "MMLU-PRO Raw": 0.3515625,
    "MMLU-PRO": 27.95138888888889,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-28",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "fblgit/UNA-ThePitbull-21.4B-v2"
  },
  {
    "eval_name": "fblgit_cybertron-v4-qw7B-MGS_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/cybertron-v4-qw7B-MGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/cybertron-v4-qw7B-MGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__cybertron-v4-qw7B-MGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/cybertron-v4-qw7B-MGS",
    "Model sha": "ea2aaf4f4000190235722a9ad4f5cd9e9091a64e",
    "Average ‚¨ÜÔ∏è": 31.207647622172544,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2467385811627305,
    "IFEval Raw": 0.6263846593704703,
    "IFEval": 62.63846593704703,
    "BBH Raw": 0.5591772533435835,
    "BBH": 37.04162311029608,
    "MATH Lvl 5 Raw": 0.277190332326284,
    "MATH Lvl 5": 27.719033232628398,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.43709375,
    "MUSR": 13.203385416666668,
    "MMLU-PRO Raw": 0.44730718085106386,
    "MMLU-PRO": 38.589686761229316,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "fblgit/cybertron-v4-qw7B-MGS (Merge)"
  },
  {
    "eval_name": "fblgit_cybertron-v4-qw7B-UNAMGS_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/cybertron-v4-qw7B-UNAMGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/cybertron-v4-qw7B-UNAMGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__cybertron-v4-qw7B-UNAMGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/cybertron-v4-qw7B-UNAMGS",
    "Model sha": "ce9b1e991908f5b89f63a2e3212cf9a066906ed2",
    "Average ‚¨ÜÔ∏è": 31.8158666745255,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3325919554946777,
    "IFEval Raw": 0.6084245357872666,
    "IFEval": 60.84245357872666,
    "BBH Raw": 0.5642509108139038,
    "BBH": 37.70717271699329,
    "MATH Lvl 5 Raw": 0.2990936555891239,
    "MATH Lvl 5": 29.909365558912388,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.4343333333333333,
    "MUSR": 12.691666666666668,
    "MMLU-PRO Raw": 0.4500498670212766,
    "MMLU-PRO": 38.894429669030735,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "fblgit/cybertron-v4-qw7B-UNAMGS (Merge)"
  },
  {
    "eval_name": "fblgit_juanako-7b-UNA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/juanako-7b-UNA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/juanako-7b-UNA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__juanako-7b-UNA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/juanako-7b-UNA",
    "Model sha": "b8ac85b603d5ee1ac619b2e1d0b3bb86c4eecb0c",
    "Average ‚¨ÜÔ∏è": 20.825303665535404,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6317904924898066,
    "IFEval Raw": 0.4837276204914073,
    "IFEval": 48.37276204914072,
    "BBH Raw": 0.507001145736535,
    "BBH": 30.415072015961297,
    "MATH Lvl 5 Raw": 0.03172205438066465,
    "MATH Lvl 5": 3.1722054380664653,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.46449999999999997,
    "MUSR": 17.162499999999994,
    "MMLU-PRO Raw": 0.277094414893617,
    "MMLU-PRO": 19.67715721040189,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-11-27",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "fblgit/juanako-7b-UNA"
  },
  {
    "eval_name": "fblgit_miniclaus-qw1.5B-UNAMGS_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/miniclaus-qw1.5B-UNAMGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/miniclaus-qw1.5B-UNAMGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__miniclaus-qw1.5B-UNAMGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/miniclaus-qw1.5B-UNAMGS",
    "Model sha": "de590536ba82ffb7b4001dffb5f8b60d2087c319",
    "Average ‚¨ÜÔ∏è": 16.868868412531032,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5917429865324039,
    "IFEval Raw": 0.3348005514257725,
    "IFEval": 33.48005514257725,
    "BBH Raw": 0.4238588294007628,
    "BBH": 18.562863710456668,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.42934374999999997,
    "MUSR": 12.234635416666668,
    "MMLU-PRO Raw": 0.2937167553191489,
    "MMLU-PRO": 21.52408392434988,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-01",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-1.5B"
  },
  {
    "eval_name": "fblgit_pancho-v1-qw25-3B-UNAMGS_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/pancho-v1-qw25-3B-UNAMGS\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/pancho-v1-qw25-3B-UNAMGS</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__pancho-v1-qw25-3B-UNAMGS-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/pancho-v1-qw25-3B-UNAMGS",
    "Model sha": "01143501cbc2c90961be5397c6945c6789815a60",
    "Average ‚¨ÜÔ∏è": 23.646636908286848,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5667288706433231,
    "IFEval Raw": 0.536134124123991,
    "IFEval": 53.6134124123991,
    "BBH Raw": 0.49258278193390775,
    "BBH": 28.66965021792132,
    "MATH Lvl 5 Raw": 0.14425981873111782,
    "MATH Lvl 5": 14.425981873111782,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4027395833333333,
    "MUSR": 8.17578125,
    "MMLU-PRO Raw": 0.3765791223404255,
    "MMLU-PRO": 30.731013593380606,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-04",
    "Submission Date": "2024-11-12",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-3B"
  },
  {
    "eval_name": "fblgit_una-cybertron-7b-v2-bf16_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/fblgit/una-cybertron-7b-v2-bf16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">fblgit/una-cybertron-7b-v2-bf16</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/fblgit__una-cybertron-7b-v2-bf16-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "fblgit/una-cybertron-7b-v2-bf16",
    "Model sha": "7ab101a153740aec39e95ec02831c56f4eab7910",
    "Average ‚¨ÜÔ∏è": 17.179560369346195,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 116,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6342055748605666,
    "IFEval Raw": 0.47371086494944525,
    "IFEval": 47.37108649494452,
    "BBH Raw": 0.3973388920486269,
    "BBH": 14.966964848379982,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.4473229166666666,
    "MUSR": 14.482031250000004,
    "MMLU-PRO Raw": 0.2442652925531915,
    "MMLU-PRO": 16.02947695035461,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-02",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "fblgit/una-cybertron-7b-v2-bf16"
  },
  {
    "eval_name": "flammenai_Llama3.1-Flammades-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/Llama3.1-Flammades-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/Llama3.1-Flammades-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__Llama3.1-Flammades-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/Llama3.1-Flammades-70B",
    "Model sha": "48909a734460e667e3a7e91bd25f124ec3b2ba74",
    "Average ‚¨ÜÔ∏è": 35.898954369866175,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.284832878308851,
    "IFEval Raw": 0.7058438277104748,
    "IFEval": 70.58438277104749,
    "BBH Raw": 0.6659721866694542,
    "BBH": 52.54794346693766,
    "MATH Lvl 5 Raw": 0.14350453172205438,
    "MATH Lvl 5": 14.350453172205437,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.48705208333333333,
    "MUSR": 22.348177083333326,
    "MMLU-PRO Raw": 0.47523271276595747,
    "MMLU-PRO": 41.692523640661946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "flammenai/Llama3.1-Flammades-70B (Merge)"
  },
  {
    "eval_name": "flammenai_Mahou-1.2a-llama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/Mahou-1.2a-llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/Mahou-1.2a-llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__Mahou-1.2a-llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/Mahou-1.2a-llama3-8B",
    "Model sha": "3318b6f5f1839644bee287a3e5390f3e9f565a9e",
    "Average ‚¨ÜÔ∏è": 21.841614118479715,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.932412235058643,
    "IFEval Raw": 0.50925655039739,
    "IFEval": 50.925655039739,
    "BBH Raw": 0.5093660540433169,
    "BBH": 28.972587655292433,
    "MATH Lvl 5 Raw": 0.08685800604229608,
    "MATH Lvl 5": 8.685800604229609,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.38466666666666666,
    "MUSR": 6.016666666666668,
    "MMLU-PRO Raw": 0.38173204787234044,
    "MMLU-PRO": 31.303560874704488,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "flammenai/Mahou-1.2a-llama3-8B (Merge)"
  },
  {
    "eval_name": "flammenai_Mahou-1.2a-mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/Mahou-1.2a-mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/Mahou-1.2a-mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__Mahou-1.2a-mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/Mahou-1.2a-mistral-7B",
    "Model sha": "d45f61cca04da0c3359573102853fca1a0d3b252",
    "Average ‚¨ÜÔ∏è": 19.50346196795837,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8056222637238803,
    "IFEval Raw": 0.4552010886669592,
    "IFEval": 45.52010886669592,
    "BBH Raw": 0.5118111474458115,
    "BBH": 31.166750037107487,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.38962500000000005,
    "MUSR": 6.969791666666668,
    "MMLU-PRO Raw": 0.31632313829787234,
    "MMLU-PRO": 24.03590425531915,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-18",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "flammenai/Mahou-1.2a-mistral-7B (Merge)"
  },
  {
    "eval_name": "flammenai_Mahou-1.5-llama3.1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/Mahou-1.5-llama3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/Mahou-1.5-llama3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__Mahou-1.5-llama3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/Mahou-1.5-llama3.1-70B",
    "Model sha": "49f45cc4c21e2ba7ed5c5e71f90ffd0bd9169e2d",
    "Average ‚¨ÜÔ∏è": 36.23715891973573,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.259992267649446,
    "IFEval Raw": 0.7146615424850509,
    "IFEval": 71.46615424850509,
    "BBH Raw": 0.6650860641288713,
    "BBH": 52.36957740630965,
    "MATH Lvl 5 Raw": 0.1435045317220544,
    "MATH Lvl 5": 14.350453172205441,
    "GPQA Raw": 0.3540268456375839,
    "GPQA": 13.870246085011187,
    "MUSR Raw": 0.4950208333333333,
    "MUSR": 23.7109375,
    "MMLU-PRO Raw": 0.47490026595744683,
    "MMLU-PRO": 41.65558510638298,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "flammenai/Mahou-1.5-llama3.1-70B (Merge)"
  },
  {
    "eval_name": "flammenai_Mahou-1.5-mistral-nemo-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/Mahou-1.5-mistral-nemo-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/Mahou-1.5-mistral-nemo-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__Mahou-1.5-mistral-nemo-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/Mahou-1.5-mistral-nemo-12B",
    "Model sha": "852561e74f1785bf7225bb28395db1fd9431fe31",
    "Average ‚¨ÜÔ∏è": 26.38180122650539,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4826319967777066,
    "IFEval Raw": 0.6751441730164851,
    "IFEval": 67.5144173016485,
    "BBH Raw": 0.5522361927910235,
    "BBH": 36.260510188013406,
    "MATH Lvl 5 Raw": 0.05664652567975832,
    "MATH Lvl 5": 5.664652567975832,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.4520416666666667,
    "MUSR": 16.471874999999997,
    "MMLU-PRO Raw": 0.3602061170212766,
    "MMLU-PRO": 28.911790780141843,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "flammenai/Mahou-1.5-mistral-nemo-12B (Merge)"
  },
  {
    "eval_name": "flammenai_flammen15-gutenberg-DPO-v1-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/flammenai/flammen15-gutenberg-DPO-v1-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">flammenai/flammen15-gutenberg-DPO-v1-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/flammenai__flammen15-gutenberg-DPO-v1-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "flammenai/flammen15-gutenberg-DPO-v1-7B",
    "Model sha": "550cd9548cba1265cb1771c85ebe498789fdecb5",
    "Average ‚¨ÜÔ∏è": 21.57493407874559,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6275296069142738,
    "IFEval Raw": 0.47980580415519714,
    "IFEval": 47.98058041551971,
    "BBH Raw": 0.5202983979716951,
    "BBH": 32.66511308584827,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4293125,
    "MUSR": 12.530729166666669,
    "MMLU-PRO Raw": 0.3185671542553192,
    "MMLU-PRO": 24.28523936170213,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-05",
    "Submission Date": "2024-07-10",
    "Generation": 1,
    "Base Model": "flammenai/flammen15-gutenberg-DPO-v1-7B (Merge)"
  },
  {
    "eval_name": "freewheelin_free-evo-qwen72b-v0.8-re_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/freewheelin/free-evo-qwen72b-v0.8-re\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">freewheelin/free-evo-qwen72b-v0.8-re</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/freewheelin__free-evo-qwen72b-v0.8-re-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "freewheelin/free-evo-qwen72b-v0.8-re",
    "Model sha": "24e301d8fbef8ada12be42156b01c827ff594962",
    "Average ‚¨ÜÔ∏è": 32.424578496240706,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.789790603600165,
    "IFEval Raw": 0.533086654521115,
    "IFEval": 53.30866545211151,
    "BBH Raw": 0.6127477065378042,
    "BBH": 45.31740264996691,
    "MATH Lvl 5 Raw": 0.17749244712990936,
    "MATH Lvl 5": 17.749244712990937,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.4871666666666667,
    "MUSR": 20.962499999999995,
    "MMLU-PRO Raw": 0.4870345744680851,
    "MMLU-PRO": 43.00384160756501,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-02",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "freewheelin/free-evo-qwen72b-v0.8-re"
  },
  {
    "eval_name": "freewheelin_free-solar-evo-v0.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/freewheelin/free-solar-evo-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">freewheelin/free-solar-evo-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/freewheelin__free-solar-evo-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "freewheelin/free-solar-evo-v0.1",
    "Model sha": "233efd607ae0abbd7b46eded2ee7889892b7bdbb",
    "Average ‚¨ÜÔ∏è": 16.295570875162888,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8011110551571363,
    "IFEval Raw": 0.20500715878313985,
    "IFEval": 20.500715878313983,
    "BBH Raw": 0.4502211109638701,
    "BBH": 22.635182738331654,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.4945833333333334,
    "MUSR": 22.256249999999998,
    "MMLU-PRO Raw": 0.3414228723404255,
    "MMLU-PRO": 26.824763593380613,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-18",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "freewheelin/free-solar-evo-v0.1"
  },
  {
    "eval_name": "freewheelin_free-solar-evo-v0.11_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/freewheelin/free-solar-evo-v0.11\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">freewheelin/free-solar-evo-v0.11</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/freewheelin__free-solar-evo-v0.11-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "freewheelin/free-solar-evo-v0.11",
    "Model sha": "17fc24a557bd3c3836abc9f6a367c803cba0cccd",
    "Average ‚¨ÜÔ∏è": 16.641293877570874,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8135019786882783,
    "IFEval Raw": 0.20265894493277836,
    "IFEval": 20.265894493277834,
    "BBH Raw": 0.4545155032474882,
    "BBH": 23.18242496978891,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.5052187499999999,
    "MUSR": 24.285677083333326,
    "MMLU-PRO Raw": 0.34674202127659576,
    "MMLU-PRO": 27.41578014184397,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-24",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "freewheelin/free-solar-evo-v0.11"
  },
  {
    "eval_name": "freewheelin_free-solar-evo-v0.13_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/freewheelin/free-solar-evo-v0.13\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">freewheelin/free-solar-evo-v0.13</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/freewheelin__free-solar-evo-v0.13-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "freewheelin/free-solar-evo-v0.13",
    "Model sha": "2a7eb72f84c54898630f9db470eee0f936a64396",
    "Average ‚¨ÜÔ∏è": 17.20449145515274,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8159558847201767,
    "IFEval Raw": 0.2320598234905606,
    "IFEval": 23.20598234905606,
    "BBH Raw": 0.4554839670962904,
    "BBH": 23.35420388572778,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.50515625,
    "MUSR": 24.077864583333326,
    "MMLU-PRO Raw": 0.34699135638297873,
    "MMLU-PRO": 27.44348404255319,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-28",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "freewheelin/free-solar-evo-v0.13"
  },
  {
    "eval_name": "gabrielmbmb_SmolLM-1.7B-Instruct-IFEval_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gabrielmbmb/SmolLM-1.7B-Instruct-IFEval\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gabrielmbmb/SmolLM-1.7B-Instruct-IFEval</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gabrielmbmb__SmolLM-1.7B-Instruct-IFEval-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gabrielmbmb/SmolLM-1.7B-Instruct-IFEval",
    "Model sha": "ac5d711adc05ccfe1b1b912d5561d98f6afeeb40",
    "Average ‚¨ÜÔ∏è": 5.222835726155844,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.13474538412568127,
    "IFEval Raw": 0.23058595637353335,
    "IFEval": 23.058595637353335,
    "BBH Raw": 0.313843378282092,
    "BBH": 4.50167515878636,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.33276041666666667,
    "MUSR": 1.5950520833333328,
    "MMLU-PRO Raw": 0.11560837765957446,
    "MMLU-PRO": 1.7342641843971618,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-11",
    "Generation": 2,
    "Base Model": "HuggingFaceTB/SmolLM-1.7B"
  },
  {
    "eval_name": "gaverfraxz_Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gaverfraxz__Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA",
    "Model sha": "6b0271a98b8875a65972ed54b0d636d8236ea60b",
    "Average ‚¨ÜÔ∏è": 11.919581981532884,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3456739571963208,
    "IFEval Raw": 0.40094615619888563,
    "IFEval": 40.094615619888565,
    "BBH Raw": 0.3984844272016949,
    "BBH": 15.276579446085892,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.36504166666666665,
    "MUSR": 3.463541666666668,
    "MMLU-PRO Raw": 0.16539228723404256,
    "MMLU-PRO": 7.265809692671395,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-DELLA (Merge)"
  },
  {
    "eval_name": "gaverfraxz_Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gaverfraxz__Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES",
    "Model sha": "80569e49b5aba960a5cd91281dd9eef92aeff9a3",
    "Average ‚¨ÜÔ∏è": 20.986453628374324,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9613568355598231,
    "IFEval Raw": 0.45505148561372716,
    "IFEval": 45.50514856137272,
    "BBH Raw": 0.5043660783243713,
    "BBH": 28.91423515333936,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.37375,
    "MUSR": 6.58541666666667,
    "MMLU-PRO Raw": 0.36785239361702127,
    "MMLU-PRO": 29.76137706855792,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "gaverfraxz/Meta-Llama-3.1-8B-Instruct-HalfAbliterated-TIES (Merge)"
  },
  {
    "eval_name": "gbueno86_Brinebreath-Llama-3.1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gbueno86/Brinebreath-Llama-3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gbueno86/Brinebreath-Llama-3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gbueno86__Brinebreath-Llama-3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gbueno86/Brinebreath-Llama-3.1-70B",
    "Model sha": "c508ecf356167e8c498c6fa3937ba30a82208983",
    "Average ‚¨ÜÔ∏è": 36.29275614704682,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.559754400328728,
    "IFEval Raw": 0.5532952565858589,
    "IFEval": 55.32952565858589,
    "BBH Raw": 0.6880562247706813,
    "BBH": 55.46361848802468,
    "MATH Lvl 5 Raw": 0.2998489425981873,
    "MATH Lvl 5": 29.984894259818727,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.45406250000000004,
    "MUSR": 17.49114583333333,
    "MMLU-PRO Raw": 0.5196143617021277,
    "MMLU-PRO": 46.62381796690308,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-08-29",
    "Generation": 1,
    "Base Model": "gbueno86/Brinebreath-Llama-3.1-70B (Merge)"
  },
  {
    "eval_name": "gbueno86_Meta-LLama-3-Cat-Smaug-LLama-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gbueno86/Meta-LLama-3-Cat-Smaug-LLama-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gbueno86/Meta-LLama-3-Cat-Smaug-LLama-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gbueno86__Meta-LLama-3-Cat-Smaug-LLama-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gbueno86/Meta-LLama-3-Cat-Smaug-LLama-70b",
    "Model sha": "2d73b7e1c7157df482555944d6a6b1362bc6c3c5",
    "Average ‚¨ÜÔ∏è": 38.26813710120556,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 10.902293212970626,
    "IFEval Raw": 0.8071849359698933,
    "IFEval": 80.71849359698932,
    "BBH Raw": 0.6674314931312052,
    "BBH": 51.50838639894525,
    "MATH Lvl 5 Raw": 0.2681268882175227,
    "MATH Lvl 5": 26.812688821752268,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.43682291666666667,
    "MUSR": 15.002864583333329,
    "MMLU-PRO Raw": 0.5074800531914894,
    "MMLU-PRO": 45.27556146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-24",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "gbueno86/Meta-LLama-3-Cat-Smaug-LLama-70b (Merge)"
  },
  {
    "eval_name": "ghost-x_ghost-8b-beta-1608_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ghost-x/ghost-8b-beta-1608\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ghost-x/ghost-8b-beta-1608</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ghost-x__ghost-8b-beta-1608-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ghost-x/ghost-8b-beta-1608",
    "Model sha": "6d1b3853aab774af5a4db21ff9d5764918fb48f5",
    "Average ‚¨ÜÔ∏è": 15.103134921172604,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 29,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8489310874346324,
    "IFEval Raw": 0.42727407722620425,
    "IFEval": 42.72740772262043,
    "BBH Raw": 0.45165496100352914,
    "BBH": 23.463963859654836,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.35158333333333336,
    "MUSR": 1.58125,
    "MMLU-PRO Raw": 0.2839926861702128,
    "MMLU-PRO": 20.44363179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "ghost-x/ghost-8b-beta"
  },
  {
    "eval_name": "glaiveai_Reflection-Llama-3.1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/glaiveai/Reflection-Llama-3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">glaiveai/Reflection-Llama-3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/glaiveai__Reflection-Llama-3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "glaiveai/Reflection-Llama-3.1-70B",
    "Model sha": "086bd2658e00345808b31758ebb8f7e2c6d9897c",
    "Average ‚¨ÜÔ∏è": 29.92481574831361,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 69,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 25.24377639772199,
    "IFEval Raw": 0.5990571683134085,
    "IFEval": 59.90571683134084,
    "BBH Raw": 0.5681010035620444,
    "BBH": 37.96048632437056,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.43803125000000004,
    "MUSR": 13.720572916666663,
    "MMLU-PRO Raw": 0.6341422872340425,
    "MMLU-PRO": 59.349143026004725,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "glaiveai/Reflection-Llama-3.1-70B"
  },
  {
    "eval_name": "gmonsoon_SahabatAI-MediChatIndo-8B-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gmonsoon/SahabatAI-MediChatIndo-8B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gmonsoon/SahabatAI-MediChatIndo-8B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gmonsoon__SahabatAI-MediChatIndo-8B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gmonsoon/SahabatAI-MediChatIndo-8B-v1",
    "Model sha": "2f7daa8eb5ad216ce9ebcd70dc77e5b44fb977b0",
    "Average ‚¨ÜÔ∏è": 17.29986493129456,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6767219468478284,
    "IFEval Raw": 0.41628323958208663,
    "IFEval": 41.62832395820867,
    "BBH Raw": 0.4508834027881236,
    "BBH": 23.64009974170933,
    "MATH Lvl 5 Raw": 0.061933534743202415,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3753958333333333,
    "MUSR": 4.557812500000001,
    "MMLU-PRO Raw": 0.3107546542553192,
    "MMLU-PRO": 23.417183806146575,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-19",
    "Submission Date": "2024-11-19",
    "Generation": 1,
    "Base Model": "gmonsoon/SahabatAI-MediChatIndo-8B-v1 (Merge)"
  },
  {
    "eval_name": "gmonsoon_StockSeaLLMs-7B-v1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gmonsoon/StockSeaLLMs-7B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gmonsoon/StockSeaLLMs-7B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gmonsoon__StockSeaLLMs-7B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gmonsoon/StockSeaLLMs-7B-v1",
    "Model sha": "2431fe5e4a3f63984c2936cf1cf68b3c7172cc20",
    "Average ‚¨ÜÔ∏è": 24.753571377438103,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6968903124772825,
    "IFEval Raw": 0.4599218961245052,
    "IFEval": 45.99218961245052,
    "BBH Raw": 0.5271087932535433,
    "BBH": 34.01262496222566,
    "MATH Lvl 5 Raw": 0.17598187311178248,
    "MATH Lvl 5": 17.598187311178247,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.421375,
    "MUSR": 11.071875000000004,
    "MMLU-PRO Raw": 0.39519614361702127,
    "MMLU-PRO": 32.79957151300236,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "gmonsoon/StockSeaLLMs-7B-v1 (Merge)"
  },
  {
    "eval_name": "gmonsoon_gemma2-9b-sahabatai-v1-instruct-BaseTIES_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gmonsoon/gemma2-9b-sahabatai-v1-instruct-BaseTIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gmonsoon/gemma2-9b-sahabatai-v1-instruct-BaseTIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gmonsoon__gemma2-9b-sahabatai-v1-instruct-BaseTIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gmonsoon/gemma2-9b-sahabatai-v1-instruct-BaseTIES",
    "Model sha": "43296081051afe5d7a426b86a6d73104efab440b",
    "Average ‚¨ÜÔ∏è": 33.70386408911091,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7814847186971148,
    "IFEval Raw": 0.7377923908562614,
    "IFEval": 73.77923908562614,
    "BBH Raw": 0.6077244532441547,
    "BBH": 43.401341987357206,
    "MATH Lvl 5 Raw": 0.1933534743202417,
    "MATH Lvl 5": 19.335347432024168,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.47780208333333335,
    "MUSR": 19.12526041666666,
    "MMLU-PRO Raw": 0.43467420212765956,
    "MMLU-PRO": 37.186022458628834,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "gmonsoon/gemma2-9b-sahabatai-v1-instruct-BaseTIES (Merge)"
  },
  {
    "eval_name": "google_codegemma-1.1-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/codegemma-1.1-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/codegemma-1.1-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__codegemma-1.1-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/codegemma-1.1-2b",
    "Model sha": "9d69e500da236427eab5867552ffc87108964f4d",
    "Average ‚¨ÜÔ∏è": 7.03316296850576,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9498831960526669,
    "IFEval Raw": 0.22936253584932426,
    "IFEval": 22.936253584932423,
    "BBH Raw": 0.3353417790248454,
    "BBH": 7.551225280004151,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3871458333333333,
    "MUSR": 5.9265625,
    "MMLU-PRO Raw": 0.1278257978723404,
    "MMLU-PRO": 3.0917553191489344,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-30",
    "Submission Date": "2024-08-12",
    "Generation": 0,
    "Base Model": "google/codegemma-1.1-2b"
  },
  {
    "eval_name": "google_flan-t5-base_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-base",
    "Model sha": "7bcac572ce56db69c1ea7c8af255c5d7c9672fc2",
    "Average ‚¨ÜÔ∏è": 6.239408489533947,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 803,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.1566214404244561,
    "IFEval Raw": 0.18907055501624578,
    "IFEval": 18.907055501624576,
    "BBH Raw": 0.3525980599300322,
    "BBH": 11.337693677304879,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23825503355704697,
    "GPQA": 0.0,
    "MUSR Raw": 0.36711458333333336,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.13572140957446807,
    "MMLU-PRO": 3.9690455082742293,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-08-14",
    "Generation": 0,
    "Base Model": "google/flan-t5-base"
  },
  {
    "eval_name": "google_flan-t5-large_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-large</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-large-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-large",
    "Model sha": "0613663d0d48ea86ba8cb3d7a44f0f65dc596a2a",
    "Average ‚¨ÜÔ∏è": 9.418948706006079,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 614,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.23349137163530823,
    "IFEval Raw": 0.22009490374428736,
    "IFEval": 22.009490374428736,
    "BBH Raw": 0.41531150356794316,
    "BBH": 17.510018280067285,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.40832291666666665,
    "MUSR": 9.007031249999999,
    "MMLU-PRO Raw": 0.17087765957446807,
    "MMLU-PRO": 7.87529550827423,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-08-14",
    "Generation": 0,
    "Base Model": "google/flan-t5-large"
  },
  {
    "eval_name": "google_flan-t5-small_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-small</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-small-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-small",
    "Model sha": "0fc9ddf78a1e988dac52e2dac162b0ede4fd74ab",
    "Average ‚¨ÜÔ∏è": 6.003780642360629,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 276,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.14313024227513882,
    "IFEval Raw": 0.1524255641697363,
    "IFEval": 15.24255641697363,
    "BBH Raw": 0.3282901097640842,
    "BBH": 6.36311196167965,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.41229166666666667,
    "MUSR": 10.36979166666667,
    "MMLU-PRO Raw": 0.1233377659574468,
    "MMLU-PRO": 2.5930851063829774,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "google/flan-t5-small"
  },
  {
    "eval_name": "google_flan-t5-xl_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-xl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-xl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-xl",
    "Model sha": "7d6315df2c2fb742f0f5b556879d730926ca9001",
    "Average ‚¨ÜÔ∏è": 11.591779528533316,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 468,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3489293442933807,
    "IFEval Raw": 0.22374189373085634,
    "IFEval": 22.374189373085635,
    "BBH Raw": 0.45310636062112314,
    "BBH": 22.695055811215397,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.41809375,
    "MUSR": 11.328385416666668,
    "MMLU-PRO Raw": 0.21467752659574468,
    "MMLU-PRO": 12.741947399527188,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "google/flan-t5-xl"
  },
  {
    "eval_name": "google_flan-t5-xl_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-xl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-xl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-xl",
    "Model sha": "7d6315df2c2fb742f0f5b556879d730926ca9001",
    "Average ‚¨ÜÔ∏è": 11.58716743755607,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 468,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.28535173017878435,
    "IFEval Raw": 0.2206944241279804,
    "IFEval": 22.06944241279804,
    "BBH Raw": 0.45372172155693963,
    "BBH": 22.837587663523298,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.42203125,
    "MUSR": 11.853906250000001,
    "MMLU-PRO Raw": 0.21417885638297873,
    "MMLU-PRO": 12.686539598108748,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "google/flan-t5-xl"
  },
  {
    "eval_name": "google_flan-t5-xxl_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-t5-xxl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-t5-xxl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-t5-xxl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-t5-xxl",
    "Model sha": "ae7c9136adc7555eeccc78cdd960dfd60fb346ce",
    "Average ‚¨ÜÔ∏è": 13.485843425522548,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1201,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7064768273146297,
    "IFEval Raw": 0.2200450360598767,
    "IFEval": 22.00450360598767,
    "BBH Raw": 0.5065888015776924,
    "BBH": 30.119255600105877,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.42175,
    "MUSR": 11.185416666666669,
    "MMLU-PRO Raw": 0.23429188829787234,
    "MMLU-PRO": 14.921320921985814,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-21",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/flan-t5-xxl"
  },
  {
    "eval_name": "google_flan-ul2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/flan-ul2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/flan-ul2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__flan-ul2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/flan-ul2",
    "Model sha": "452d74ce28ac4a7f211d6ba3ef0717027f7a8074",
    "Average ‚¨ÜÔ∏è": 13.550117524788796,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 554,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5599662834484231,
    "IFEval Raw": 0.23925406809487715,
    "IFEval": 23.925406809487715,
    "BBH Raw": 0.5053738049125648,
    "BBH": 30.02029012567709,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.3843541666666666,
    "MUSR": 5.577604166666667,
    "MMLU-PRO Raw": 0.24933510638297873,
    "MMLU-PRO": 16.592789598108748,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-03-03",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "google/flan-ul2"
  },
  {
    "eval_name": "google_gemma-1.1-2b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-1.1-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-1.1-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-1.1-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-1.1-2b-it",
    "Model sha": "bf4924f313df5166dee1467161e886e55f2eb4d4",
    "Average ‚¨ÜÔ∏è": 7.776435284352048,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 151,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3292147814227601,
    "IFEval Raw": 0.30674831668860847,
    "IFEval": 30.674831668860847,
    "BBH Raw": 0.3184634974814922,
    "BBH": 5.862826722774347,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.33939583333333334,
    "MUSR": 2.024479166666666,
    "MMLU-PRO Raw": 0.14835438829787234,
    "MMLU-PRO": 5.372709810874704,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-26",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "google/gemma-1.1-2b-it"
  },
  {
    "eval_name": "google_gemma-1.1-7b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-1.1-7b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-1.1-7b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-1.1-7b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-1.1-7b-it",
    "Model sha": "16128b0aeb50762ea96430c0c06a37941bf9f274",
    "Average ‚¨ÜÔ∏è": 17.479586243071306,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 265,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.578299189963354,
    "IFEval Raw": 0.5039107346285633,
    "IFEval": 50.391073462856326,
    "BBH Raw": 0.3935297962833251,
    "BBH": 15.93420938501317,
    "MATH Lvl 5 Raw": 0.03625377643504532,
    "MATH Lvl 5": 3.625377643504532,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.42302083333333335,
    "MUSR": 11.510937500000002,
    "MMLU-PRO Raw": 0.2583942819148936,
    "MMLU-PRO": 17.5993646572104,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-26",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "google/gemma-1.1-7b-it"
  },
  {
    "eval_name": "google_gemma-2-27b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-27b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-27b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-27b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-27b",
    "Model sha": "938270f5272feb02779b55c2bb2fffdd0f53ff0c",
    "Average ‚¨ÜÔ∏è": 23.850638639876475,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 178,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.61424927838634,
    "IFEval Raw": 0.24752213017017072,
    "IFEval": 24.75221301701707,
    "BBH Raw": 0.5642908317482057,
    "BBH": 37.390737454186464,
    "MATH Lvl 5 Raw": 0.16163141993957705,
    "MATH Lvl 5": 16.163141993957705,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.43963541666666667,
    "MUSR": 13.921093749999997,
    "MMLU-PRO Raw": 0.4370844414893617,
    "MMLU-PRO": 37.4538268321513,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-08-24",
    "Generation": 0,
    "Base Model": "google/gemma-2-27b"
  },
  {
    "eval_name": "google_gemma-2-27b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-27b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-27b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-27b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-27b-it",
    "Model sha": "f6c533e5eb013c7e31fc74ef042ac4f3fb5cf40b",
    "Average ‚¨ÜÔ∏è": 32.32231876887986,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 445,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.826211186692737,
    "IFEval Raw": 0.7977677008116243,
    "IFEval": 79.77677008116243,
    "BBH Raw": 0.6451387433168799,
    "BBH": 49.27284215130387,
    "MATH Lvl 5 Raw": 0.007552870090634441,
    "MATH Lvl 5": 0.755287009063444,
    "GPQA Raw": 0.375,
    "GPQA": 16.666666666666664,
    "MUSR Raw": 0.40330208333333334,
    "MUSR": 9.112760416666667,
    "MMLU-PRO Raw": 0.4451462765957447,
    "MMLU-PRO": 38.34958628841608,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-08-07",
    "Generation": 1,
    "Base Model": "google/gemma-2-27b"
  },
  {
    "eval_name": "google_gemma-2-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-2b",
    "Model sha": "4d05c88d00441bf62bf87dcfd29e204c05089f36",
    "Average ‚¨ÜÔ∏è": 10.129463155055184,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 430,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5187957227139828,
    "IFEval Raw": 0.19931226922343825,
    "IFEval": 19.931226922343825,
    "BBH Raw": 0.3655966996422591,
    "BBH": 11.755807532236112,
    "MATH Lvl 5 Raw": 0.028700906344410877,
    "MATH Lvl 5": 2.8700906344410875,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.4231770833333333,
    "MUSR": 11.430468750000001,
    "MMLU-PRO Raw": 0.21800199468085107,
    "MMLU-PRO": 13.111332742316787,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "google_gemma-2-2b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-2b",
    "Model sha": "0738188b3055bc98daf0fe7211f0091357e5b979",
    "Average ‚¨ÜÔ∏è": 10.334439334831469,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 430,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4182573511315797,
    "IFEval Raw": 0.20176021844262113,
    "IFEval": 20.176021844262113,
    "BBH Raw": 0.3708674612470255,
    "BBH": 12.497306228573644,
    "MATH Lvl 5 Raw": 0.02870090634441088,
    "MATH Lvl 5": 2.870090634441088,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.421875,
    "MUSR": 11.267708333333333,
    "MMLU-PRO Raw": 0.22165890957446807,
    "MMLU-PRO": 13.51765661938534,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "google_gemma-2-2b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-2b-it",
    "Model sha": "2b6ac3ff954ad896c115bbfa1b571cd93ea2c20f",
    "Average ‚¨ÜÔ∏è": 17.046939294966545,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 684,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2347432742058528,
    "IFEval Raw": 0.5668337788179807,
    "IFEval": 56.68337788179808,
    "BBH Raw": 0.41992308914274706,
    "BBH": 17.980792881523424,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.39288541666666665,
    "MUSR": 7.077343750000001,
    "MMLU-PRO Raw": 0.25498670212765956,
    "MMLU-PRO": 17.22074468085106,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-07-31",
    "Generation": 1,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "google_gemma-2-2b-jpn-it_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-2b-jpn-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-2b-jpn-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-2b-jpn-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-2b-jpn-it",
    "Model sha": "6b046bbc091084a1ec89fe03e58871fde10868eb",
    "Average ‚¨ÜÔ∏è": 17.11540570593849,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 136,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.011437210514093,
    "IFEval Raw": 0.5077826832803628,
    "IFEval": 50.778268328036276,
    "BBH Raw": 0.42255698900658106,
    "BBH": 18.525626449832735,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.39638541666666666,
    "MUSR": 7.681510416666669,
    "MMLU-PRO Raw": 0.2578125,
    "MMLU-PRO": 17.53472222222222,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-10-11",
    "Generation": 2,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "google_gemma-2-2b-jpn-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-2b-jpn-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-2b-jpn-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-2b-jpn-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-2b-jpn-it",
    "Model sha": "6b046bbc091084a1ec89fe03e58871fde10868eb",
    "Average ‚¨ÜÔ∏è": 15.885578707405609,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 136,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8544002490400107,
    "IFEval Raw": 0.5288401441508531,
    "IFEval": 52.88401441508531,
    "BBH Raw": 0.4178440226217119,
    "BBH": 17.848086390818036,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.37276041666666665,
    "MUSR": 4.928385416666668,
    "MMLU-PRO Raw": 0.2466755319148936,
    "MMLU-PRO": 16.297281323877066,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-10-14",
    "Generation": 2,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "google_gemma-2-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-9b",
    "Model sha": "beb0c08e9eeb0548f3aca2ac870792825c357b7d",
    "Average ‚¨ÜÔ∏è": 21.154934308829795,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 590,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.663185691790397,
    "IFEval Raw": 0.20398320899657355,
    "IFEval": 20.398320899657357,
    "BBH Raw": 0.5377373397621884,
    "BBH": 34.09681853589784,
    "MATH Lvl 5 Raw": 0.1314199395770393,
    "MATH Lvl 5": 13.14199395770393,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.4461145833333333,
    "MUSR": 14.297656250000001,
    "MMLU-PRO Raw": 0.4103224734042553,
    "MMLU-PRO": 34.48027482269504,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "google_gemma-2-9b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2-9b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2-9b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2-9b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2-9b-it",
    "Model sha": "1937c70277fcc5f7fb0fc772fc5bc69378996e71",
    "Average ‚¨ÜÔ∏è": 28.86279046415118,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 559,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.014497020774933,
    "IFEval Raw": 0.7435626360279614,
    "IFEval": 74.35626360279613,
    "BBH Raw": 0.5990342504164132,
    "BBH": 42.136619683664655,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.36073825503355705,
    "GPQA": 14.76510067114094,
    "MUSR Raw": 0.4072708333333333,
    "MUSR": 9.742187500000002,
    "MMLU-PRO Raw": 0.3875498670212766,
    "MMLU-PRO": 31.949985224586293,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-07-11",
    "Generation": 1,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "google_gemma-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2b",
    "Model sha": "2ac59a5d7bf4e1425010f0d457dde7d146658953",
    "Average ‚¨ÜÔ∏è": 7.35870138352179,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 915,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2362508245033688,
    "IFEval Raw": 0.20375825033134307,
    "IFEval": 20.375825033134305,
    "BBH Raw": 0.3380993975829239,
    "BBH": 8.466712864840373,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.39778125,
    "MUSR": 7.555989583333336,
    "MMLU-PRO Raw": 0.13655252659574468,
    "MMLU-PRO": 4.061391843971631,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "google/gemma-2b"
  },
  {
    "eval_name": "google_gemma-2b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-2b-it",
    "Model sha": "de144fb2268dee1066f515465df532c05e699d48",
    "Average ‚¨ÜÔ∏è": 7.221453677142921,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 671,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3529503310477593,
    "IFEval Raw": 0.26902950837112194,
    "IFEval": 26.902950837112197,
    "BBH Raw": 0.31508191988788464,
    "BBH": 5.214303022163619,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.334125,
    "MUSR": 3.0322916666666675,
    "MMLU-PRO Raw": 0.13530585106382978,
    "MMLU-PRO": 3.9228723404255303,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "google/gemma-2b-it"
  },
  {
    "eval_name": "google_gemma-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-7b",
    "Model sha": "a0eac5b80dba224e6ed79d306df50b1e92c2125d",
    "Average ‚¨ÜÔ∏è": 15.455406687090031,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 3056,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2549140578199318,
    "IFEval Raw": 0.2659321710838353,
    "IFEval": 26.593217108383534,
    "BBH Raw": 0.43615285239286355,
    "BBH": 21.11609932329174,
    "MATH Lvl 5 Raw": 0.07477341389728098,
    "MATH Lvl 5": 7.477341389728098,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4062395833333334,
    "MUSR": 10.979947916666669,
    "MMLU-PRO Raw": 0.2947972074468085,
    "MMLU-PRO": 21.644134160756497,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-08",
    "Submission Date": "2024-06-08",
    "Generation": 0,
    "Base Model": "google/gemma-7b"
  },
  {
    "eval_name": "google_gemma-7b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/gemma-7b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/gemma-7b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__gemma-7b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/gemma-7b-it",
    "Model sha": "18329f019fb74ca4b24f97371785268543d687d2",
    "Average ‚¨ÜÔ∏è": 12.868141901410231,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1138,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0999544452692995,
    "IFEval Raw": 0.3868324933398937,
    "IFEval": 38.68324933398937,
    "BBH Raw": 0.36455829222701713,
    "BBH": 11.880091344549442,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.42742708333333335,
    "MUSR": 12.528385416666667,
    "MMLU-PRO Raw": 0.16946476063829788,
    "MMLU-PRO": 7.7183067375886525,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-13",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "google/gemma-7b"
  },
  {
    "eval_name": "google_mt5-base_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MT5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/mt5-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/mt5-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__mt5-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/mt5-base",
    "Model sha": "2eb15465c5dd7f72a8f7984306ad05ebc3dd1e1f",
    "Average ‚¨ÜÔ∏è": 3.5652821226493496,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 189,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.20003981816446528,
    "IFEval Raw": 0.1645157072124186,
    "IFEval": 16.45157072124186,
    "BBH Raw": 0.28831600228488835,
    "BBH": 1.29855138817669,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23909395973154363,
    "GPQA": 0.0,
    "MUSR Raw": 0.36720833333333336,
    "MUSR": 2.8677083333333346,
    "MMLU-PRO Raw": 0.10696476063829788,
    "MMLU-PRO": 0.7738622931442081,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/mt5-base"
  },
  {
    "eval_name": "google_mt5-small_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MT5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/mt5-small\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/mt5-small</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__mt5-small-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/mt5-small",
    "Model sha": "73fb5dbe4756edadc8fbe8c769b0a109493acf7a",
    "Average ‚¨ÜÔ∏è": 4.2559281732773515,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 106,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.18049367436077327,
    "IFEval Raw": 0.17180968718555653,
    "IFEval": 17.180968718555654,
    "BBH Raw": 0.2765842029929075,
    "BBH": 1.0709714795008913,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.38575,
    "MUSR": 5.91875,
    "MMLU-PRO Raw": 0.11228390957446809,
    "MMLU-PRO": 1.3648788416075646,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/mt5-small"
  },
  {
    "eval_name": "google_mt5-xl_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MT5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/mt5-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/mt5-xl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__mt5-xl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/mt5-xl",
    "Model sha": "63fc6450d80515b48e026b69ef2fbbd426433e84",
    "Average ‚¨ÜÔ∏è": 5.191420153031625,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9037672245771817,
    "IFEval Raw": 0.19596448534333347,
    "IFEval": 19.596448534333348,
    "BBH Raw": 0.304735837080435,
    "BBH": 3.2824619143354035,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3795208333333333,
    "MUSR": 5.040104166666665,
    "MMLU-PRO Raw": 0.11195146276595745,
    "MMLU-PRO": 1.3279403073286051,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/mt5-xl"
  },
  {
    "eval_name": "google_mt5-xxl_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "T5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/mt5-xxl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/mt5-xxl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__mt5-xxl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/mt5-xxl",
    "Model sha": "e07c395916dfbc315d4e5e48b4a54a1e8821b5c0",
    "Average ‚¨ÜÔ∏è": 5.10307678308611,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 66,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2819385026222916,
    "IFEval Raw": 0.23575668116154028,
    "IFEval": 23.575668116154027,
    "BBH Raw": 0.2959344159116905,
    "BBH": 2.504710800447747,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24161073825503357,
    "GPQA": 0.0,
    "MUSR Raw": 0.36894791666666665,
    "MUSR": 3.5518229166666675,
    "MMLU-PRO Raw": 0.10887632978723404,
    "MMLU-PRO": 0.9862588652482256,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/mt5-xxl"
  },
  {
    "eval_name": "google_recurrentgemma-2b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "RecurrentGemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/recurrentgemma-2b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/recurrentgemma-2b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__recurrentgemma-2b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/recurrentgemma-2b",
    "Model sha": "195f13c55b371fc721eda0662c00c64642c70e17",
    "Average ‚¨ÜÔ∏è": 6.952186412902159,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 91,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.692652871561375,
    "IFEval Raw": 0.3017028151970106,
    "IFEval": 30.170281519701057,
    "BBH Raw": 0.31973582830084474,
    "BBH": 4.8203622310347365,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.3445729166666667,
    "MUSR": 3.1049479166666667,
    "MMLU-PRO Raw": 0.11760305851063829,
    "MMLU-PRO": 1.9558953900709206,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-06",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "google/recurrentgemma-2b"
  },
  {
    "eval_name": "google_recurrentgemma-2b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "RecurrentGemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/recurrentgemma-2b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/recurrentgemma-2b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__recurrentgemma-2b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/recurrentgemma-2b-it",
    "Model sha": "150248167d171fbdf4b02e7d28a4b3d749e570f6",
    "Average ‚¨ÜÔ∏è": 7.945552906776601,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 109,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.933035926041196,
    "IFEval Raw": 0.2949329999955673,
    "IFEval": 29.493299999556733,
    "BBH Raw": 0.33300047272606553,
    "BBH": 7.978763840391559,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.3340625,
    "MUSR": 3.6244791666666676,
    "MMLU-PRO Raw": 0.1402094414893617,
    "MMLU-PRO": 4.467715721040189,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "google/recurrentgemma-2b-it"
  },
  {
    "eval_name": "google_recurrentgemma-9b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "RecurrentGemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/recurrentgemma-9b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/recurrentgemma-9b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__recurrentgemma-9b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/recurrentgemma-9b",
    "Model sha": "7b0ed98fb889ba8bdfa7c690f08f2e57a7c48dae",
    "Average ‚¨ÜÔ∏è": 13.684284622472417,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 59,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 23.20619009817378,
    "IFEval Raw": 0.31159434744256354,
    "IFEval": 31.15943474425635,
    "BBH Raw": 0.39562568669428394,
    "BBH": 15.323368888997413,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.3802604166666667,
    "MUSR": 6.599218750000001,
    "MMLU-PRO Raw": 0.2604720744680851,
    "MMLU-PRO": 17.830230496453904,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "google/recurrentgemma-9b"
  },
  {
    "eval_name": "google_recurrentgemma-9b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "RecurrentGemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/recurrentgemma-9b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/recurrentgemma-9b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__recurrentgemma-9b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/recurrentgemma-9b-it",
    "Model sha": "43e62f98c3d496a5469ef4b18c1b11e417d68d1d",
    "Average ‚¨ÜÔ∏è": 19.23070312312456,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 50,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.362608412740016,
    "IFEval Raw": 0.5010383560065071,
    "IFEval": 50.10383560065072,
    "BBH Raw": 0.4367189649027647,
    "BBH": 21.621580084740117,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.43790625,
    "MUSR": 13.771614583333337,
    "MMLU-PRO Raw": 0.2843251329787234,
    "MMLU-PRO": 20.480570330969268,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-07-05",
    "Generation": 0,
    "Base Model": "google/recurrentgemma-9b-it"
  },
  {
    "eval_name": "google_switch-base-8_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "SwitchTransformersForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/switch-base-8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/switch-base-8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__switch-base-8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/switch-base-8",
    "Model sha": "92fe2d22b024d9937146fe097ba3d3a7ba146e1b",
    "Average ‚¨ÜÔ∏è": 3.2959502683966075,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.1467029338774024,
    "IFEval Raw": 0.15852050337548815,
    "IFEval": 15.852050337548814,
    "BBH Raw": 0.28763132730669333,
    "BBH": 1.7024781049821334,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.35173958333333327,
    "MUSR": 1.133333333333333,
    "MMLU-PRO Raw": 0.10979055851063829,
    "MMLU-PRO": 1.087839834515365,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-10-24",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/switch-base-8"
  },
  {
    "eval_name": "google_umt5-base_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "UMT5ForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/google/umt5-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">google/umt5-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/google__umt5-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "google/umt5-base",
    "Model sha": "0de9394d54f8975e71838d309de1cb496c894ab9",
    "Average ‚¨ÜÔ∏è": 3.441046025501144,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": -1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6680460445581992,
    "IFEval Raw": 0.174632198123202,
    "IFEval": 17.463219812320197,
    "BBH Raw": 0.27877262328945457,
    "BBH": 0.8135531788472962,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.33821875,
    "MUSR": 0.9440104166666662,
    "MMLU-PRO Raw": 0.10779587765957446,
    "MMLU-PRO": 0.8662086288416063,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-02",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "google/umt5-base"
  },
  {
    "eval_name": "gpt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gpt2",
    "Model sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
    "Average ‚¨ÜÔ∏è": 6.39102973137443,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2370,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.32392801241644914,
    "IFEval Raw": 0.1934168007553292,
    "IFEval": 19.34168007553292,
    "BBH Raw": 0.3036385401516729,
    "BBH": 2.7142978473877357,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.43241666666666667,
    "MUSR": 12.985416666666667,
    "MMLU-PRO Raw": 0.1149434840425532,
    "MMLU-PRO": 1.6603871158392434,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "gpt2"
  },
  {
    "eval_name": "gpt2_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gpt2",
    "Model sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
    "Average ‚¨ÜÔ∏è": 5.977736928104574,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2370,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.039245173068546815,
    "IFEval Raw": 0.08333333333333333,
    "IFEval": 8.333333333333332,
    "BBH Raw": 0.30833333333333335,
    "BBH": 9.199754901960786,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23333333333333334,
    "GPQA": 0.0,
    "MUSR Raw": 0.4333333333333333,
    "MUSR": 18.333333333333332,
    "MMLU-PRO Raw": 0.1,
    "MMLU-PRO": 0.0,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "gpt2"
  },
  {
    "eval_name": "gradientai_Llama-3-8B-Instruct-Gradient-1048k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gradientai/Llama-3-8B-Instruct-Gradient-1048k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gradientai/Llama-3-8B-Instruct-Gradient-1048k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gradientai__Llama-3-8B-Instruct-Gradient-1048k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gradientai/Llama-3-8B-Instruct-Gradient-1048k",
    "Model sha": "8697fb25cb77c852311e03b4464b8467471d56a4",
    "Average ‚¨ÜÔ∏è": 18.2455696265917,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 675,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8871644867662838,
    "IFEval Raw": 0.4455588948434598,
    "IFEval": 44.55588948434598,
    "BBH Raw": 0.4345903107069573,
    "BBH": 21.01052898715872,
    "MATH Lvl 5 Raw": 0.051359516616314195,
    "MATH Lvl 5": 5.135951661631419,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.42975,
    "MUSR": 13.518749999999997,
    "MMLU-PRO Raw": 0.29404920212765956,
    "MMLU-PRO": 21.56102245862884,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "gradientai/Llama-3-8B-Instruct-Gradient-1048k"
  },
  {
    "eval_name": "grimjim_Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge",
    "Model sha": "7a8d334dce0a2ce948f75612b8d3a61c53d094aa",
    "Average ‚¨ÜÔ∏è": 20.887036378959028,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5475483539199297,
    "IFEval Raw": 0.42712447417297217,
    "IFEval": 42.71244741729721,
    "BBH Raw": 0.4961694535006833,
    "BBH": 28.258014912987704,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.40432291666666664,
    "MUSR": 9.540364583333334,
    "MMLU-PRO Raw": 0.3625332446808511,
    "MMLU-PRO": 29.170360520094558,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-06-29",
    "Generation": 1,
    "Base Model": "grimjim/Llama-3-Instruct-8B-SPPO-Iter3-SimPO-merge (Merge)"
  },
  {
    "eval_name": "grimjim_Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge",
    "Model sha": "8f4d460ea20e24e48914156af7def305c0cd347f",
    "Average ‚¨ÜÔ∏è": 23.688474780111147,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6169416731127875,
    "IFEval Raw": 0.6805897241541332,
    "IFEval": 68.05897241541332,
    "BBH Raw": 0.5021734091176594,
    "BBH": 29.073285914476486,
    "MATH Lvl 5 Raw": 0.06797583081570996,
    "MATH Lvl 5": 6.797583081570996,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.38851041666666664,
    "MUSR": 6.697135416666669,
    "MMLU-PRO Raw": 0.3684341755319149,
    "MMLU-PRO": 29.826019503546092,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "grimjim/Llama-3-Instruct-8B-SimPO-SPPO-Iter3-merge (Merge)"
  },
  {
    "eval_name": "grimjim_Llama-3.1-8B-Instruct-abliterated_via_adapter_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/Llama-3.1-8B-Instruct-abliterated_via_adapter\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/Llama-3.1-8B-Instruct-abliterated_via_adapter</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__Llama-3.1-8B-Instruct-abliterated_via_adapter-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/Llama-3.1-8B-Instruct-abliterated_via_adapter",
    "Model sha": "b37ab2f859c96b125ff1c45c7ff0e267aa229156",
    "Average ‚¨ÜÔ∏è": 23.179537374596944,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9019147184130103,
    "IFEval Raw": 0.48695018107510296,
    "IFEval": 48.69501810751029,
    "BBH Raw": 0.510526564708187,
    "BBH": 29.415990262794168,
    "MATH Lvl 5 Raw": 0.13746223564954685,
    "MATH Lvl 5": 13.746223564954684,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.40103125,
    "MUSR": 9.262239583333335,
    "MMLU-PRO Raw": 0.3651097074468085,
    "MMLU-PRO": 29.4566341607565,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "grimjim/Llama-3.1-8B-Instruct-abliterated_via_adapter (Merge)"
  },
  {
    "eval_name": "grimjim_Magot-v1-Gemma2-8k-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/Magot-v1-Gemma2-8k-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/Magot-v1-Gemma2-8k-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__Magot-v1-Gemma2-8k-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/Magot-v1-Gemma2-8k-9B",
    "Model sha": "afae94acb42bc0dcf1d31b7338cb79c0bcab1829",
    "Average ‚¨ÜÔ∏è": 23.706234655492324,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9540369804066744,
    "IFEval Raw": 0.29967818720993633,
    "IFEval": 29.96781872099363,
    "BBH Raw": 0.6019447732218105,
    "BBH": 42.81812817526611,
    "MATH Lvl 5 Raw": 0.046072507552870096,
    "MATH Lvl 5": 4.6072507552870094,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.44884375,
    "MUSR": 14.905468750000004,
    "MMLU-PRO Raw": 0.43367686170212766,
    "MMLU-PRO": 37.07520685579196,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "grimjim/Magot-v1-Gemma2-8k-9B (Merge)"
  },
  {
    "eval_name": "grimjim_llama-3-Nephilim-v1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/llama-3-Nephilim-v1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/llama-3-Nephilim-v1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__llama-3-Nephilim-v1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/llama-3-Nephilim-v1-8B",
    "Model sha": "642799c8c768c53e831a03a1224db875116be866",
    "Average ‚¨ÜÔ∏è": 21.742325255357272,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8568726202971928,
    "IFEval Raw": 0.4277239945566652,
    "IFEval": 42.772399455666516,
    "BBH Raw": 0.5131817939007638,
    "BBH": 29.907537489079242,
    "MATH Lvl 5 Raw": 0.09138972809667675,
    "MATH Lvl 5": 9.138972809667676,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.41362499999999996,
    "MUSR": 10.63645833333333,
    "MMLU-PRO Raw": 0.37957114361702127,
    "MMLU-PRO": 31.063460401891252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-21",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "grimjim/llama-3-Nephilim-v1-8B (Merge)"
  },
  {
    "eval_name": "grimjim_llama-3-Nephilim-v2-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/llama-3-Nephilim-v2-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/llama-3-Nephilim-v2-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__llama-3-Nephilim-v2-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/llama-3-Nephilim-v2-8B",
    "Model sha": "924f56cdefbfaf38deb6aee3ad301ced027e142d",
    "Average ‚¨ÜÔ∏è": 20.58766179624135,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7008734157127516,
    "IFEval Raw": 0.39222817679313116,
    "IFEval": 39.22281767931311,
    "BBH Raw": 0.5048214936442625,
    "BBH": 29.896263840620197,
    "MATH Lvl 5 Raw": 0.10574018126888218,
    "MATH Lvl 5": 10.574018126888218,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.3895,
    "MUSR": 7.887500000000002,
    "MMLU-PRO Raw": 0.3641123670212766,
    "MMLU-PRO": 29.34581855791962,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "grimjim/llama-3-Nephilim-v2-8B (Merge)"
  },
  {
    "eval_name": "grimjim_llama-3-Nephilim-v2.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/llama-3-Nephilim-v2.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/llama-3-Nephilim-v2.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__llama-3-Nephilim-v2.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/llama-3-Nephilim-v2.1-8B",
    "Model sha": "5f516d9df1778dbe53ea941a754aef73b87e8eaa",
    "Average ‚¨ÜÔ∏è": 20.44755524463176,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.713158330577237,
    "IFEval Raw": 0.38950540122430705,
    "IFEval": 38.95054012243071,
    "BBH Raw": 0.5095042703104161,
    "BBH": 29.81966445991054,
    "MATH Lvl 5 Raw": 0.10045317220543808,
    "MATH Lvl 5": 10.045317220543808,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.3935,
    "MUSR": 7.887500000000002,
    "MMLU-PRO Raw": 0.3644448138297872,
    "MMLU-PRO": 29.38275709219858,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "grimjim/llama-3-Nephilim-v2.1-8B (Merge)"
  },
  {
    "eval_name": "grimjim_llama-3-Nephilim-v3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/grimjim/llama-3-Nephilim-v3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">grimjim/llama-3-Nephilim-v3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/grimjim__llama-3-Nephilim-v3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "grimjim/llama-3-Nephilim-v3-8B",
    "Model sha": "fd012ba05116aad7dc297d0a866ddb3345a056a1",
    "Average ‚¨ÜÔ∏è": 20.663929456309727,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5640896846190612,
    "IFEval Raw": 0.4173825449806513,
    "IFEval": 41.73825449806513,
    "BBH Raw": 0.5012671264428366,
    "BBH": 28.955635498374203,
    "MATH Lvl 5 Raw": 0.09894259818731119,
    "MATH Lvl 5": 9.894259818731118,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.3989270833333334,
    "MUSR": 8.332552083333338,
    "MMLU-PRO Raw": 0.3612034574468085,
    "MMLU-PRO": 29.022606382978722,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-08-26",
    "Generation": 1,
    "Base Model": "grimjim/llama-3-Nephilim-v3-8B (Merge)"
  },
  {
    "eval_name": "gupta-tanish_llama-7b-dpo-baseline_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/gupta-tanish/llama-7b-dpo-baseline\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">gupta-tanish/llama-7b-dpo-baseline</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/gupta-tanish__llama-7b-dpo-baseline-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "gupta-tanish/llama-7b-dpo-baseline",
    "Model sha": "1b5f1ef3ffa3b550619fbf64c33b6fd79e1bd559",
    "Average ‚¨ÜÔ∏è": 11.857290104453797,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7813164603497033,
    "IFEval Raw": 0.26930433472076315,
    "IFEval": 26.930433472076317,
    "BBH Raw": 0.3896894398264714,
    "BBH": 14.380522116367189,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.445625,
    "MUSR": 14.769791666666668,
    "MMLU-PRO Raw": 0.20279255319148937,
    "MMLU-PRO": 11.421394799054372,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "gupta-tanish/llama-7b-dpo-baseline (Merge)"
  },
  {
    "eval_name": "h2oai_h2o-danube3-4b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/h2oai/h2o-danube3-4b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">h2oai/h2o-danube3-4b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/h2oai__h2o-danube3-4b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "h2oai/h2o-danube3-4b-base",
    "Model sha": "6bdf2f1e317143c998b88d9e9d72facc621a863f",
    "Average ‚¨ÜÔ∏è": 10.015320048495058,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 20,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44450196460680436,
    "IFEval Raw": 0.23380851695722904,
    "IFEval": 23.380851695722903,
    "BBH Raw": 0.3599083951265592,
    "BBH": 10.564444044561542,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.37781250000000005,
    "MUSR": 6.526562500000002,
    "MMLU-PRO Raw": 0.2109375,
    "MMLU-PRO": 12.326388888888888,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-04",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "h2oai/h2o-danube3-4b-base"
  },
  {
    "eval_name": "h2oai_h2o-danube3-4b-chat_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/h2oai/h2o-danube3-4b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">h2oai/h2o-danube3-4b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/h2oai__h2o-danube3-4b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "h2oai/h2o-danube3-4b-chat",
    "Model sha": "1e5c6fa6620f8bf078958069ab4581cd88e0202c",
    "Average ‚¨ÜÔ∏è": 11.39501377161943,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 65,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4626213369360926,
    "IFEval Raw": 0.3628771659197596,
    "IFEval": 36.28771659197596,
    "BBH Raw": 0.3466170643135169,
    "BBH": 8.839702966263845,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.378125,
    "MUSR": 5.232291666666669,
    "MMLU-PRO Raw": 0.22282247340425532,
    "MMLU-PRO": 13.6469414893617,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-04",
    "Submission Date": "2024-07-15",
    "Generation": 0,
    "Base Model": "h2oai/h2o-danube3-4b-chat"
  },
  {
    "eval_name": "h2oai_h2o-danube3-500m-chat_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/h2oai/h2o-danube3-500m-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">h2oai/h2o-danube3-500m-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/h2oai__h2o-danube3-500m-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "h2oai/h2o-danube3-500m-chat",
    "Model sha": "c202f976c26875541e738ea978c8158fa536da9a",
    "Average ‚¨ÜÔ∏è": 5.028206641571718,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 31,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.21890335109136297,
    "IFEval Raw": 0.2207941594968018,
    "IFEval": 22.079415949680183,
    "BBH Raw": 0.3034691168308313,
    "BBH": 3.065370444981646,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.23070469798657717,
    "GPQA": 0.0,
    "MUSR Raw": 0.34339583333333334,
    "MUSR": 2.824479166666667,
    "MMLU-PRO Raw": 0.11436170212765957,
    "MMLU-PRO": 1.595744680851063,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-04",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "h2oai/h2o-danube3-500m-chat"
  },
  {
    "eval_name": "haoranxu_ALMA-13B-R_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/haoranxu/ALMA-13B-R\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">haoranxu/ALMA-13B-R</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/haoranxu__ALMA-13B-R-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "haoranxu/ALMA-13B-R",
    "Model sha": "b69ebad694274b929cfcf3db29dd7bb93d752e39",
    "Average ‚¨ÜÔ∏è": 3.587775291474459,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 78,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9626298626305162,
    "IFEval Raw": 0.003921816336210145,
    "IFEval": 0.3921816336210145,
    "BBH Raw": 0.345656261205981,
    "BBH": 8.819669166822672,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.35279166666666667,
    "MUSR": 2.232291666666667,
    "MMLU-PRO Raw": 0.18168218085106383,
    "MMLU-PRO": 9.075797872340424,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-17",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "haoranxu/ALMA-13B-R"
  },
  {
    "eval_name": "haoranxu_Llama-3-Instruct-8B-CPO-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/haoranxu/Llama-3-Instruct-8B-CPO-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">haoranxu/Llama-3-Instruct-8B-CPO-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/haoranxu__Llama-3-Instruct-8B-CPO-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "haoranxu/Llama-3-Instruct-8B-CPO-SimPO",
    "Model sha": "3ca4b5c3a6395ff090e1039d55ac1f6120777302",
    "Average ‚¨ÜÔ∏è": 24.570858101407055,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.745335038770084,
    "IFEval Raw": 0.7046447869430887,
    "IFEval": 70.46447869430888,
    "BBH Raw": 0.5048301774821616,
    "BBH": 29.762188091412483,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3566666666666667,
    "MUSR": 3.4166666666666674,
    "MMLU-PRO Raw": 0.3686003989361702,
    "MMLU-PRO": 29.844488770685572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-19",
    "Submission Date": "2024-07-28",
    "Generation": 0,
    "Base Model": "haoranxu/Llama-3-Instruct-8B-CPO-SimPO"
  },
  {
    "eval_name": "haoranxu_Llama-3-Instruct-8B-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/haoranxu/Llama-3-Instruct-8B-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">haoranxu/Llama-3-Instruct-8B-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/haoranxu__Llama-3-Instruct-8B-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "haoranxu/Llama-3-Instruct-8B-SimPO",
    "Model sha": "8346770280fa169d41d737785dd63a66e9d94501",
    "Average ‚¨ÜÔ∏è": 24.827083715980304,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5795779980701291,
    "IFEval Raw": 0.7347449212533854,
    "IFEval": 73.47449212533854,
    "BBH Raw": 0.49792360151415016,
    "BBH": 28.2263760762505,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.35660416666666667,
    "MUSR": 3.7421875000000004,
    "MMLU-PRO Raw": 0.37333776595744683,
    "MMLU-PRO": 30.37086288416076,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-07-28",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "hon9kon9ize_CantoneseLLMChat-v0.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/hon9kon9ize/CantoneseLLMChat-v0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">hon9kon9ize/CantoneseLLMChat-v0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/hon9kon9ize__CantoneseLLMChat-v0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "hon9kon9ize/CantoneseLLMChat-v0.5",
    "Model sha": "812eb4f168c3ea258ebb220393401db9578e0f67",
    "Average ‚¨ÜÔ∏è": 15.783567182822972,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8336341749315878,
    "IFEval Raw": 0.3230849701015528,
    "IFEval": 32.30849701015528,
    "BBH Raw": 0.43452388803059244,
    "BBH": 20.761385180655164,
    "MATH Lvl 5 Raw": 0.030966767371601214,
    "MATH Lvl 5": 3.0966767371601214,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4706458333333334,
    "MUSR": 18.13072916666667,
    "MMLU-PRO Raw": 0.2504155585106383,
    "MMLU-PRO": 16.712839834515364,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-01",
    "Submission Date": "2024-07-07",
    "Generation": 0,
    "Base Model": "hon9kon9ize/CantoneseLLMChat-v0.5"
  },
  {
    "eval_name": "hon9kon9ize_CantoneseLLMChat-v1.0-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/hon9kon9ize/CantoneseLLMChat-v1.0-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">hon9kon9ize/CantoneseLLMChat-v1.0-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/hon9kon9ize__CantoneseLLMChat-v1.0-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "hon9kon9ize/CantoneseLLMChat-v1.0-7B",
    "Model sha": "4703b1afc7aab8e3a8059432fd1c4b0aba011482",
    "Average ‚¨ÜÔ∏è": 23.252107598622704,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8313964563876477,
    "IFEval Raw": 0.44548353923146145,
    "IFEval": 44.54835392314614,
    "BBH Raw": 0.4865734655539633,
    "BBH": 28.53613616746739,
    "MATH Lvl 5 Raw": 0.19561933534743203,
    "MATH Lvl 5": 19.561933534743204,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.3882916666666667,
    "MUSR": 6.303125000000004,
    "MMLU-PRO Raw": 0.3784906914893617,
    "MMLU-PRO": 30.94341016548463,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-10",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "huggyllama_llama-13b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-13b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/huggyllama__llama-13b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "huggyllama/llama-13b",
    "Model sha": "bf57045473f207bb1de1ed035ace226f4d9f9bba",
    "Average ‚¨ÜÔ∏è": 9.291479464313436,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 137,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1061405985715995,
    "IFEval Raw": 0.24105262924595627,
    "IFEval": 24.10526292459563,
    "BBH Raw": 0.39878925581174585,
    "BBH": 16.145707376925767,
    "MATH Lvl 5 Raw": 0.01435045317220544,
    "MATH Lvl 5": 1.435045317220544,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.34621875,
    "MUSR": 2.8106770833333345,
    "MMLU-PRO Raw": 0.19522938829787234,
    "MMLU-PRO": 10.581043144208037,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-04-03",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "huggyllama/llama-13b"
  },
  {
    "eval_name": "huggyllama_llama-65b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-65b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-65b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/huggyllama__llama-65b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "huggyllama/llama-65b",
    "Model sha": "49707c5313d34d1c5a846e29cf2a2a650c22c8ee",
    "Average ‚¨ÜÔ∏è": 13.587326620388723,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 74,
    "#Params (B)": 65,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.330108081950852,
    "IFEval Raw": 0.25259311958935626,
    "IFEval": 25.259311958935623,
    "BBH Raw": 0.4702556052882764,
    "BBH": 25.254277114598622,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.35945833333333327,
    "MUSR": 1.9656249999999986,
    "MMLU-PRO Raw": 0.3077626329787234,
    "MMLU-PRO": 23.084736997635936,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-04-04",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "huggyllama/llama-65b"
  },
  {
    "eval_name": "huggyllama_llama-7b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/huggyllama/llama-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huggyllama/llama-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/huggyllama__llama-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "huggyllama/llama-7b",
    "Model sha": "4782ad278652c7c71b72204d462d6d01eaaf7549",
    "Average ‚¨ÜÔ∏è": 6.389823692285343,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 296,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5636042156771623,
    "IFEval Raw": 0.25009530268576263,
    "IFEval": 25.009530268576263,
    "BBH Raw": 0.32773134782898566,
    "BBH": 7.076660678102023,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.33539583333333334,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.13131648936170212,
    "MMLU-PRO": 3.4796099290780127,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-04-03",
    "Submission Date": "2024-07-04",
    "Generation": 0,
    "Base Model": "huggyllama/llama-7b"
  },
  {
    "eval_name": "huihui-ai_Qwen2.5-7B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/huihui-ai/Qwen2.5-7B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huihui-ai/Qwen2.5-7B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/huihui-ai__Qwen2.5-7B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "huihui-ai/Qwen2.5-7B-Instruct-abliterated",
    "Model sha": "c04c14c82962506e2b16f58f9f6b0a2e60a6afde",
    "Average ‚¨ÜÔ∏è": 26.6475056681965,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.166124305693874,
    "IFEval Raw": 0.7546033413564897,
    "IFEval": 75.46033413564896,
    "BBH Raw": 0.5261589972829911,
    "BBH": 32.886673214320496,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.39666666666666667,
    "MUSR": 7.483333333333333,
    "MMLU-PRO Raw": 0.41796875,
    "MMLU-PRO": 35.32986111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-24",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "huihui-ai_Qwen2.5-7B-Instruct-abliterated-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/huihui-ai__Qwen2.5-7B-Instruct-abliterated-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "huihui-ai/Qwen2.5-7B-Instruct-abliterated-v2",
    "Model sha": "05d179c1108cc2dc1c1a16a8255ac6f57eac5d32",
    "Average ‚¨ÜÔ∏è": 26.999904985788007,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.219740015468243,
    "IFEval Raw": 0.7606484128778308,
    "IFEval": 76.06484128778308,
    "BBH Raw": 0.5376688442794247,
    "BBH": 34.369626512494015,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.3980625,
    "MUSR": 8.091145833333334,
    "MMLU-PRO Raw": 0.42079454787234044,
    "MMLU-PRO": 35.643838652482266,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-24",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "iRyanBell_ARC1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/iRyanBell/ARC1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">iRyanBell/ARC1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/iRyanBell__ARC1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "iRyanBell/ARC1",
    "Model sha": "28176c0fb77fa43e1410766faf35d2a2681566e9",
    "Average ‚¨ÜÔ∏è": 19.623910591690436,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.924664498382478,
    "IFEval Raw": 0.441112913735555,
    "IFEval": 44.1112913735555,
    "BBH Raw": 0.4902999658144703,
    "BBH": 26.564495132631716,
    "MATH Lvl 5 Raw": 0.0664652567975831,
    "MATH Lvl 5": 6.64652567975831,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3990520833333333,
    "MUSR": 8.148177083333335,
    "MMLU-PRO Raw": 0.3371010638297872,
    "MMLU-PRO": 26.34456264775413,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "iRyanBell/ARC1"
  },
  {
    "eval_name": "iRyanBell_ARC1-II_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/iRyanBell/ARC1-II\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">iRyanBell/ARC1-II</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/iRyanBell__ARC1-II-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "iRyanBell/ARC1-II",
    "Model sha": "c81076b9bdaac0722b33e411a49b07a296e8fae8",
    "Average ‚¨ÜÔ∏è": 9.320256405776588,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.895276262218848,
    "IFEval Raw": 0.17083560508340093,
    "IFEval": 17.083560508340092,
    "BBH Raw": 0.33817781029884353,
    "BBH": 7.246229410679451,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.4912916666666667,
    "MUSR": 20.311458333333334,
    "MMLU-PRO Raw": 0.1685505319148936,
    "MMLU-PRO": 7.616725768321511,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-12",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "iRyanBell/ARC1-II"
  },
  {
    "eval_name": "ibivibiv_colossus_120b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibivibiv/colossus_120b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibivibiv/colossus_120b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibivibiv__colossus_120b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibivibiv/colossus_120b",
    "Model sha": "b4c11f98bd874bfa454a0bb46153335cfb9b06a3",
    "Average ‚¨ÜÔ∏è": 25.377438954944267,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 117,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.75243173308043,
    "IFEval Raw": 0.42759877126025614,
    "IFEval": 42.759877126025614,
    "BBH Raw": 0.6061408586494191,
    "BBH": 44.071497527478364,
    "MATH Lvl 5 Raw": 0.054380664652567974,
    "MATH Lvl 5": 5.438066465256798,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.4733125,
    "MUSR": 19.264062499999998,
    "MMLU-PRO Raw": 0.3961103723404255,
    "MMLU-PRO": 32.901152482269495,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-12",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "ibivibiv/colossus_120b"
  },
  {
    "eval_name": "ibivibiv_multimaster-7b-v6_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibivibiv/multimaster-7b-v6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibivibiv/multimaster-7b-v6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibivibiv__multimaster-7b-v6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibivibiv/multimaster-7b-v6",
    "Model sha": "7b3bfecb654c86565c65cd510dd1138cb3e75087",
    "Average ‚¨ÜÔ∏è": 21.127533023833326,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 35,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.5746805043774152,
    "IFEval Raw": 0.4473075883101283,
    "IFEval": 44.73075883101282,
    "BBH Raw": 0.519351871026721,
    "BBH": 32.40128043389345,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.43957291666666665,
    "MUSR": 13.379947916666671,
    "MMLU-PRO Raw": 0.30950797872340424,
    "MMLU-PRO": 23.278664302600472,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-24",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "ibivibiv/multimaster-7b-v6"
  },
  {
    "eval_name": "ibm_merlinite-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm/merlinite-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm/merlinite-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm__merlinite-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm/merlinite-7b",
    "Model sha": "233d12759d5bb9344231dafdb51310ec19d79c0e",
    "Average ‚¨ÜÔ∏è": 16.763622002709972,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 103,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5503968344485408,
    "IFEval Raw": 0.2498703440205322,
    "IFEval": 24.98703440205322,
    "BBH Raw": 0.50071326118705,
    "BBH": 29.97724776968684,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.44115624999999997,
    "MUSR": 13.877864583333334,
    "MMLU-PRO Raw": 0.3068484042553192,
    "MMLU-PRO": 22.983156028368796,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-02",
    "Submission Date": "2024-06-09",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-1b-a400m-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-1b-a400m-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-1b-a400m-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-1b-a400m-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-1b-a400m-base",
    "Model sha": "8f3d6d6fb24a1d2528f24bad0d2ae3e8fc6f3232",
    "Average ‚¨ÜÔ∏è": 5.917496921650671,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4764253621446746,
    "IFEval Raw": 0.24040324117785256,
    "IFEval": 24.040324117785254,
    "BBH Raw": 0.3221205531032148,
    "BBH": 6.055007672005359,
    "MATH Lvl 5 Raw": 0.019637462235649546,
    "MATH Lvl 5": 1.9637462235649545,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.3367291666666667,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.11519281914893617,
    "MMLU-PRO": 1.6880910165484628,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "",
    "Generation": 0,
    "Base Model": "ibm-granite/granite-3.0-1b-a400m-base"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-1b-a400m-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-1b-a400m-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-1b-a400m-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-1b-a400m-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-1b-a400m-instruct",
    "Model sha": "acb9675a7d67b8657d9b8105d5cbd5818408293f",
    "Average ‚¨ÜÔ∏è": 8.018876027775205,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.184326487724173,
    "IFEval Raw": 0.33315159332792543,
    "IFEval": 33.31515933279255,
    "BBH Raw": 0.3223950988485842,
    "BBH": 5.453219408698861,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.36228124999999994,
    "MUSR": 2.6851562500000004,
    "MMLU-PRO Raw": 0.12441821808510638,
    "MMLU-PRO": 2.713135342789597,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "",
    "Generation": 1,
    "Base Model": "ibm-granite/granite-3.0-1b-a400m-instruct (Merge)"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-2b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-2b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-2b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-2b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-2b-base",
    "Model sha": "532f55c03d71a31905c0b825eba4b24fe7f7936b",
    "Average ‚¨ÜÔ∏è": 14.108372991614473,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0461668914642688,
    "IFEval Raw": 0.3873821460391761,
    "IFEval": 38.738214603917605,
    "BBH Raw": 0.40474805593806223,
    "BBH": 17.563749725828334,
    "MATH Lvl 5 Raw": 0.05513595166163142,
    "MATH Lvl 5": 5.5135951661631415,
    "GPQA Raw": 0.28020134228187926,
    "GPQA": 4.026845637583901,
    "MUSR Raw": 0.3434270833333333,
    "MUSR": 3.461718750000001,
    "MMLU-PRO Raw": 0.23811502659574468,
    "MMLU-PRO": 15.346114066193852,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "",
    "Generation": 0,
    "Base Model": "ibm-granite/granite-3.0-2b-base"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-2b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-2b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-2b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-2b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-2b-instruct",
    "Model sha": "342f92f4a0b4d6d83c0b61dc6c122e253a4efebd",
    "Average ‚¨ÜÔ∏è": 18.32056641337788,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 41,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.018948081568125,
    "IFEval Raw": 0.513977357854936,
    "IFEval": 51.3977357854936,
    "BBH Raw": 0.44119772062630297,
    "BBH": 21.73789141090241,
    "MATH Lvl 5 Raw": 0.08761329305135952,
    "MATH Lvl 5": 8.761329305135952,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.35148958333333336,
    "MUSR": 1.2695312499999996,
    "MMLU-PRO Raw": 0.2814162234042553,
    "MMLU-PRO": 20.15735815602837,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "",
    "Generation": 1,
    "Base Model": "ibm-granite/granite-3.0-2b-instruct (Merge)"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-3b-a800m-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-3b-a800m-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-3b-a800m-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-3b-a800m-base",
    "Model sha": "0d1d12f91791b25289ef407e39d88f00d1256d10",
    "Average ‚¨ÜÔ∏è": 9.426900867369772,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.5357838236007946,
    "IFEval Raw": 0.2732261510569733,
    "IFEval": 27.32261510569733,
    "BBH Raw": 0.36674974971308566,
    "BBH": 11.3484424218006,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.34196875,
    "MUSR": 3.312760416666667,
    "MMLU-PRO Raw": 0.18907912234042554,
    "MMLU-PRO": 9.89768026004728,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "",
    "Generation": 0,
    "Base Model": "ibm-granite/granite-3.0-3b-a800m-base"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-3b-a800m-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-3b-a800m-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-3b-a800m-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-3b-a800m-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-3b-a800m-instruct",
    "Model sha": "ab0c732243cfd50a601fa393dd46a2c5993746f7",
    "Average ‚¨ÜÔ∏è": 13.660359975521821,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.076680553126776,
    "IFEval Raw": 0.4298217618142085,
    "IFEval": 42.982176181420854,
    "BBH Raw": 0.37527805291733446,
    "BBH": 13.163009595010001,
    "MATH Lvl 5 Raw": 0.06797583081570996,
    "MATH Lvl 5": 6.797583081570996,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.3486666666666667,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.21517619680851063,
    "MMLU-PRO": 12.797355200945626,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "ibm-granite/granite-3.0-3b-a800m-instruct (Merge)"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-8b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-8b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-8b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-8b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-8b-base",
    "Model sha": "1edd1f646abfcd90ed5d6c0d9711fbb02c947884",
    "Average ‚¨ÜÔ∏è": 21.653160048346322,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 21,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.885655879319516,
    "IFEval Raw": 0.4583482936386566,
    "IFEval": 45.83482936386566,
    "BBH Raw": 0.4943760637365333,
    "BBH": 27.974358298982427,
    "MATH Lvl 5 Raw": 0.09894259818731119,
    "MATH Lvl 5": 9.894259818731118,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.40813541666666664,
    "MUSR": 10.450260416666664,
    "MMLU-PRO Raw": 0.3312832446808511,
    "MMLU-PRO": 25.69813829787234,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-20",
    "Generation": 0,
    "Base Model": "ibm-granite/granite-3.0-8b-base"
  },
  {
    "eval_name": "ibm-granite_granite-3.0-8b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GraniteForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-3.0-8b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-3.0-8b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-3.0-8b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-3.0-8b-instruct",
    "Model sha": "e0a466fb25b9e07e9c2dc93380a360189700d1f8",
    "Average ‚¨ÜÔ∏è": 23.864033234852883,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 171,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.712992587007898,
    "IFEval Raw": 0.5309633993359841,
    "IFEval": 53.09633993359841,
    "BBH Raw": 0.5191874631840226,
    "BBH": 31.588159064715125,
    "MATH Lvl 5 Raw": 0.13217522658610273,
    "MATH Lvl 5": 13.217522658610273,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.3900625,
    "MUSR": 7.024479166666669,
    "MMLU-PRO Raw": 0.34566156914893614,
    "MMLU-PRO": 27.295729905437348,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "ibm-granite/granite-3.0-8b-instruct (Merge)"
  },
  {
    "eval_name": "ibm-granite_granite-7b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-7b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-7b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-7b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-7b-base",
    "Model sha": "23fcb4cb5b69f8a122fb944491e9f1ad664ba37b",
    "Average ‚¨ÜÔ∏è": 7.7576445280227295,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6526238385611688,
    "IFEval Raw": 0.24142719096441884,
    "IFEval": 24.142719096441887,
    "BBH Raw": 0.34804372716106186,
    "BBH": 9.050800002899097,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.35548958333333336,
    "MUSR": 3.4028645833333346,
    "MMLU-PRO Raw": 0.18342752659574468,
    "MMLU-PRO": 9.269725177304965,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-19",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "ibm-granite/granite-7b-base"
  },
  {
    "eval_name": "ibm-granite_granite-7b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ibm-granite/granite-7b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ibm-granite/granite-7b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ibm-granite__granite-7b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ibm-granite/granite-7b-instruct",
    "Model sha": "c6d1adfa5cdba2c8344e055bb7de87b7935250a8",
    "Average ‚¨ÜÔ∏è": 11.808373451644258,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7114519189650554,
    "IFEval Raw": 0.2972313461615181,
    "IFEval": 29.72313461615181,
    "BBH Raw": 0.37229529603269523,
    "BBH": 12.639328702465264,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40199999999999997,
    "MUSR": 8.816666666666668,
    "MMLU-PRO Raw": 0.2286402925531915,
    "MMLU-PRO": 14.293365839243496,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-19",
    "Submission Date": "2024-10-02",
    "Generation": 1,
    "Base Model": "ibm/granite-7b-base"
  },
  {
    "eval_name": "icefog72_Ice0.15-02.10-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.15-02.10-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.15-02.10-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.15-02.10-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.15-02.10-RP",
    "Model sha": "ab67a8b63836ec7c8e6729d79d9dfd2708b20eb3",
    "Average ‚¨ÜÔ∏è": 21.49132746100335,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5928221604212743,
    "IFEval Raw": 0.5343355629729118,
    "IFEval": 53.43355629729119,
    "BBH Raw": 0.4976384736188401,
    "BBH": 30.130104071068587,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.43197916666666664,
    "MUSR": 12.997395833333337,
    "MMLU-PRO Raw": 0.30659906914893614,
    "MMLU-PRO": 22.95545212765957,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-02",
    "Generation": 0,
    "Base Model": "icefog72/Ice0.15-02.10-RP"
  },
  {
    "eval_name": "icefog72_Ice0.16-02.10-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.16-02.10-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.16-02.10-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.16-02.10-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.16-02.10-RP",
    "Model sha": "cb5c4d8a2e74efb41eae8b6dff8d06252c0a795d",
    "Average ‚¨ÜÔ∏è": 21.051242383176817,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5967558303861016,
    "IFEval Raw": 0.5069083365470286,
    "IFEval": 50.69083365470286,
    "BBH Raw": 0.4945564313654156,
    "BBH": 29.58232083302582,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.433375,
    "MUSR": 13.405208333333329,
    "MMLU-PRO Raw": 0.3067652925531915,
    "MMLU-PRO": 22.973921394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-02",
    "Generation": 0,
    "Base Model": "icefog72/Ice0.16-02.10-RP"
  },
  {
    "eval_name": "icefog72_Ice0.17-03.10-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.17-03.10-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.17-03.10-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.17-03.10-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.17-03.10-RP",
    "Model sha": "ca5a429546334784d94bcab0eb52c5f22f433680",
    "Average ‚¨ÜÔ∏è": 21.41440427176671,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6102062530197291,
    "IFEval Raw": 0.5123538876846767,
    "IFEval": 51.235388768467665,
    "BBH Raw": 0.5006815748225494,
    "BBH": 30.376262438172095,
    "MATH Lvl 5 Raw": 0.06117824773413898,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.433375,
    "MUSR": 13.338541666666663,
    "MMLU-PRO Raw": 0.30851063829787234,
    "MMLU-PRO": 23.167848699763592,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "icefog72/Ice0.17-03.10-RP"
  },
  {
    "eval_name": "icefog72_Ice0.27-06.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.27-06.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.27-06.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.27-06.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.27-06.11-RP",
    "Model sha": "f2c78e71b59e0d36475217e3f265bc135f7c8505",
    "Average ‚¨ÜÔ∏è": 21.83125244134635,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4143506326946029,
    "IFEval Raw": 0.49182059158588104,
    "IFEval": 49.1820591585881,
    "BBH Raw": 0.5111654648230625,
    "BBH": 31.364751797095256,
    "MATH Lvl 5 Raw": 0.05664652567975831,
    "MATH Lvl 5": 5.664652567975831,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.43278125000000006,
    "MUSR": 12.564322916666669,
    "MMLU-PRO Raw": 0.3154089095744681,
    "MMLU-PRO": 23.93432328605201,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.27-06.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.29-06.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.29-06.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.29-06.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.29-06.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.29-06.11-RP",
    "Model sha": "932f16ea3f790553904f0d2dfcdc861d737cbaf7",
    "Average ‚¨ÜÔ∏è": 21.703644196424325,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4298166096071882,
    "IFEval Raw": 0.486050346414181,
    "IFEval": 48.6050346414181,
    "BBH Raw": 0.5087880173407883,
    "BBH": 31.359453902395284,
    "MATH Lvl 5 Raw": 0.055891238670694864,
    "MATH Lvl 5": 5.589123867069486,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4458958333333333,
    "MUSR": 14.370312499999995,
    "MMLU-PRO Raw": 0.30925864361702127,
    "MMLU-PRO": 23.250960401891252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.29-06.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.31-08.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.31-08.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.31-08.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.31-08.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.31-08.11-RP",
    "Model sha": "52d947b170ee72c7f4c2b63b11f00330847e44f9",
    "Average ‚¨ÜÔ∏è": 21.886899252180523,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.46346938365663953,
    "IFEval Raw": 0.5145768782386291,
    "IFEval": 51.457687823862905,
    "BBH Raw": 0.5032134100285419,
    "BBH": 30.460341897671253,
    "MATH Lvl 5 Raw": 0.06117824773413897,
    "MATH Lvl 5": 6.117824773413897,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.42766666666666664,
    "MUSR": 11.891666666666667,
    "MMLU-PRO Raw": 0.3130817819148936,
    "MMLU-PRO": 23.675753546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.31-08.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.32-10.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.32-10.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.32-10.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.32-10.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.32-10.11-RP",
    "Model sha": "a05dbb7fe0e756afb73c19e6f33c5481a9ac2ba8",
    "Average ‚¨ÜÔ∏è": 21.63483115225961,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4240323946721359,
    "IFEval Raw": 0.49154576523623983,
    "IFEval": 49.15457652362399,
    "BBH Raw": 0.5047695597611622,
    "BBH": 30.430940035916453,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.4382083333333333,
    "MUSR": 13.476041666666669,
    "MMLU-PRO Raw": 0.3100066489361702,
    "MMLU-PRO": 23.33407210401891,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.32-10.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.34b-14.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.34b-14.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.34b-14.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.34b-14.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.34b-14.11-RP",
    "Model sha": "5362f57fd0402c7c14c8dbe6b55c8b979cc8f475",
    "Average ‚¨ÜÔ∏è": 21.681833980913552,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.42982302330077976,
    "IFEval Raw": 0.47620868185303883,
    "IFEval": 47.620868185303884,
    "BBH Raw": 0.5067195329696937,
    "BBH": 30.80635727588579,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4419895833333333,
    "MUSR": 13.615364583333333,
    "MMLU-PRO Raw": 0.3125,
    "MMLU-PRO": 23.61111111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.34b-14.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.34n-14.11-RP_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.34n-14.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.34n-14.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.34n-14.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.34n-14.11-RP",
    "Model sha": "1a39b99112926fc8dd44c3be35d99c04388d3078",
    "Average ‚¨ÜÔ∏è": 21.828057183799743,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4485484687885171,
    "IFEval Raw": 0.47865663107222167,
    "IFEval": 47.86566310722217,
    "BBH Raw": 0.5091090160356474,
    "BBH": 31.206252799751898,
    "MATH Lvl 5 Raw": 0.06948640483383686,
    "MATH Lvl 5": 6.948640483383686,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.4379583333333333,
    "MUSR": 12.844791666666666,
    "MMLU-PRO Raw": 0.31241688829787234,
    "MMLU-PRO": 23.60187647754137,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.34n-14.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.37-18.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.37-18.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.37-18.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.37-18.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.37-18.11-RP",
    "Model sha": "4d9dfaa52efdaede3291c85ccb9c5966636298e0",
    "Average ‚¨ÜÔ∏è": 21.913941249727642,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4145128109049576,
    "IFEval Raw": 0.4972162750391184,
    "IFEval": 49.72162750391184,
    "BBH Raw": 0.5084310833712639,
    "BBH": 31.042850362735788,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.43392708333333335,
    "MUSR": 12.207552083333335,
    "MMLU-PRO Raw": 0.3143284574468085,
    "MMLU-PRO": 23.814273049645386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.37-18.11-RP (Merge)"
  },
  {
    "eval_name": "icefog72_Ice0.38-19.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.38-19.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.38-19.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.38-19.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.38-19.11-RP",
    "Model sha": "5d35120e4511369d97441c1732b3abf02bcc27ff",
    "Average ‚¨ÜÔ∏è": 20.851529815240237,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.42273775323924095,
    "IFEval Raw": 0.44033830237104216,
    "IFEval": 44.03383023710421,
    "BBH Raw": 0.510108216407024,
    "BBH": 31.3306289411177,
    "MATH Lvl 5 Raw": 0.05740181268882175,
    "MATH Lvl 5": 5.740181268882175,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.43671875,
    "MUSR": 12.956510416666667,
    "MMLU-PRO Raw": 0.31399601063829785,
    "MMLU-PRO": 23.777334515366427,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "icefog72_Ice0.39-19.11-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.39-19.11-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.39-19.11-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.39-19.11-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.39-19.11-RP",
    "Model sha": "044d7404646a13187ecabc5f87480a4e6bcaf18c",
    "Average ‚¨ÜÔ∏è": 21.326177223206326,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4252933563188715,
    "IFEval Raw": 0.47565902915375646,
    "IFEval": 47.565902915375645,
    "BBH Raw": 0.5092985137525424,
    "BBH": 31.263627378198198,
    "MATH Lvl 5 Raw": 0.04909365558912387,
    "MATH Lvl 5": 4.909365558912387,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.4341458333333333,
    "MUSR": 12.534895833333335,
    "MMLU-PRO Raw": 0.3126662234042553,
    "MMLU-PRO": 23.62958037825059,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "icefog72_Ice0.7-29.09-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/Ice0.7-29.09-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/Ice0.7-29.09-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__Ice0.7-29.09-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/Ice0.7-29.09-RP",
    "Model sha": "932f2687137eebcafa9b90fe06e73ed272e0be81",
    "Average ‚¨ÜÔ∏è": 21.58781076739574,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5868542314197152,
    "IFEval Raw": 0.5175744801570943,
    "IFEval": 51.75744801570943,
    "BBH Raw": 0.5047661992357916,
    "BBH": 30.72587571429055,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.4237916666666666,
    "MUSR": 11.507291666666669,
    "MMLU-PRO Raw": 0.3126662234042553,
    "MMLU-PRO": 23.62958037825059,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "icefog72/Ice0.7-29.09-RP (Merge)"
  },
  {
    "eval_name": "icefog72_IceCocoaRP-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceCocoaRP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceCocoaRP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceCocoaRP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceCocoaRP-7b",
    "Model sha": "001beaf88932f7e010af21bbdeff0079bda73b1d",
    "Average ‚¨ÜÔ∏è": 20.959618631101474,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5849415728108589,
    "IFEval Raw": 0.4962421929369628,
    "IFEval": 49.62421929369628,
    "BBH Raw": 0.4937902147076245,
    "BBH": 29.63689549472275,
    "MATH Lvl 5 Raw": 0.05966767371601209,
    "MATH Lvl 5": 5.966767371601208,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.4197916666666666,
    "MUSR": 11.17395833333333,
    "MMLU-PRO Raw": 0.3098404255319149,
    "MMLU-PRO": 23.315602836879428,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "icefog72/IceCocoaRP-7b (Merge)"
  },
  {
    "eval_name": "icefog72_IceCoffeeRP-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceCoffeeRP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceCoffeeRP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceCoffeeRP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceCoffeeRP-7b",
    "Model sha": "131c0f7c0809a9d23b05b63cb550a586c3c7b372",
    "Average ‚¨ÜÔ∏è": 20.34382476557608,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5717445464225894,
    "IFEval Raw": 0.4959174989029109,
    "IFEval": 49.59174989029109,
    "BBH Raw": 0.48887216244327214,
    "BBH": 29.39810739240589,
    "MATH Lvl 5 Raw": 0.05438066465256798,
    "MATH Lvl 5": 5.4380664652567985,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.4159791666666666,
    "MUSR": 10.997395833333329,
    "MMLU-PRO Raw": 0.2974567819148936,
    "MMLU-PRO": 21.939642434988176,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "icefog72/IceCoffeeRP-7b"
  },
  {
    "eval_name": "icefog72_IceDrinkByFrankensteinV3RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceDrinkByFrankensteinV3RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceDrinkByFrankensteinV3RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceDrinkByFrankensteinV3RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceDrinkByFrankensteinV3RP",
    "Model sha": "a4d2eb422867ea28860ad3b983b93bc97ca91719",
    "Average ‚¨ÜÔ∏è": 19.830521930024645,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7559605381525063,
    "IFEval Raw": 0.4974911013887596,
    "IFEval": 49.74911013887596,
    "BBH Raw": 0.4832523723413275,
    "BBH": 28.84588139816073,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.4253125,
    "MUSR": 12.19739583333333,
    "MMLU-PRO Raw": 0.292719414893617,
    "MMLU-PRO": 21.413268321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "icefog72/IceDrinkByFrankensteinV3RP"
  },
  {
    "eval_name": "icefog72_IceDrinkNameGoesHereRP-7b-Model_Stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceDrinkNameGoesHereRP-7b-Model_Stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceDrinkNameGoesHereRP-7b-Model_Stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceDrinkNameGoesHereRP-7b-Model_Stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceDrinkNameGoesHereRP-7b-Model_Stock",
    "Model sha": "78f7625f85c3cb150565ebb68c3f8d47d48325c8",
    "Average ‚¨ÜÔ∏è": 18.61930264775693,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6930989912609933,
    "IFEval Raw": 0.49684171332065585,
    "IFEval": 49.68417133206558,
    "BBH Raw": 0.46578646938927254,
    "BBH": 26.22465405632467,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.4067395833333334,
    "MUSR": 9.309114583333338,
    "MMLU-PRO Raw": 0.2816655585106383,
    "MMLU-PRO": 20.18506205673759,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-24",
    "Generation": 0,
    "Base Model": "icefog72/IceDrinkNameGoesHereRP-7b-Model_Stock"
  },
  {
    "eval_name": "icefog72_IceDrinkNameNotFoundRP-7b-Model_Stock_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceDrinkNameNotFoundRP-7b-Model_Stock\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceDrinkNameNotFoundRP-7b-Model_Stock</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceDrinkNameNotFoundRP-7b-Model_Stock-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceDrinkNameNotFoundRP-7b-Model_Stock",
    "Model sha": "35db2bf9e6812c5819378be68f94159e962fd1cb",
    "Average ‚¨ÜÔ∏è": 21.393849887148946,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6014486077275787,
    "IFEval Raw": 0.5130032757527804,
    "IFEval": 51.30032757527805,
    "BBH Raw": 0.502625425089929,
    "BBH": 30.668251445896402,
    "MATH Lvl 5 Raw": 0.06117824773413897,
    "MATH Lvl 5": 6.117824773413897,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4371875,
    "MUSR": 13.648437500000005,
    "MMLU-PRO Raw": 0.3064328457446808,
    "MMLU-PRO": 22.936982860520093,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "icefog72/IceDrinkNameNotFoundRP-7b-Model_Stock"
  },
  {
    "eval_name": "icefog72_IceDrunkCherryRP-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceDrunkCherryRP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceDrunkCherryRP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceDrunkCherryRP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceDrunkCherryRP-7b",
    "Model sha": "160b01e50d9c9441886f6cf987a3495bd8fa1c49",
    "Average ‚¨ÜÔ∏è": 20.28423097800195,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5449298001274147,
    "IFEval Raw": 0.48982255969715904,
    "IFEval": 48.98225596971591,
    "BBH Raw": 0.4846629039263151,
    "BBH": 28.241090201205953,
    "MATH Lvl 5 Raw": 0.06193353474320243,
    "MATH Lvl 5": 6.193353474320243,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.4291875,
    "MUSR": 12.381770833333336,
    "MMLU-PRO Raw": 0.3009474734042553,
    "MMLU-PRO": 22.327497044917255,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 0,
    "Base Model": "icefog72/IceDrunkCherryRP-7b"
  },
  {
    "eval_name": "icefog72_IceEspressoRPv2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceEspressoRPv2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceEspressoRPv2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceEspressoRPv2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceEspressoRPv2-7b",
    "Model sha": "d71a4c2ae25c063fd4c3d3df039908c648a8bab4",
    "Average ‚¨ÜÔ∏è": 21.340100248235235,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5725338105644989,
    "IFEval Raw": 0.4977160600539901,
    "IFEval": 49.77160600539901,
    "BBH Raw": 0.5054890156350785,
    "BBH": 31.303238558418098,
    "MATH Lvl 5 Raw": 0.06042296072507554,
    "MATH Lvl 5": 6.042296072507554,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.43306249999999996,
    "MUSR": 12.766145833333335,
    "MMLU-PRO Raw": 0.3061003989361702,
    "MMLU-PRO": 22.90004432624113,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-11",
    "Generation": 1,
    "Base Model": "icefog72/IceEspressoRPv2-7b (Merge)"
  },
  {
    "eval_name": "icefog72_IceLemonTeaRP-32k-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceLemonTeaRP-32k-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceLemonTeaRP-32k-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceLemonTeaRP-32k-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceLemonTeaRP-32k-7b",
    "Model sha": "7ea0bdf873c535b73ca20db46db0799bac433662",
    "Average ‚¨ÜÔ∏è": 21.35984770816403,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5714350519540776,
    "IFEval Raw": 0.5212214701436633,
    "IFEval": 52.12214701436632,
    "BBH Raw": 0.49973852418379305,
    "BBH": 30.135779642023177,
    "MATH Lvl 5 Raw": 0.053625377643504536,
    "MATH Lvl 5": 5.362537764350454,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.42903125,
    "MUSR": 12.195572916666668,
    "MMLU-PRO Raw": 0.3067652925531915,
    "MMLU-PRO": 22.973921394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-07-27",
    "Generation": 1,
    "Base Model": "icefog72/IceLemonTeaRP-32k-7b (Merge)"
  },
  {
    "eval_name": "icefog72_IceMartiniRP-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceMartiniRP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceMartiniRP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceMartiniRP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceMartiniRP-7b",
    "Model sha": "e5be38a55d2d9877fbb61cffc7f48402ac0193fc",
    "Average ‚¨ÜÔ∏è": 21.183767054299327,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5694376222129963,
    "IFEval Raw": 0.5044603873278457,
    "IFEval": 50.446038732784565,
    "BBH Raw": 0.4972421837639585,
    "BBH": 29.685367916429147,
    "MATH Lvl 5 Raw": 0.06873111782477342,
    "MATH Lvl 5": 6.873111782477342,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.4344895833333333,
    "MUSR": 13.144531250000005,
    "MMLU-PRO Raw": 0.3073470744680851,
    "MMLU-PRO": 23.038563829787233,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-24",
    "Generation": 0,
    "Base Model": "icefog72/IceMartiniRP-7b"
  },
  {
    "eval_name": "icefog72_IceSakeRP-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceSakeRP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceSakeRP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceSakeRP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceSakeRP-7b",
    "Model sha": "3b6b00bc48cd99e9b28e5aa8293dc987a0cf069a",
    "Average ‚¨ÜÔ∏è": 21.576224480293906,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.292448762812455,
    "IFEval Raw": 0.5227950726295119,
    "IFEval": 52.279507262951185,
    "BBH Raw": 0.5119287057484642,
    "BBH": 31.6512550721568,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.41300000000000003,
    "MUSR": 10.225000000000001,
    "MMLU-PRO Raw": 0.3176529255319149,
    "MMLU-PRO": 24.183658392434985,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-07",
    "Submission Date": "2024-08-22",
    "Generation": 1,
    "Base Model": "icefog72/IceSakeRP-7b (Merge)"
  },
  {
    "eval_name": "icefog72_IceSakeV4RP-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceSakeV4RP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceSakeV4RP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceSakeV4RP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceSakeV4RP-7b",
    "Model sha": "e8cb50b78918149c7d1bf663bcb807e7bfac3eed",
    "Average ‚¨ÜÔ∏è": 20.05225099073927,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.54807863292486,
    "IFEval Raw": 0.4634192830578421,
    "IFEval": 46.34192830578421,
    "BBH Raw": 0.4929557826908731,
    "BBH": 29.234193217077536,
    "MATH Lvl 5 Raw": 0.05589123867069487,
    "MATH Lvl 5": 5.589123867069487,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.40819791666666666,
    "MUSR": 9.858072916666663,
    "MMLU-PRO Raw": 0.31025598404255317,
    "MMLU-PRO": 23.361776004728128,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "icefog72_IceSakeV6RP-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceSakeV6RP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceSakeV6RP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceSakeV6RP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceSakeV6RP-7b",
    "Model sha": "6838e68d35d037b0ef9b04a9de1ebc8ab508cd45",
    "Average ‚¨ÜÔ∏è": 21.22705439016619,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5554427665167333,
    "IFEval Raw": 0.5032613465604596,
    "IFEval": 50.32613465604596,
    "BBH Raw": 0.49760336362566354,
    "BBH": 30.391494717552195,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.42001041666666666,
    "MUSR": 11.634635416666663,
    "MMLU-PRO Raw": 0.3093417553191489,
    "MMLU-PRO": 23.26019503546099,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "icefog72/IceSakeV6RP-7b"
  },
  {
    "eval_name": "icefog72_IceSakeV8RP-7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceSakeV8RP-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceSakeV8RP-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceSakeV8RP-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceSakeV8RP-7b",
    "Model sha": "0f8f73fe356583e561479c689aa6597435327f4e",
    "Average ‚¨ÜÔ∏è": 21.76501539901592,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6482854097341464,
    "IFEval Raw": 0.6085741388404988,
    "IFEval": 60.857413884049876,
    "BBH Raw": 0.48847141337960176,
    "BBH": 28.966258233266576,
    "MATH Lvl 5 Raw": 0.06419939577039276,
    "MATH Lvl 5": 6.419939577039275,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3992708333333333,
    "MUSR": 8.542187500000004,
    "MMLU-PRO Raw": 0.301030585106383,
    "MMLU-PRO": 22.336731678486995,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "icefog72/IceSakeV8RP-7b"
  },
  {
    "eval_name": "icefog72_IceTea21EnergyDrinkRPV13-DPOv3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceTea21EnergyDrinkRPV13-DPOv3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceTea21EnergyDrinkRPV13-DPOv3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceTea21EnergyDrinkRPV13-DPOv3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceTea21EnergyDrinkRPV13-DPOv3",
    "Model sha": "2d4b4fd596ff0f6706a5752198e59da6ffc08067",
    "Average ‚¨ÜÔ∏è": 21.684258727932463,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5799420472010602,
    "IFEval Raw": 0.5263423272472595,
    "IFEval": 52.63423272472595,
    "BBH Raw": 0.5019587584232624,
    "BBH": 30.6127340167025,
    "MATH Lvl 5 Raw": 0.05891238670694864,
    "MATH Lvl 5": 5.8912386706948645,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.4371875,
    "MUSR": 13.648437500000005,
    "MMLU-PRO Raw": 0.30560172872340424,
    "MMLU-PRO": 22.844636524822693,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-06",
    "Generation": 1,
    "Base Model": "icefog72/IceTea21EnergyDrinkRPV13-DPOv3 (Merge)"
  },
  {
    "eval_name": "icefog72_IceTea21EnergyDrinkRPV13-DPOv3.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/icefog72/IceTea21EnergyDrinkRPV13-DPOv3.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">icefog72/IceTea21EnergyDrinkRPV13-DPOv3.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/icefog72__IceTea21EnergyDrinkRPV13-DPOv3.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "icefog72/IceTea21EnergyDrinkRPV13-DPOv3.5",
    "Model sha": "0b0b0864347c3fad2b4d3e102f2f9839d20e296c",
    "Average ‚¨ÜÔ∏è": 17.308799492462867,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5007286994284761,
    "IFEval Raw": 0.48709978412833504,
    "IFEval": 48.70997841283351,
    "BBH Raw": 0.4399660013109026,
    "BBH": 22.57322577923665,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.39641666666666664,
    "MUSR": 7.78541666666667,
    "MMLU-PRO Raw": 0.24983377659574468,
    "MMLU-PRO": 16.648197399527188,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "ifable_gemma-2-Ifable-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ifable/gemma-2-Ifable-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ifable/gemma-2-Ifable-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ifable__gemma-2-Ifable-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ifable/gemma-2-Ifable-9B",
    "Model sha": "d3dbde4efb93ea0a4f247de82541479de6b03160",
    "Average ‚¨ÜÔ∏è": 22.888690555751907,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 35,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 4.317604421462027,
    "IFEval Raw": 0.2984292787581395,
    "IFEval": 29.842927875813952,
    "BBH Raw": 0.5866115556693244,
    "BBH": 41.032644646265275,
    "MATH Lvl 5 Raw": 0.0989425981873112,
    "MATH Lvl 5": 9.89425981873112,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.40525000000000005,
    "MUSR": 8.522916666666669,
    "MMLU-PRO Raw": 0.4226230053191489,
    "MMLU-PRO": 35.847000591016545,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "ifable/gemma-2-Ifable-9B"
  },
  {
    "eval_name": "informatiker_Qwen2-7B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/informatiker/Qwen2-7B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">informatiker/Qwen2-7B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/informatiker__Qwen2-7B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "informatiker/Qwen2-7B-Instruct-abliterated",
    "Model sha": "7577d60acfe4544d5ab303f0a4d69a9fcb9cf1aa",
    "Average ‚¨ÜÔ∏è": 25.12160361348914,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.060605045683814,
    "IFEval Raw": 0.5821708622011817,
    "IFEval": 58.217086220118176,
    "BBH Raw": 0.5534265515936739,
    "BBH": 37.79572344136589,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.38879166666666665,
    "MUSR": 6.83229166666667,
    "MMLU-PRO Raw": 0.3873005319148936,
    "MMLU-PRO": 31.922281323877062,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-10",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "informatiker/Qwen2-7B-Instruct-abliterated"
  },
  {
    "eval_name": "instruction-pretrain_InstructLM-500M_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/instruction-pretrain/InstructLM-500M\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">instruction-pretrain/InstructLM-500M</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/instruction-pretrain__InstructLM-500M-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "instruction-pretrain/InstructLM-500M",
    "Model sha": "e9d33823c76303dfaff6a8397a8b70d0118ea350",
    "Average ‚¨ÜÔ∏è": 2.8543503197666724,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 34,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2457921450668255,
    "IFEval Raw": 0.1027662158627996,
    "IFEval": 10.27662158627996,
    "BBH Raw": 0.29408717872529677,
    "BBH": 2.317053716048478,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.3528229166666667,
    "MUSR": 2.0695312500000003,
    "MMLU-PRO Raw": 0.1141123670212766,
    "MMLU-PRO": 1.5680407801418434,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-18",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "instruction-pretrain/InstructLM-500M"
  },
  {
    "eval_name": "internlm_internlm2-1_8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm2-1_8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm2-1_8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2-1_8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "internlm/internlm2-1_8b",
    "Model sha": "c24f301c7374ad9f9b58d1ea80f68b5f57cbca13",
    "Average ‚¨ÜÔ∏è": 8.597072451460065,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 28,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6636455169122932,
    "IFEval Raw": 0.2197702097102355,
    "IFEval": 21.97702097102355,
    "BBH Raw": 0.3879732800028095,
    "BBH": 13.633857965906719,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.38128125,
    "MUSR": 8.226822916666668,
    "MMLU-PRO Raw": 0.15882646276595744,
    "MMLU-PRO": 6.536273640661936,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "internlm/internlm2-1_8b"
  },
  {
    "eval_name": "internlm_internlm2-chat-1_8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm2-chat-1_8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm2-chat-1_8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2-chat-1_8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "internlm/internlm2-chat-1_8b",
    "Model sha": "4e226eeb354499f4d34ef4c27f6939f377475cc1",
    "Average ‚¨ÜÔ∏è": 10.553683634515037,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 29,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.596422562282053,
    "IFEval Raw": 0.2386545477111841,
    "IFEval": 23.865454771118408,
    "BBH Raw": 0.4452271664119214,
    "BBH": 20.67235743256185,
    "MATH Lvl 5 Raw": 0.027190332326283987,
    "MATH Lvl 5": 2.719033232628399,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.36305208333333333,
    "MUSR": 4.61484375,
    "MMLU-PRO Raw": 0.18392619680851063,
    "MMLU-PRO": 9.325132978723403,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "internlm/internlm2-chat-1_8b"
  },
  {
    "eval_name": "internlm_internlm2_5-1_8b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm2_5-1_8b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm2_5-1_8b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2_5-1_8b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "internlm/internlm2_5-1_8b-chat",
    "Model sha": "4426f00b854561fa60d555d2b628064b56bcb758",
    "Average ‚¨ÜÔ∏è": 12.106337611274624,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 24,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7716656473585425,
    "IFEval Raw": 0.38490870889240547,
    "IFEval": 38.490870889240554,
    "BBH Raw": 0.4488926786996439,
    "BBH": 21.03092693656956,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.35939583333333336,
    "MUSR": 4.424479166666669,
    "MMLU-PRO Raw": 0.12990359042553193,
    "MMLU-PRO": 3.322621158392436,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "internlm/internlm2_5-1_8b-chat"
  },
  {
    "eval_name": "internlm_internlm2_5-20b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm2_5-20b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm2_5-20b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2_5-20b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "internlm/internlm2_5-20b-chat",
    "Model sha": "ef17bde929761255fee76d95e2c25969ccd93b0d",
    "Average ‚¨ÜÔ∏è": 32.08201273924976,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 83,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.732707871332407,
    "IFEval Raw": 0.7009977969565198,
    "IFEval": 70.09977969565199,
    "BBH Raw": 0.7473580533672884,
    "BBH": 62.83245915287989,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.4558229166666667,
    "MUSR": 16.744531249999994,
    "MMLU-PRO Raw": 0.39976728723404253,
    "MMLU-PRO": 33.30747635933806,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-08-12",
    "Generation": 0,
    "Base Model": "internlm/internlm2_5-20b-chat"
  },
  {
    "eval_name": "internlm_internlm2_5-7b-chat_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "InternLM2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/internlm/internlm2_5-7b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">internlm/internlm2_5-7b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/internlm__internlm2_5-7b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "internlm/internlm2_5-7b-chat",
    "Model sha": "bebb00121ee105b823647c3ba2b1e152652edc33",
    "Average ‚¨ÜÔ∏è": 30.576855562826193,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 179,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4533815135153885,
    "IFEval Raw": 0.6140196899781469,
    "IFEval": 61.40196899781469,
    "BBH Raw": 0.710773697280275,
    "BBH": 57.67364804232054,
    "MATH Lvl 5 Raw": 0.08987915407854985,
    "MATH Lvl 5": 8.987915407854985,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.4415,
    "MUSR": 14.35416666666667,
    "MMLU-PRO Raw": 0.3737533244680851,
    "MMLU-PRO": 30.417036052009454,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "internlm/internlm2_5-7b-chat"
  },
  {
    "eval_name": "intervitens_mini-magnum-12b-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/intervitens/mini-magnum-12b-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">intervitens/mini-magnum-12b-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/intervitens__mini-magnum-12b-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "intervitens/mini-magnum-12b-v1.1",
    "Model sha": "3b19e12711d3f4d9b81fdeb73860e9019ebe2404",
    "Average ‚¨ÜÔ∏è": 20.63850354349346,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 69,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.230948348432325,
    "IFEval Raw": 0.5155509603407846,
    "IFEval": 51.55509603407847,
    "BBH Raw": 0.506180035650624,
    "BBH": 29.731186868686873,
    "MATH Lvl 5 Raw": 0.03851963746223565,
    "MATH Lvl 5": 3.8519637462235647,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.4004479166666666,
    "MUSR": 8.089322916666669,
    "MMLU-PRO Raw": 0.3291223404255319,
    "MMLU-PRO": 25.458037825059098,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "intervitens/mini-magnum-12b-v1.1"
  },
  {
    "eval_name": "invalid-coder_Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/invalid-coder/Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">invalid-coder/Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/invalid-coder__Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "invalid-coder/Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp",
    "Model sha": "39a1c76ddb5fa3a82c5b4071121d2e4866a25300",
    "Average ‚¨ÜÔ∏è": 19.52985136315749,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7642507363470094,
    "IFEval Raw": 0.45547591501660034,
    "IFEval": 45.54759150166004,
    "BBH Raw": 0.5158439010792586,
    "BBH": 31.635374808026484,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.3992395833333333,
    "MUSR": 8.77161458333333,
    "MMLU-PRO Raw": 0.3145777925531915,
    "MMLU-PRO": 23.841976950354614,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-10",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "invalid-coder/Sakura-SOLAR-Instruct-CarbonVillain-en-10.7B-v2-slerp"
  },
  {
    "eval_name": "invisietch_EtherealRainbow-v0.2-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/invisietch/EtherealRainbow-v0.2-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">invisietch/EtherealRainbow-v0.2-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/invisietch__EtherealRainbow-v0.2-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "invisietch/EtherealRainbow-v0.2-8B",
    "Model sha": "46611fbb6aac0f33478c8401488d3ec7763c04d0",
    "Average ‚¨ÜÔ∏è": 20.156929106658144,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8714165424651581,
    "IFEval Raw": 0.39032988027323057,
    "IFEval": 39.03298802732306,
    "BBH Raw": 0.5102035205059678,
    "BBH": 30.283791366541095,
    "MATH Lvl 5 Raw": 0.0853474320241692,
    "MATH Lvl 5": 8.53474320241692,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.38267708333333333,
    "MUSR": 6.5679687499999995,
    "MMLU-PRO Raw": 0.36527593085106386,
    "MMLU-PRO": 29.47510342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-12",
    "Submission Date": "2024-07-01",
    "Generation": 0,
    "Base Model": "invisietch/EtherealRainbow-v0.2-8B"
  },
  {
    "eval_name": "invisietch_EtherealRainbow-v0.3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/invisietch/EtherealRainbow-v0.3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">invisietch/EtherealRainbow-v0.3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/invisietch__EtherealRainbow-v0.3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "invisietch/EtherealRainbow-v0.3-8B",
    "Model sha": "c986c4ca5a5b8474820a59d3e911a431cf26938d",
    "Average ‚¨ÜÔ∏è": 19.778643538982763,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2812671972183436,
    "IFEval Raw": 0.36822298168858625,
    "IFEval": 36.82229816885863,
    "BBH Raw": 0.5096758454539693,
    "BBH": 30.080258475101616,
    "MATH Lvl 5 Raw": 0.07552870090634439,
    "MATH Lvl 5": 7.552870090634439,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.39039583333333333,
    "MUSR": 7.766145833333333,
    "MMLU-PRO Raw": 0.36261635638297873,
    "MMLU-PRO": 29.179595153664305,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-19",
    "Submission Date": "2024-07-01",
    "Generation": 0,
    "Base Model": "invisietch/EtherealRainbow-v0.3-8B"
  },
  {
    "eval_name": "invisietch_MiS-Firefly-v0.2-22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/invisietch/MiS-Firefly-v0.2-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">invisietch/MiS-Firefly-v0.2-22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/invisietch__MiS-Firefly-v0.2-22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "invisietch/MiS-Firefly-v0.2-22B",
    "Model sha": "02dd13deefc5ff516edb59070ad66bd9f2831f4c",
    "Average ‚¨ÜÔ∏è": 26.6534812159811,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0252777960394084,
    "IFEval Raw": 0.5371082062261466,
    "IFEval": 53.71082062261466,
    "BBH Raw": 0.5513523591170696,
    "BBH": 36.082656217719574,
    "MATH Lvl 5 Raw": 0.1593655589123867,
    "MATH Lvl 5": 15.93655589123867,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.46937500000000004,
    "MUSR": 17.80520833333333,
    "MMLU-PRO Raw": 0.3620345744680851,
    "MMLU-PRO": 29.11495271867612,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-07",
    "Generation": 0,
    "Base Model": "invisietch/MiS-Firefly-v0.2-22B"
  },
  {
    "eval_name": "invisietch_Nimbus-Miqu-v0.1-70B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/invisietch/Nimbus-Miqu-v0.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">invisietch/Nimbus-Miqu-v0.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/invisietch__Nimbus-Miqu-v0.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "invisietch/Nimbus-Miqu-v0.1-70B",
    "Model sha": "3209583a0849383daf8faa7b819f29726b8806cf",
    "Average ‚¨ÜÔ∏è": 24.782875921300192,
    "Hub License": "unknown",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.143677368740989,
    "IFEval Raw": 0.46466819150963884,
    "IFEval": 46.466819150963886,
    "BBH Raw": 0.601030667794844,
    "BBH": 43.4509951550532,
    "MATH Lvl 5 Raw": 0.058912386706948636,
    "MATH Lvl 5": 5.891238670694864,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.41331249999999997,
    "MUSR": 9.330729166666666,
    "MMLU-PRO Raw": 0.3853058510638298,
    "MMLU-PRO": 31.700650118203306,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-30",
    "Submission Date": "2024-07-03",
    "Generation": 0,
    "Base Model": "invisietch/Nimbus-Miqu-v0.1-70B"
  },
  {
    "eval_name": "jaredjoss_pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jaredjoss/pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jaredjoss/pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jaredjoss__pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jaredjoss/pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model",
    "Model sha": "048bc8edfc32fdcf6d957332d5f4c0d4e5950746",
    "Average ‚¨ÜÔ∏è": 3.81661033477558,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.23306374663685697,
    "IFEval Raw": 0.15722172723928066,
    "IFEval": 15.722172723928066,
    "BBH Raw": 0.2863444769655102,
    "BBH": 1.8203742908537424,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3606979166666667,
    "MUSR": 2.2539062500000013,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-23",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "jaredjoss/pythia-410m-roberta-lr_8e7-kl_01-steps_12000-rlhf-model"
  },
  {
    "eval_name": "jebcarter_psyonic-cetacean-20B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jebcarter/psyonic-cetacean-20B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jebcarter/psyonic-cetacean-20B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jebcarter__psyonic-cetacean-20B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jebcarter/psyonic-cetacean-20B",
    "Model sha": "298d2086a949d53af06096d229f64f4719261698",
    "Average ‚¨ÜÔ∏è": 15.898965686506898,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 38,
    "#Params (B)": 19,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.144948163311079,
    "IFEval Raw": 0.25436619281284767,
    "IFEval": 25.43661928128477,
    "BBH Raw": 0.4907386156835858,
    "BBH": 27.843060379681305,
    "MATH Lvl 5 Raw": 0.011329305135951661,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.46611458333333333,
    "MUSR": 16.897656249999997,
    "MMLU-PRO Raw": 0.28856382978723405,
    "MMLU-PRO": 20.951536643026003,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-11-28",
    "Submission Date": "2024-06-30",
    "Generation": 0,
    "Base Model": "jebcarter/psyonic-cetacean-20B"
  },
  {
    "eval_name": "jeffmeloy_Qwen-7B-nerd-uncensored-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen-7B-nerd-uncensored-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen-7B-nerd-uncensored-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen-7B-nerd-uncensored-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen-7B-nerd-uncensored-v1.0",
    "Model sha": "245a9a038ea9cfdc214a5e24a2e7ff9362f56b4a",
    "Average ‚¨ÜÔ∏è": 31.178621367854532,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7269106695409595,
    "IFEval Raw": 0.6151189953767116,
    "IFEval": 61.51189953767116,
    "BBH Raw": 0.5421083753999172,
    "BBH": 34.18632007953577,
    "MATH Lvl 5 Raw": 0.24697885196374622,
    "MATH Lvl 5": 24.69788519637462,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.47929166666666667,
    "MUSR": 18.911458333333332,
    "MMLU-PRO Raw": 0.4362533244680851,
    "MMLU-PRO": 37.3614804964539,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-10-30",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen-7B-nerd-uncensored-v1.0 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v0.9_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v0.9\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v0.9</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v0.9-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v0.9",
    "Model sha": "7eb2a19e13fb32c1bab751eb89fed33f6c66b4e6",
    "Average ‚¨ÜÔ∏è": 31.166804259039225,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7230310643297215,
    "IFEval Raw": 0.6048274134851084,
    "IFEval": 60.48274134851084,
    "BBH Raw": 0.5469701834138724,
    "BBH": 34.79159883396676,
    "MATH Lvl 5 Raw": 0.25075528700906347,
    "MATH Lvl 5": 25.075528700906347,
    "GPQA Raw": 0.32298657718120805,
    "GPQA": 9.731543624161072,
    "MUSR Raw": 0.48198958333333336,
    "MUSR": 19.548697916666665,
    "MMLU-PRO Raw": 0.4363364361702128,
    "MMLU-PRO": 37.37071513002365,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-02",
    "Submission Date": "2024-11-13",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v0.9 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.0",
    "Model sha": "8f478661c654990358904e2159252d5c5236b80f",
    "Average ‚¨ÜÔ∏è": 28.363890212782554,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7315881773747307,
    "IFEval Raw": 0.7695159953368174,
    "IFEval": 76.95159953368174,
    "BBH Raw": 0.541762771903226,
    "BBH": 34.737157075604436,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4551145833333334,
    "MUSR": 16.822656250000005,
    "MMLU-PRO Raw": 0.4253656914893617,
    "MMLU-PRO": 36.151743498817964,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.0 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.1",
    "Model sha": "e757ba9e4c1a5a43ba3a3e98b44ebbbfe7bf831a",
    "Average ‚¨ÜÔ∏è": 23.70241029390584,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.684100532704964,
    "IFEval Raw": 0.6626296005709296,
    "IFEval": 66.26296005709295,
    "BBH Raw": 0.48640249867140106,
    "BBH": 26.661152209882328,
    "MATH Lvl 5 Raw": 0.07401812688821752,
    "MATH Lvl 5": 7.401812688821751,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.38429166666666664,
    "MUSR": 5.303125,
    "MMLU-PRO Raw": 0.3849734042553192,
    "MMLU-PRO": 31.663711583924346,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-09",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.1 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.2",
    "Model sha": "8ba84532e3eea17c821f96f3e80bec7c9d8b3799",
    "Average ‚¨ÜÔ∏è": 23.14683268365377,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6259135424099636,
    "IFEval Raw": 0.49646715160219335,
    "IFEval": 49.64671516021933,
    "BBH Raw": 0.494592979290867,
    "BBH": 27.660911941379897,
    "MATH Lvl 5 Raw": 0.10574018126888217,
    "MATH Lvl 5": 10.574018126888216,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.41724999999999995,
    "MUSR": 10.856249999999998,
    "MMLU-PRO Raw": 0.3968583776595745,
    "MMLU-PRO": 32.98426418439716,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-09",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.2 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.3",
    "Model sha": "db61c49ae128777c4b893ab544975df349052d66",
    "Average ‚¨ÜÔ∏è": 23.873164644487787,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6153502238793745,
    "IFEval Raw": 0.49951462120506923,
    "IFEval": 49.95146212050692,
    "BBH Raw": 0.5026055485090198,
    "BBH": 28.900263386108733,
    "MATH Lvl 5 Raw": 0.11178247734138973,
    "MATH Lvl 5": 11.178247734138973,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.41873958333333333,
    "MUSR": 11.309114583333331,
    "MMLU-PRO Raw": 0.4015957446808511,
    "MMLU-PRO": 33.51063829787233,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-03",
    "Submission Date": "2024-11-09",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.3 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.4",
    "Model sha": "7da97922062bae96d0e694fbd3a5f1c06cf375b6",
    "Average ‚¨ÜÔ∏è": 30.823333760632693,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6613080657836182,
    "IFEval Raw": 0.6078748830879843,
    "IFEval": 60.78748830879843,
    "BBH Raw": 0.5467076263362468,
    "BBH": 34.85601776135507,
    "MATH Lvl 5 Raw": 0.236404833836858,
    "MATH Lvl 5": 23.6404833836858,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.47138541666666667,
    "MUSR": 17.82317708333333,
    "MMLU-PRO Raw": 0.44190492021276595,
    "MMLU-PRO": 37.98943557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.4 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.5",
    "Model sha": "055cf43cab9027de7e548728dc231afea6a3dfd1",
    "Average ‚¨ÜÔ∏è": 30.95377666364284,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6841011391675504,
    "IFEval Raw": 0.5650352176669016,
    "IFEval": 56.50352176669016,
    "BBH Raw": 0.5522599149696679,
    "BBH": 35.92532095316597,
    "MATH Lvl 5 Raw": 0.22280966767371602,
    "MATH Lvl 5": 22.280966767371602,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.49820833333333336,
    "MUSR": 22.409375,
    "MMLU-PRO Raw": 0.44481382978723405,
    "MMLU-PRO": 38.31264775413712,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.5 (Merge)"
  },
  {
    "eval_name": "jeffmeloy_Qwen2.5-7B-nerd-uncensored-v1.7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeffmeloy__Qwen2.5-7B-nerd-uncensored-v1.7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.7",
    "Model sha": "a27917d12ac64d91a86afee9953bce6d1a9b6424",
    "Average ‚¨ÜÔ∏è": 28.312593949496755,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6648698088219905,
    "IFEval Raw": 0.4201551882338861,
    "IFEval": 42.01551882338861,
    "BBH Raw": 0.5391718355132782,
    "BBH": 33.83351101288335,
    "MATH Lvl 5 Raw": 0.26963746223564955,
    "MATH Lvl 5": 26.963746223564954,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.48484375,
    "MUSR": 20.77213541666666,
    "MMLU-PRO Raw": 0.42802526595744683,
    "MMLU-PRO": 36.44725177304965,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "jeffmeloy/Qwen2.5-7B-nerd-uncensored-v1.7 (Merge)"
  },
  {
    "eval_name": "jeonsworld_CarbonVillain-en-10.7B-v4_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jeonsworld/CarbonVillain-en-10.7B-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jeonsworld/CarbonVillain-en-10.7B-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jeonsworld__CarbonVillain-en-10.7B-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jeonsworld/CarbonVillain-en-10.7B-v4",
    "Model sha": "57d6ad4d705d336aba228356683d9f221507440a",
    "Average ‚¨ÜÔ∏è": 19.548213143143965,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7697308773313882,
    "IFEval Raw": 0.45792386423578324,
    "IFEval": 45.79238642357833,
    "BBH Raw": 0.516795955873779,
    "BBH": 31.80563982727619,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.3965416666666666,
    "MUSR": 8.401041666666663,
    "MMLU-PRO Raw": 0.31416223404255317,
    "MMLU-PRO": 23.795803782505907,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "jeonsworld/CarbonVillain-en-10.7B-v4"
  },
  {
    "eval_name": "jiangxinyang-shanda_Homer-LLama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jiangxinyang-shanda/Homer-LLama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jiangxinyang-shanda/Homer-LLama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jiangxinyang-shanda__Homer-LLama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jiangxinyang-shanda/Homer-LLama3-8B",
    "Model sha": "550cdaea5feac5df9b0984bda14d00570daa4437",
    "Average ‚¨ÜÔ∏è": 18.89619333590397,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6964260107431075,
    "IFEval Raw": 0.3991719748046295,
    "IFEval": 39.917197480462946,
    "BBH Raw": 0.5173242047543128,
    "BBH": 31.69897508701321,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.40562499999999996,
    "MUSR": 9.236458333333337,
    "MMLU-PRO Raw": 0.3139128989361702,
    "MMLU-PRO": 23.768099881796687,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "jieliu_Storm-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jieliu/Storm-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jieliu/Storm-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jieliu__Storm-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jieliu/Storm-7B",
    "Model sha": "71edab8ee6c2578e428b0359158fb0d43133e989",
    "Average ‚¨ÜÔ∏è": 19.789054068751515,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 40,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6119134897746422,
    "IFEval Raw": 0.3424192254329623,
    "IFEval": 34.24192254329623,
    "BBH Raw": 0.5187285371254579,
    "BBH": 32.33028437916087,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4428958333333333,
    "MUSR": 14.628645833333332,
    "MMLU-PRO Raw": 0.3119182180851064,
    "MMLU-PRO": 23.54646867612293,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01",
    "Model sha": "f4ebbf27d586e94c63f0a7293f565cbd947b824f",
    "Average ‚¨ÜÔ∏è": 22.379186218052393,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9935181977957601,
    "IFEval Raw": 0.42712447417297217,
    "IFEval": 42.71244741729721,
    "BBH Raw": 0.5035519809362171,
    "BBH": 29.550013804457734,
    "MATH Lvl 5 Raw": 0.04154078549848943,
    "MATH Lvl 5": 4.1540785498489425,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.4637604166666667,
    "MUSR": 17.80338541666666,
    "MMLU-PRO Raw": 0.37391954787234044,
    "MMLU-PRO": 30.43550531914893,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1",
    "Model sha": "66c7330e9d04b13a68ea7dcf25bc0a71d144221a",
    "Average ‚¨ÜÔ∏è": 21.408591601787467,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.808220322486785,
    "IFEval Raw": 0.42532591302189304,
    "IFEval": 42.5325913021893,
    "BBH Raw": 0.5018845446835877,
    "BBH": 28.607718394442788,
    "MATH Lvl 5 Raw": 0.09441087613293052,
    "MATH Lvl 5": 9.441087613293051,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.41502083333333334,
    "MUSR": 10.777604166666672,
    "MMLU-PRO Raw": 0.37242353723404253,
    "MMLU-PRO": 30.269281914893615,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.1-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01",
    "Model sha": "4a432be239528ffc654955338982f1f32eb12901",
    "Average ‚¨ÜÔ∏è": 20.103542136013562,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0365630515642712,
    "IFEval Raw": 0.33774828565982706,
    "IFEval": 33.7748285659827,
    "BBH Raw": 0.4917135045463188,
    "BBH": 28.135682301211705,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.5017708333333334,
    "MUSR": 22.288020833333334,
    "MMLU-PRO Raw": 0.3533078457446808,
    "MMLU-PRO": 28.14531619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1",
    "Model sha": "d6f8ed8dc4b7f74b4312bc0d24aaac275c61958d",
    "Average ‚¨ÜÔ∏è": 21.833220529017808,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8182986474011524,
    "IFEval Raw": 0.4273993005226133,
    "IFEval": 42.73993005226133,
    "BBH Raw": 0.5125777877188348,
    "BBH": 30.51494334374904,
    "MATH Lvl 5 Raw": 0.08081570996978854,
    "MATH Lvl 5": 8.081570996978854,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.42264583333333333,
    "MUSR": 11.397395833333336,
    "MMLU-PRO Raw": 0.37391954787234044,
    "MMLU-PRO": 30.43550531914893,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.3-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01",
    "Model sha": "6ab1392c825907b08eff8fbed4c97a3e6e0d6dd9",
    "Average ‚¨ÜÔ∏è": 19.38459094574601,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0279018780334526,
    "IFEval Raw": 0.32036219453272874,
    "IFEval": 32.03621945327288,
    "BBH Raw": 0.48835763921755193,
    "BBH": 27.665794638508448,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.5097708333333334,
    "MUSR": 23.621354166666674,
    "MMLU-PRO Raw": 0.33444148936170215,
    "MMLU-PRO": 26.049054373522463,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1",
    "Model sha": "a481edaceeaab34f4dc0e90c4d8ec0f72658bbdd",
    "Average ‚¨ÜÔ∏è": 22.35855717338799,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8151268001288113,
    "IFEval Raw": 0.43963904661852776,
    "IFEval": 43.96390466185278,
    "BBH Raw": 0.5140041302485145,
    "BBH": 30.854731427683628,
    "MATH Lvl 5 Raw": 0.07930513595166162,
    "MATH Lvl 5": 7.930513595166162,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.43979166666666664,
    "MUSR": 13.840625000000001,
    "MMLU-PRO Raw": 0.36959773936170215,
    "MMLU-PRO": 29.955304373522463,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.5-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01",
    "Model sha": "61f4b44fb917cdb46f0ade9f8fc2a382e0cf67af",
    "Average ‚¨ÜÔ∏è": 18.442433340954473,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.024438148186672,
    "IFEval Raw": 0.2814443454478561,
    "IFEval": 28.14443454478561,
    "BBH Raw": 0.4854325756272537,
    "BBH": 27.16443115792157,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.5163125000000001,
    "MUSR": 24.472395833333337,
    "MMLU-PRO Raw": 0.3295378989361702,
    "MMLU-PRO": 25.5042109929078,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1",
    "Model sha": "139a9bccd0ffb284e670a181a5986a01b1420c6c",
    "Average ‚¨ÜÔ∏è": 21.8356864933274,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9341308193595685,
    "IFEval Raw": 0.4302218114602588,
    "IFEval": 43.02218114602588,
    "BBH Raw": 0.5157097379648965,
    "BBH": 31.163507714744895,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.43315624999999996,
    "MUSR": 12.877864583333336,
    "MMLU-PRO Raw": 0.36627327127659576,
    "MMLU-PRO": 29.58591903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.7-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01",
    "Model sha": "c88c6b65f751156e7bc04c738947387eb55747e9",
    "Average ‚¨ÜÔ∏è": 18.483613463733477,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.019765793293449,
    "IFEval Raw": 0.2789963962286732,
    "IFEval": 27.899639622867326,
    "BBH Raw": 0.48611535229340735,
    "BBH": 27.22486881424896,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.5150104166666667,
    "MUSR": 24.242968750000003,
    "MMLU-PRO Raw": 0.3304521276595745,
    "MMLU-PRO": 25.60579196217494,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1",
    "Model sha": "818f7e586444b551200862fb234c39bd48d69ae8",
    "Average ‚¨ÜÔ∏è": 21.99460647263732,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9352257723152003,
    "IFEval Raw": 0.4222784434190171,
    "IFEval": 42.22784434190171,
    "BBH Raw": 0.5153764046315631,
    "BBH": 31.124765884679533,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4384270833333333,
    "MUSR": 13.670052083333337,
    "MMLU-PRO Raw": 0.3650265957446808,
    "MMLU-PRO": 29.44739952718675,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-08",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs-density-0.9-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01",
    "Model sha": "861347cd643d396877d8e560367cf0717c671228",
    "Average ‚¨ÜÔ∏è": 22.199406131853124,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0348643968356555,
    "IFEval Raw": 0.4358923212631374,
    "IFEval": 43.58923212631373,
    "BBH Raw": 0.5040935986635269,
    "BBH": 29.530012820716838,
    "MATH Lvl 5 Raw": 0.04984894259818732,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.45315625,
    "MUSR": 16.344531249999992,
    "MMLU-PRO Raw": 0.3762466755319149,
    "MMLU-PRO": 30.69407505910165,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1",
    "Model sha": "2647bc863e6ee686e7174366107eecbd4b37f62e",
    "Average ‚¨ÜÔ∏è": 21.278225896655268,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8413158307025352,
    "IFEval Raw": 0.4201551882338861,
    "IFEval": 42.01551882338861,
    "BBH Raw": 0.501124270710985,
    "BBH": 28.504906370089667,
    "MATH Lvl 5 Raw": 0.09667673716012085,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.41502083333333334,
    "MUSR": 10.777604166666672,
    "MMLU-PRO Raw": 0.3699301861702128,
    "MMLU-PRO": 29.99224290780142,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.1-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01",
    "Model sha": "fa77530fe3723d7b15b06b88c3ca6110a8421742",
    "Average ‚¨ÜÔ∏è": 20.41033875046328,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0557926311746306,
    "IFEval Raw": 0.35178659290682057,
    "IFEval": 35.178659290682056,
    "BBH Raw": 0.49985217584312186,
    "BBH": 29.136918888444114,
    "MATH Lvl 5 Raw": 0.012839879154078549,
    "MATH Lvl 5": 1.2839879154078548,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.48710416666666667,
    "MUSR": 20.3546875,
    "MMLU-PRO Raw": 0.3611203457446808,
    "MMLU-PRO": 29.01337174940898,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1",
    "Model sha": "6fe73aa7f9c5b59297739166e9557089d39e5fc7",
    "Average ‚¨ÜÔ∏è": 21.773954256833775,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.851835662764295,
    "IFEval Raw": 0.42038014689911657,
    "IFEval": 42.03801468991166,
    "BBH Raw": 0.5107301269172088,
    "BBH": 30.244175919150706,
    "MATH Lvl 5 Raw": 0.09063444108761329,
    "MATH Lvl 5": 9.06344410876133,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.42785416666666665,
    "MUSR": 11.915104166666671,
    "MMLU-PRO Raw": 0.37101063829787234,
    "MMLU-PRO": 30.112293144208035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.3-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01",
    "Model sha": "a31f86b538ba8b2983620cc27a741bc9a81a7e2f",
    "Average ‚¨ÜÔ∏è": 20.08669537382087,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9526847714993242,
    "IFEval Raw": 0.34541682735142754,
    "IFEval": 34.54168273514276,
    "BBH Raw": 0.4983827321097329,
    "BBH": 29.32060751365874,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.49113541666666666,
    "MUSR": 21.058593749999996,
    "MMLU-PRO Raw": 0.3531416223404255,
    "MMLU-PRO": 28.126846926713938,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1",
    "Model sha": "f9d5bab1c1d0d6890e89b513225d13f68a1c6d75",
    "Average ‚¨ÜÔ∏è": 21.442231277824096,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7933573433291264,
    "IFEval Raw": 0.40916435058976847,
    "IFEval": 40.916435058976845,
    "BBH Raw": 0.513665952913411,
    "BBH": 30.693077471988918,
    "MATH Lvl 5 Raw": 0.08081570996978853,
    "MATH Lvl 5": 8.081570996978853,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.43569791666666663,
    "MUSR": 13.262239583333335,
    "MMLU-PRO Raw": 0.366938164893617,
    "MMLU-PRO": 29.659796099290777,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.5-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01",
    "Model sha": "d30c75506feaec957dc73bc5c040159c310ecf4c",
    "Average ‚¨ÜÔ∏è": 19.14961218122289,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9084814212653319,
    "IFEval Raw": 0.29038728351884113,
    "IFEval": 29.03872835188411,
    "BBH Raw": 0.4967337534367295,
    "BBH": 28.739266057268594,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.4990729166666667,
    "MUSR": 22.250781249999992,
    "MMLU-PRO Raw": 0.34898603723404253,
    "MMLU-PRO": 27.665115248226947,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1",
    "Model sha": "cd52bafe64e82d466d0bc590da5399f2299d24e1",
    "Average ‚¨ÜÔ∏è": 21.626887508307224,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8062624080333317,
    "IFEval Raw": 0.41988036188424493,
    "IFEval": 41.98803618842449,
    "BBH Raw": 0.5146905664948336,
    "BBH": 31.007758447741608,
    "MATH Lvl 5 Raw": 0.08081570996978853,
    "MATH Lvl 5": 8.081570996978853,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.43576041666666665,
    "MUSR": 13.136718750000002,
    "MMLU-PRO Raw": 0.3615359042553192,
    "MMLU-PRO": 29.05954491725768,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.7-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01",
    "Model sha": "4c30fdbe0708afefe50788ea640c3dfab294c77f",
    "Average ‚¨ÜÔ∏è": 18.884376758560723,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9006309261833836,
    "IFEval Raw": 0.29131149793658606,
    "IFEval": 29.131149793658604,
    "BBH Raw": 0.49182964384768835,
    "BBH": 28.219373273671142,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4976770833333333,
    "MUSR": 21.976302083333326,
    "MMLU-PRO Raw": 0.34541223404255317,
    "MMLU-PRO": 27.268026004728128,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.01 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1",
    "Model sha": "378a7cad3e34a1a8b11e77edd95b02ff0d228da2",
    "Average ‚¨ÜÔ∏è": 21.374085275651822,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9299117948129532,
    "IFEval Raw": 0.41623337189767595,
    "IFEval": 41.6233371897676,
    "BBH Raw": 0.5138610942606995,
    "BBH": 30.841602413783743,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.43172916666666666,
    "MUSR": 12.499479166666669,
    "MMLU-PRO Raw": 0.3624501329787234,
    "MMLU-PRO": 29.161125886524825,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_breadcrumbs_ties-density-0.9-gamma-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_dare_linear_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_dare_linear\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_dare_linear</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_dare_linear-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_dare_linear",
    "Model sha": "abb81fd8fdc2ad32f65befcb7ae369c9837cd563",
    "Average ‚¨ÜÔ∏è": 14.123522915539402,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9102559390833451,
    "IFEval Raw": 0.21454961723781787,
    "IFEval": 21.454961723781786,
    "BBH Raw": 0.4282807940700452,
    "BBH": 19.610998997495773,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.49792708333333335,
    "MUSR": 21.807552083333334,
    "MMLU-PRO Raw": 0.24143949468085107,
    "MMLU-PRO": 15.715499408983453,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_dare_linear (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_dare_ties-density-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_dare_ties-density-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.1",
    "Model sha": "e7a3a3b955d945f53da8301b958f0b90a28a62d3",
    "Average ‚¨ÜÔ∏è": 11.619907050970184,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9111340828720692,
    "IFEval Raw": 0.18907055501624578,
    "IFEval": 18.907055501624576,
    "BBH Raw": 0.41187360174735804,
    "BBH": 16.85891694951123,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.46580208333333334,
    "MUSR": 16.991927083333334,
    "MMLU-PRO Raw": 0.22647938829787234,
    "MMLU-PRO": 14.05326536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_dare_ties-density-0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_dare_ties-density-0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.3",
    "Model sha": "6f966d14d7236f3da6d1ea9ce3bd9b20808e02a9",
    "Average ‚¨ÜÔ∏è": 15.9437708061195,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.923384742054847,
    "IFEval Raw": 0.21132705665412216,
    "IFEval": 21.132705665412217,
    "BBH Raw": 0.4558569854124363,
    "BBH": 23.0949356647322,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.5069479166666667,
    "MUSR": 22.50182291666667,
    "MMLU-PRO Raw": 0.30402260638297873,
    "MMLU-PRO": 22.669178486997634,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.3 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_dare_ties-density-0.7_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_dare_ties-density-0.7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.7",
    "Model sha": "b14b5cd07feb749e42b0567b1e387b390bed033e",
    "Average ‚¨ÜÔ∏è": 16.721677967899154,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0377121651606747,
    "IFEval Raw": 0.20338368861288048,
    "IFEval": 20.338368861288046,
    "BBH Raw": 0.4722858888388635,
    "BBH": 25.253545989338345,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.5110104166666667,
    "MUSR": 23.709635416666668,
    "MMLU-PRO Raw": 0.3148271276595745,
    "MMLU-PRO": 23.869680851063833,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.7 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_dare_ties-density-0.9_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.9\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.9</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_dare_ties-density-0.9-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.9",
    "Model sha": "",
    "Average ‚¨ÜÔ∏è": 17.28459343949862,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.329660431636281,
    "IFEval Raw": 0.21607335203925582,
    "IFEval": 21.607335203925583,
    "BBH Raw": 0.46639610671811504,
    "BBH": 24.68762324471831,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.5230416666666667,
    "MUSR": 25.88020833333333,
    "MMLU-PRO Raw": 0.3143284574468085,
    "MMLU-PRO": 23.814273049645386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_dare_ties-density-0.9 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_linear_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_linear\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_linear</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_linear-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_linear",
    "Model sha": "7449157fbc2e8b02e5b6e8ad56b4b2bd7ea82e9d",
    "Average ‚¨ÜÔ∏è": 21.35828447877297,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8261061780318913,
    "IFEval Raw": 0.4308213318439518,
    "IFEval": 43.08213318439518,
    "BBH Raw": 0.5031496839210309,
    "BBH": 28.778577217548463,
    "MATH Lvl 5 Raw": 0.09969788519637462,
    "MATH Lvl 5": 9.969788519637463,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.40971874999999996,
    "MUSR": 10.14817708333333,
    "MMLU-PRO Raw": 0.37117686170212766,
    "MMLU-PRO": 30.13076241134751,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_linear (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_ties-density-0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_ties-density-0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_ties-density-0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_ties-density-0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_ties-density-0.1",
    "Model sha": "84793f89ebe3be5b5bd9a797d4bbdf374c07419d",
    "Average ‚¨ÜÔ∏è": 20.428512140604372,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7863609574915011,
    "IFEval Raw": 0.41161229980895137,
    "IFEval": 41.161229980895136,
    "BBH Raw": 0.5021445196013956,
    "BBH": 28.768718884316495,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.417375,
    "MUSR": 10.671875000000002,
    "MMLU-PRO Raw": 0.36003989361702127,
    "MMLU-PRO": 28.893321513002363,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_ties-density-0.1 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_ties-density-0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_ties-density-0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_ties-density-0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_ties-density-0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_ties-density-0.3",
    "Model sha": "8d051f3eec3fc93a4521073c2d290c4ff9144fc1",
    "Average ‚¨ÜÔ∏è": 18.842381597176864,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9573353325540769,
    "IFEval Raw": 0.3626278274977061,
    "IFEval": 36.26278274977061,
    "BBH Raw": 0.49061122520005807,
    "BBH": 27.724506656987163,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40248958333333335,
    "MUSR": 10.477864583333336,
    "MMLU-PRO Raw": 0.33211436170212766,
    "MMLU-PRO": 25.790484633569736,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_ties-density-0.3 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_ties-density-0.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_ties-density-0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_ties-density-0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_ties-density-0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_ties-density-0.5",
    "Model sha": "c857e33c30016960f114e3a049f5dae41d68bfe7",
    "Average ‚¨ÜÔ∏è": 18.209007549281022,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8419330956629213,
    "IFEval Raw": 0.37966373666316483,
    "IFEval": 37.966373666316485,
    "BBH Raw": 0.47931248948849836,
    "BBH": 26.01209708592899,
    "MATH Lvl 5 Raw": 0.06042296072507553,
    "MATH Lvl 5": 6.042296072507553,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.3879791666666667,
    "MUSR": 7.797395833333334,
    "MMLU-PRO Raw": 0.31748670212765956,
    "MMLU-PRO": 24.165189125295505,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_ties-density-0.5 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_ties-density-0.7_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_ties-density-0.7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_ties-density-0.7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_ties-density-0.7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_ties-density-0.7",
    "Model sha": "8d7d8bbb1e8cba5e51337f97bc3d6d8ae40544d5",
    "Average ‚¨ÜÔ∏è": 18.00619026682418,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8980509507576335,
    "IFEval Raw": 0.3681232463197649,
    "IFEval": 36.81232463197649,
    "BBH Raw": 0.4738186124296502,
    "BBH": 25.371407671115207,
    "MATH Lvl 5 Raw": 0.06419939577039274,
    "MATH Lvl 5": 6.419939577039274,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.3880729166666667,
    "MUSR": 7.575781250000001,
    "MMLU-PRO Raw": 0.3152426861702128,
    "MMLU-PRO": 23.91585401891253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_ties-density-0.7 (Merge)"
  },
  {
    "eval_name": "johnsutor_Llama-3-8B-Instruct_ties-density-0.9_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/johnsutor/Llama-3-8B-Instruct_ties-density-0.9\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">johnsutor/Llama-3-8B-Instruct_ties-density-0.9</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/johnsutor__Llama-3-8B-Instruct_ties-density-0.9-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "johnsutor/Llama-3-8B-Instruct_ties-density-0.9",
    "Model sha": "57c280ce43fe81a23c966b48de6db7f4a85383a3",
    "Average ‚¨ÜÔ∏è": 18.13585052124981,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9010580927313352,
    "IFEval Raw": 0.3858085435533274,
    "IFEval": 38.58085435533274,
    "BBH Raw": 0.47354321136013144,
    "BBH": 25.46373486461887,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.3880416666666667,
    "MUSR": 7.738541666666667,
    "MMLU-PRO Raw": 0.3181515957446808,
    "MMLU-PRO": 24.23906619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "johnsutor/Llama-3-8B-Instruct_ties-density-0.9 (Merge)"
  },
  {
    "eval_name": "jpacifico_Chocolatine-14B-Instruct-4k-DPO_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-4k-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jpacifico/Chocolatine-14B-Instruct-4k-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jpacifico__Chocolatine-14B-Instruct-4k-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jpacifico/Chocolatine-14B-Instruct-4k-DPO",
    "Model sha": "30677e58010979af26b70240846fdf7ff38cbbf2",
    "Average ‚¨ÜÔ∏è": 30.02689405117682,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 8.248900623072105,
    "IFEval Raw": 0.4688648341954902,
    "IFEval": 46.886483419549016,
    "BBH Raw": 0.6299582409761587,
    "BBH": 48.02072159780435,
    "MATH Lvl 5 Raw": 0.16087613293051362,
    "MATH Lvl 5": 16.087613293051362,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.44388541666666664,
    "MUSR": 15.15234375,
    "MMLU-PRO Raw": 0.4763962765957447,
    "MMLU-PRO": 41.8218085106383,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-01",
    "Submission Date": "2024-08-08",
    "Generation": 0,
    "Base Model": "jpacifico/Chocolatine-14B-Instruct-4k-DPO"
  },
  {
    "eval_name": "jpacifico_Chocolatine-14B-Instruct-DPO-v1.2_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-DPO-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jpacifico/Chocolatine-14B-Instruct-DPO-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jpacifico__Chocolatine-14B-Instruct-DPO-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jpacifico/Chocolatine-14B-Instruct-DPO-v1.2",
    "Model sha": "d34bbd55b48e553f28579d86f3ccae19726c6b39",
    "Average ‚¨ÜÔ∏è": 33.54404861417634,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.540603253288398,
    "IFEval Raw": 0.6852107962428579,
    "IFEval": 68.52107962428579,
    "BBH Raw": 0.6438408959901142,
    "BBH": 49.845064475726,
    "MATH Lvl 5 Raw": 0.19410876132930513,
    "MATH Lvl 5": 19.410876132930515,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.4267708333333333,
    "MUSR": 12.346354166666671,
    "MMLU-PRO Raw": 0.46966422872340424,
    "MMLU-PRO": 41.073803191489354,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "jpacifico/Chocolatine-14B-Instruct-DPO-v1.2"
  },
  {
    "eval_name": "jpacifico_Chocolatine-3B-Instruct-DPO-Revised_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jpacifico/Chocolatine-3B-Instruct-DPO-Revised\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jpacifico/Chocolatine-3B-Instruct-DPO-Revised</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jpacifico__Chocolatine-3B-Instruct-DPO-Revised-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jpacifico/Chocolatine-3B-Instruct-DPO-Revised",
    "Model sha": "c403df6c0f78148cfb477972455cbd859149311a",
    "Average ‚¨ÜÔ∏è": 27.91192830004516,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7547244952049539,
    "IFEval Raw": 0.5622625744136669,
    "IFEval": 56.226257441366684,
    "BBH Raw": 0.5539982344792619,
    "BBH": 37.1552860906475,
    "MATH Lvl 5 Raw": 0.16163141993957705,
    "MATH Lvl 5": 16.163141993957705,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.44534375,
    "MUSR": 15.101302083333328,
    "MMLU-PRO Raw": 0.3988530585106383,
    "MMLU-PRO": 33.205895390070914,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-07-19",
    "Generation": 0,
    "Base Model": "jpacifico/Chocolatine-3B-Instruct-DPO-Revised"
  },
  {
    "eval_name": "jpacifico_Chocolatine-3B-Instruct-DPO-v1.0_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jpacifico/Chocolatine-3B-Instruct-DPO-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jpacifico/Chocolatine-3B-Instruct-DPO-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jpacifico__Chocolatine-3B-Instruct-DPO-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jpacifico/Chocolatine-3B-Instruct-DPO-v1.0",
    "Model sha": "98d049b8f8c305cfba81adae498a95e6b5647d4a",
    "Average ‚¨ÜÔ∏è": 25.2030043992137,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7992229434580425,
    "IFEval Raw": 0.3737184005106451,
    "IFEval": 37.37184005106451,
    "BBH Raw": 0.5471398082537478,
    "BBH": 36.554520056455814,
    "MATH Lvl 5 Raw": 0.16465256797583083,
    "MATH Lvl 5": 16.465256797583084,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.4754791666666667,
    "MUSR": 19.468229166666664,
    "MMLU-PRO Raw": 0.3937001329787234,
    "MMLU-PRO": 32.63334810874704,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-11",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "jpacifico/Chocolatine-3B-Instruct-DPO-v1.0"
  },
  {
    "eval_name": "jpacifico_Chocolatine-3B-Instruct-DPO-v1.2_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jpacifico/Chocolatine-3B-Instruct-DPO-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jpacifico/Chocolatine-3B-Instruct-DPO-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jpacifico__Chocolatine-3B-Instruct-DPO-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jpacifico/Chocolatine-3B-Instruct-DPO-v1.2",
    "Model sha": "ebc9de6c266586adb1ec0db31bf050d1cd8fdffe",
    "Average ‚¨ÜÔ∏è": 26.80451120922204,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9744445535056264,
    "IFEval Raw": 0.5455014915978493,
    "IFEval": 54.55014915978493,
    "BBH Raw": 0.5487182027245813,
    "BBH": 35.99938785144921,
    "MATH Lvl 5 Raw": 0.14123867069486407,
    "MATH Lvl 5": 14.123867069486407,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.41542708333333334,
    "MUSR": 12.328385416666668,
    "MMLU-PRO Raw": 0.3877160904255319,
    "MMLU-PRO": 31.96845449172577,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "jpacifico/Chocolatine-3B-Instruct-DPO-v1.2"
  },
  {
    "eval_name": "jsfs11_MixtureofMerges-MoE-4x7b-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jsfs11/MixtureofMerges-MoE-4x7b-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jsfs11/MixtureofMerges-MoE-4x7b-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jsfs11__MixtureofMerges-MoE-4x7b-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jsfs11/MixtureofMerges-MoE-4x7b-v4",
    "Model sha": "2b98406f20a874184dbffb5ed24e1f4b5063ec4b",
    "Average ‚¨ÜÔ∏è": 20.047537447594227,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3838278065408878,
    "IFEval Raw": 0.40299405577201824,
    "IFEval": 40.299405577201824,
    "BBH Raw": 0.5169007103786006,
    "BBH": 32.21799819533692,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.43855208333333334,
    "MUSR": 13.885677083333334,
    "MMLU-PRO Raw": 0.30319148936170215,
    "MMLU-PRO": 22.576832151300238,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-11",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "jsfs11/MixtureofMerges-MoE-4x7b-v4 (Merge)"
  },
  {
    "eval_name": "jsfs11_MixtureofMerges-MoE-4x7b-v5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/jsfs11/MixtureofMerges-MoE-4x7b-v5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">jsfs11/MixtureofMerges-MoE-4x7b-v5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/jsfs11__MixtureofMerges-MoE-4x7b-v5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "jsfs11/MixtureofMerges-MoE-4x7b-v5",
    "Model sha": "c1b5ce7144b966062df7627d2482a59e0df3757c",
    "Average ‚¨ÜÔ∏è": 20.44752918938526,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4312724034989237,
    "IFEval Raw": 0.41993022956865567,
    "IFEval": 41.99302295686556,
    "BBH Raw": 0.5198481257083689,
    "BBH": 32.82672418068055,
    "MATH Lvl 5 Raw": 0.07628398791540784,
    "MATH Lvl 5": 7.628398791540785,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4304895833333333,
    "MUSR": 12.344531250000003,
    "MMLU-PRO Raw": 0.3097573138297872,
    "MMLU-PRO": 23.30636820330969,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-25",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "jsfs11/MixtureofMerges-MoE-4x7b-v5 (Merge)"
  },
  {
    "eval_name": "kaist-ai_janus-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kaist-ai/janus-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kaist-ai/janus-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kaist-ai__janus-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kaist-ai/janus-7b",
    "Model sha": "f19c614ae7c81db06af1655d297c67afa99ad286",
    "Average ‚¨ÜÔ∏è": 17.654763138167564,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6066034983407438,
    "IFEval Raw": 0.37751499355044615,
    "IFEval": 37.751499355044615,
    "BBH Raw": 0.4693667591541633,
    "BBH": 25.749870021061568,
    "MATH Lvl 5 Raw": 0.04305135951661632,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.4401041666666667,
    "MUSR": 14.279687500000003,
    "MMLU-PRO Raw": 0.28740026595744683,
    "MMLU-PRO": 20.822251773049647,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-04",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "alpindale/Mistral-7B-v0.2-hf"
  },
  {
    "eval_name": "kaist-ai_janus-dpo-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kaist-ai/janus-dpo-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kaist-ai/janus-dpo-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kaist-ai__janus-dpo-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kaist-ai/janus-dpo-7b",
    "Model sha": "a414396b6d03fba75d12ccf7d8391186b4b639ce",
    "Average ‚¨ÜÔ∏è": 18.53164895276002,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6264283536746461,
    "IFEval Raw": 0.4002712802031942,
    "IFEval": 40.027128020319424,
    "BBH Raw": 0.4772581104894978,
    "BBH": 27.090901576814417,
    "MATH Lvl 5 Raw": 0.04154078549848943,
    "MATH Lvl 5": 4.1540785498489425,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.43873958333333335,
    "MUSR": 13.709114583333337,
    "MMLU-PRO Raw": 0.2976230053191489,
    "MMLU-PRO": 21.95811170212766,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "kaist-ai_janus-rm-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LLMForSequenceRegression",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kaist-ai/janus-rm-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kaist-ai/janus-rm-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kaist-ai__janus-rm-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kaist-ai/janus-rm-7b",
    "Model sha": "ffdbcc353ad4034fdfa68a767d265920d5f3e71c",
    "Average ‚¨ÜÔ∏è": 4.775598832496902,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5391106010185486,
    "IFEval Raw": 0.177804891022487,
    "IFEval": 17.7804891022487,
    "BBH Raw": 0.3056467446788138,
    "BBH": 3.2777812036470793,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.38829166666666665,
    "MUSR": 5.969791666666667,
    "MMLU-PRO Raw": 0.11261635638297872,
    "MMLU-PRO": 1.4018173758865236,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-09",
    "Submission Date": "2024-10-09",
    "Generation": 0,
    "Base Model": "kaist-ai/janus-rm-7b"
  },
  {
    "eval_name": "kaist-ai_mistral-orpo-capybara-7k_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kaist-ai/mistral-orpo-capybara-7k\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kaist-ai/mistral-orpo-capybara-7k</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kaist-ai__mistral-orpo-capybara-7k-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kaist-ai/mistral-orpo-capybara-7k",
    "Model sha": "24c1172060658a1923c9b454796857e2cc59fbeb",
    "Average ‚¨ÜÔ∏è": 19.18313022987992,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 26,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6607435339837496,
    "IFEval Raw": 0.536733644507684,
    "IFEval": 53.6733644507684,
    "BBH Raw": 0.4488995185492166,
    "BBH": 23.434359116276923,
    "MATH Lvl 5 Raw": 0.037009063444108765,
    "MATH Lvl 5": 3.7009063444108765,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3963541666666666,
    "MUSR": 7.57760416666667,
    "MMLU-PRO Raw": 0.297124335106383,
    "MMLU-PRO": 21.90270390070922,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-23",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "kaist-ai/mistral-orpo-capybara-7k (Merge)"
  },
  {
    "eval_name": "keeeeenw_MicroLlama_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/keeeeenw/MicroLlama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">keeeeenw/MicroLlama</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/keeeeenw__MicroLlama-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "keeeeenw/MicroLlama",
    "Model sha": "8d5874ca07b86ea1ea2e71eea96212278506ba65",
    "Average ‚¨ÜÔ∏è": 5.077266589541096,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 39,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.1857678895350065,
    "IFEval Raw": 0.19853765785892544,
    "IFEval": 19.85376578589254,
    "BBH Raw": 0.3007313991347165,
    "BBH": 2.831363636363637,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.36981249999999993,
    "MUSR": 4.793229166666666,
    "MMLU-PRO Raw": 0.11377992021276596,
    "MMLU-PRO": 1.5311022458628842,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-29",
    "Submission Date": "2024-09-15",
    "Generation": 0,
    "Base Model": "keeeeenw/MicroLlama"
  },
  {
    "eval_name": "kekmodel_StopCarbon-10.7B-v5_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kekmodel/StopCarbon-10.7B-v5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kekmodel/StopCarbon-10.7B-v5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kekmodel__StopCarbon-10.7B-v5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kekmodel/StopCarbon-10.7B-v5",
    "Model sha": "7d59819dce2439f6c83b4f5c21a68aa882ff5ac9",
    "Average ‚¨ÜÔ∏è": 20.001471576043677,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7452936846364598,
    "IFEval Raw": 0.47283651821611106,
    "IFEval": 47.2836518216111,
    "BBH Raw": 0.5177716413471513,
    "BBH": 31.9932224557197,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4019375,
    "MUSR": 9.27552083333333,
    "MMLU-PRO Raw": 0.3156582446808511,
    "MMLU-PRO": 23.96202718676123,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "kekmodel/StopCarbon-10.7B-v5"
  },
  {
    "eval_name": "kevin009_llamaRAGdrama_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kevin009/llamaRAGdrama\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kevin009/llamaRAGdrama</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kevin009__llamaRAGdrama-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kevin009/llamaRAGdrama",
    "Model sha": "8c103ca8fa6dd9a8d3dab81b319408095e9a1ad8",
    "Average ‚¨ÜÔ∏è": 13.222836027835152,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6396390467938745,
    "IFEval Raw": 0.2598372318780835,
    "IFEval": 25.98372318780835,
    "BBH Raw": 0.4007385667099335,
    "BBH": 16.637813694151937,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.43157291666666664,
    "MUSR": 12.113281250000002,
    "MMLU-PRO Raw": 0.27235704787234044,
    "MMLU-PRO": 19.150783096926716,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-04",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "kevin009/llamaRAGdrama"
  },
  {
    "eval_name": "kms7530_chemeng_llama-3-8b-Instruct-bnb-4bit_24_1_100_1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kms7530/chemeng_llama-3-8b-Instruct-bnb-4bit_24_1_100_1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kms7530/chemeng_llama-3-8b-Instruct-bnb-4bit_24_1_100_1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kms7530__chemeng_llama-3-8b-Instruct-bnb-4bit_24_1_100_1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kms7530/chemeng_llama-3-8b-Instruct-bnb-4bit_24_1_100_1",
    "Model sha": "f296897830363557c84cc4a942c2cd1f91818ae4",
    "Average ‚¨ÜÔ∏è": 17.556133044401136,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2999777248574724,
    "IFEval Raw": 0.5455014915978493,
    "IFEval": 54.55014915978493,
    "BBH Raw": 0.42890394469736065,
    "BBH": 19.079190454097787,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.38206249999999997,
    "MUSR": 5.491145833333334,
    "MMLU-PRO Raw": 0.2798371010638298,
    "MMLU-PRO": 19.98190011820331,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "unsloth/llama-3-8b-Instruct-bnb-4bit"
  },
  {
    "eval_name": "kms7530_chemeng_qwen-math-7b_24_1_100_1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kms7530/chemeng_qwen-math-7b_24_1_100_1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kms7530/chemeng_qwen-math-7b_24_1_100_1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kms7530__chemeng_qwen-math-7b_24_1_100_1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kms7530/chemeng_qwen-math-7b_24_1_100_1",
    "Model sha": "b3c1a1875fe4679e8c402b2bde02ae6c1127eb63",
    "Average ‚¨ÜÔ∏è": 12.972013212286393,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.623590045757661,
    "IFEval Raw": 0.17975305522679824,
    "IFEval": 17.97530552267982,
    "BBH Raw": 0.4040009337917377,
    "BBH": 16.463149308014337,
    "MATH Lvl 5 Raw": 0.14803625377643503,
    "MATH Lvl 5": 14.803625377643503,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.38857291666666666,
    "MUSR": 6.238281250000001,
    "MMLU-PRO Raw": 0.2598902925531915,
    "MMLU-PRO": 17.765588061465724,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-14",
    "Generation": 4,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "kno10_ende-chat-0.0.5_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kno10/ende-chat-0.0.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kno10/ende-chat-0.0.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kno10__ende-chat-0.0.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kno10/ende-chat-0.0.5",
    "Model sha": "fff913e8ce204bab72b02582b663db669cb61412",
    "Average ‚¨ÜÔ∏è": 10.636087137871412,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9749664085473255,
    "IFEval Raw": 0.3404455733010634,
    "IFEval": 34.04455733010634,
    "BBH Raw": 0.3604365707523862,
    "BBH": 11.125830654491324,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.39384375,
    "MUSR": 7.097135416666668,
    "MMLU-PRO Raw": 0.17902260638297873,
    "MMLU-PRO": 8.780289598108746,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "kno10/ende-chat-0.0.5"
  },
  {
    "eval_name": "kno10_ende-chat-0.0.7_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/kno10/ende-chat-0.0.7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">kno10/ende-chat-0.0.7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/kno10__ende-chat-0.0.7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "kno10/ende-chat-0.0.7",
    "Model sha": "1d45f51e5a3387378cea1036b0c65f2893466dd6",
    "Average ‚¨ÜÔ∏è": 13.082387129788158,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9583658306092072,
    "IFEval Raw": 0.440063476021401,
    "IFEval": 44.006347602140096,
    "BBH Raw": 0.37918745577624335,
    "BBH": 13.57894913417845,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.386125,
    "MUSR": 6.032291666666666,
    "MMLU-PRO Raw": 0.19664228723404256,
    "MMLU-PRO": 10.738031914893616,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-30",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "kno10/ende-chat-0.0.7"
  },
  {
    "eval_name": "ladydaina_ECE-FDF_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ladydaina/ECE-FDF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ladydaina/ECE-FDF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ladydaina__ECE-FDF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ladydaina/ECE-FDF",
    "Model sha": "81e709d727e9ba5cf8707fe0c5c08e688a4cc6bd",
    "Average ‚¨ÜÔ∏è": 20.004600935202415,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.44622408644020733,
    "IFEval Raw": 0.3728440537773109,
    "IFEval": 37.28440537773109,
    "BBH Raw": 0.5150177593278346,
    "BBH": 32.250998220060005,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.45039583333333333,
    "MUSR": 15.899479166666666,
    "MMLU-PRO Raw": 0.30069813829787234,
    "MMLU-PRO": 22.299793144208035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "ladydaina/ECE-FDF (Merge)"
  },
  {
    "eval_name": "laislemke_LLaMA-2-vicuna-7b-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/laislemke/LLaMA-2-vicuna-7b-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">laislemke/LLaMA-2-vicuna-7b-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/laislemke__LLaMA-2-vicuna-7b-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "laislemke/LLaMA-2-vicuna-7b-slerp",
    "Model sha": "84a64f0ac8ff7db632a9d012fd5f4dcdf1eff950",
    "Average ‚¨ÜÔ∏è": 7.669226122472586,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5970757462040044,
    "IFEval Raw": 0.29320979445648654,
    "IFEval": 29.320979445648653,
    "BBH Raw": 0.29862163052356266,
    "BBH": 2.598263938597983,
    "MATH Lvl 5 Raw": 0.009818731117824773,
    "MATH Lvl 5": 0.9818731117824773,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.3833020833333333,
    "MUSR": 6.179427083333336,
    "MMLU-PRO Raw": 0.13422539893617022,
    "MMLU-PRO": 3.8028221040189125,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-03",
    "Submission Date": "2024-07-03",
    "Generation": 1,
    "Base Model": "laislemke/LLaMA-2-vicuna-7b-slerp (Merge)"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-0.5B-FT-V5-MUSR_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-0.5B-FT-V5-MUSR\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-0.5B-FT-V5-MUSR</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-0.5B-FT-V5-MUSR-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-0.5B-FT-V5-MUSR",
    "Model sha": "bf80bf3d14a79b5dcb322b97b6dbaf10e316a3ee",
    "Average ‚¨ÜÔ∏è": 6.529137553770025,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5829399791876275,
    "IFEval Raw": 0.21377500587330506,
    "IFEval": 21.377500587330506,
    "BBH Raw": 0.32694393820046386,
    "BBH": 6.485922419195989,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.32625,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.15334109042553193,
    "MMLU-PRO": 5.926787825059102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-0.5B-FT-V5-MUSR"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-0.5B-SLERP-V4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-0.5B-SLERP-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-0.5B-SLERP-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-0.5B-SLERP-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-0.5B-SLERP-V4",
    "Model sha": "3a34c33dba0f02cd8c5172f45b6f6510cad1563d",
    "Average ‚¨ÜÔ∏è": 4.3809433295396625,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2679730880886273,
    "IFEval Raw": 0.15639724819035714,
    "IFEval": 15.639724819035713,
    "BBH Raw": 0.2894308596288922,
    "BBH": 2.096080371265706,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.37892708333333336,
    "MUSR": 4.99921875,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-0.5B-SLERP-V4"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1",
    "Model sha": "7865b6f386969b831e9c1754914463154fecbda2",
    "Average ‚¨ÜÔ∏è": 3.5981341522342523,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5252733567929562,
    "IFEval Raw": 0.1437075847639818,
    "IFEval": 14.37075847639818,
    "BBH Raw": 0.3031946898842932,
    "BBH": 2.92944936253925,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2348993288590604,
    "GPQA": 0.0,
    "MUSR Raw": 0.3646041666666667,
    "MUSR": 2.9421874999999997,
    "MMLU-PRO Raw": 0.11211768617021277,
    "MMLU-PRO": 1.3464095744680846,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-09",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-YL-0.5B-SLERP-BIS-V1"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-YL-1B-SLERP-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-YL-1B-SLERP-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-YL-1B-SLERP-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-YL-1B-SLERP-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-YL-1B-SLERP-V3",
    "Model sha": "eef4293be744aef0524f00a7657e915a6601a459",
    "Average ‚¨ÜÔ∏è": 16.271667851438323,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5931069916412925,
    "IFEval Raw": 0.325008754549041,
    "IFEval": 32.500875454904104,
    "BBH Raw": 0.42245501886651654,
    "BBH": 18.22865501035824,
    "MATH Lvl 5 Raw": 0.08685800604229607,
    "MATH Lvl 5": 8.685800604229607,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.42128125,
    "MUSR": 10.82682291666667,
    "MMLU-PRO Raw": 0.2931349734042553,
    "MMLU-PRO": 21.4594414893617,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-YL-1B-SLERP-V3"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-YL-1B-SLERP-V4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-YL-1B-SLERP-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-YL-1B-SLERP-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-YL-1B-SLERP-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-YL-1B-SLERP-V4",
    "Model sha": "dfa5e42b6f4f83cacc3b9e7d0ff05fec7f941835",
    "Average ‚¨ÜÔ∏è": 16.148858435149524,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6118712565682319,
    "IFEval Raw": 0.33235260220658963,
    "IFEval": 33.23526022065896,
    "BBH Raw": 0.4170742409015322,
    "BBH": 17.411751961605876,
    "MATH Lvl 5 Raw": 0.08308157099697885,
    "MATH Lvl 5": 8.308157099697885,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4306145833333333,
    "MUSR": 12.093489583333332,
    "MMLU-PRO Raw": 0.289311835106383,
    "MMLU-PRO": 21.034648345153663,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-YL-1B-SLERP-V4"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-YL-6B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-YL-6B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-YL-6B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-YL-6B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-YL-6B-SLERP-V1",
    "Model sha": "56789ff5fcc863460fce652ebe6ed6bb5a4bd30c",
    "Average ‚¨ÜÔ∏è": 19.86080810918431,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5015803959113865,
    "IFEval Raw": 0.3264072660540699,
    "IFEval": 32.64072660540699,
    "BBH Raw": 0.46293726502592586,
    "BBH": 24.515258836802445,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.48639583333333336,
    "MUSR": 20.632812500000004,
    "MMLU-PRO Raw": 0.32139295212765956,
    "MMLU-PRO": 24.599216903073284,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-08",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-YL-6B-SLERP-V1"
  },
  {
    "eval_name": "lalainy_ECE-PRYMMAL-YL-6B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lalainy/ECE-PRYMMAL-YL-6B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lalainy/ECE-PRYMMAL-YL-6B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lalainy__ECE-PRYMMAL-YL-6B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lalainy/ECE-PRYMMAL-YL-6B-SLERP-V2",
    "Model sha": "18d282d0206ae8f878a9cfa80ce4eaf042056569",
    "Average ‚¨ÜÔ∏è": 19.83541252916034,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.498131132253495,
    "IFEval Raw": 0.3248835312526319,
    "IFEval": 32.48835312526319,
    "BBH Raw": 0.46293726502592586,
    "BBH": 24.515258836802445,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.48639583333333336,
    "MUSR": 20.632812500000004,
    "MMLU-PRO Raw": 0.32139295212765956,
    "MMLU-PRO": 24.599216903073284,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-09",
    "Submission Date": "2024-11-09",
    "Generation": 0,
    "Base Model": "lalainy/ECE-PRYMMAL-YL-6B-SLERP-V2"
  },
  {
    "eval_name": "langgptai_Qwen-las-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/langgptai/Qwen-las-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">langgptai/Qwen-las-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/langgptai__Qwen-las-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "langgptai/Qwen-las-v0.1",
    "Model sha": "a7a4d4945d28bac955554c9abd2f74a71ebbf22f",
    "Average ‚¨ÜÔ∏è": 11.394004277470655,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7980964152747454,
    "IFEval Raw": 0.33010412372504955,
    "IFEval": 33.01041237250495,
    "BBH Raw": 0.38925525629956187,
    "BBH": 14.698639898107368,
    "MATH Lvl 5 Raw": 0.02265861027190333,
    "MATH Lvl 5": 2.2658610271903328,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.37009374999999994,
    "MUSR": 3.66171875,
    "MMLU-PRO Raw": 0.2325465425531915,
    "MMLU-PRO": 14.727393617021276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "Qwen/Qwen1.5-4B-Chat"
  },
  {
    "eval_name": "langgptai_qwen1.5-7b-chat-sa-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/langgptai/qwen1.5-7b-chat-sa-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">langgptai/qwen1.5-7b-chat-sa-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/langgptai__qwen1.5-7b-chat-sa-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "langgptai/qwen1.5-7b-chat-sa-v0.1",
    "Model sha": "5f4f5e69ac7f1d508f8369e977de208b4803444b",
    "Average ‚¨ÜÔ∏è": 16.580170752646193,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 15,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.464321083426148,
    "IFEval Raw": 0.42677429221133256,
    "IFEval": 42.67742922113326,
    "BBH Raw": 0.4325267992878656,
    "BBH": 20.302342129934097,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.3551458333333333,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.29928523936170215,
    "MMLU-PRO": 22.14280437352246,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "Qwen/Qwen1.5-7B-Chat"
  },
  {
    "eval_name": "leafspark_Llama-3.1-8B-MultiReflection-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/leafspark/Llama-3.1-8B-MultiReflection-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">leafspark/Llama-3.1-8B-MultiReflection-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/leafspark__Llama-3.1-8B-MultiReflection-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "leafspark/Llama-3.1-8B-MultiReflection-Instruct",
    "Model sha": "b748441154efdbd7690d773b0194197bfc136ed0",
    "Average ‚¨ÜÔ∏è": 26.311881352660507,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8484468327525838,
    "IFEval Raw": 0.7125382872999197,
    "IFEval": 71.25382872999197,
    "BBH Raw": 0.5009088261495708,
    "BBH": 28.448045037118618,
    "MATH Lvl 5 Raw": 0.13670694864048338,
    "MATH Lvl 5": 13.670694864048338,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3681979166666667,
    "MUSR": 8.524739583333336,
    "MMLU-PRO Raw": 0.37242353723404253,
    "MMLU-PRO": 30.269281914893615,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "leafspark/Llama-3.1-8B-MultiReflection-Instruct (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-9B",
    "Model sha": "fb22193268c7a6c3b4598255999ce2de3af8c256",
    "Average ‚¨ÜÔ∏è": 22.465284493940924,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 65,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9106881804306695,
    "IFEval Raw": 0.3008772279773224,
    "IFEval": 30.08772279773224,
    "BBH Raw": 0.5931298417725773,
    "BBH": 42.03199052898647,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.3347315436241611,
    "GPQA": 11.297539149888143,
    "MUSR Raw": 0.4424270833333333,
    "MUSR": 14.47005208333333,
    "MMLU-PRO Raw": 0.4226230053191489,
    "MMLU-PRO": 35.847000591016545,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-14",
    "Submission Date": "2024-08-27",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-Advanced-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-Advanced-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-Advanced-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-Advanced-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-Advanced-9B",
    "Model sha": "960654f5780f0b458367a6b591ad8440892c2aad",
    "Average ‚¨ÜÔ∏è": 25.10969386534957,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.227277245236008,
    "IFEval Raw": 0.5515964308036011,
    "IFEval": 55.15964308036011,
    "BBH Raw": 0.5889067263184956,
    "BBH": 41.16143815473681,
    "MATH Lvl 5 Raw": 0.0037764350453172208,
    "MATH Lvl 5": 0.3776435045317221,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.3760729166666667,
    "MUSR": 6.509114583333333,
    "MMLU-PRO Raw": 0.4243683510638298,
    "MMLU-PRO": 36.040927895981085,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-Advanced-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-Remix-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-Remix-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-Remix-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-Remix-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-Remix-9B",
    "Model sha": "f917a9be9f86d58fe122d58ba84cf4b08e4a975e",
    "Average ‚¨ÜÔ∏è": 29.261671253970402,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.157440904099819,
    "IFEval Raw": 0.7083416446140685,
    "IFEval": 70.83416446140684,
    "BBH Raw": 0.5892021015046846,
    "BBH": 41.59231281593379,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.4371875,
    "MUSR": 13.715104166666663,
    "MMLU-PRO Raw": 0.42386968085106386,
    "MMLU-PRO": 35.985520094562645,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-Remix-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v2-9B",
    "Model sha": "77aca48ac25eb2cbe8c0751a4ef77e5face34d80",
    "Average ‚¨ÜÔ∏è": 19.173830206434758,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.996241894044154,
    "IFEval Raw": 0.21362429464930827,
    "IFEval": 21.362429464930827,
    "BBH Raw": 0.5765835815625312,
    "BBH": 39.79685359725269,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.3422818791946309,
    "GPQA": 12.304250559284117,
    "MUSR Raw": 0.34838541666666667,
    "MUSR": 4.881510416666667,
    "MMLU-PRO Raw": 0.422124335106383,
    "MMLU-PRO": 35.791592789598106,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v2-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v2a-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v2a-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v2a-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v2a-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v2a-9B",
    "Model sha": "899fb093d80569fc919f53217e3acf031dde89a5",
    "Average ‚¨ÜÔ∏è": 15.01936138376009,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9813962356480848,
    "IFEval Raw": 0.15946909755005606,
    "IFEval": 15.946909755005606,
    "BBH Raw": 0.518248966271832,
    "BBH": 31.19852836941699,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33976510067114096,
    "GPQA": 11.968680089485462,
    "MUSR Raw": 0.31647916666666664,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.35147938829787234,
    "MMLU-PRO": 27.942154255319146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v2a-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v2f-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v2f-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v2f-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v2f-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v2f-9B",
    "Model sha": "44da9d6a9bc7be5a9af24fb0951047849d5f717d",
    "Average ‚¨ÜÔ∏è": 18.765944401672623,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.3985955549788076,
    "IFEval Raw": 0.37911408396388246,
    "IFEval": 37.911408396388246,
    "BBH Raw": 0.5192845467961766,
    "BBH": 31.421336195418803,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.3231458333333333,
    "MUSR": 3.5932291666666667,
    "MMLU-PRO Raw": 0.3503158244680851,
    "MMLU-PRO": 27.812869385342786,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v2f-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v3-Advanced-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v3-Advanced-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v3-Advanced-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v3-Advanced-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v3-Advanced-9B",
    "Model sha": "318afe2b44a150780e44483a0f90a499e81f946f",
    "Average ‚¨ÜÔ∏è": 28.333876970643033,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.7879959794883304,
    "IFEval Raw": 0.6601816513517467,
    "IFEval": 66.01816513517468,
    "BBH Raw": 0.5935146853737787,
    "BBH": 42.21047229127766,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.44496874999999997,
    "MUSR": 14.58776041666667,
    "MMLU-PRO Raw": 0.41963098404255317,
    "MMLU-PRO": 35.51455378250591,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v3-Advanced-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v3b-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v3b-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v3b-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v3b-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v3b-9B",
    "Model sha": "de8bbacddabf22dad89658d3b3d358b3eccbd59c",
    "Average ‚¨ÜÔ∏è": 29.016329177068368,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3037277659902227,
    "IFEval Raw": 0.6809144181881852,
    "IFEval": 68.09144181881851,
    "BBH Raw": 0.5907698162898164,
    "BBH": 41.62398549212332,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.44887499999999997,
    "MUSR": 15.209374999999996,
    "MMLU-PRO Raw": 0.4204621010638298,
    "MMLU-PRO": 35.606900118203306,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v3b-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v3i-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v3i-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v3i-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v3i-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v3i-9B",
    "Model sha": "8bd1ce81b6f42ebeebd9957b605c7313eedbe0a8",
    "Average ‚¨ÜÔ∏è": 21.293827573842577,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.4755098819392485,
    "IFEval Raw": 0.4203047912871182,
    "IFEval": 42.03047912871182,
    "BBH Raw": 0.5625750779805955,
    "BBH": 38.238824874777286,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.31806249999999997,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.41663896276595747,
    "MMLU-PRO": 35.182106973995275,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v3i-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v3j-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v3j-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v3j-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v3j-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v3j-9B",
    "Model sha": "7ad4a1bf604f37bd82f3470dbc24870896d7287d",
    "Average ‚¨ÜÔ∏è": 21.180095735103666,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.4138224767880523,
    "IFEval Raw": 0.4169326276501904,
    "IFEval": 41.69326276501904,
    "BBH Raw": 0.5632286961183511,
    "BBH": 38.16656919949616,
    "MATH Lvl 5 Raw": 0.0007552870090634442,
    "MATH Lvl 5": 0.07552870090634442,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.31803125,
    "MUSR": 1.920572916666666,
    "MMLU-PRO Raw": 0.41339760638297873,
    "MMLU-PRO": 34.821956264775416,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v3j-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v4-Advanced-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v4-Advanced-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v4-Advanced-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v4-Advanced-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v4-Advanced-9B",
    "Model sha": "bc9edb78753fc60a22268cd91e93e43dd9fbc648",
    "Average ‚¨ÜÔ∏è": 30.93550287526195,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.329564240607788,
    "IFEval Raw": 0.7015474496558022,
    "IFEval": 70.15474496558022,
    "BBH Raw": 0.6023627309683861,
    "BBH": 43.18189722480503,
    "MATH Lvl 5 Raw": 0.06722054380664652,
    "MATH Lvl 5": 6.722054380664652,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.4580520833333333,
    "MUSR": 16.289843750000003,
    "MMLU-PRO Raw": 0.4366688829787234,
    "MMLU-PRO": 37.4076536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v4-Advanced-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v4a-Advanced-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v4a-Advanced-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v4a-Advanced-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v4a-Advanced-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v4a-Advanced-9B",
    "Model sha": "78dca140ec1b704233c932706fc9640404433cc5",
    "Average ‚¨ÜÔ∏è": 30.16400805293473,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2896179468099698,
    "IFEval Raw": 0.7135123694020753,
    "IFEval": 71.35123694020754,
    "BBH Raw": 0.598838715496553,
    "BBH": 42.73751687792403,
    "MATH Lvl 5 Raw": 0.02416918429003021,
    "MATH Lvl 5": 2.416918429003021,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.44890625,
    "MUSR": 15.179947916666663,
    "MMLU-PRO Raw": 0.4309341755319149,
    "MMLU-PRO": 36.77046394799054,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v4a-Advanced-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v4b-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v4b-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v4b-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v4b-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v4b-9B",
    "Model sha": "70dc6ddfaede76ff01584922fca53ba90837cd52",
    "Average ‚¨ÜÔ∏è": 31.290605069393816,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4250168402421792,
    "IFEval Raw": 0.6878338364428604,
    "IFEval": 68.78338364428605,
    "BBH Raw": 0.6039158192304305,
    "BBH": 43.44273930792989,
    "MATH Lvl 5 Raw": 0.1027190332326284,
    "MATH Lvl 5": 10.27190332326284,
    "GPQA Raw": 0.34060402684563756,
    "GPQA": 12.080536912751676,
    "MUSR Raw": 0.45547916666666666,
    "MUSR": 15.868229166666667,
    "MMLU-PRO Raw": 0.4356715425531915,
    "MMLU-PRO": 37.29683806146573,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v4b-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v4c-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v4c-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v4c-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v4c-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v4c-9B",
    "Model sha": "26f2619a432266a5f73c135804b1aa34f00ec689",
    "Average ‚¨ÜÔ∏è": 32.878261787684,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.7652375901844013,
    "IFEval Raw": 0.6945282960323054,
    "IFEval": 69.45282960323055,
    "BBH Raw": 0.6084319292299174,
    "BBH": 44.12536650674094,
    "MATH Lvl 5 Raw": 0.19486404833836862,
    "MATH Lvl 5": 19.48640483383686,
    "GPQA Raw": 0.3338926174496644,
    "GPQA": 11.185682326621922,
    "MUSR Raw": 0.45278124999999997,
    "MUSR": 15.297656249999998,
    "MMLU-PRO Raw": 0.43949468085106386,
    "MMLU-PRO": 37.72163120567376,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v4c-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Gemma-2-Ataraxy-v4d-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Gemma-2-Ataraxy-v4d-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Gemma-2-Ataraxy-v4d-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Gemma-2-Ataraxy-v4d-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Gemma-2-Ataraxy-v4d-9B",
    "Model sha": "24f9ad78e42c92df5277b3aea4deb4083a8625d9",
    "Average ‚¨ÜÔ∏è": 33.17239600127745,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.6852774281542287,
    "IFEval Raw": 0.7250029920610646,
    "IFEval": 72.50029920610646,
    "BBH Raw": 0.6054158192304304,
    "BBH": 43.595239307929894,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.34731543624161076,
    "GPQA": 12.975391498881436,
    "MUSR Raw": 0.4541458333333333,
    "MUSR": 15.868229166666664,
    "MMLU-PRO Raw": 0.4345910904255319,
    "MMLU-PRO": 37.1767878250591,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "lemon07r/Gemma-2-Ataraxy-v4d-9B (Merge)"
  },
  {
    "eval_name": "lemon07r_Llama-3-RedMagic4-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/Llama-3-RedMagic4-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/Llama-3-RedMagic4-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__Llama-3-RedMagic4-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/Llama-3-RedMagic4-8B",
    "Model sha": "65ee08a0434f1903a8971640fc3cca6c8ae8590e",
    "Average ‚¨ÜÔ∏è": 19.49393108204256,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7985960119985078,
    "IFEval Raw": 0.4864005283758206,
    "IFEval": 48.64005283758206,
    "BBH Raw": 0.42560489470390417,
    "BBH": 19.475746974326068,
    "MATH Lvl 5 Raw": 0.09365558912386708,
    "MATH Lvl 5": 9.365558912386708,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.3766354166666666,
    "MUSR": 4.379427083333333,
    "MMLU-PRO Raw": 0.3676030585106383,
    "MMLU-PRO": 29.733673167848696,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-19",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "lemon07r/Llama-3-RedMagic4-8B (Merge)"
  },
  {
    "eval_name": "lemon07r_llama-3-NeuralMahou-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lemon07r/llama-3-NeuralMahou-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lemon07r/llama-3-NeuralMahou-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lemon07r__llama-3-NeuralMahou-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lemon07r/llama-3-NeuralMahou-8b",
    "Model sha": "59a0937df85f9d6d65d15dbb4a7c06b6ad8a0305",
    "Average ‚¨ÜÔ∏è": 19.871250447473276,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8462277043082896,
    "IFEval Raw": 0.49009738604680025,
    "IFEval": 49.009738604680024,
    "BBH Raw": 0.41841123683301523,
    "BBH": 18.69206874721008,
    "MATH Lvl 5 Raw": 0.10347432024169184,
    "MATH Lvl 5": 10.347432024169184,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.3872708333333333,
    "MUSR": 6.142187500000001,
    "MMLU-PRO Raw": 0.3690159574468085,
    "MMLU-PRO": 29.89066193853428,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "lemon07r/llama-3-NeuralMahou-8b (Merge)"
  },
  {
    "eval_name": "lesubra_ECE-EIFFEL-3B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-EIFFEL-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-EIFFEL-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-EIFFEL-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-EIFFEL-3B",
    "Model sha": "aa56433ac824d245ac82d5e55ce8e589df0711ec",
    "Average ‚¨ÜÔ∏è": 22.013486779642253,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.539546381109783,
    "IFEval Raw": 0.3469405621528655,
    "IFEval": 34.69405621528655,
    "BBH Raw": 0.5101583259186949,
    "BBH": 31.286439186186243,
    "MATH Lvl 5 Raw": 0.09214501510574019,
    "MATH Lvl 5": 9.214501510574019,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.43622916666666667,
    "MUSR": 14.6953125,
    "MMLU-PRO Raw": 0.3820644946808511,
    "MMLU-PRO": 31.340499408983447,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "lesubra/ECE-EIFFEL-3B"
  },
  {
    "eval_name": "lesubra_ECE-EIFFEL-3Bv2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-EIFFEL-3Bv2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-EIFFEL-3Bv2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-EIFFEL-3Bv2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-EIFFEL-3Bv2",
    "Model sha": "b059d1a0d49f09d6df34d93f133d24f6641bc535",
    "Average ‚¨ÜÔ∏è": 22.10886589241117,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8603449555758487,
    "IFEval Raw": 0.30130276555096036,
    "IFEval": 30.13027655509603,
    "BBH Raw": 0.5424007873371969,
    "BBH": 36.35313296509659,
    "MATH Lvl 5 Raw": 0.056646525679758315,
    "MATH Lvl 5": 5.664652567975832,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.4442916666666667,
    "MUSR": 15.769791666666665,
    "MMLU-PRO Raw": 0.39993351063829785,
    "MMLU-PRO": 33.32594562647754,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "lesubra/ECE-EIFFEL-3Bv2"
  },
  {
    "eval_name": "lesubra_ECE-EIFFEL-3Bv3_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-EIFFEL-3Bv3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-EIFFEL-3Bv3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-EIFFEL-3Bv3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-EIFFEL-3Bv3",
    "Model sha": "2cd31e58d38b96626a8a83192b5d2eec6669f5e2",
    "Average ‚¨ÜÔ∏è": 25.33758187443696,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7171468373544209,
    "IFEval Raw": 0.3786142989490109,
    "IFEval": 37.861429894901086,
    "BBH Raw": 0.5469446669064592,
    "BBH": 36.46408334995511,
    "MATH Lvl 5 Raw": 0.15709969788519637,
    "MATH Lvl 5": 15.709969788519636,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.46751041666666665,
    "MUSR": 18.30546875,
    "MMLU-PRO Raw": 0.39752327127659576,
    "MMLU-PRO": 33.05814125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "lesubra/ECE-EIFFEL-3Bv3"
  },
  {
    "eval_name": "lesubra_ECE-PRYMMAL-3B-SLERP-V1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-PRYMMAL-3B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-PRYMMAL-3B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-PRYMMAL-3B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-PRYMMAL-3B-SLERP-V1",
    "Model sha": "e46f1de93f10b1a57f9175653fd29dda355a61e6",
    "Average ‚¨ÜÔ∏è": 22.002429182591055,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7135939533547564,
    "IFEval Raw": 0.2932840418977203,
    "IFEval": 29.328404189772023,
    "BBH Raw": 0.5340594627933309,
    "BBH": 35.05306761164019,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.45951041666666664,
    "MUSR": 16.638802083333335,
    "MMLU-PRO Raw": 0.3900432180851064,
    "MMLU-PRO": 32.227024231678485,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "lesubra/ECE-PRYMMAL-3B-SLERP-V1"
  },
  {
    "eval_name": "lesubra_ECE-PRYMMAL-3B-SLERP-V2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-PRYMMAL-3B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-PRYMMAL-3B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-PRYMMAL-3B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-PRYMMAL-3B-SLERP-V2",
    "Model sha": "ba617ea0b1ed5497f62bf49635c30bcfb0547133",
    "Average ‚¨ÜÔ∏è": 22.002429182591055,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7303252562075893,
    "IFEval Raw": 0.2932840418977203,
    "IFEval": 29.328404189772023,
    "BBH Raw": 0.5340594627933309,
    "BBH": 35.05306761164019,
    "MATH Lvl 5 Raw": 0.09818731117824774,
    "MATH Lvl 5": 9.818731117824774,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.45951041666666664,
    "MUSR": 16.638802083333335,
    "MMLU-PRO Raw": 0.3900432180851064,
    "MMLU-PRO": 32.227024231678485,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 0,
    "Base Model": "lesubra/ECE-PRYMMAL-3B-SLERP-V2"
  },
  {
    "eval_name": "lesubra_ECE-PRYMMAL-3B-SLERP_2-V1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-PRYMMAL-3B-SLERP_2-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-PRYMMAL-3B-SLERP_2-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-PRYMMAL-3B-SLERP_2-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-PRYMMAL-3B-SLERP_2-V1",
    "Model sha": "354e5c732dd2fde016da1e33a018d2d2787f7805",
    "Average ‚¨ÜÔ∏è": 24.634133139185625,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6394276779244312,
    "IFEval Raw": 0.3649006857360692,
    "IFEval": 36.49006857360692,
    "BBH Raw": 0.5411447467732948,
    "BBH": 35.710681082357645,
    "MATH Lvl 5 Raw": 0.14803625377643503,
    "MATH Lvl 5": 14.803625377643503,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.4661458333333333,
    "MUSR": 18.068229166666665,
    "MMLU-PRO Raw": 0.3990192819148936,
    "MMLU-PRO": 33.2243646572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "lesubra/ECE-PRYMMAL-3B-SLERP_2-V1"
  },
  {
    "eval_name": "lesubra_ECE-PRYMMAL-3B-SLERP_2-V2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/ECE-PRYMMAL-3B-SLERP_2-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/ECE-PRYMMAL-3B-SLERP_2-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__ECE-PRYMMAL-3B-SLERP_2-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/ECE-PRYMMAL-3B-SLERP_2-V2",
    "Model sha": "d5074a951206f946a6be331a74bd4fa381d348eb",
    "Average ‚¨ÜÔ∏è": 24.659528719209593,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5355167901301285,
    "IFEval Raw": 0.3664244205375071,
    "IFEval": 36.64244205375071,
    "BBH Raw": 0.5411447467732948,
    "BBH": 35.710681082357645,
    "MATH Lvl 5 Raw": 0.14803625377643503,
    "MATH Lvl 5": 14.803625377643503,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.4661458333333333,
    "MUSR": 18.068229166666665,
    "MMLU-PRO Raw": 0.3990192819148936,
    "MMLU-PRO": 33.2243646572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 0,
    "Base Model": "lesubra/ECE-PRYMMAL-3B-SLERP_2-V2"
  },
  {
    "eval_name": "lesubra_merge-test_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lesubra/merge-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lesubra/merge-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lesubra__merge-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lesubra/merge-test",
    "Model sha": "39895c64dd646443719873a2ab2b19d3afe4f86c",
    "Average ‚¨ÜÔ∏è": 25.735641766923777,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9685256255921008,
    "IFEval Raw": 0.538257379309122,
    "IFEval": 53.8257379309122,
    "BBH Raw": 0.5240434385320306,
    "BBH": 33.353311441745,
    "MATH Lvl 5 Raw": 0.10045317220543809,
    "MATH Lvl 5": 10.04531722054381,
    "GPQA Raw": 0.3221476510067114,
    "GPQA": 9.61968680089485,
    "MUSR Raw": 0.44190625,
    "MUSR": 15.638281249999997,
    "MMLU-PRO Raw": 0.38738364361702127,
    "MMLU-PRO": 31.93151595744681,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "lesubra/merge-test"
  },
  {
    "eval_name": "lightblue_suzume-llama-3-8B-multilingual_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lightblue/suzume-llama-3-8B-multilingual</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lightblue__suzume-llama-3-8B-multilingual-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lightblue/suzume-llama-3-8B-multilingual",
    "Model sha": "0cb15aa9ec685eef494f9a15f65aefcfe3c04c66",
    "Average ‚¨ÜÔ∏è": 23.885601415742673,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 105,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8409896248922019,
    "IFEval Raw": 0.6678003253589365,
    "IFEval": 66.78003253589365,
    "BBH Raw": 0.49499524187359745,
    "BBH": 28.895092037237777,
    "MATH Lvl 5 Raw": 0.08836858006042295,
    "MATH Lvl 5": 8.836858006042295,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.39768749999999997,
    "MUSR": 7.8442708333333355,
    "MMLU-PRO Raw": 0.33834773936170215,
    "MMLU-PRO": 26.483082151300234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-23",
    "Submission Date": "2024-07-30",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "lightblue_suzume-llama-3-8B-multilingual-orpo-borda-full_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual-orpo-borda-full\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lightblue/suzume-llama-3-8B-multilingual-orpo-borda-full</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lightblue__suzume-llama-3-8B-multilingual-orpo-borda-full-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lightblue/suzume-llama-3-8B-multilingual-orpo-borda-full",
    "Model sha": "ac04e23fb8861c188f8ecddfecc4250b40aee04d",
    "Average ‚¨ÜÔ∏è": 19.571596882248173,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8025665878425922,
    "IFEval Raw": 0.5817464327983085,
    "IFEval": 58.174643279830846,
    "BBH Raw": 0.4714219934773132,
    "BBH": 25.075474888496917,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3221875,
    "MUSR": 4.040104166666668,
    "MMLU-PRO Raw": 0.33095079787234044,
    "MMLU-PRO": 25.66119976359338,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-07-29",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "lightblue_suzume-llama-3-8B-multilingual-orpo-borda-half_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lightblue__suzume-llama-3-8B-multilingual-orpo-borda-half-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lightblue/suzume-llama-3-8B-multilingual-orpo-borda-half",
    "Model sha": "b82150a9840ba5ba93918c745adc70afc6ad2ce1",
    "Average ‚¨ÜÔ∏è": 21.409091772671264,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8863369451745771,
    "IFEval Raw": 0.6249107922534431,
    "IFEval": 62.49107922534431,
    "BBH Raw": 0.47074584910573014,
    "BBH": 26.34859792572119,
    "MATH Lvl 5 Raw": 0.08459214501510574,
    "MATH Lvl 5": 8.459214501510575,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.35158333333333336,
    "MUSR": 2.114583333333334,
    "MMLU-PRO Raw": 0.36136968085106386,
    "MMLU-PRO": 29.041075650118202,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-25",
    "Submission Date": "2024-06-29",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "lightblue_suzume-llama-3-8B-multilingual-orpo-borda-top25_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top25\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top25</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lightblue__suzume-llama-3-8B-multilingual-orpo-borda-top25-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top25",
    "Model sha": "5a2f17238cc83932e00613d285f8bf6b8f4a0c3a",
    "Average ‚¨ÜÔ∏è": 23.533710710608293,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8348683505398102,
    "IFEval Raw": 0.6636535503574958,
    "IFEval": 66.36535503574959,
    "BBH Raw": 0.4864641205580417,
    "BBH": 27.665285015300114,
    "MATH Lvl 5 Raw": 0.09516616314199394,
    "MATH Lvl 5": 9.516616314199394,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.35660416666666667,
    "MUSR": 4.808854166666668,
    "MMLU-PRO Raw": 0.3684341755319149,
    "MMLU-PRO": 29.826019503546092,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-06-29",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "lightblue_suzume-llama-3-8B-multilingual-orpo-borda-top75_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top75\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top75</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lightblue__suzume-llama-3-8B-multilingual-orpo-borda-top75-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lightblue/suzume-llama-3-8B-multilingual-orpo-borda-top75",
    "Model sha": "555f4a0092f239557e1aa34f9d489e8156b907bb",
    "Average ‚¨ÜÔ∏è": 23.596767310523163,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9465042585533999,
    "IFEval Raw": 0.6687245397766814,
    "IFEval": 66.87245397766813,
    "BBH Raw": 0.48333166095856117,
    "BBH": 28.056255938988922,
    "MATH Lvl 5 Raw": 0.07552870090634442,
    "MATH Lvl 5": 7.552870090634442,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3816875,
    "MUSR": 5.3109375000000005,
    "MMLU-PRO Raw": 0.37691156914893614,
    "MMLU-PRO": 30.767952127659566,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-06-29",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "llmat_Mistral-v0.3-7B-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llmat/Mistral-v0.3-7B-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llmat/Mistral-v0.3-7B-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llmat__Mistral-v0.3-7B-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llmat/Mistral-v0.3-7B-ORPO",
    "Model sha": "868d8a51e8deb6fd948eabe5bc296c53bcf41073",
    "Average ‚¨ÜÔ∏è": 12.084587090107753,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2590159657534048,
    "IFEval Raw": 0.3770406964631622,
    "IFEval": 37.70406964631621,
    "BBH Raw": 0.39776607302918093,
    "BBH": 14.86315851911541,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.35552083333333334,
    "MUSR": 2.973437500000001,
    "MMLU-PRO Raw": 0.2278091755319149,
    "MMLU-PRO": 14.2010195035461,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-04",
    "Submission Date": "2024-09-02",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-v0.3-bnb-4bit"
  },
  {
    "eval_name": "llmat_Mistral-v0.3-7B-ORPO_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llmat/Mistral-v0.3-7B-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llmat/Mistral-v0.3-7B-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llmat__Mistral-v0.3-7B-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llmat/Mistral-v0.3-7B-ORPO",
    "Model sha": "868d8a51e8deb6fd948eabe5bc296c53bcf41073",
    "Average ‚¨ÜÔ∏è": 12.024321589275658,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6268180652325449,
    "IFEval Raw": 0.3639764713183243,
    "IFEval": 36.39764713183243,
    "BBH Raw": 0.400465557804411,
    "BBH": 15.59149132338697,
    "MATH Lvl 5 Raw": 0.0015105740181268882,
    "MATH Lvl 5": 0.1510574018126888,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.3528541666666667,
    "MUSR": 2.973437500000001,
    "MMLU-PRO Raw": 0.23013630319148937,
    "MMLU-PRO": 14.459589243498819,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-04",
    "Submission Date": "2024-08-06",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-v0.3-bnb-4bit"
  },
  {
    "eval_name": "llnYou_ECE-PRYMMAL-YL-1B-SLERP-V5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llnYou/ECE-PRYMMAL-YL-1B-SLERP-V5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llnYou/ECE-PRYMMAL-YL-1B-SLERP-V5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llnYou__ECE-PRYMMAL-YL-1B-SLERP-V5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llnYou/ECE-PRYMMAL-YL-1B-SLERP-V5",
    "Model sha": "6facb36cea2f670e32d6571846f00aa4cf5aaa86",
    "Average ‚¨ÜÔ∏è": 15.509045365139842,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6401782543004979,
    "IFEval Raw": 0.33125329680802496,
    "IFEval": 33.125329680802494,
    "BBH Raw": 0.42329545804357255,
    "BBH": 18.879659027462832,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3868020833333334,
    "MUSR": 5.650260416666666,
    "MMLU-PRO Raw": 0.29305186170212766,
    "MMLU-PRO": 21.45020685579196,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "llnYou/ECE-PRYMMAL-YL-1B-SLERP-V5"
  },
  {
    "eval_name": "llnYou_ECE-PRYMMAL-YL-1B-SLERP-V6_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llnYou/ECE-PRYMMAL-YL-1B-SLERP-V6\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llnYou/ECE-PRYMMAL-YL-1B-SLERP-V6</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llnYou__ECE-PRYMMAL-YL-1B-SLERP-V6-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llnYou/ECE-PRYMMAL-YL-1B-SLERP-V6",
    "Model sha": "f15fb39e40475348e7d349c3ec2f346ffca39377",
    "Average ‚¨ÜÔ∏è": 9.35750923320331,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5554145432363247,
    "IFEval Raw": 0.13876181864120535,
    "IFEval": 13.876181864120534,
    "BBH Raw": 0.3944027089700251,
    "BBH": 14.538923027777068,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.39279166666666665,
    "MUSR": 7.365624999999999,
    "MMLU-PRO Raw": 0.2349567819148936,
    "MMLU-PRO": 14.995197990543732,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-13",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "llnYou/ECE-PRYMMAL-YL-1B-SLERP-V6"
  },
  {
    "eval_name": "llnYou_ECE-PRYMMAL-YL-3B-SLERP-V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llnYou/ECE-PRYMMAL-YL-3B-SLERP-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llnYou/ECE-PRYMMAL-YL-3B-SLERP-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llnYou__ECE-PRYMMAL-YL-3B-SLERP-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V1",
    "Model sha": "4918220543f4923137d20204a5ea396f65f6b956",
    "Average ‚¨ÜÔ∏è": 11.475736756355928,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5793884284358181,
    "IFEval Raw": 0.23463299600615256,
    "IFEval": 23.463299600615258,
    "BBH Raw": 0.4018418465179459,
    "BBH": 15.79746247814972,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.3364479166666667,
    "MUSR": 3.2226562499999996,
    "MMLU-PRO Raw": 0.2849900265957447,
    "MMLU-PRO": 20.554447399527188,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V1"
  },
  {
    "eval_name": "llnYou_ECE-PRYMMAL-YL-3B-SLERP-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llnYou/ECE-PRYMMAL-YL-3B-SLERP-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llnYou/ECE-PRYMMAL-YL-3B-SLERP-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llnYou__ECE-PRYMMAL-YL-3B-SLERP-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V2",
    "Model sha": "c3d4fbef1a10ef2746c47c0379b4247c784758e5",
    "Average ‚¨ÜÔ∏è": 11.599470354824822,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5438091790854709,
    "IFEval Raw": 0.2309361383351729,
    "IFEval": 23.09361383351729,
    "BBH Raw": 0.39897709281426197,
    "BBH": 15.20224370386771,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.3587708333333333,
    "MUSR": 6.613020833333334,
    "MMLU-PRO Raw": 0.28997672872340424,
    "MMLU-PRO": 21.108525413711583,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V2"
  },
  {
    "eval_name": "llnYou_ECE-PRYMMAL-YL-3B-SLERP-V3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/llnYou/ECE-PRYMMAL-YL-3B-SLERP-V3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">llnYou/ECE-PRYMMAL-YL-3B-SLERP-V3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/llnYou__ECE-PRYMMAL-YL-3B-SLERP-V3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V3",
    "Model sha": "90648507743059de96334fdc97309b6f2af3d01d",
    "Average ‚¨ÜÔ∏è": 22.910741927151804,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.543325489591988,
    "IFEval Raw": 0.35808100285021516,
    "IFEval": 35.80810028502152,
    "BBH Raw": 0.5473121918055145,
    "BBH": 36.62575632451354,
    "MATH Lvl 5 Raw": 0.09894259818731117,
    "MATH Lvl 5": 9.894259818731117,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.43613541666666666,
    "MUSR": 14.050260416666662,
    "MMLU-PRO Raw": 0.40433843085106386,
    "MMLU-PRO": 33.81538120567376,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-13",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "llnYou/ECE-PRYMMAL-YL-3B-SLERP-V3"
  },
  {
    "eval_name": "lmsys_vicuna-13b-v1.3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-13b-v1.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lmsys/vicuna-13b-v1.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lmsys__vicuna-13b-v1.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lmsys/vicuna-13b-v1.3",
    "Model sha": "6566e9cb1787585d1147dcf4f9bc48f29e1328d2",
    "Average ‚¨ÜÔ∏è": 10.284476273840957,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 194,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.094232905688383,
    "IFEval Raw": 0.3343506340953115,
    "IFEval": 33.43506340953115,
    "BBH Raw": 0.3384399312777569,
    "BBH": 7.4897893116292105,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3727291666666666,
    "MUSR": 4.091145833333333,
    "MMLU-PRO Raw": 0.2243184840425532,
    "MMLU-PRO": 13.813164893617023,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-06-18",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "lmsys/vicuna-13b-v1.3"
  },
  {
    "eval_name": "lmsys_vicuna-7b-v1.3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-7b-v1.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lmsys/vicuna-7b-v1.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lmsys__vicuna-7b-v1.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lmsys/vicuna-7b-v1.3",
    "Model sha": "236eeeab96f0dc2e463f2bebb7bb49809279c6d6",
    "Average ‚¨ÜÔ∏è": 8.31181120581355,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 129,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5633782031203843,
    "IFEval Raw": 0.29086158060612505,
    "IFEval": 29.086158060612505,
    "BBH Raw": 0.3298410006592924,
    "BBH": 6.461378796018201,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2424496644295302,
    "GPQA": 0.0,
    "MUSR Raw": 0.3793333333333333,
    "MUSR": 5.0166666666666675,
    "MMLU-PRO Raw": 0.18375997340425532,
    "MMLU-PRO": 9.306663711583923,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-06-18",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "lmsys/vicuna-7b-v1.3"
  },
  {
    "eval_name": "lmsys_vicuna-7b-v1.5_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lmsys/vicuna-7b-v1.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lmsys/vicuna-7b-v1.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lmsys__vicuna-7b-v1.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lmsys/vicuna-7b-v1.5",
    "Model sha": "3321f76e3f527bd14065daf69dad9344000a201d",
    "Average ‚¨ÜÔ∏è": 10.784447380313544,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 308,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6027181051322219,
    "IFEval Raw": 0.23515716077784724,
    "IFEval": 23.515716077784724,
    "BBH Raw": 0.39470436842233775,
    "BBH": 15.15250931284372,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.42311458333333335,
    "MUSR": 11.422656250000001,
    "MMLU-PRO Raw": 0.21467752659574468,
    "MMLU-PRO": 12.741947399527188,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-29",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "lmsys/vicuna-7b-v1.5"
  },
  {
    "eval_name": "lodrick-the-lafted_llama-3.1-8b-instruct-ortho-v7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lodrick-the-lafted/llama-3.1-8b-instruct-ortho-v7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lodrick-the-lafted/llama-3.1-8b-instruct-ortho-v7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lodrick-the-lafted__llama-3.1-8b-instruct-ortho-v7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lodrick-the-lafted/llama-3.1-8b-instruct-ortho-v7",
    "Model sha": "6b7673cd78398c3a8c92f8e759aaae6409e96978",
    "Average ‚¨ÜÔ∏è": 11.661761952832547,
    "Hub License": "wtfpl",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9315738571879582,
    "IFEval Raw": 0.3514618988727687,
    "IFEval": 35.14618988727687,
    "BBH Raw": 0.39069140261362917,
    "BBH": 14.43786307942362,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.36159375,
    "MUSR": 4.732552083333332,
    "MMLU-PRO Raw": 0.1973902925531915,
    "MMLU-PRO": 10.821143617021276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "lodrick-the-lafted/llama-3.1-8b-instruct-ortho-v7"
  },
  {
    "eval_name": "lordjia_Llama-3-Cantonese-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lordjia/Llama-3-Cantonese-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lordjia/Llama-3-Cantonese-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lordjia__Llama-3-Cantonese-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lordjia/Llama-3-Cantonese-8B-Instruct",
    "Model sha": "ea98e9b1ab3ea0d66e5270816e43d7a70aaaa151",
    "Average ‚¨ÜÔ∏è": 24.259120767899372,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7677029628444899,
    "IFEval Raw": 0.6669259786256023,
    "IFEval": 66.69259786256023,
    "BBH Raw": 0.4814148018954038,
    "BBH": 26.79103884029782,
    "MATH Lvl 5 Raw": 0.08836858006042296,
    "MATH Lvl 5": 8.836858006042297,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.40460416666666665,
    "MUSR": 9.47552083333334,
    "MMLU-PRO Raw": 0.35147938829787234,
    "MMLU-PRO": 27.942154255319146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "lordjia/Llama-3-Cantonese-8B-Instruct"
  },
  {
    "eval_name": "lordjia_Qwen2-Cantonese-7B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lordjia/Qwen2-Cantonese-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lordjia/Qwen2-Cantonese-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lordjia__Qwen2-Cantonese-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lordjia/Qwen2-Cantonese-7B-Instruct",
    "Model sha": "eb8b0faee749d167fd70e74f5e579094c4cfe7fb",
    "Average ‚¨ÜÔ∏è": 23.640515390226003,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.016006795896826,
    "IFEval Raw": 0.5435278394659503,
    "IFEval": 54.35278394659503,
    "BBH Raw": 0.5215311346221223,
    "BBH": 32.45321665791298,
    "MATH Lvl 5 Raw": 0.09592145015105741,
    "MATH Lvl 5": 9.592145015105741,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.40038541666666666,
    "MUSR": 7.814843749999999,
    "MMLU-PRO Raw": 0.38430851063829785,
    "MMLU-PRO": 31.589834515366427,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-13",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "lordjia/Qwen2-Cantonese-7B-Instruct"
  },
  {
    "eval_name": "lt-asset_nova-1.3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "NovaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/lt-asset/nova-1.3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">lt-asset/nova-1.3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/lt-asset__nova-1.3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "lt-asset/nova-1.3b",
    "Model sha": "766eb459b5aa1e084b5474bb86ade09f9bed8fca",
    "Average ‚¨ÜÔ∏è": 3.803298133544272,
    "Hub License": "bsd-3-clause-clear",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2477466571993112,
    "IFEval Raw": 0.1214255951985177,
    "IFEval": 12.14255951985177,
    "BBH Raw": 0.31700122104895806,
    "BBH": 4.437619873492811,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.36978125,
    "MUSR": 3.755989583333333,
    "MMLU-PRO Raw": 0.11419547872340426,
    "MMLU-PRO": 1.5772754137115832,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-20",
    "Submission Date": "2024-11-16",
    "Generation": 0,
    "Base Model": "lt-asset/nova-1.3b"
  },
  {
    "eval_name": "macadeliccc_Samantha-Qwen-2-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/macadeliccc/Samantha-Qwen-2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">macadeliccc/Samantha-Qwen-2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/macadeliccc__Samantha-Qwen-2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "macadeliccc/Samantha-Qwen-2-7B",
    "Model sha": "59058972fa9b56d132d04589eb17cbba277c2826",
    "Average ‚¨ÜÔ∏è": 24.976968946780264,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3398073956474044,
    "IFEval Raw": 0.4377152621710395,
    "IFEval": 43.771526217103954,
    "BBH Raw": 0.5082341412476951,
    "BBH": 31.41189390746123,
    "MATH Lvl 5 Raw": 0.20619335347432025,
    "MATH Lvl 5": 20.619335347432024,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.4799479166666667,
    "MUSR": 20.160156250000004,
    "MMLU-PRO Raw": 0.3779089095744681,
    "MMLU-PRO": 30.87876773049646,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-15",
    "Submission Date": "2024-08-05",
    "Generation": 1,
    "Base Model": "Qwen/Qwen2-7B"
  },
  {
    "eval_name": "macadeliccc_magistrate-3.2-3b-base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/macadeliccc/magistrate-3.2-3b-base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">macadeliccc/magistrate-3.2-3b-base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/macadeliccc__magistrate-3.2-3b-base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "macadeliccc/magistrate-3.2-3b-base",
    "Model sha": "2a40ac9ca1904fca2c1e69573e27f0ff8039b738",
    "Average ‚¨ÜÔ∏è": 5.970568662218972,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7303429628875555,
    "IFEval Raw": 0.1159301763764589,
    "IFEval": 11.59301763764589,
    "BBH Raw": 0.3342701056047533,
    "BBH": 6.910280939116192,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.39759374999999997,
    "MUSR": 7.532552083333333,
    "MMLU-PRO Raw": 0.16888297872340424,
    "MMLU-PRO": 7.65366430260047,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B"
  },
  {
    "eval_name": "macadeliccc_magistrate-3.2-3b-it_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/macadeliccc/magistrate-3.2-3b-it\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">macadeliccc/magistrate-3.2-3b-it</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/macadeliccc__magistrate-3.2-3b-it-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "macadeliccc/magistrate-3.2-3b-it",
    "Model sha": "122961278c97195dd59d67b244907359013e4de5",
    "Average ‚¨ÜÔ∏è": 7.037723786407888,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7029510293033439,
    "IFEval Raw": 0.22918744486850445,
    "IFEval": 22.918744486850443,
    "BBH Raw": 0.3256506790327196,
    "BBH": 5.323155419813335,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.3763229166666667,
    "MUSR": 5.740364583333334,
    "MMLU-PRO Raw": 0.15924202127659576,
    "MMLU-PRO": 6.582446808510639,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-01",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B"
  },
  {
    "eval_name": "maldv_badger-kappa-llama-3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/maldv/badger-kappa-llama-3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">maldv/badger-kappa-llama-3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/maldv__badger-kappa-llama-3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "maldv/badger-kappa-llama-3-8b",
    "Model sha": "aa6863eb816ca6ad29453b8aaf846962c4328998",
    "Average ‚¨ÜÔ∏è": 21.166688498001093,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9591253228649962,
    "IFEval Raw": 0.46946435457918323,
    "IFEval": 46.94643545791833,
    "BBH Raw": 0.5084927997756815,
    "BBH": 30.153238604373765,
    "MATH Lvl 5 Raw": 0.08610271903323262,
    "MATH Lvl 5": 8.610271903323262,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.3765104166666666,
    "MUSR": 4.297135416666666,
    "MMLU-PRO Raw": 0.3695146276595745,
    "MMLU-PRO": 29.94606973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "maldv/badger-kappa-llama-3-8b"
  },
  {
    "eval_name": "maldv_badger-lambda-llama-3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/maldv/badger-lambda-llama-3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">maldv/badger-lambda-llama-3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/maldv__badger-lambda-llama-3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "maldv/badger-lambda-llama-3-8b",
    "Model sha": "8ef157d0d3c12212ca5e70d354869aed90e03f22",
    "Average ‚¨ÜÔ∏è": 20.893497780988604,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1110222176474127,
    "IFEval Raw": 0.4860758343417687,
    "IFEval": 48.607583434176874,
    "BBH Raw": 0.49634866510444836,
    "BBH": 28.103050014353716,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.3753645833333333,
    "MUSR": 4.520572916666667,
    "MMLU-PRO Raw": 0.37666223404255317,
    "MMLU-PRO": 30.740248226950357,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-10",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "maldv/badger-lambda-llama-3-8b"
  },
  {
    "eval_name": "maldv_badger-mu-llama-3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/maldv/badger-mu-llama-3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">maldv/badger-mu-llama-3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/maldv__badger-mu-llama-3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "maldv/badger-mu-llama-3-8b",
    "Model sha": "952a269bb1e6c18ee772c6d088e74d305df4425d",
    "Average ‚¨ÜÔ∏è": 19.793469619296484,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9046353132269994,
    "IFEval Raw": 0.49194581488229006,
    "IFEval": 49.194581488229005,
    "BBH Raw": 0.514287576852281,
    "BBH": 30.51396514214568,
    "MATH Lvl 5 Raw": 0.024169184290030215,
    "MATH Lvl 5": 2.4169184290030215,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.35545833333333327,
    "MUSR": 5.69895833333333,
    "MMLU-PRO Raw": 0.3673537234042553,
    "MMLU-PRO": 29.70596926713948,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-27",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "maldv/badger-mu-llama-3-8b"
  },
  {
    "eval_name": "maldv_badger-writer-llama-3-8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/maldv/badger-writer-llama-3-8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">maldv/badger-writer-llama-3-8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/maldv__badger-writer-llama-3-8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "maldv/badger-writer-llama-3-8b",
    "Model sha": "1d8134d01af87e994571ae16ccd7b31cce42418f",
    "Average ‚¨ÜÔ∏è": 21.046535160595596,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2358105142700981,
    "IFEval Raw": 0.5303140112678804,
    "IFEval": 53.03140112678803,
    "BBH Raw": 0.4863893856673737,
    "BBH": 26.878360614538398,
    "MATH Lvl 5 Raw": 0.07250755287009064,
    "MATH Lvl 5": 7.250755287009064,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.35809375000000004,
    "MUSR": 3.195052083333334,
    "MMLU-PRO Raw": 0.3759973404255319,
    "MMLU-PRO": 30.666371158392437,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-17",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "maldv/badger-writer-llama-3-8b (Merge)"
  },
  {
    "eval_name": "matouLeLoup_ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/matouLeLoup/ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matouLeLoup/ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/matouLeLoup__ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3",
    "Model sha": "60c5853d376d4b62b19dd4c4741224d0246ec5b4",
    "Average ‚¨ÜÔ∏è": 7.287062057653856,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8517719561741942,
    "IFEval Raw": 0.18732186154957736,
    "IFEval": 18.732186154957734,
    "BBH Raw": 0.3239117424825444,
    "BBH": 7.9185120409032566,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.37520833333333337,
    "MUSR": 4.601041666666668,
    "MMLU-PRO Raw": 0.17195811170212766,
    "MMLU-PRO": 7.99534574468085,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-01",
    "Generation": 0,
    "Base Model": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-EnhancedMUSREnsembleV3"
  },
  {
    "eval_name": "matouLeLoup_ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/matouLeLoup/ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matouLeLoup/ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/matouLeLoup__ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis",
    "Model sha": "3fd229bcc3b4d2502ed7f3bdd48ccb5c97e83212",
    "Average ‚¨ÜÔ∏è": 7.287062057653856,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8509404483701427,
    "IFEval Raw": 0.18732186154957736,
    "IFEval": 18.732186154957734,
    "BBH Raw": 0.3239117424825444,
    "BBH": 7.9185120409032566,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.37520833333333337,
    "MUSR": 4.601041666666668,
    "MMLU-PRO Raw": 0.17195811170212766,
    "MMLU-PRO": 7.99534574468085,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-10-31",
    "Generation": 0,
    "Base Model": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-MUSR-ENSEMBLE-V2Mathis"
  },
  {
    "eval_name": "matouLeLoup_ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/matouLeLoup__ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis",
    "Model sha": "455945ed4318bbeae008a253f877f56a68291b8b",
    "Average ‚¨ÜÔ∏è": 7.287062057653856,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8607415912894161,
    "IFEval Raw": 0.18732186154957736,
    "IFEval": 18.732186154957734,
    "BBH Raw": 0.3239117424825444,
    "BBH": 7.9185120409032566,
    "MATH Lvl 5 Raw": 0.030211480362537766,
    "MATH Lvl 5": 3.0211480362537766,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.37520833333333337,
    "MUSR": 4.601041666666668,
    "MMLU-PRO Raw": 0.17195811170212766,
    "MMLU-PRO": 7.99534574468085,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-10-31",
    "Generation": 0,
    "Base Model": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-ENSEMBLE-Mathis"
  },
  {
    "eval_name": "matouLeLoup_ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/matouLeLoup__ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis",
    "Model sha": "dd86c3d7f77748a0ba18d911ceb93358a69ce160",
    "Average ‚¨ÜÔ∏è": 7.257344997193762,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9037178751918784,
    "IFEval Raw": 0.18824607596732226,
    "IFEval": 18.824607596732225,
    "BBH Raw": 0.32327887380902803,
    "BBH": 8.079577103291848,
    "MATH Lvl 5 Raw": 0.027190332326283987,
    "MATH Lvl 5": 2.719033232628399,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.3684791666666667,
    "MUSR": 4.1265625,
    "MMLU-PRO Raw": 0.17204122340425532,
    "MMLU-PRO": 8.00458037825059,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-31",
    "Generation": 0,
    "Base Model": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V4-MUSR-Mathis"
  },
  {
    "eval_name": "matouLeLoup_ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/matouLeLoup/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">matouLeLoup/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/matouLeLoup__ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis",
    "Model sha": "7a9d848188a674302d64a865786d4508be19571a",
    "Average ‚¨ÜÔ∏è": 5.812746004584803,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0989509784510771,
    "IFEval Raw": 0.16521496296493304,
    "IFEval": 16.521496296493304,
    "BBH Raw": 0.30237295164613204,
    "BBH": 3.0833518776266557,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.42730208333333336,
    "MUSR": 12.179427083333335,
    "MMLU-PRO Raw": 0.1116190159574468,
    "MMLU-PRO": 1.2910017730496441,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "matouLeLoup/ECE-PRYMMAL-0.5B-FT-V5-MUSR-Mathis"
  },
  {
    "eval_name": "mattshumer_Reflection-Llama-3.1-70B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mattshumer/Reflection-Llama-3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mattshumer/Reflection-Llama-3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mattshumer__Reflection-Llama-3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mattshumer/Reflection-Llama-3.1-70B",
    "Model sha": "458962ed801fac4eadd01a91a2029a3a82f4cd84",
    "Average ‚¨ÜÔ∏è": 26.561461535316194,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1705,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 28.65084585457039,
    "IFEval Raw": 0.6562598350155366,
    "IFEval": 65.62598350155366,
    "BBH Raw": 0.5998839958220125,
    "BBH": 42.38944488022307,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.4126666666666667,
    "MUSR": 10.016666666666666,
    "MMLU-PRO Raw": 0.45894281914893614,
    "MMLU-PRO": 39.882535460992905,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-05",
    "Submission Date": "2024-09-09",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "mattshumer_ref_70_e3_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mattshumer/ref_70_e3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mattshumer/ref_70_e3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mattshumer__ref_70_e3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mattshumer/ref_70_e3",
    "Model sha": "5d2d9dbb9e0bf61879255f63f1b787296fe524cc",
    "Average ‚¨ÜÔ∏è": 30.737996436280383,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 56,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 39.90530864379392,
    "IFEval Raw": 0.6294321289733462,
    "IFEval": 62.943212897334625,
    "BBH Raw": 0.6500839481104265,
    "BBH": 49.27446660003019,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.4327604166666667,
    "MUSR": 12.995052083333334,
    "MMLU-PRO Raw": 0.5302526595744681,
    "MMLU-PRO": 47.80585106382979,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-08",
    "Submission Date": "2024-09-08",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "maywell_Qwen2-7B-Multilingual-RP_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/maywell/Qwen2-7B-Multilingual-RP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">maywell/Qwen2-7B-Multilingual-RP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/maywell__Qwen2-7B-Multilingual-RP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "maywell/Qwen2-7B-Multilingual-RP",
    "Model sha": "487e8f0498419e4d1188f661dbb63bd629be4638",
    "Average ‚¨ÜÔ∏è": 23.463466810209308,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 42,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9594128357573145,
    "IFEval Raw": 0.4347176602525743,
    "IFEval": 43.47176602525742,
    "BBH Raw": 0.5062058680861069,
    "BBH": 30.54356147647468,
    "MATH Lvl 5 Raw": 0.22507552870090636,
    "MATH Lvl 5": 22.507552870090635,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.3695625,
    "MUSR": 6.228645833333334,
    "MMLU-PRO Raw": 0.3858876329787234,
    "MMLU-PRO": 31.765292553191493,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-24",
    "Submission Date": "2024-09-05",
    "Generation": 0,
    "Base Model": "maywell/Qwen2-7B-Multilingual-RP"
  },
  {
    "eval_name": "meditsolutions_Llama-3.1-MedIT-SUN-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.1-MedIT-SUN-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.1-MedIT-SUN-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.1-MedIT-SUN-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.1-MedIT-SUN-8B",
    "Model sha": "0c11abbaa40e76b538b8c0f9c50e965078999087",
    "Average ‚¨ÜÔ∏è": 30.043102315944463,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7130500345654077,
    "IFEval Raw": 0.7837293935646308,
    "IFEval": 78.37293935646308,
    "BBH Raw": 0.5186924904597405,
    "BBH": 32.001650567550215,
    "MATH Lvl 5 Raw": 0.2001510574018127,
    "MATH Lvl 5": 20.01510574018127,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.40562499999999996,
    "MUSR": 9.636458333333328,
    "MMLU-PRO Raw": 0.3916223404255319,
    "MMLU-PRO": 32.402482269503544,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.1-MedIT-SUN-8B (Merge)"
  },
  {
    "eval_name": "meditsolutions_Llama-3.2-SUN-1B-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.2-SUN-1B-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.2-SUN-1B-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.2-SUN-1B-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.2-SUN-1B-chat",
    "Model sha": "a67791cfc31d09c3e96bd8c62a386f6107378087",
    "Average ‚¨ÜÔ∏è": 13.377015111087497,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7278507003209941,
    "IFEval Raw": 0.5481743994822625,
    "IFEval": 54.81743994822626,
    "BBH Raw": 0.35144575516411386,
    "BBH": 8.690237956315015,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3249166666666667,
    "MUSR": 1.0479166666666664,
    "MMLU-PRO Raw": 0.18375997340425532,
    "MMLU-PRO": 9.306663711583923,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-03",
    "Submission Date": "2024-11-07",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.2-SUN-1B-chat (Merge)"
  },
  {
    "eval_name": "meditsolutions_Llama-3.2-SUN-2.4B-checkpoint-26000_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-26000\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-26000</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.2-SUN-2.4B-checkpoint-26000-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-26000",
    "Model sha": "1300885555ca8bbed20a57cf0ec9f7ae014200c3",
    "Average ‚¨ÜÔ∏è": 7.954663212324287,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8131702633730724,
    "IFEval Raw": 0.28139447776344545,
    "IFEval": 28.139447776344547,
    "BBH Raw": 0.3017752699243885,
    "BBH": 2.8953053502640427,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.41033333333333327,
    "MUSR": 8.491666666666665,
    "MMLU-PRO Raw": 0.1344747340425532,
    "MMLU-PRO": 3.830526004728132,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-10-04",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-26000 (Merge)"
  },
  {
    "eval_name": "meditsolutions_Llama-3.2-SUN-2.4B-checkpoint-34800_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-34800\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-34800</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.2-SUN-2.4B-checkpoint-34800-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-34800",
    "Model sha": "ef65f05f577a69a1992349c8d33c96cd099844f7",
    "Average ‚¨ÜÔ∏è": 8.042045147015047,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8155329966330017,
    "IFEval Raw": 0.25009530268576263,
    "IFEval": 25.009530268576263,
    "BBH Raw": 0.3161124673749052,
    "BBH": 5.466179719646344,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4022395833333334,
    "MUSR": 8.846614583333333,
    "MMLU-PRO Raw": 0.13572140957446807,
    "MMLU-PRO": 3.9690455082742293,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.2-SUN-2.4B-checkpoint-34800 (Merge)"
  },
  {
    "eval_name": "meditsolutions_Llama-3.2-SUN-2.4B-v1.0.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.2-SUN-2.4B-v1.0.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.2-SUN-2.4B-v1.0.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.2-SUN-2.4B-v1.0.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.2-SUN-2.4B-v1.0.0",
    "Model sha": "b8a31c62ab4acbd4c645fd882d899c4ec7280677",
    "Average ‚¨ÜÔ∏è": 12.97725036927792,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.271012081291912,
    "IFEval Raw": 0.5636865738462834,
    "IFEval": 56.36865738462834,
    "BBH Raw": 0.3390826682107771,
    "BBH": 7.211667793228333,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.32094791666666667,
    "MUSR": 3.0184895833333347,
    "MMLU-PRO Raw": 0.15425531914893617,
    "MMLU-PRO": 6.02836879432624,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.2-SUN-2.4B-v1.0.0 (Merge)"
  },
  {
    "eval_name": "meditsolutions_Llama-3.2-SUN-2.5B-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/Llama-3.2-SUN-2.5B-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/Llama-3.2-SUN-2.5B-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__Llama-3.2-SUN-2.5B-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/Llama-3.2-SUN-2.5B-chat",
    "Model sha": "2bd68a18c0f7984f430acbc2efad76344177aba0",
    "Average ‚¨ÜÔ∏è": 13.64783098911302,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1944434204951326,
    "IFEval Raw": 0.560414145578177,
    "IFEval": 56.0414145578177,
    "BBH Raw": 0.3574734302161124,
    "BBH": 9.40909318881213,
    "MATH Lvl 5 Raw": 0.05060422960725076,
    "MATH Lvl 5": 5.0604229607250755,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3155208333333333,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.1813497340425532,
    "MMLU-PRO": 9.038859338061465,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-27",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "meditsolutions/Llama-3.2-SUN-2.5B-chat (Merge)"
  },
  {
    "eval_name": "meditsolutions_MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune",
    "Model sha": "a0ffd0cd00cab2245c1f0edcef4d1d8ead4c6d6e",
    "Average ‚¨ÜÔ∏è": 14.377117270136123,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0590999906910972,
    "IFEval Raw": 0.36550020611976225,
    "IFEval": 36.55002061197622,
    "BBH Raw": 0.4034845834509661,
    "BBH": 16.138425927069104,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.42534374999999996,
    "MUSR": 11.56796875,
    "MMLU-PRO Raw": 0.21899933510638298,
    "MMLU-PRO": 13.222148345153665,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-13",
    "Generation": 1,
    "Base Model": "meditsolutions/MSH-Lite-7B-v1-Bielik-v2.3-Instruct-Llama-Prune (Merge)"
  },
  {
    "eval_name": "meditsolutions_MSH-v1-Bielik-v2.3-Instruct-MedIT-merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/MSH-v1-Bielik-v2.3-Instruct-MedIT-merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/MSH-v1-Bielik-v2.3-Instruct-MedIT-merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__MSH-v1-Bielik-v2.3-Instruct-MedIT-merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/MSH-v1-Bielik-v2.3-Instruct-MedIT-merge",
    "Model sha": "2db5e8871fb3be7e658e3bc6e2885d26b891b8b8",
    "Average ‚¨ÜÔ∏è": 27.38001107114184,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8427408287680233,
    "IFEval Raw": 0.5814217387642566,
    "IFEval": 58.14217387642566,
    "BBH Raw": 0.5671722290858499,
    "BBH": 38.0234352820579,
    "MATH Lvl 5 Raw": 0.13746223564954682,
    "MATH Lvl 5": 13.746223564954683,
    "GPQA Raw": 0.34563758389261745,
    "GPQA": 12.751677852348994,
    "MUSR Raw": 0.43845833333333334,
    "MUSR": 13.840624999999996,
    "MMLU-PRO Raw": 0.3499833776595745,
    "MMLU-PRO": 27.77593085106383,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "meditsolutions/MSH-v1-Bielik-v2.3-Instruct-MedIT-merge (Merge)"
  },
  {
    "eval_name": "meditsolutions_MedIT-Mesh-3B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meditsolutions/MedIT-Mesh-3B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meditsolutions/MedIT-Mesh-3B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meditsolutions__MedIT-Mesh-3B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meditsolutions/MedIT-Mesh-3B-Instruct",
    "Model sha": "469d1a58f7747c3d456b3308b5a7042df4ab49e3",
    "Average ‚¨ÜÔ∏è": 27.562940703396578,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5304863487951661,
    "IFEval Raw": 0.5814217387642566,
    "IFEval": 58.14217387642566,
    "BBH Raw": 0.5575523356865378,
    "BBH": 37.54705419374355,
    "MATH Lvl 5 Raw": 0.1578549848942598,
    "MATH Lvl 5": 15.785498489425981,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4047604166666667,
    "MUSR": 10.595052083333334,
    "MMLU-PRO Raw": 0.4011801861702128,
    "MMLU-PRO": 33.46446513002365,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-01",
    "Submission Date": "2024-11-01",
    "Generation": 1,
    "Base Model": "meditsolutions/MedIT-Mesh-3B-Instruct (Merge)"
  },
  {
    "eval_name": "meetkai_functionary-small-v3.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meetkai/functionary-small-v3.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meetkai/functionary-small-v3.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meetkai__functionary-small-v3.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meetkai/functionary-small-v3.1",
    "Model sha": "8e43bc1d2e259b91799e704c410a95b8ca458121",
    "Average ‚¨ÜÔ∏è": 21.641240859603382,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7043696992829962,
    "IFEval Raw": 0.6274584768414474,
    "IFEval": 62.74584768414474,
    "BBH Raw": 0.4981781042779377,
    "BBH": 28.616314665836143,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.3833645833333333,
    "MUSR": 6.1872395833333345,
    "MMLU-PRO Raw": 0.33485704787234044,
    "MMLU-PRO": 26.09522754137116,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-26",
    "Submission Date": "2024-11-10",
    "Generation": 0,
    "Base Model": "meetkai/functionary-small-v3.1"
  },
  {
    "eval_name": "meraGPT_mera-mix-4x7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meraGPT/mera-mix-4x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meraGPT/mera-mix-4x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meraGPT__mera-mix-4x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meraGPT/mera-mix-4x7B",
    "Model sha": "09d965c5ef9b66ce419986027e03a915cb869e43",
    "Average ‚¨ÜÔ∏è": 17.854958732939675,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6644006630879888,
    "IFEval Raw": 0.4831779677921249,
    "IFEval": 48.317796779212486,
    "BBH Raw": 0.40189899163661713,
    "BBH": 17.48643895465503,
    "MATH Lvl 5 Raw": 0.053625377643504536,
    "MATH Lvl 5": 5.362537764350454,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.40565625,
    "MUSR": 9.273697916666668,
    "MMLU-PRO Raw": 0.27476728723404253,
    "MMLU-PRO": 19.418587470449168,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-13",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "meraGPT/mera-mix-4x7B"
  },
  {
    "eval_name": "meta-llama_Llama-2-13b-chat-hf_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-chat-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-chat-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-13b-chat-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-13b-chat-hf",
    "Model sha": "a2cb7a712bb6e5e736ca7f8cd98167f81a0b5bd8",
    "Average ‚¨ÜÔ∏è": 11.016342275210453,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 1029,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8745695082151663,
    "IFEval Raw": 0.398472719052115,
    "IFEval": 39.8472719052115,
    "BBH Raw": 0.33427367066714186,
    "BBH": 7.155379968626988,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.23154362416107382,
    "GPQA": 0.0,
    "MUSR Raw": 0.40072916666666664,
    "MUSR": 8.1578125,
    "MMLU-PRO Raw": 0.19232047872340424,
    "MMLU-PRO": 10.257830969267138,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-13b-chat-hf"
  },
  {
    "eval_name": "meta-llama_Llama-2-13b-hf_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-13b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-13b-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-13b-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-13b-hf",
    "Model sha": "5c31dfb671ce7cfe2d7bb7c04375e44c55e815b1",
    "Average ‚¨ÜÔ∏è": 11.014833514003099,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 572,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1123801563079765,
    "IFEval Raw": 0.24824687385027283,
    "IFEval": 24.824687385027282,
    "BBH Raw": 0.41256242233835055,
    "BBH": 17.222559825058127,
    "MATH Lvl 5 Raw": 0.012084592145015107,
    "MATH Lvl 5": 1.2084592145015107,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.35375,
    "MUSR": 3.385416666666666,
    "MMLU-PRO Raw": 0.23778257978723405,
    "MMLU-PRO": 15.309175531914892,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-13b-hf"
  },
  {
    "eval_name": "meta-llama_Llama-2-70b-chat-hf_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-chat-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-chat-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-70b-chat-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-70b-chat-hf",
    "Model sha": "e9149a12809580e8602995856f8098ce973d1080",
    "Average ‚¨ÜÔ∏è": 12.74640473856668,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 2160,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 22.89845505526058,
    "IFEval Raw": 0.49579227560650185,
    "IFEval": 49.57922756065019,
    "BBH Raw": 0.30424741461642657,
    "BBH": 4.613767082590614,
    "MATH Lvl 5 Raw": 0.009818731117824775,
    "MATH Lvl 5": 0.9818731117824775,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3686666666666667,
    "MUSR": 3.4833333333333356,
    "MMLU-PRO Raw": 0.2432679521276596,
    "MMLU-PRO": 15.918661347517732,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-14",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-70b-chat-hf"
  },
  {
    "eval_name": "meta-llama_Llama-2-70b-hf_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-70b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-70b-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-70b-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-70b-hf",
    "Model sha": "3aba440b59558f995867ba6e1f58f21d0336b5bb",
    "Average ‚¨ÜÔ∏è": 18.309658021614386,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 831,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 29.621246617151854,
    "IFEval Raw": 0.2406780675274937,
    "IFEval": 24.06780675274937,
    "BBH Raw": 0.5472591190449342,
    "BBH": 35.900061863721675,
    "MATH Lvl 5 Raw": 0.028700906344410877,
    "MATH Lvl 5": 2.8700906344410875,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.41235416666666663,
    "MUSR": 9.777604166666668,
    "MMLU-PRO Raw": 0.37175864361702127,
    "MMLU-PRO": 30.1954048463357,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-70b-hf"
  },
  {
    "eval_name": "meta-llama_Llama-2-7b-chat-hf_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-chat-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-7b-chat-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-7b-chat-hf",
    "Model sha": "f5db02db724555f92da89c216ac04704f23d4590",
    "Average ‚¨ÜÔ∏è": 9.395485278250947,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 3999,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.156879036155158,
    "IFEval Raw": 0.3986478100329348,
    "IFEval": 39.86478100329349,
    "BBH Raw": 0.3113546355002185,
    "BBH": 4.459171645959485,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.3675520833333333,
    "MUSR": 3.27734375,
    "MMLU-PRO Raw": 0.16879986702127658,
    "MMLU-PRO": 7.644429669030731,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-13",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-7b-chat-hf"
  },
  {
    "eval_name": "meta-llama_Llama-2-7b-hf_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-2-7b-hf\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-2-7b-hf</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-2-7b-hf-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-2-7b-hf",
    "Model sha": "01c7f73d771dfac7d292323805ebc428287df4f9",
    "Average ‚¨ÜÔ∏è": 8.730828895633673,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 1795,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5630945627720637,
    "IFEval Raw": 0.2518938638368418,
    "IFEval": 25.18938638368418,
    "BBH Raw": 0.34961958199821835,
    "BBH": 10.35141665784897,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.37006249999999996,
    "MUSR": 3.7578125,
    "MMLU-PRO Raw": 0.18608710106382978,
    "MMLU-PRO": 9.56523345153664,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-13",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-2-7b-hf"
  },
  {
    "eval_name": "meta-llama_Llama-3.2-1B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-3.2-1B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-3.2-1B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-3.2-1B",
    "Model sha": "a7c18587d7f473bfea02aa5639aa349403307b54",
    "Average ‚¨ÜÔ∏è": 4.031494495415088,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 973,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.41912851784102,
    "IFEval Raw": 0.14777900415342402,
    "IFEval": 14.777900415342401,
    "BBH Raw": 0.31149540964608097,
    "BBH": 4.366029656556756,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.22818791946308725,
    "GPQA": 0.0,
    "MUSR Raw": 0.3447291666666667,
    "MUSR": 2.5578125000000003,
    "MMLU-PRO Raw": 0.12034574468085106,
    "MMLU-PRO": 2.2606382978723394,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-3.2-1B"
  },
  {
    "eval_name": "meta-llama_Llama-3.2-1B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-3.2-1B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-3.2-1B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-3.2-1B-Instruct",
    "Model sha": "d0a2081ed47e20ce524e8bc5d132f3fad2f69ff0",
    "Average ‚¨ÜÔ∏è": 13.813720492824933,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 561,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4049045249076224,
    "IFEval Raw": 0.5698313807364459,
    "IFEval": 56.9831380736446,
    "BBH Raw": 0.34968498061768266,
    "BBH": 8.742521312303046,
    "MATH Lvl 5 Raw": 0.032477341389728104,
    "MATH Lvl 5": 3.2477341389728105,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.3328541666666667,
    "MUSR": 2.973437500000001,
    "MMLU-PRO Raw": 0.16821808510638298,
    "MMLU-PRO": 7.579787234042552,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-3.2-1B-Instruct"
  },
  {
    "eval_name": "meta-llama_Llama-3.2-3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-3.2-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-3.2-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-3.2-3B",
    "Model sha": "95c102307f55fbd6d18ddf28bfbcb537ffdc2806",
    "Average ‚¨ÜÔ∏è": 8.584529665203304,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 330,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3318126489949191,
    "IFEval Raw": 0.13374069690643048,
    "IFEval": 13.374069690643047,
    "BBH Raw": 0.3905117116991059,
    "BBH": 14.232664884364107,
    "MATH Lvl 5 Raw": 0.012084592145015107,
    "MATH Lvl 5": 1.2084592145015107,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.35771875000000003,
    "MUSR": 3.8148437499999996,
    "MMLU-PRO Raw": 0.2487533244680851,
    "MMLU-PRO": 16.528147163120565,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-3.2-3B"
  },
  {
    "eval_name": "meta-llama_Llama-3.2-3B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Llama-3.2-3B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Llama-3.2-3B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Llama-3.2-3B-Instruct",
    "Model sha": "276b29ce8303c9b88966a9b32fc75692dce4d8e1",
    "Average ‚¨ÜÔ∏è": 24.116533990069385,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 646,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2712035198661342,
    "IFEval Raw": 0.7393161256576994,
    "IFEval": 73.93161256576994,
    "BBH Raw": 0.4610070239466069,
    "BBH": 24.059186446885473,
    "MATH Lvl 5 Raw": 0.1714501510574018,
    "MATH Lvl 5": 17.14501510574018,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.3528541666666667,
    "MUSR": 1.3734374999999996,
    "MMLU-PRO Raw": 0.3194813829787234,
    "MMLU-PRO": 24.386820330969268,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3-70B",
    "Model sha": "b4d08b7db49d488da3ac49adf25a6b9ac01ae338",
    "Average ‚¨ÜÔ∏è": 26.66758582116017,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 827,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 23.4071857793614,
    "IFEval Raw": 0.1603190645265673,
    "IFEval": 16.031906452656727,
    "BBH Raw": 0.6461074599904467,
    "BBH": 48.709812647505885,
    "MATH Lvl 5 Raw": 0.18353474320241692,
    "MATH Lvl 5": 18.35347432024169,
    "GPQA Raw": 0.3976510067114094,
    "GPQA": 19.686800894854585,
    "MUSR Raw": 0.4518229166666667,
    "MUSR": 16.011197916666664,
    "MMLU-PRO Raw": 0.4709109042553192,
    "MMLU-PRO": 41.21232269503546,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3-70B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3-70B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3-70B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3-70B-Instruct",
    "Model sha": "7129260dd854a80eb10ace5f61c20324b472b31c",
    "Average ‚¨ÜÔ∏è": 36.51069341426509,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1435,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 18.239150117739147,
    "IFEval Raw": 0.8099077115387172,
    "IFEval": 80.99077115387172,
    "BBH Raw": 0.6546699432372051,
    "BBH": 50.18513318440344,
    "MATH Lvl 5 Raw": 0.2530211480362538,
    "MATH Lvl 5": 25.30211480362538,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4153645833333333,
    "MUSR": 10.92057291666667,
    "MMLU-PRO Raw": 0.5206948138297872,
    "MMLU-PRO": 46.74386820330969,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-70B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3-8B",
    "Model sha": "62bd457b6fe961a42a631306577e622c83876cb6",
    "Average ‚¨ÜÔ∏è": 13.463211553055663,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5837,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8725684976108097,
    "IFEval Raw": 0.14550614591506092,
    "IFEval": 14.550614591506093,
    "BBH Raw": 0.4597905195240255,
    "BBH": 24.50076379676797,
    "MATH Lvl 5 Raw": 0.03549848942598188,
    "MATH Lvl 5": 3.5498489425981883,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.36140625,
    "MUSR": 6.242447916666666,
    "MMLU-PRO Raw": 0.32097739361702127,
    "MMLU-PRO": 24.553043735224584,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3-8B-Instruct",
    "Model sha": "e1945c40cd546c78e41f1151f4db032b271faeaa",
    "Average ‚¨ÜÔ∏è": 23.908735693936837,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3628,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7974996778909468,
    "IFEval Raw": 0.7408398604591373,
    "IFEval": 74.08398604591373,
    "BBH Raw": 0.49887111136169526,
    "BBH": 28.244949576343615,
    "MATH Lvl 5 Raw": 0.08685800604229607,
    "MATH Lvl 5": 8.685800604229607,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3568229166666667,
    "MUSR": 1.602864583333335,
    "MMLU-PRO Raw": 0.3664394946808511,
    "MMLU-PRO": 29.604388297872337,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3-8B-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3-8B-Instruct",
    "Model sha": "e1945c40cd546c78e41f1151f4db032b271faeaa",
    "Average ‚¨ÜÔ∏è": 20.59657132920815,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 3628,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9494732571884625,
    "IFEval Raw": 0.47823220166934843,
    "IFEval": 47.823220166934846,
    "BBH Raw": 0.4910264175128683,
    "BBH": 26.795283502573653,
    "MATH Lvl 5 Raw": 0.09063444108761332,
    "MATH Lvl 5": 9.063444108761331,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3805416666666666,
    "MUSR": 5.401041666666668,
    "MMLU-PRO Raw": 0.359125664893617,
    "MMLU-PRO": 28.791740543735223,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-17",
    "Submission Date": "2024-07-08",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3.1-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3.1-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3.1-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3.1-70B",
    "Model sha": "f7d3cc45ed4ff669a354baf2e0f05e65799a0bee",
    "Average ‚¨ÜÔ∏è": 26.200215843375947,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 312,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.601852032718597,
    "IFEval Raw": 0.16843752354862876,
    "IFEval": 16.843752354862875,
    "BBH Raw": 0.626006918317161,
    "BBH": 46.39941295581887,
    "MATH Lvl 5 Raw": 0.18429003021148038,
    "MATH Lvl 5": 18.429003021148038,
    "GPQA Raw": 0.3875838926174497,
    "GPQA": 18.34451901565996,
    "MUSR Raw": 0.4571875,
    "MUSR": 16.581770833333337,
    "MMLU-PRO Raw": 0.4654255319148936,
    "MMLU-PRO": 40.602836879432616,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-23",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3.1-70B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3.1-70B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-70B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3.1-70B-Instruct",
    "Model sha": "b9461463b511ed3c0762467538ea32cf7c9669f2",
    "Average ‚¨ÜÔ∏è": 42.17631279750882,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 691,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 26.802015716870088,
    "IFEval Raw": 0.8668854195756149,
    "IFEval": 86.6885419575615,
    "BBH Raw": 0.6917287453663654,
    "BBH": 55.92799173898473,
    "MATH Lvl 5 Raw": 0.3066465256797583,
    "MATH Lvl 5": 30.664652567975832,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.45806250000000004,
    "MUSR": 17.691145833333334,
    "MMLU-PRO Raw": 0.5309175531914894,
    "MMLU-PRO": 47.87972813238771,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-08-15",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3.1-8B",
    "Model sha": "e5c39e551424c763dbc3e58e32ef2999d33a6d8d",
    "Average ‚¨ÜÔ∏è": 13.869066261279146,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1096,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.5985230038683027,
    "IFEval Raw": 0.1269963696325749,
    "IFEval": 12.69963696325749,
    "BBH Raw": 0.4666136555003996,
    "BBH": 25.294779851087174,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.38252083333333337,
    "MUSR": 8.981770833333334,
    "MMLU-PRO Raw": 0.32455119680851063,
    "MMLU-PRO": 24.950132978723403,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-07-23",
    "Generation": 0,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "meta-llama_Meta-Llama-3.1-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">meta-llama/Meta-Llama-3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "meta-llama/Meta-Llama-3.1-8B-Instruct",
    "Model sha": "df34336b42332c6d360959e259cd6271c6a09fd4",
    "Average ‚¨ÜÔ∏è": 28.204458498101914,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3087,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.48701186834409,
    "IFEval Raw": 0.7855778224001206,
    "IFEval": 78.55778224001206,
    "BBH Raw": 0.5073267838961463,
    "BBH": 29.89275635245275,
    "MATH Lvl 5 Raw": 0.1933534743202417,
    "MATH Lvl 5": 19.335347432024168,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3869895833333333,
    "MUSR": 8.407031249999996,
    "MMLU-PRO Raw": 0.3761635638297872,
    "MMLU-PRO": 30.684840425531917,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-18",
    "Submission Date": "2024-08-15",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "microsoft_DialoGPT-medium_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/DialoGPT-medium\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/DialoGPT-medium</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__DialoGPT-medium-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/DialoGPT-medium",
    "Model sha": "7b40bb0f92c45fefa957d088000d8648e5c7fa33",
    "Average ‚¨ÜÔ∏è": 5.251433606790305,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 327,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.1294644851002661,
    "IFEval Raw": 0.14790422744983311,
    "IFEval": 14.79042274498331,
    "BBH Raw": 0.3014156380141994,
    "BBH": 2.5568557723352243,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.4286666666666667,
    "MUSR": 12.283333333333333,
    "MMLU-PRO Raw": 0.1118683510638298,
    "MMLU-PRO": 1.3187056737588652,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "microsoft/DialoGPT-medium"
  },
  {
    "eval_name": "microsoft_Orca-2-13b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Orca-2-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Orca-2-13b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Orca-2-13b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Orca-2-13b",
    "Model sha": "2539ff53e6baa4cc603774ad5a2d646f4041ea4e",
    "Average ‚¨ÜÔ∏è": 18.14940382091093,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 664,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0085816995954922,
    "IFEval Raw": 0.3127933882099496,
    "IFEval": 31.27933882099496,
    "BBH Raw": 0.48844897288396094,
    "BBH": 27.308019499942578,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.5129687500000001,
    "MUSR": 25.787760416666668,
    "MMLU-PRO Raw": 0.27493351063829785,
    "MMLU-PRO": 19.437056737588648,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-14",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "microsoft/Orca-2-13b"
  },
  {
    "eval_name": "microsoft_Orca-2-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Orca-2-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Orca-2-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Orca-2-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Orca-2-7b",
    "Model sha": "60e31e6bdcf582ad103b807cb74b73ee1d2c4b17",
    "Average ‚¨ÜÔ∏è": 14.216008329134612,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 216,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2093119327499386,
    "IFEval Raw": 0.2183462102776189,
    "IFEval": 21.83462102776189,
    "BBH Raw": 0.4452132267545943,
    "BBH": 22.429468402818458,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.5026145833333333,
    "MUSR": 24.093489583333326,
    "MMLU-PRO Raw": 0.23188164893617022,
    "MMLU-PRO": 14.653516548463358,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-14",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "microsoft/Orca-2-7b"
  },
  {
    "eval_name": "microsoft_Phi-3-medium-128k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-medium-128k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-medium-128k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-medium-128k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-medium-128k-instruct",
    "Model sha": "fa7d2aa4f5ea69b2e36b20d050cdae79c9bfbb3f",
    "Average ‚¨ÜÔ∏è": 31.711653255665578,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 371,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9475587445261677,
    "IFEval Raw": 0.6040029344361849,
    "IFEval": 60.400293443618494,
    "BBH Raw": 0.6382322530870549,
    "BBH": 48.46045127399018,
    "MATH Lvl 5 Raw": 0.1729607250755287,
    "MATH Lvl 5": 17.29607250755287,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.4129479166666667,
    "MUSR": 11.351822916666663,
    "MMLU-PRO Raw": 0.47116023936170215,
    "MMLU-PRO": 41.24002659574468,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-08-21",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-medium-128k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-medium-4k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-medium-4k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-medium-4k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-medium-4k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-medium-4k-instruct",
    "Model sha": "d194e4e74ffad5a5e193e26af25bcfc80c7f1ffc",
    "Average ‚¨ÜÔ∏è": 32.89624957029284,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 212,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4552625233293661,
    "IFEval Raw": 0.6422713954529538,
    "IFEval": 64.22713954529537,
    "BBH Raw": 0.6412464890555547,
    "BBH": 49.38061007422016,
    "MATH Lvl 5 Raw": 0.18353474320241692,
    "MATH Lvl 5": 18.35347432024169,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.42575,
    "MUSR": 13.052083333333334,
    "MMLU-PRO Raw": 0.4675864361702128,
    "MMLU-PRO": 40.84293735224587,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-medium-4k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-mini-128k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-mini-128k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-mini-128k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-mini-128k-instruct",
    "Model sha": "5be6479b4bc06a081e8f4c6ece294241ccd32dec",
    "Average ‚¨ÜÔ∏è": 25.62628727325536,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1605,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 24.22225159268882,
    "IFEval Raw": 0.5976331688807919,
    "IFEval": 59.76331688807919,
    "BBH Raw": 0.5574531792679852,
    "BBH": 37.09976663224031,
    "MATH Lvl 5 Raw": 0.0974320241691843,
    "MATH Lvl 5": 9.74320241691843,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.3936875,
    "MUSR": 7.710937500000003,
    "MMLU-PRO Raw": 0.3734208776595745,
    "MMLU-PRO": 30.38009751773049,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-22",
    "Submission Date": "2024-08-21",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-mini-128k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-mini-4k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-mini-4k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-mini-4k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-mini-4k-instruct",
    "Model sha": "ff07dc01615f8113924aed013115ab2abd32115b",
    "Average ‚¨ÜÔ∏è": 25.967732638041607,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1078,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8040748299123722,
    "IFEval Raw": 0.5612884923115112,
    "IFEval": 56.12884923115112,
    "BBH Raw": 0.5675972626334875,
    "BBH": 39.2693352377728,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.3196308724832215,
    "GPQA": 9.284116331096197,
    "MUSR Raw": 0.3950208333333333,
    "MUSR": 7.644270833333336,
    "MMLU-PRO Raw": 0.38663563829787234,
    "MMLU-PRO": 31.848404255319146,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-mini-4k-instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-mini-4k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-mini-4k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-mini-4k-instruct",
    "Model sha": "c1358f8a35e6d2af81890deffbbfa575b978c62f",
    "Average ‚¨ÜÔ∏è": 27.411116641779596,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1078,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7866992654543408,
    "IFEval Raw": 0.547674614467391,
    "IFEval": 54.76746144673909,
    "BBH Raw": 0.5490718919495822,
    "BBH": 36.55985530518785,
    "MATH Lvl 5 Raw": 0.15483383685800603,
    "MATH Lvl 5": 15.483383685800604,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.42841666666666667,
    "MUSR": 13.118749999999997,
    "MMLU-PRO Raw": 0.4021775265957447,
    "MMLU-PRO": 33.57528073286053,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-22",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-small-128k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3SmallForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-small-128k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-small-128k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-small-128k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-small-128k-instruct",
    "Model sha": "f80aaa30bfc64c2b8ab214b541d9050e97163bc4",
    "Average ‚¨ÜÔ∏è": 28.590991569522757,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 171,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.508467906194247,
    "IFEval Raw": 0.6368258443153056,
    "IFEval": 63.68258443153056,
    "BBH Raw": 0.6202176778696983,
    "BBH": 45.63406964144793,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.43784375000000003,
    "MUSR": 14.49713541666666,
    "MMLU-PRO Raw": 0.4490525265957447,
    "MMLU-PRO": 38.783614066193856,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-small-128k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3-small-8k-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3SmallForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3-small-8k-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3-small-8k-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3-small-8k-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3-small-8k-instruct",
    "Model sha": "1535ae26fb4faada95c6950e8bc6e867cdad6b00",
    "Average ‚¨ÜÔ∏è": 29.670921852105653,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 158,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.025453603521799,
    "IFEval Raw": 0.6496651107949131,
    "IFEval": 64.96651107949131,
    "BBH Raw": 0.6208364880870563,
    "BBH": 46.205570366389075,
    "MATH Lvl 5 Raw": 0.028430143077010778,
    "MATH Lvl 5": 2.8430143077010777,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.45579166666666665,
    "MUSR": 16.77395833333333,
    "MMLU-PRO Raw": 0.4506316489361702,
    "MMLU-PRO": 38.95907210401891,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3-small-8k-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3.5-MoE-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3.5-MoE-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3.5-MoE-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3.5-MoE-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3.5-MoE-instruct",
    "Model sha": "482a9ba0eb0e1fa1671e3560e009d7cec2e5147c",
    "Average ‚¨ÜÔ∏è": 35.45650752160648,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 519,
    "#Params (B)": 42,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.632278613944312,
    "IFEval Raw": 0.692454908531585,
    "IFEval": 69.2454908531585,
    "BBH Raw": 0.640762564622586,
    "BBH": 48.77464635932187,
    "MATH Lvl 5 Raw": 0.22658610271903323,
    "MATH Lvl 5": 22.658610271903324,
    "GPQA Raw": 0.35570469798657717,
    "GPQA": 14.093959731543624,
    "MUSR Raw": 0.4564791666666667,
    "MUSR": 17.326562499999998,
    "MMLU-PRO Raw": 0.46575797872340424,
    "MMLU-PRO": 40.639775413711575,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-17",
    "Submission Date": "2024-08-21",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3.5-MoE-instruct"
  },
  {
    "eval_name": "microsoft_Phi-3.5-mini-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/Phi-3.5-mini-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/Phi-3.5-mini-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__Phi-3.5-mini-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/Phi-3.5-mini-instruct",
    "Model sha": "64963004ad95869fa73a30279371c8778509ac84",
    "Average ‚¨ÜÔ∏è": 27.567573468796144,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 643,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.696004302337374,
    "IFEval Raw": 0.5774500547436359,
    "IFEval": 57.74500547436358,
    "BBH Raw": 0.5517785126111956,
    "BBH": 36.74585390851661,
    "MATH Lvl 5 Raw": 0.1593655589123867,
    "MATH Lvl 5": 15.93655589123867,
    "GPQA Raw": 0.33976510067114096,
    "GPQA": 11.968680089485462,
    "MUSR Raw": 0.402125,
    "MUSR": 10.098958333333334,
    "MMLU-PRO Raw": 0.39619348404255317,
    "MMLU-PRO": 32.91038711583924,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-08-21",
    "Generation": 0,
    "Base Model": "microsoft/Phi-3.5-mini-instruct"
  },
  {
    "eval_name": "microsoft_phi-1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/phi-1",
    "Model sha": "b9ac0e6d78d43970ecf88e9e0154b3a7da20ed89",
    "Average ‚¨ÜÔ∏è": 5.523965728106273,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 207,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.28622924752807954,
    "IFEval Raw": 0.20680571993421898,
    "IFEval": 20.6805719934219,
    "BBH Raw": 0.31394755895837845,
    "BBH": 4.273999212214679,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.35251041666666666,
    "MUSR": 3.697135416666667,
    "MMLU-PRO Raw": 0.11619015957446809,
    "MMLU-PRO": 1.798906619385342,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-10",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "microsoft/phi-1"
  },
  {
    "eval_name": "microsoft_phi-1_5_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-1_5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-1_5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-1_5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/phi-1_5",
    "Model sha": "675aa382d814580b22651a30acb1a585d7c25963",
    "Average ‚¨ÜÔ∏è": 7.057673794439714,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1315,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.34086206905763317,
    "IFEval Raw": 0.2032839532440591,
    "IFEval": 20.32839532440591,
    "BBH Raw": 0.33597583211996657,
    "BBH": 7.468938770070243,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.34041666666666665,
    "MUSR": 3.385416666666666,
    "MMLU-PRO Raw": 0.16913231382978725,
    "MMLU-PRO": 7.6813682033096935,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-10",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "microsoft/phi-1_5"
  },
  {
    "eval_name": "microsoft_phi-2_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/microsoft/phi-2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">microsoft/phi-2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/microsoft__phi-2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "microsoft/phi-2",
    "Model sha": "ef382358ec9e382308935a992d908de099b64c23",
    "Average ‚¨ÜÔ∏è": 15.471350974126281,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3243,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.42352098259023907,
    "IFEval Raw": 0.273875539125077,
    "IFEval": 27.3875539125077,
    "BBH Raw": 0.4881208771249696,
    "BBH": 28.038519293439304,
    "MATH Lvl 5 Raw": 0.0256797583081571,
    "MATH Lvl 5": 2.56797583081571,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.4098958333333333,
    "MUSR": 13.83697916666666,
    "MMLU-PRO Raw": 0.26279920212765956,
    "MMLU-PRO": 18.088800236406616,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-13",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "migtissera_Llama-3-70B-Synthia-v3.5_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Llama-3-70B-Synthia-v3.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Llama-3-70B-Synthia-v3.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Llama-3-70B-Synthia-v3.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Llama-3-70B-Synthia-v3.5",
    "Model sha": "8744db0bccfc18f1847633da9d29fc89b35b4190",
    "Average ‚¨ÜÔ∏è": 35.20429856307621,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.769697989560003,
    "IFEval Raw": 0.6076499244227538,
    "IFEval": 60.764992442275386,
    "BBH Raw": 0.6488638026271278,
    "BBH": 49.118159695748176,
    "MATH Lvl 5 Raw": 0.18957703927492447,
    "MATH Lvl 5": 18.957703927492446,
    "GPQA Raw": 0.3875838926174497,
    "GPQA": 18.34451901565996,
    "MUSR Raw": 0.49219791666666673,
    "MUSR": 23.39140625,
    "MMLU-PRO Raw": 0.4658410904255319,
    "MMLU-PRO": 40.64901004728132,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "migtissera/Llama-3-70B-Synthia-v3.5"
  },
  {
    "eval_name": "migtissera_Llama-3-8B-Synthia-v3.5_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Llama-3-8B-Synthia-v3.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Llama-3-8B-Synthia-v3.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Llama-3-8B-Synthia-v3.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Llama-3-8B-Synthia-v3.5",
    "Model sha": "af4990801a24fee7acf16370cb5aa5643b5e9d6c",
    "Average ‚¨ÜÔ∏è": 19.696677808834853,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8286983662741313,
    "IFEval Raw": 0.5069582042314393,
    "IFEval": 50.69582042314393,
    "BBH Raw": 0.4887940933660044,
    "BBH": 27.542339430057652,
    "MATH Lvl 5 Raw": 0.05060422960725076,
    "MATH Lvl 5": 5.0604229607250755,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.40438541666666666,
    "MUSR": 9.414843749999996,
    "MMLU-PRO Raw": 0.30302526595744683,
    "MMLU-PRO": 22.558362884160758,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-08-28",
    "Generation": 0,
    "Base Model": "migtissera/Llama-3-8B-Synthia-v3.5"
  },
  {
    "eval_name": "migtissera_Tess-3-7B-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Tess-3-7B-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Tess-3-7B-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Tess-3-7B-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Tess-3-7B-SFT",
    "Model sha": "404de3b56564dbd43cd64d97f8574b43189462f3",
    "Average ‚¨ÜÔ∏è": 17.096163150722504,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6471697737859032,
    "IFEval Raw": 0.3946262583279033,
    "IFEval": 39.46262583279033,
    "BBH Raw": 0.46073483895076217,
    "BBH": 24.123847398237004,
    "MATH Lvl 5 Raw": 0.03323262839879154,
    "MATH Lvl 5": 3.3232628398791544,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.4112708333333333,
    "MUSR": 10.275520833333333,
    "MMLU-PRO Raw": 0.30335771276595747,
    "MMLU-PRO": 22.595301418439718,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-09",
    "Submission Date": "2024-07-20",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.3"
  },
  {
    "eval_name": "migtissera_Tess-3-Mistral-Nemo-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Tess-3-Mistral-Nemo-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Tess-3-Mistral-Nemo-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Tess-3-Mistral-Nemo-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Tess-3-Mistral-Nemo-12B",
    "Model sha": "0b82dea6e8f4aed4a1c2e10198d68991c30d171b",
    "Average ‚¨ÜÔ∏è": 16.543939390579638,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8899918565127223,
    "IFEval Raw": 0.335499807178287,
    "IFEval": 33.549980717828696,
    "BBH Raw": 0.489942302453045,
    "BBH": 28.042728344416503,
    "MATH Lvl 5 Raw": 0.04682779456193353,
    "MATH Lvl 5": 4.682779456193353,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.44578125,
    "MUSR": 15.489322916666671,
    "MMLU-PRO Raw": 0.25648271276595747,
    "MMLU-PRO": 17.386968085106382,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "migtissera/Tess-3-Mistral-Nemo-12B"
  },
  {
    "eval_name": "migtissera_Tess-v2.5-Phi-3-medium-128k-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Phi3ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Tess-v2.5-Phi-3-medium-128k-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Tess-v2.5-Phi-3-medium-128k-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Tess-v2.5-Phi-3-medium-128k-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Tess-v2.5-Phi-3-medium-128k-14B",
    "Model sha": "3a4dbce32e765f659d418c57f0040d290b8b480d",
    "Average ‚¨ÜÔ∏è": 23.738381656622384,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2381702245810446,
    "IFEval Raw": 0.45387682460316403,
    "IFEval": 45.3876824603164,
    "BBH Raw": 0.6206613823135703,
    "BBH": 46.21582810863877,
    "MATH Lvl 5 Raw": 0.026435045317220542,
    "MATH Lvl 5": 2.643504531722054,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.41130208333333335,
    "MUSR": 10.112760416666669,
    "MMLU-PRO Raw": 0.3731715425531915,
    "MMLU-PRO": 30.352393617021285,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-05",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3-medium-128k-instruct"
  },
  {
    "eval_name": "migtissera_Tess-v2.5.2-Qwen2-72B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Tess-v2.5.2-Qwen2-72B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Tess-v2.5.2-Qwen2-72B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Tess-v2.5.2-Qwen2-72B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Tess-v2.5.2-Qwen2-72B",
    "Model sha": "0435e634ad9bc8b1172395a535b78e6f25f3594f",
    "Average ‚¨ÜÔ∏è": 33.27604657472896,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 14.613087266966872,
    "IFEval Raw": 0.44943084349525925,
    "IFEval": 44.943084349525925,
    "BBH Raw": 0.6646791891060648,
    "BBH": 52.30813577387958,
    "MATH Lvl 5 Raw": 0.27416918429003023,
    "MATH Lvl 5": 27.416918429003022,
    "GPQA Raw": 0.35067114093959734,
    "GPQA": 13.422818791946312,
    "MUSR Raw": 0.41883333333333334,
    "MUSR": 10.887500000000003,
    "MMLU-PRO Raw": 0.5561003989361702,
    "MMLU-PRO": 50.67782210401892,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-13",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "migtissera/Tess-v2.5.2-Qwen2-72B"
  },
  {
    "eval_name": "migtissera_Trinity-2-Codestral-22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Trinity-2-Codestral-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Trinity-2-Codestral-22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Trinity-2-Codestral-22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Trinity-2-Codestral-22B",
    "Model sha": "5f20b9d8af1a75c135c70bd7295e58301cce63fc",
    "Average ‚¨ÜÔ∏è": 21.81901085635334,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5021573106630008,
    "IFEval Raw": 0.4202050559182968,
    "IFEval": 42.020505591829675,
    "BBH Raw": 0.5593244825460373,
    "BBH": 36.41273800501431,
    "MATH Lvl 5 Raw": 0.08610271903323263,
    "MATH Lvl 5": 8.610271903323262,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.4110520833333333,
    "MUSR": 9.61484375,
    "MMLU-PRO Raw": 0.3307845744680851,
    "MMLU-PRO": 25.642730496453904,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-09-16",
    "Generation": 1,
    "Base Model": "mistralai/Codestral-22B-v0.1"
  },
  {
    "eval_name": "migtissera_Trinity-2-Codestral-22B-v0.2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Trinity-2-Codestral-22B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Trinity-2-Codestral-22B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Trinity-2-Codestral-22B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Trinity-2-Codestral-22B-v0.2",
    "Model sha": "63513c3eb9b7c552fc163f58a2e7dc1fa09573b5",
    "Average ‚¨ÜÔ∏è": 21.869825084599302,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5535215928704613,
    "IFEval Raw": 0.43446832183052075,
    "IFEval": 43.44683218305208,
    "BBH Raw": 0.5686364683055418,
    "BBH": 37.61424608895926,
    "MATH Lvl 5 Raw": 0.08383685800604229,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.40447916666666667,
    "MUSR": 9.059895833333337,
    "MMLU-PRO Raw": 0.33402593085106386,
    "MMLU-PRO": 26.002881205673763,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "mistralai/Codestral-22B-v0.1"
  },
  {
    "eval_name": "migtissera_Trinity-2-Codestral-22B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/migtissera/Trinity-2-Codestral-22B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">migtissera/Trinity-2-Codestral-22B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/migtissera__Trinity-2-Codestral-22B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "migtissera/Trinity-2-Codestral-22B-v0.2",
    "Model sha": "9452a82ac7bfa9092a061ec913e9078ef3525a03",
    "Average ‚¨ÜÔ∏è": 22.1117998201449,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.561207568663611,
    "IFEval Raw": 0.44301121025545553,
    "IFEval": 44.301121025545555,
    "BBH Raw": 0.5706466356198404,
    "BBH": 37.78604101957199,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4031458333333333,
    "MUSR": 8.859895833333338,
    "MMLU-PRO Raw": 0.3353557180851064,
    "MMLU-PRO": 26.150635342789595,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-16",
    "Generation": 1,
    "Base Model": "mistralai/Codestral-22B-v0.1"
  },
  {
    "eval_name": "minghaowu_Qwen1.5-1.8B-OpenHermes-2.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/minghaowu/Qwen1.5-1.8B-OpenHermes-2.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">minghaowu/Qwen1.5-1.8B-OpenHermes-2.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/minghaowu__Qwen1.5-1.8B-OpenHermes-2.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "minghaowu/Qwen1.5-1.8B-OpenHermes-2.5",
    "Model sha": "40700de82968350c192318877fe522630d0ef76d",
    "Average ‚¨ÜÔ∏è": 8.319695788165424,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0949003455424968,
    "IFEval Raw": 0.27779735546128714,
    "IFEval": 27.779735546128713,
    "BBH Raw": 0.33746396801266015,
    "BBH": 7.561477534247794,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3528854166666667,
    "MUSR": 1.0773437500000014,
    "MMLU-PRO Raw": 0.17918882978723405,
    "MMLU-PRO": 8.798758865248226,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-10",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "Qwen/Qwen1.5-1.8B"
  },
  {
    "eval_name": "ministral_Ministral-3b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ministral/Ministral-3b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ministral/Ministral-3b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ministral__Ministral-3b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ministral/Ministral-3b-instruct",
    "Model sha": "2c95908929198d6e69af8638f0dbbd9bc6b93f9e",
    "Average ‚¨ÜÔ∏è": 3.381613488411341,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 29,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.26448693051646205,
    "IFEval Raw": 0.1357642167227401,
    "IFEval": 13.57642167227401,
    "BBH Raw": 0.31918598478332383,
    "BBH": 4.675863578564667,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.33825,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.10929188829787234,
    "MMLU-PRO": 1.032432033096926,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-14",
    "Submission Date": "2024-10-25",
    "Generation": 0,
    "Base Model": "ministral/Ministral-3b-instruct"
  },
  {
    "eval_name": "mistral-community_Mistral-7B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistral-community/Mistral-7B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistral-community/Mistral-7B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistral-community__Mistral-7B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistral-community/Mistral-7B-v0.2",
    "Model sha": "2c3e624962b1a3f3fbf52e15969565caa7bc064a",
    "Average ‚¨ÜÔ∏è": 14.215362442692104,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 232,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5532132779204353,
    "IFEval Raw": 0.22663976028050017,
    "IFEval": 22.663976028050016,
    "BBH Raw": 0.4510187962797583,
    "BBH": 23.950865383029594,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4031770833333333,
    "MUSR": 8.363802083333333,
    "MMLU-PRO Raw": 0.2952958776595745,
    "MMLU-PRO": 21.699541962174944,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-23",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mistral-community/Mistral-7B-v0.2"
  },
  {
    "eval_name": "mistral-community_mixtral-8x22B-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistral-community/mixtral-8x22B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistral-community/mixtral-8x22B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistral-community__mixtral-8x22B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistral-community/mixtral-8x22B-v0.3",
    "Model sha": "211b177b79ab5ef245ee334d106c27623e786882",
    "Average ‚¨ÜÔ∏è": 25.789406608528008,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 140,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 52.49448516855476,
    "IFEval Raw": 0.25826362939223485,
    "IFEval": 25.826362939223486,
    "BBH Raw": 0.6250002178435845,
    "BBH": 45.73104089763324,
    "MATH Lvl 5 Raw": 0.1827794561933535,
    "MATH Lvl 5": 18.277945619335352,
    "GPQA Raw": 0.3775167785234899,
    "GPQA": 17.00223713646532,
    "MUSR Raw": 0.4036979166666667,
    "MUSR": 7.462239583333335,
    "MMLU-PRO Raw": 0.46392952127659576,
    "MMLU-PRO": 40.4366134751773,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "mistral-community/mixtral-8x22B-v0.3"
  },
  {
    "eval_name": "mistralai_Codestral-22B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Codestral-22B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Codestral-22B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Codestral-22B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Codestral-22B-v0.1",
    "Model sha": "8f5fe23af91885222a1563283c87416745a5e212",
    "Average ‚¨ÜÔ∏è": 23.27991740686463,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1155,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3066695609381,
    "IFEval Raw": 0.5771752283939946,
    "IFEval": 57.717522839399464,
    "BBH Raw": 0.5139136921003167,
    "BBH": 30.737634411945635,
    "MATH Lvl 5 Raw": 0.10045317220543806,
    "MATH Lvl 5": 10.045317220543806,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4187083333333333,
    "MUSR": 10.738541666666668,
    "MMLU-PRO Raw": 0.3155751329787234,
    "MMLU-PRO": 23.95279255319149,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-29",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "mistralai/Codestral-22B-v0.1"
  },
  {
    "eval_name": "mistralai_Mistral-7B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-7B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-7B-Instruct-v0.1",
    "Model sha": "73068f3702d050a2fd5aa2ca1e612e5036429398",
    "Average ‚¨ÜÔ∏è": 12.695700694124275,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1528,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2160447174800653,
    "IFEval Raw": 0.4487060998151571,
    "IFEval": 44.87060998151571,
    "BBH Raw": 0.33548084759810987,
    "BBH": 7.647020535827543,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.38476041666666666,
    "MUSR": 6.12838541666667,
    "MMLU-PRO Raw": 0.24143949468085107,
    "MMLU-PRO": 15.715499408983453,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-27",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "mistralai_Mistral-7B-Instruct-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-Instruct-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-7B-Instruct-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-7B-Instruct-v0.2",
    "Model sha": "41b61a33a2483885c981aa79e0df6b32407ed873",
    "Average ‚¨ÜÔ∏è": 18.45753912546675,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2577,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5344066290509847,
    "IFEval Raw": 0.5496227786717023,
    "IFEval": 54.96227786717022,
    "BBH Raw": 0.44597355203292793,
    "BBH": 22.910601936713604,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.39660416666666665,
    "MUSR": 7.608854166666667,
    "MMLU-PRO Raw": 0.2716921542553192,
    "MMLU-PRO": 19.076906028368796,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-11",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-7B-Instruct-v0.2"
  },
  {
    "eval_name": "mistralai_Mistral-7B-Instruct-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-Instruct-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-7B-Instruct-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-7B-Instruct-v0.3",
    "Model sha": "83e9aa141f2e28c82232fea5325f54edf17c43de",
    "Average ‚¨ÜÔ∏è": 19.174746309634973,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1137,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5377834643891,
    "IFEval Raw": 0.5465254413844156,
    "IFEval": 54.652544138441556,
    "BBH Raw": 0.47219631712648397,
    "BBH": 25.56911494885904,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.37390625000000005,
    "MUSR": 4.304947916666669,
    "MMLU-PRO Raw": 0.30751329787234044,
    "MMLU-PRO": 23.057033096926716,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.3"
  },
  {
    "eval_name": "mistralai_Mistral-7B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-7B-v0.1",
    "Model sha": "26bca36bde8333b5d7f72e9ed20ccda6a618af24",
    "Average ‚¨ÜÔ∏è": 14.562619083433885,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3449,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6755344655681169,
    "IFEval Raw": 0.2385548123423627,
    "IFEval": 23.855481234236272,
    "BBH Raw": 0.44310678121837116,
    "BBH": 22.16840245789813,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4139375,
    "MUSR": 10.675520833333335,
    "MMLU-PRO Raw": 0.30127992021276595,
    "MMLU-PRO": 22.364435579196215,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-20",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "mistralai_Mistral-7B-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-7B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-7B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-7B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-7B-v0.3",
    "Model sha": "b67d6a03ca097c5122fa65904fce0413500bf8c8",
    "Average ‚¨ÜÔ∏è": 14.215362442692104,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 390,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6604756000812907,
    "IFEval Raw": 0.22663976028050017,
    "IFEval": 22.663976028050016,
    "BBH Raw": 0.4510187962797583,
    "BBH": 23.950865383029594,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4031770833333333,
    "MUSR": 8.363802083333333,
    "MMLU-PRO Raw": 0.2952958776595745,
    "MMLU-PRO": 21.699541962174944,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-7B-v0.3"
  },
  {
    "eval_name": "mistralai_Mistral-Large-Instruct-2411_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-Large-Instruct-2411\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-Large-Instruct-2411</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Large-Instruct-2411-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-Large-Instruct-2411",
    "Model sha": "3a5cb136f6106edf5c1210369068eb5a4f787cab",
    "Average ‚¨ÜÔ∏è": 38.45523147580388,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 131,
    "#Params (B)": 122,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 26.27230522887299,
    "IFEval Raw": 0.8400577135334246,
    "IFEval": 84.00577135334247,
    "BBH Raw": 0.6746647735675069,
    "BBH": 52.7448919952634,
    "MATH Lvl 5 Raw": 0.011329305135951661,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.43708053691275167,
    "GPQA": 24.94407158836689,
    "MUSR Raw": 0.454,
    "MUSR": 17.216666666666665,
    "MMLU-PRO Raw": 0.5561835106382979,
    "MMLU-PRO": 50.687056737588655,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-Large-Instruct-2411"
  },
  {
    "eval_name": "mistralai_Mistral-Nemo-Base-2407_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-Nemo-Base-2407\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-Nemo-Base-2407</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Nemo-Base-2407-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-Nemo-Base-2407",
    "Model sha": "d2efb15544d5401f761235bef327babb850887d0",
    "Average ‚¨ÜÔ∏è": 15.138651108214134,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 258,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7029954549103143,
    "IFEval Raw": 0.16299197241098062,
    "IFEval": 16.299197241098064,
    "BBH Raw": 0.5035062000369291,
    "BBH": 29.374736440966874,
    "MATH Lvl 5 Raw": 0.053625377643504536,
    "MATH Lvl 5": 5.362537764350454,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.3921354166666667,
    "MUSR": 6.516927083333336,
    "MMLU-PRO Raw": 0.34715757978723405,
    "MMLU-PRO": 27.46195330969267,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-18",
    "Submission Date": "2024-07-19",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "mistralai_Mistral-Nemo-Instruct-2407_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-Nemo-Instruct-2407</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Nemo-Instruct-2407-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-Nemo-Instruct-2407",
    "Model sha": "4d14c1db68fe20dbf80b8eca85d39b909c5fe1d5",
    "Average ‚¨ÜÔ∏è": 23.633374362251303,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1233,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9976005624836857,
    "IFEval Raw": 0.6380248850826917,
    "IFEval": 63.80248850826917,
    "BBH Raw": 0.5036523950310812,
    "BBH": 29.679970381152803,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.38999999999999996,
    "MUSR": 8.483333333333333,
    "MMLU-PRO Raw": 0.3517287234042553,
    "MMLU-PRO": 27.969858156028373,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-08-29",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-Nemo-Base-2407"
  },
  {
    "eval_name": "mistralai_Mistral-Small-Instruct-2409_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-Small-Instruct-2409\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-Small-Instruct-2409</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Small-Instruct-2409-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-Small-Instruct-2409",
    "Model sha": "63e53df6575e7085d62113f4383835ff979b3795",
    "Average ‚¨ÜÔ∏è": 26.262748976418276,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 351,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3793375088517446,
    "IFEval Raw": 0.666975846310013,
    "IFEval": 66.69758463100129,
    "BBH Raw": 0.5213075098146217,
    "BBH": 30.7920960925092,
    "MATH Lvl 5 Raw": 0.14350453172205438,
    "MATH Lvl 5": 14.350453172205437,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.36320833333333336,
    "MUSR": 3.001041666666666,
    "MMLU-PRO Raw": 0.39602726063829785,
    "MMLU-PRO": 32.89191784869976,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-Small-Instruct-2409"
  },
  {
    "eval_name": "mistralai_Mistral-Small-Instruct-2409_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mistral-Small-Instruct-2409\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mistral-Small-Instruct-2409</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mistral-Small-Instruct-2409-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mistral-Small-Instruct-2409",
    "Model sha": "63e53df6575e7085d62113f4383835ff979b3795",
    "Average ‚¨ÜÔ∏è": 29.81824256993342,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 351,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6100074880413604,
    "IFEval Raw": 0.6282829558903709,
    "IFEval": 62.82829558903709,
    "BBH Raw": 0.5830283846898211,
    "BBH": 40.559713034899204,
    "MATH Lvl 5 Raw": 0.19788519637462237,
    "MATH Lvl 5": 19.788519637462237,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.4063333333333334,
    "MUSR": 10.225000000000001,
    "MMLU-PRO Raw": 0.409906914893617,
    "MMLU-PRO": 34.43410165484633,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "mistralai/Mistral-Small-Instruct-2409"
  },
  {
    "eval_name": "mistralai_Mixtral-8x22B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x22B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x22B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mixtral-8x22B-Instruct-v0.1",
    "Model sha": "b0c3516041d014f640267b14feb4e9a84c8e8c71",
    "Average ‚¨ÜÔ∏è": 33.88568028808198,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 690,
    "#Params (B)": 140,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 47.14757859237797,
    "IFEval Raw": 0.7183584001560305,
    "IFEval": 71.83584001560305,
    "BBH Raw": 0.6124924926272018,
    "BBH": 44.11434558724835,
    "MATH Lvl 5 Raw": 0.18731117824773413,
    "MATH Lvl 5": 18.731117824773413,
    "GPQA Raw": 0.3733221476510067,
    "GPQA": 16.442953020134222,
    "MUSR Raw": 0.43111458333333336,
    "MUSR": 13.489322916666664,
    "MMLU-PRO Raw": 0.44830452127659576,
    "MMLU-PRO": 38.70050236406619,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-16",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mixtral-8x22B-v0.1"
  },
  {
    "eval_name": "mistralai_Mixtral-8x22B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x22B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x22B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x22B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mixtral-8x22B-v0.1",
    "Model sha": "b03e260818710044a2f088d88fab12bb220884fb",
    "Average ‚¨ÜÔ∏è": 25.72834815840493,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 199,
    "#Params (B)": 140,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 104.69731627511149,
    "IFEval Raw": 0.25826362939223485,
    "IFEval": 25.826362939223486,
    "BBH Raw": 0.6239807473187268,
    "BBH": 45.58840384342722,
    "MATH Lvl 5 Raw": 0.1827794561933535,
    "MATH Lvl 5": 18.277945619335352,
    "GPQA Raw": 0.37583892617449666,
    "GPQA": 16.778523489932887,
    "MUSR Raw": 0.4036979166666667,
    "MUSR": 7.462239583333335,
    "MMLU-PRO Raw": 0.46392952127659576,
    "MMLU-PRO": 40.4366134751773,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-16",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mistralai/Mixtral-8x22B-v0.1"
  },
  {
    "eval_name": "mistralai_Mixtral-8x7B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x7B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "Model sha": "1e637f2d7cb0a9d6fb1922f305cb784995190a83",
    "Average ‚¨ÜÔ∏è": 23.842278939481744,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4206,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.76493936614677,
    "IFEval Raw": 0.5599143605633053,
    "IFEval": 55.991436056330535,
    "BBH Raw": 0.49623654013356494,
    "BBH": 29.742398380967334,
    "MATH Lvl 5 Raw": 0.09290030211480362,
    "MATH Lvl 5": 9.290030211480362,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.42032291666666666,
    "MUSR": 11.073697916666667,
    "MMLU-PRO Raw": 0.36918218085106386,
    "MMLU-PRO": 29.909131205673756,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-10",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "mistralai_Mixtral-8x7B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mixtral-8x7B-v0.1",
    "Model sha": "985aa055896a8f943d4a9f2572e6ea1341823841",
    "Average ‚¨ÜÔ∏è": 19.45198796144032,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1648,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 18.387864579754865,
    "IFEval Raw": 0.24152692633324024,
    "IFEval": 24.152692633324023,
    "BBH Raw": 0.508666743762444,
    "BBH": 30.294194918961484,
    "MATH Lvl 5 Raw": 0.09516616314199396,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.43213541666666666,
    "MUSR": 12.58359375,
    "MMLU-PRO Raw": 0.3849734042553192,
    "MMLU-PRO": 31.663711583924346,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-01",
    "Submission Date": "2024-08-20",
    "Generation": 0,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "mistralai_Mixtral-8x7B-v0.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mistralai/Mixtral-8x7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mistralai/Mixtral-8x7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mistralai__Mixtral-8x7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mistralai/Mixtral-8x7B-v0.1",
    "Model sha": "985aa055896a8f943d4a9f2572e6ea1341823841",
    "Average ‚¨ÜÔ∏è": 19.665108918316083,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1648,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.135099854813959,
    "IFEval Raw": 0.23260947618984296,
    "IFEval": 23.260947618984297,
    "BBH Raw": 0.5097711377553386,
    "BBH": 30.4002992674255,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.32046979865771813,
    "GPQA": 9.395973154362418,
    "MUSR Raw": 0.4413125,
    "MUSR": 13.6640625,
    "MMLU-PRO Raw": 0.3871343085106383,
    "MMLU-PRO": 31.903812056737586,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-01",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "mistralai/Mixtral-8x7B-v0.1"
  },
  {
    "eval_name": "mixtao_MixTAO-7Bx2-MoE-v8.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mixtao/MixTAO-7Bx2-MoE-v8.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mixtao/MixTAO-7Bx2-MoE-v8.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mixtao__MixTAO-7Bx2-MoE-v8.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mixtao/MixTAO-7Bx2-MoE-v8.1",
    "Model sha": "339130b87b6ef2484fea9fbfacba8a714ac03347",
    "Average ‚¨ÜÔ∏è": 21.07792698379691,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 54,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.924035071773507,
    "IFEval Raw": 0.41623337189767595,
    "IFEval": 41.6233371897676,
    "BBH Raw": 0.5189059391733521,
    "BBH": 32.31034233969924,
    "MATH Lvl 5 Raw": 0.09063444108761327,
    "MATH Lvl 5": 9.063444108761328,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.4463333333333333,
    "MUSR": 15.291666666666664,
    "MMLU-PRO Raw": 0.3123337765957447,
    "MMLU-PRO": 23.592641843971627,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-26",
    "Submission Date": "2024-10-04",
    "Generation": 0,
    "Base Model": "mixtao/MixTAO-7Bx2-MoE-v8.1"
  },
  {
    "eval_name": "mlabonne_AlphaMonarch-7B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/AlphaMonarch-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/AlphaMonarch-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__AlphaMonarch-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/AlphaMonarch-7B",
    "Model sha": "3de065d84411d74e5b3590f67f52b0b71faf6161",
    "Average ‚¨ÜÔ∏è": 17.655797425645098,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 148,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5725717098809129,
    "IFEval Raw": 0.49394384677101205,
    "IFEval": 49.39438467710121,
    "BBH Raw": 0.4625522037183211,
    "BBH": 23.947378025426246,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.41213541666666664,
    "MUSR": 9.316927083333331,
    "MMLU-PRO Raw": 0.24725731382978725,
    "MMLU-PRO": 16.36192375886525,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-02-14",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mlabonne/AlphaMonarch-7B (Merge)"
  },
  {
    "eval_name": "mlabonne_Beyonder-4x7B-v3_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/Beyonder-4x7B-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/Beyonder-4x7B-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__Beyonder-4x7B-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/Beyonder-4x7B-v3",
    "Model sha": "8e923fa480f511ab54d79b44b0487768bdd3de4e",
    "Average ‚¨ÜÔ∏è": 19.381682464691185,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 58,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3863027070825318,
    "IFEval Raw": 0.5608385749810503,
    "IFEval": 56.08385749810503,
    "BBH Raw": 0.4670522037183211,
    "BBH": 24.557209180110334,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40454166666666663,
    "MUSR": 8.934375000000001,
    "MMLU-PRO Raw": 0.2512466755319149,
    "MMLU-PRO": 16.805186170212764,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-21",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mlabonne/Beyonder-4x7B-v3 (Merge)"
  },
  {
    "eval_name": "mlabonne_BigQwen2.5-52B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/BigQwen2.5-52B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/BigQwen2.5-52B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__BigQwen2.5-52B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/BigQwen2.5-52B-Instruct",
    "Model sha": "425b9bffc9871085cc0d42c34138ce776f96ba02",
    "Average ‚¨ÜÔ∏è": 37.55828058974402,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 52,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 27.563507714868514,
    "IFEval Raw": 0.7928718023732585,
    "IFEval": 79.28718023732586,
    "BBH Raw": 0.7121004678698547,
    "BBH": 59.80960695923371,
    "MATH Lvl 5 Raw": 0.1865558912386707,
    "MATH Lvl 5": 18.65558912386707,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.41130208333333335,
    "MUSR": 10.446093750000001,
    "MMLU-PRO Raw": 0.5519448138297872,
    "MMLU-PRO": 50.21609042553191,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-25",
    "Generation": 1,
    "Base Model": "mlabonne/BigQwen2.5-52B-Instruct (Merge)"
  },
  {
    "eval_name": "mlabonne_BigQwen2.5-Echo-47B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/BigQwen2.5-Echo-47B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/BigQwen2.5-Echo-47B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__BigQwen2.5-Echo-47B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/BigQwen2.5-Echo-47B-Instruct",
    "Model sha": "f95fcf22f8ab87c2dbb1893b87c8a132820acb5e",
    "Average ‚¨ÜÔ∏è": 30.322428753939732,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 47,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 8.523076769430045,
    "IFEval Raw": 0.7356691356711305,
    "IFEval": 73.56691356711303,
    "BBH Raw": 0.6125111878044905,
    "BBH": 44.52224375363397,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.4124791666666667,
    "MUSR": 10.193229166666667,
    "MMLU-PRO Raw": 0.4734042553191489,
    "MMLU-PRO": 41.48936170212765,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "mlabonne/BigQwen2.5-Echo-47B-Instruct (Merge)"
  },
  {
    "eval_name": "mlabonne_ChimeraLlama-3-8B-v2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/ChimeraLlama-3-8B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/ChimeraLlama-3-8B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__ChimeraLlama-3-8B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/ChimeraLlama-3-8B-v2",
    "Model sha": "d90a12b1574d7be084e53e0ad610282638ab29cf",
    "Average ‚¨ÜÔ∏è": 20.133093296942747,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8374097575107993,
    "IFEval Raw": 0.44688315890725494,
    "IFEval": 44.6883158907255,
    "BBH Raw": 0.5045597361952603,
    "BBH": 28.47879573339652,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.3790833333333334,
    "MUSR": 5.252083333333334,
    "MMLU-PRO Raw": 0.3568816489361702,
    "MMLU-PRO": 28.54240543735224,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-22",
    "Submission Date": "2024-08-25",
    "Generation": 1,
    "Base Model": "mlabonne/ChimeraLlama-3-8B-v2 (Merge)"
  },
  {
    "eval_name": "mlabonne_ChimeraLlama-3-8B-v3_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/ChimeraLlama-3-8B-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/ChimeraLlama-3-8B-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__ChimeraLlama-3-8B-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/ChimeraLlama-3-8B-v3",
    "Model sha": "c8c1787e1426e3979ae82134f4eb7fa332f58ae0",
    "Average ‚¨ÜÔ∏è": 20.684541936885832,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8237395942584049,
    "IFEval Raw": 0.44078821970150317,
    "IFEval": 44.078821970150315,
    "BBH Raw": 0.49781902726529204,
    "BBH": 27.64609355033005,
    "MATH Lvl 5 Raw": 0.08761329305135952,
    "MATH Lvl 5": 8.761329305135952,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4003541666666666,
    "MUSR": 8.37760416666667,
    "MMLU-PRO Raw": 0.36685505319148937,
    "MMLU-PRO": 29.650561465721044,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-01",
    "Submission Date": "2024-08-25",
    "Generation": 1,
    "Base Model": "mlabonne/ChimeraLlama-3-8B-v3 (Merge)"
  },
  {
    "eval_name": "mlabonne_Daredevil-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/Daredevil-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/Daredevil-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__Daredevil-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/Daredevil-8B",
    "Model sha": "717953c83631cc9adf2dddccfff06739308f10f7",
    "Average ‚¨ÜÔ∏è": 22.308941454445293,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 34,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5119146885760388,
    "IFEval Raw": 0.45477665926408595,
    "IFEval": 45.4776659264086,
    "BBH Raw": 0.5194408746721715,
    "BBH": 31.626854762529916,
    "MATH Lvl 5 Raw": 0.10045317220543808,
    "MATH Lvl 5": 10.045317220543808,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.393875,
    "MUSR": 7.534375000000003,
    "MMLU-PRO Raw": 0.383061835106383,
    "MMLU-PRO": 31.451315011820324,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-07-02",
    "Generation": 1,
    "Base Model": "mlabonne/Daredevil-8B (Merge)"
  },
  {
    "eval_name": "mlabonne_Daredevil-8B-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/Daredevil-8B-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/Daredevil-8B-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__Daredevil-8B-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/Daredevil-8B-abliterated",
    "Model sha": "034c0ce8ceeba075d1dff2bac1b113a017c79390",
    "Average ‚¨ÜÔ∏è": 19.725759923200936,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 33,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.198361881597928,
    "IFEval Raw": 0.44263664853699297,
    "IFEval": 44.2636648536993,
    "BBH Raw": 0.4254272523147253,
    "BBH": 19.865777111108127,
    "MATH Lvl 5 Raw": 0.09667673716012083,
    "MATH Lvl 5": 9.667673716012084,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.40702083333333333,
    "MUSR": 9.17760416666667,
    "MMLU-PRO Raw": 0.3700964095744681,
    "MMLU-PRO": 30.010712174940902,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-07-02",
    "Generation": 0,
    "Base Model": "mlabonne/Daredevil-8B-abliterated"
  },
  {
    "eval_name": "mlabonne_Hermes-3-Llama-3.1-70B-lorablated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/Hermes-3-Llama-3.1-70B-lorablated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/Hermes-3-Llama-3.1-70B-lorablated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__Hermes-3-Llama-3.1-70B-lorablated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/Hermes-3-Llama-3.1-70B-lorablated",
    "Model sha": "4303ff3b524418e9aa5e787d60595a44a6173b02",
    "Average ‚¨ÜÔ∏è": 35.90028415826159,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 10.434414090317043,
    "IFEval Raw": 0.7143867161354096,
    "IFEval": 71.43867161354096,
    "BBH Raw": 0.6644396676156855,
    "BBH": 52.34174899453114,
    "MATH Lvl 5 Raw": 0.15030211480362538,
    "MATH Lvl 5": 15.030211480362537,
    "GPQA Raw": 0.348993288590604,
    "GPQA": 13.19910514541387,
    "MUSR Raw": 0.48444791666666664,
    "MUSR": 22.022656249999994,
    "MMLU-PRO Raw": 0.47232380319148937,
    "MMLU-PRO": 41.36931146572104,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "mlabonne/Hermes-3-Llama-3.1-70B-lorablated (Merge)"
  },
  {
    "eval_name": "mlabonne_Meta-Llama-3.1-8B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__Meta-Llama-3.1-8B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/Meta-Llama-3.1-8B-Instruct-abliterated",
    "Model sha": "aef878bdf42c119d007322967006fcdef5ae6ee1",
    "Average ‚¨ÜÔ∏è": 23.290888435550304,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 134,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.454049515488103,
    "IFEval Raw": 0.7344700949037443,
    "IFEval": 73.44700949037443,
    "BBH Raw": 0.48740648734902187,
    "BBH": 27.12916478111247,
    "MATH Lvl 5 Raw": 0.07250755287009063,
    "MATH Lvl 5": 7.250755287009063,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.36488541666666663,
    "MUSR": 3.2106770833333336,
    "MMLU-PRO Raw": 0.3503158244680851,
    "MMLU-PRO": 27.812869385342786,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-10-13",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "mlabonne_NeuralBeagle14-7B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/NeuralBeagle14-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/NeuralBeagle14-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__NeuralBeagle14-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/NeuralBeagle14-7B",
    "Model sha": "1567ad618a0998139654cb355738bb9bc018ca64",
    "Average ‚¨ÜÔ∏è": 18.94784069619808,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 158,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6717071150134716,
    "IFEval Raw": 0.49351941736813876,
    "IFEval": 49.35194173681387,
    "BBH Raw": 0.46278709452353844,
    "BBH": 23.959695145493203,
    "MATH Lvl 5 Raw": 0.05438066465256798,
    "MATH Lvl 5": 5.4380664652567985,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.43194791666666665,
    "MUSR": 12.893489583333336,
    "MMLU-PRO Raw": 0.2601396276595745,
    "MMLU-PRO": 17.793291962174944,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-15",
    "Submission Date": "2024-06-27",
    "Generation": 2,
    "Base Model": "mlabonne/Beagle14-7B (Merge)"
  },
  {
    "eval_name": "mlabonne_NeuralDaredevil-8B-abliterated_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/NeuralDaredevil-8B-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__NeuralDaredevil-8B-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/NeuralDaredevil-8B-abliterated",
    "Model sha": "2f4a5e8a8522f19dff345c7189b7891468763061",
    "Average ‚¨ÜÔ∏è": 27.14897632598957,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 149,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7077438813018277,
    "IFEval Raw": 0.756077208473517,
    "IFEval": 75.60772084735169,
    "BBH Raw": 0.5110566504436299,
    "BBH": 30.30798586214647,
    "MATH Lvl 5 Raw": 0.08836858006042297,
    "MATH Lvl 5": 8.836858006042297,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4019375,
    "MUSR": 9.075520833333337,
    "MMLU-PRO Raw": 0.38414228723404253,
    "MMLU-PRO": 31.57136524822695,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "mlabonne/NeuralDaredevil-8B-abliterated"
  },
  {
    "eval_name": "mlabonne_NeuralDaredevil-8B-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/NeuralDaredevil-8B-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/NeuralDaredevil-8B-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__NeuralDaredevil-8B-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/NeuralDaredevil-8B-abliterated",
    "Model sha": "89b01e3292e031ed85ad21545849182f5627021e",
    "Average ‚¨ÜÔ∏è": 21.499914415098534,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 149,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9850067516335185,
    "IFEval Raw": 0.41623337189767595,
    "IFEval": 41.6233371897676,
    "BBH Raw": 0.5123964057729099,
    "BBH": 29.763198395755456,
    "MATH Lvl 5 Raw": 0.08534743202416918,
    "MATH Lvl 5": 8.534743202416918,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4149583333333333,
    "MUSR": 10.903124999999996,
    "MMLU-PRO Raw": 0.3801529255319149,
    "MMLU-PRO": 31.128102836879428,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "mlabonne/NeuralDaredevil-8B-abliterated"
  },
  {
    "eval_name": "mlabonne_OrpoLlama-3-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/OrpoLlama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/OrpoLlama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__OrpoLlama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/OrpoLlama-3-8B",
    "Model sha": "7f200e4c84ad0daa3ff6bc414012d8d0bacbf90e",
    "Average ‚¨ÜÔ∏è": 14.980803094668337,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 54,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.890299820776769,
    "IFEval Raw": 0.36527524745453177,
    "IFEval": 36.52752474545318,
    "BBH Raw": 0.4424079063503051,
    "BBH": 21.95410762879941,
    "MATH Lvl 5 Raw": 0.04531722054380666,
    "MATH Lvl 5": 4.5317220543806656,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.3579375,
    "MUSR": 4.0088541666666675,
    "MMLU-PRO Raw": 0.2705285904255319,
    "MMLU-PRO": 18.947621158392433,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-18",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "mlabonne_phixtral-2x2_8_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mlabonne/phixtral-2x2_8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mlabonne/phixtral-2x2_8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mlabonne__phixtral-2x2_8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mlabonne/phixtral-2x2_8",
    "Model sha": "7744a977d83f132ae5808d8c3b70157031f7de44",
    "Average ‚¨ÜÔ∏è": 15.46499677396425,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 146,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9609510670681426,
    "IFEval Raw": 0.3431184811854767,
    "IFEval": 34.31184811854767,
    "BBH Raw": 0.48885941873076205,
    "BBH": 28.502644855771297,
    "MATH Lvl 5 Raw": 0.030211480362537763,
    "MATH Lvl 5": 3.021148036253776,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3643541666666667,
    "MUSR": 7.710937500000003,
    "MMLU-PRO Raw": 0.2550698138297872,
    "MMLU-PRO": 17.229979314420802,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-07",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "mlabonne/phixtral-2x2_8"
  },
  {
    "eval_name": "mmnga_Llama-3-70B-japanese-suzume-vector-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mmnga/Llama-3-70B-japanese-suzume-vector-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mmnga/Llama-3-70B-japanese-suzume-vector-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mmnga__Llama-3-70B-japanese-suzume-vector-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mmnga/Llama-3-70B-japanese-suzume-vector-v0.1",
    "Model sha": "16f98b2d45684af2c4a9ff5da75b00ef13cca808",
    "Average ‚¨ÜÔ∏è": 30.883569145740612,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 16.09712501603509,
    "IFEval Raw": 0.4648931501748693,
    "IFEval": 46.48931501748693,
    "BBH Raw": 0.6541763652331517,
    "BBH": 50.02266053282724,
    "MATH Lvl 5 Raw": 0.2628398791540785,
    "MATH Lvl 5": 26.283987915407852,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.4140625,
    "MUSR": 10.757812500000002,
    "MMLU-PRO Raw": 0.5224401595744681,
    "MMLU-PRO": 46.937795508274235,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-28",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "mmnga/Llama-3-70B-japanese-suzume-vector-v0.1"
  },
  {
    "eval_name": "moeru-ai_L3.1-Moe-2x8B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/moeru-ai/L3.1-Moe-2x8B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">moeru-ai/L3.1-Moe-2x8B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/moeru-ai__L3.1-Moe-2x8B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "moeru-ai/L3.1-Moe-2x8B-v0.2",
    "Model sha": "1a0b4d4d1e839e332c67c9c16a2fc1f7ccc7f81e",
    "Average ‚¨ÜÔ∏è": 28.588567615094785,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9265679456770795,
    "IFEval Raw": 0.7347947889377962,
    "IFEval": 73.47947889377961,
    "BBH Raw": 0.5255688392585965,
    "BBH": 32.94589106477927,
    "MATH Lvl 5 Raw": 0.15256797583081572,
    "MATH Lvl 5": 15.256797583081571,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.41985416666666664,
    "MUSR": 11.381770833333329,
    "MMLU-PRO Raw": 0.38580452127659576,
    "MMLU-PRO": 31.756057919621743,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "moeru-ai/L3.1-Moe-2x8B-v0.2 (Merge)"
  },
  {
    "eval_name": "moeru-ai_L3.1-Moe-4x8B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/moeru-ai/L3.1-Moe-4x8B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">moeru-ai/L3.1-Moe-4x8B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/moeru-ai__L3.1-Moe-4x8B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "moeru-ai/L3.1-Moe-4x8B-v0.1",
    "Model sha": "f8d477fad4c02c099c80ef38865c01e2c882e996",
    "Average ‚¨ÜÔ∏è": 19.126854370106415,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.805487386857481,
    "IFEval Raw": 0.433219413378724,
    "IFEval": 43.3219413378724,
    "BBH Raw": 0.49392781736367014,
    "BBH": 27.856764788876916,
    "MATH Lvl 5 Raw": 0.11102719033232629,
    "MATH Lvl 5": 11.10271903323263,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3609166666666667,
    "MUSR": 3.9812500000000024,
    "MMLU-PRO Raw": 0.34541223404255317,
    "MMLU-PRO": 27.268026004728128,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "moeru-ai/L3.1-Moe-4x8B-v0.1 (Merge)"
  },
  {
    "eval_name": "moeru-ai_L3.1-Moe-4x8B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/moeru-ai/L3.1-Moe-4x8B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">moeru-ai/L3.1-Moe-4x8B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/moeru-ai__L3.1-Moe-4x8B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "moeru-ai/L3.1-Moe-4x8B-v0.2",
    "Model sha": "fab49d865eb51f00e955c5624712184c39d207c9",
    "Average ‚¨ÜÔ∏è": 17.467109243624378,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.4222179841587153,
    "IFEval Raw": 0.5406554608438943,
    "IFEval": 54.065546084389425,
    "BBH Raw": 0.446625675582615,
    "BBH": 21.33700714055054,
    "MATH Lvl 5 Raw": 0.052870090634441085,
    "MATH Lvl 5": 5.287009063444108,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.3233958333333333,
    "MUSR": 2.291145833333333,
    "MMLU-PRO Raw": 0.27626329787234044,
    "MMLU-PRO": 19.58481087470449,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-10-30",
    "Generation": 1,
    "Base Model": "moeru-ai/L3.1-Moe-4x8B-v0.2 (Merge)"
  },
  {
    "eval_name": "monsterapi_Llama-3_1-8B-Instruct-orca-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/monsterapi/Llama-3_1-8B-Instruct-orca-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">monsterapi/Llama-3_1-8B-Instruct-orca-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/monsterapi__Llama-3_1-8B-Instruct-orca-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "monsterapi/Llama-3_1-8B-Instruct-orca-ORPO",
    "Model sha": "5206a32e0bd3067aef1ce90f5528ade7d866253f",
    "Average ‚¨ÜÔ∏è": 4.832138103419669,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.532680261546071,
    "IFEval Raw": 0.22728914834860392,
    "IFEval": 22.728914834860394,
    "BBH Raw": 0.28653625778742803,
    "BBH": 1.3404688979507675,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.34447916666666667,
    "MUSR": 3.0598958333333326,
    "MMLU-PRO Raw": 0.11677194148936171,
    "MMLU-PRO": 1.8635490543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-01",
    "Submission Date": "2024-08-30",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "monsterapi_gemma-2-2b-LoRA-MonsterInstruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/monsterapi/gemma-2-2b-LoRA-MonsterInstruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">monsterapi/gemma-2-2b-LoRA-MonsterInstruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/monsterapi__gemma-2-2b-LoRA-MonsterInstruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "monsterapi/gemma-2-2b-LoRA-MonsterInstruct",
    "Model sha": "6422e27e96e15cf93b966c973aacc15f8a27a458",
    "Average ‚¨ÜÔ∏è": 11.86529091716785,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3386875489969998,
    "IFEval Raw": 0.3902545246612322,
    "IFEval": 39.02545246612322,
    "BBH Raw": 0.36496861927498697,
    "BBH": 11.965057260762338,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.3643854166666667,
    "MUSR": 5.41484375,
    "MMLU-PRO Raw": 0.19872007978723405,
    "MMLU-PRO": 10.968897754137116,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "monsterapi/gemma-2-2b-LoRA-MonsterInstruct"
  },
  {
    "eval_name": "mosaicml_mpt-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MPTForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/mosaicml/mpt-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">mosaicml/mpt-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/mosaicml__mpt-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "mosaicml/mpt-7b",
    "Model sha": "039e37745f00858f0e01e988383a8c4393b1a4f5",
    "Average ‚¨ÜÔ∏è": 5.994264988690563,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1161,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6435034251930756,
    "IFEval Raw": 0.21519900530592162,
    "IFEval": 21.51990053059216,
    "BBH Raw": 0.32997415960801324,
    "BBH": 6.550600790794161,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.36723958333333334,
    "MUSR": 2.904947916666668,
    "MMLU-PRO Raw": 0.12059507978723404,
    "MMLU-PRO": 2.288342198581559,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-05",
    "Submission Date": "2024-06-08",
    "Generation": 0,
    "Base Model": "mosaicml/mpt-7b"
  },
  {
    "eval_name": "natong19_Mistral-Nemo-Instruct-2407-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/natong19/Mistral-Nemo-Instruct-2407-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">natong19/Mistral-Nemo-Instruct-2407-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/natong19__Mistral-Nemo-Instruct-2407-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "natong19/Mistral-Nemo-Instruct-2407-abliterated",
    "Model sha": "9c7087f62e6ab10ec4aeeb268e25cb3d4000696b",
    "Average ‚¨ÜÔ∏è": 23.872106790135238,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2378570942899771,
    "IFEval Raw": 0.6392239258500778,
    "IFEval": 63.92239258500778,
    "BBH Raw": 0.5048447739625885,
    "BBH": 29.915044266358993,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4033333333333333,
    "MUSR": 10.149999999999999,
    "MMLU-PRO Raw": 0.351811835106383,
    "MMLU-PRO": 27.979092789598102,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-15",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "natong19/Mistral-Nemo-Instruct-2407-abliterated"
  },
  {
    "eval_name": "natong19_Qwen2-7B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/natong19/Qwen2-7B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">natong19/Qwen2-7B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/natong19__Qwen2-7B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "natong19/Qwen2-7B-Instruct-abliterated",
    "Model sha": "127962453ae87879719a82a97384ac1859787a25",
    "Average ‚¨ÜÔ∏è": 25.758544017443402,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0758356037929617,
    "IFEval Raw": 0.5836945970026197,
    "IFEval": 58.36945970026197,
    "BBH Raw": 0.5553035842403061,
    "BBH": 37.746833853463094,
    "MATH Lvl 5 Raw": 0.1110271903323263,
    "MATH Lvl 5": 11.10271903323263,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4034270833333333,
    "MUSR": 8.92838541666667,
    "MMLU-PRO Raw": 0.3842253989361702,
    "MMLU-PRO": 31.580599881796683,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-14",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "natong19/Qwen2-7B-Instruct-abliterated"
  },
  {
    "eval_name": "nazimali_Mistral-Nemo-Kurdish_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nazimali/Mistral-Nemo-Kurdish\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nazimali/Mistral-Nemo-Kurdish</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nazimali__Mistral-Nemo-Kurdish-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nazimali/Mistral-Nemo-Kurdish",
    "Model sha": "1eb544577a2874d8df0b77ca83ff1c88dd20f481",
    "Average ‚¨ÜÔ∏è": 19.368944765378615,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8497272285708921,
    "IFEval Raw": 0.3401208792670115,
    "IFEval": 34.012087926701156,
    "BBH Raw": 0.5133321102266589,
    "BBH": 29.855897080399064,
    "MATH Lvl 5 Raw": 0.08912386706948641,
    "MATH Lvl 5": 8.912386706948642,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4115729166666667,
    "MUSR": 11.77994791666667,
    "MMLU-PRO Raw": 0.3234707446808511,
    "MMLU-PRO": 24.830082742316783,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "nazimali/Mistral-Nemo-Kurdish (Merge)"
  },
  {
    "eval_name": "nazimali_Mistral-Nemo-Kurdish-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nazimali/Mistral-Nemo-Kurdish-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nazimali/Mistral-Nemo-Kurdish-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nazimali__Mistral-Nemo-Kurdish-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nazimali/Mistral-Nemo-Kurdish-Instruct",
    "Model sha": "512140572f11203441e60ca26b5ede2b9979cb1d",
    "Average ‚¨ÜÔ∏è": 18.555957634452803,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7021165130013975,
    "IFEval Raw": 0.4963917959901949,
    "IFEval": 49.63917959901949,
    "BBH Raw": 0.4699417600389813,
    "BBH": 25.56142263482943,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.397875,
    "MUSR": 8.40104166666667,
    "MMLU-PRO Raw": 0.3062666223404255,
    "MMLU-PRO": 22.91851359338061,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "nazimali/Mistral-Nemo-Kurdish-Instruct (Merge)"
  },
  {
    "eval_name": "nazimali_Mistral-Nemo-Kurdish-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nazimali/Mistral-Nemo-Kurdish-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nazimali/Mistral-Nemo-Kurdish-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nazimali__Mistral-Nemo-Kurdish-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nazimali/Mistral-Nemo-Kurdish-Instruct",
    "Model sha": "512140572f11203441e60ca26b5ede2b9979cb1d",
    "Average ‚¨ÜÔ∏è": 18.58910550024965,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.751720675119927,
    "IFEval Raw": 0.4860004787297703,
    "IFEval": 48.60004787297703,
    "BBH Raw": 0.47214400722999256,
    "BBH": 26.02174135407743,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.40057291666666667,
    "MUSR": 8.838281250000003,
    "MMLU-PRO Raw": 0.30867686170212766,
    "MMLU-PRO": 23.186317966903072,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-09",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "nazimali/Mistral-Nemo-Kurdish-Instruct (Merge)"
  },
  {
    "eval_name": "nbeerbower_Flammades-Mistral-Nemo-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Flammades-Mistral-Nemo-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Flammades-Mistral-Nemo-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Flammades-Mistral-Nemo-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Flammades-Mistral-Nemo-12B",
    "Model sha": "ddc76d1976af06aedc7f06bbffcaa34166c1cbdd",
    "Average ‚¨ÜÔ∏è": 22.466019281270402,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.627419781527904,
    "IFEval Raw": 0.38415958545548035,
    "IFEval": 38.41595854554804,
    "BBH Raw": 0.5299609345270283,
    "BBH": 32.39377187272594,
    "MATH Lvl 5 Raw": 0.06948640483383685,
    "MATH Lvl 5": 6.948640483383685,
    "GPQA Raw": 0.3036912751677852,
    "GPQA": 7.158836689038028,
    "MUSR Raw": 0.480625,
    "MUSR": 20.31145833333333,
    "MMLU-PRO Raw": 0.36610704787234044,
    "MMLU-PRO": 29.56744976359338,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-05",
    "Submission Date": "2024-10-06",
    "Generation": 1,
    "Base Model": "nbeerbower/Flammades-Mistral-Nemo-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Gemma2-Gutenberg-Doppel-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Gemma2-Gutenberg-Doppel-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Gemma2-Gutenberg-Doppel-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Gemma2-Gutenberg-Doppel-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Gemma2-Gutenberg-Doppel-9B",
    "Model sha": "f425bc69783891088e89e0afe44ec62b730567ba",
    "Average ‚¨ÜÔ∏è": 29.835999159589488,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9473156288396591,
    "IFEval Raw": 0.7171094917042337,
    "IFEval": 71.71094917042336,
    "BBH Raw": 0.5870114193661848,
    "BBH": 41.08306318800703,
    "MATH Lvl 5 Raw": 0.03549848942598188,
    "MATH Lvl 5": 3.5498489425981883,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.46078125,
    "MUSR": 17.29765625,
    "MMLU-PRO Raw": 0.41273271276595747,
    "MMLU-PRO": 34.748079196217496,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "nbeerbower/Gemma2-Gutenberg-Doppel-9B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Gutensuppe-mistral-nemo-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Gutensuppe-mistral-nemo-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Gutensuppe-mistral-nemo-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Gutensuppe-mistral-nemo-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Gutensuppe-mistral-nemo-12B",
    "Model sha": "6ee13f347071bc3c4ee95c9dc3488a4093927143",
    "Average ‚¨ÜÔ∏è": 22.080224004729754,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5562493274373266,
    "IFEval Raw": 0.29161070404305023,
    "IFEval": 29.161070404305022,
    "BBH Raw": 0.5486832203098263,
    "BBH": 35.56934797458052,
    "MATH Lvl 5 Raw": 0.12009063444108761,
    "MATH Lvl 5": 12.009063444108762,
    "GPQA Raw": 0.337248322147651,
    "GPQA": 11.633109619686799,
    "MUSR Raw": 0.42903125,
    "MUSR": 14.328906249999998,
    "MMLU-PRO Raw": 0.3680186170212766,
    "MMLU-PRO": 29.779846335697403,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/Gutensuppe-mistral-nemo-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Hermes2-Gutenberg2-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Hermes2-Gutenberg2-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Hermes2-Gutenberg2-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Hermes2-Gutenberg2-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Hermes2-Gutenberg2-Mistral-7B",
    "Model sha": "5eec0dfd29999ef1d7775010b7e9c7be9ed89bfd",
    "Average ‚¨ÜÔ∏è": 19.376448875678378,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.581121007587213,
    "IFEval Raw": 0.37214479802479644,
    "IFEval": 37.214479802479644,
    "BBH Raw": 0.4981450458280896,
    "BBH": 28.907334664767347,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.46230208333333334,
    "MUSR": 16.92109375,
    "MMLU-PRO Raw": 0.29928523936170215,
    "MMLU-PRO": 22.14280437352246,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "nbeerbower/Hermes2-Gutenberg2-Mistral-7B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Llama-3.1-Nemotron-lorablated-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Llama-3.1-Nemotron-lorablated-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Llama-3.1-Nemotron-lorablated-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Llama-3.1-Nemotron-lorablated-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Llama-3.1-Nemotron-lorablated-70B",
    "Model sha": "f335a582cdb7fb0e63a7343a908766ebd0ed9882",
    "Average ‚¨ÜÔ∏è": 33.96666257180438,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 13.805207720814407,
    "IFEval Raw": 0.7146615424850509,
    "IFEval": 71.46615424850509,
    "BBH Raw": 0.637904241694439,
    "BBH": 48.06205502704251,
    "MATH Lvl 5 Raw": 0.25000000000000006,
    "MATH Lvl 5": 25.000000000000007,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.44336458333333334,
    "MUSR": 14.92057291666667,
    "MMLU-PRO Raw": 0.49110704787234044,
    "MMLU-PRO": 43.456338652482266,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-11-08",
    "Generation": 1,
    "Base Model": "nbeerbower/Llama-3.1-Nemotron-lorablated-70B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Llama3.1-Gutenberg-Doppel-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Llama3.1-Gutenberg-Doppel-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Llama3.1-Gutenberg-Doppel-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Llama3.1-Gutenberg-Doppel-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Llama3.1-Gutenberg-Doppel-70B",
    "Model sha": "5de156e97f776ce1b88ce5b2e2dc1e7709205a82",
    "Average ‚¨ÜÔ∏è": 35.80304762542396,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.993593035315849,
    "IFEval Raw": 0.7092159913474027,
    "IFEval": 70.92159913474026,
    "BBH Raw": 0.6660891255994471,
    "BBH": 52.556778995199046,
    "MATH Lvl 5 Raw": 0.14501510574018125,
    "MATH Lvl 5": 14.501510574018125,
    "GPQA Raw": 0.3447986577181208,
    "GPQA": 12.639821029082773,
    "MUSR Raw": 0.48971875,
    "MUSR": 22.681510416666665,
    "MMLU-PRO Raw": 0.4736535904255319,
    "MMLU-PRO": 41.51706560283688,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "nbeerbower/Llama3.1-Gutenberg-Doppel-70B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Lyra-Gutenberg-mistral-nemo-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Lyra-Gutenberg-mistral-nemo-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Lyra-Gutenberg-mistral-nemo-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Lyra-Gutenberg-mistral-nemo-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Lyra-Gutenberg-mistral-nemo-12B",
    "Model sha": "5c506391eb02075e02f4cf5953b443505d646bce",
    "Average ‚¨ÜÔ∏è": 22.7037183339816,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9186019400856846,
    "IFEval Raw": 0.34948824674086976,
    "IFEval": 34.94882467408698,
    "BBH Raw": 0.5586245741555749,
    "BBH": 36.99243243937594,
    "MATH Lvl 5 Raw": 0.09138972809667674,
    "MATH Lvl 5": 9.138972809667674,
    "GPQA Raw": 0.3338926174496644,
    "GPQA": 11.185682326621922,
    "MUSR Raw": 0.43566666666666665,
    "MUSR": 14.758333333333331,
    "MMLU-PRO Raw": 0.36278257978723405,
    "MMLU-PRO": 29.19806442080379,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/Lyra-Gutenberg-mistral-nemo-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Lyra4-Gutenberg-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Lyra4-Gutenberg-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Lyra4-Gutenberg-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Lyra4-Gutenberg-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Lyra4-Gutenberg-12B",
    "Model sha": "cb6911be3475da99a810071c04803d6edfb5965b",
    "Average ‚¨ÜÔ∏è": 19.81894299634541,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.690533585792432,
    "IFEval Raw": 0.2212185888996751,
    "IFEval": 22.12185888996751,
    "BBH Raw": 0.538669487933139,
    "BBH": 34.23559275480162,
    "MATH Lvl 5 Raw": 0.1283987915407855,
    "MATH Lvl 5": 12.83987915407855,
    "GPQA Raw": 0.3187919463087248,
    "GPQA": 9.172259507829976,
    "MUSR Raw": 0.4037916666666666,
    "MUSR": 11.973958333333329,
    "MMLU-PRO Raw": 0.35713098404255317,
    "MMLU-PRO": 28.570109338061467,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "nbeerbower/Lyra4-Gutenberg-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Lyra4-Gutenberg2-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Lyra4-Gutenberg2-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Lyra4-Gutenberg2-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Lyra4-Gutenberg2-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Lyra4-Gutenberg2-12B",
    "Model sha": "6a5f117695cc729de16da87654b979e6df72ed2f",
    "Average ‚¨ÜÔ∏è": 19.932293679463253,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8093395714036888,
    "IFEval Raw": 0.25851296781428834,
    "IFEval": 25.851296781428832,
    "BBH Raw": 0.5344527944750038,
    "BBH": 33.73063962440059,
    "MATH Lvl 5 Raw": 0.11631419939577038,
    "MATH Lvl 5": 11.631419939577038,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.39721874999999995,
    "MUSR": 11.485677083333329,
    "MMLU-PRO Raw": 0.35654920212765956,
    "MMLU-PRO": 28.50546690307328,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "nbeerbower/Lyra4-Gutenberg2-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mahou-1.5-mistral-nemo-12B-lorablated_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mahou-1.5-mistral-nemo-12B-lorablated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mahou-1.5-mistral-nemo-12B-lorablated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mahou-1.5-mistral-nemo-12B-lorablated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mahou-1.5-mistral-nemo-12B-lorablated",
    "Model sha": "8c9eecaace50659647c7d8b569237ad068a6c837",
    "Average ‚¨ÜÔ∏è": 26.534810293518046,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4054244954299064,
    "IFEval Raw": 0.6824880206740338,
    "IFEval": 68.24880206740337,
    "BBH Raw": 0.5496040380079439,
    "BBH": 36.077381004161374,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.45216666666666666,
    "MUSR": 16.554166666666664,
    "MMLU-PRO Raw": 0.35738031914893614,
    "MMLU-PRO": 28.59781323877068,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-19",
    "Submission Date": "2024-10-19",
    "Generation": 1,
    "Base Model": "nbeerbower/Mahou-1.5-mistral-nemo-12B-lorablated (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Gutenberg-Doppel-7B-FFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Gutenberg-Doppel-7B-FFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Gutenberg-Doppel-7B-FFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Gutenberg-Doppel-7B-FFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Gutenberg-Doppel-7B-FFT",
    "Model sha": "5735876465b6f2523fdedb73120c3f97d04556d3",
    "Average ‚¨ÜÔ∏è": 18.33827572865676,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.43681628343655166,
    "IFEval Raw": 0.5716798095719358,
    "IFEval": 57.16798095719358,
    "BBH Raw": 0.40762540890255944,
    "BBH": 17.34657495584369,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.4059375,
    "MUSR": 9.342187500000001,
    "MMLU-PRO Raw": 0.2728557180851064,
    "MMLU-PRO": 19.206190898345156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Gutenberg-Doppel-7B-FFT (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Nemo-Gutenberg-Doppel-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Nemo-Gutenberg-Doppel-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B",
    "Model sha": "0eaaac89d4b53e94d5b78220b24439a026ee29e6",
    "Average ‚¨ÜÔ∏è": 21.47504639542385,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.776771295147254,
    "IFEval Raw": 0.3567068711020093,
    "IFEval": 35.67068711020093,
    "BBH Raw": 0.5274606999473499,
    "BBH": 32.42152675939865,
    "MATH Lvl 5 Raw": 0.1178247734138973,
    "MATH Lvl 5": 11.782477341389729,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.41321874999999997,
    "MUSR": 11.485677083333329,
    "MMLU-PRO Raw": 0.35787898936170215,
    "MMLU-PRO": 28.65322104018913,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Nemo-Gutenberg-Doppel-12B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Nemo-Gutenberg-Doppel-12B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B-v2",
    "Model sha": "adc1ccd9d83d24e41bed895f989803af87ea2d2c",
    "Average ‚¨ÜÔ∏è": 24.717980596521073,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4048565334201466,
    "IFEval Raw": 0.6535869271311232,
    "IFEval": 65.35869271311232,
    "BBH Raw": 0.5374496172235809,
    "BBH": 34.35741284991507,
    "MATH Lvl 5 Raw": 0.0445619335347432,
    "MATH Lvl 5": 4.45619335347432,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.42330208333333336,
    "MUSR": 13.046093749999997,
    "MMLU-PRO Raw": 0.3546376329787234,
    "MMLU-PRO": 28.29307033096927,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-09",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Nemo-Gutenberg-Doppel-12B-v2 (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Nemo-Prism-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Nemo-Prism-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Nemo-Prism-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Nemo-Prism-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Nemo-Prism-12B",
    "Model sha": "a39e1c8c083c172aaa3ca81faf8ba3b4799a888f",
    "Average ‚¨ÜÔ∏è": 27.344953735247795,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9592880026978776,
    "IFEval Raw": 0.6858103166265509,
    "IFEval": 68.58103166265508,
    "BBH Raw": 0.5475186352291487,
    "BBH": 35.91800839002647,
    "MATH Lvl 5 Raw": 0.05211480362537765,
    "MATH Lvl 5": 5.211480362537765,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.46261458333333333,
    "MUSR": 17.960156249999997,
    "MMLU-PRO Raw": 0.3581283244680851,
    "MMLU-PRO": 28.680924940898343,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Nemo-Prism-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Small-Drummer-22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Small-Drummer-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Small-Drummer-22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Small-Drummer-22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Small-Drummer-22B",
    "Model sha": "53b21ece0c64ffc8aba81f294ad19e2c06e9852c",
    "Average ‚¨ÜÔ∏è": 29.74388009670839,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.612721703579025,
    "IFEval Raw": 0.6331289866443259,
    "IFEval": 63.312898664432595,
    "BBH Raw": 0.5793201948136216,
    "BBH": 40.12177010845507,
    "MATH Lvl 5 Raw": 0.18429003021148038,
    "MATH Lvl 5": 18.429003021148038,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.40636458333333336,
    "MUSR": 9.795572916666668,
    "MMLU-PRO Raw": 0.40949135638297873,
    "MMLU-PRO": 34.38792848699764,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-10-01",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Small-Drummer-22B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Mistral-Small-Gutenberg-Doppel-22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Mistral-Small-Gutenberg-Doppel-22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Mistral-Small-Gutenberg-Doppel-22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Mistral-Small-Gutenberg-Doppel-22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Mistral-Small-Gutenberg-Doppel-22B",
    "Model sha": "d8091aad5f882b714321e4d51f504cc61996ee67",
    "Average ‚¨ÜÔ∏è": 27.85874668172723,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5886032328121744,
    "IFEval Raw": 0.48932277468228746,
    "IFEval": 48.93227746822875,
    "BBH Raw": 0.5858932329112819,
    "BBH": 40.931345197471124,
    "MATH Lvl 5 Raw": 0.21148036253776437,
    "MATH Lvl 5": 21.148036253776436,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.39706250000000004,
    "MUSR": 8.566145833333334,
    "MMLU-PRO Raw": 0.41240026595744683,
    "MMLU-PRO": 34.711140661938536,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-09-25",
    "Generation": 1,
    "Base Model": "nbeerbower/Mistral-Small-Gutenberg-Doppel-22B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Qwen2.5-Gutenberg-Doppel-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Qwen2.5-Gutenberg-Doppel-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Qwen2.5-Gutenberg-Doppel-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Qwen2.5-Gutenberg-Doppel-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Qwen2.5-Gutenberg-Doppel-14B",
    "Model sha": "11a5060f9e7315ea07241106f086ac4694dded60",
    "Average ‚¨ÜÔ∏è": 32.30211513411884,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6906118654504574,
    "IFEval Raw": 0.8090832324897937,
    "IFEval": 80.90832324897937,
    "BBH Raw": 0.6381735755183319,
    "BBH": 48.23890863039215,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33305369127516776,
    "GPQA": 11.073825503355701,
    "MUSR Raw": 0.4100625,
    "MUSR": 10.024479166666667,
    "MMLU-PRO Raw": 0.49210438829787234,
    "MMLU-PRO": 43.567154255319146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "nbeerbower/Qwen2.5-Gutenberg-Doppel-14B (Merge)"
  },
  {
    "eval_name": "nbeerbower_Stella-mistral-nemo-12B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/Stella-mistral-nemo-12B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/Stella-mistral-nemo-12B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__Stella-mistral-nemo-12B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/Stella-mistral-nemo-12B-v2",
    "Model sha": "b81bab28f7dcb25a0aa0fe4dcf957f3083ee6b43",
    "Average ‚¨ÜÔ∏è": 22.430369133905156,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.7408720915078146,
    "IFEval Raw": 0.32743121584063617,
    "IFEval": 32.743121584063616,
    "BBH Raw": 0.5483750956495209,
    "BBH": 35.364516100686416,
    "MATH Lvl 5 Raw": 0.11253776435045319,
    "MATH Lvl 5": 11.253776435045319,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.4303958333333333,
    "MUSR": 14.432812499999997,
    "MMLU-PRO Raw": 0.3684341755319149,
    "MMLU-PRO": 29.826019503546092,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-07",
    "Submission Date": "2024-09-14",
    "Generation": 1,
    "Base Model": "nbeerbower/Stella-mistral-nemo-12B-v2 (Merge)"
  },
  {
    "eval_name": "nbeerbower_gemma2-gutenberg-27B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/gemma2-gutenberg-27B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/gemma2-gutenberg-27B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__gemma2-gutenberg-27B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/gemma2-gutenberg-27B",
    "Model sha": "d4febe52e8b7b13a98126dbf1716ed1329f48922",
    "Average ‚¨ÜÔ∏è": 10.108960954327799,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 27,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 7.695458269738416,
    "IFEval Raw": 0.29470804133033685,
    "IFEval": 29.47080413303368,
    "BBH Raw": 0.37965683503451614,
    "BBH": 13.091524912026523,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.3727291666666666,
    "MUSR": 4.1578125,
    "MMLU-PRO Raw": 0.19822140957446807,
    "MMLU-PRO": 10.913489952718674,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "nbeerbower/gemma2-gutenberg-27B (Merge)"
  },
  {
    "eval_name": "nbeerbower_gemma2-gutenberg-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/gemma2-gutenberg-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/gemma2-gutenberg-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__gemma2-gutenberg-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/gemma2-gutenberg-9B",
    "Model sha": "ebdab2d41f257fc9e7c858498653644d13386ce5",
    "Average ‚¨ÜÔ∏è": 22.649256523301194,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 12,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.8096087688490474,
    "IFEval Raw": 0.2795948084416016,
    "IFEval": 27.95948084416016,
    "BBH Raw": 0.5950904001490335,
    "BBH": 42.35561106809721,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.45951041666666664,
    "MUSR": 16.705468749999994,
    "MMLU-PRO Raw": 0.4192154255319149,
    "MMLU-PRO": 35.4683806146572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-14",
    "Submission Date": "2024-08-03",
    "Generation": 1,
    "Base Model": "nbeerbower/gemma2-gutenberg-9B (Merge)"
  },
  {
    "eval_name": "nbeerbower_llama-3-gutenberg-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/llama-3-gutenberg-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/llama-3-gutenberg-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__llama-3-gutenberg-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/llama-3-gutenberg-8B",
    "Model sha": "4ed3aac5e30c078bee79ae193c2d301d38860b20",
    "Average ‚¨ÜÔ∏è": 21.296228939756496,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8835693905046188,
    "IFEval Raw": 0.4371910973993448,
    "IFEval": 43.71910973993448,
    "BBH Raw": 0.49936002561994197,
    "BBH": 27.958132724191334,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.40730208333333334,
    "MUSR": 10.046093749999999,
    "MMLU-PRO Raw": 0.383061835106383,
    "MMLU-PRO": 31.451315011820324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-05",
    "Submission Date": "2024-07-10",
    "Generation": 1,
    "Base Model": "nbeerbower/llama-3-gutenberg-8B (Merge)"
  },
  {
    "eval_name": "nbeerbower_llama3.1-cc-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/llama3.1-cc-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/llama3.1-cc-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__llama3.1-cc-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/llama3.1-cc-8B",
    "Model sha": "5269bb26f1afe005f144564f484e7554f185239f",
    "Average ‚¨ÜÔ∏è": 20.256041660753873,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9372374607461671,
    "IFEval Raw": 0.5068086011782071,
    "IFEval": 50.680860117820714,
    "BBH Raw": 0.4871187428614386,
    "BBH": 26.48381169342659,
    "MATH Lvl 5 Raw": 0.07099697885196375,
    "MATH Lvl 5": 7.099697885196375,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.38851041666666664,
    "MUSR": 6.497135416666668,
    "MMLU-PRO Raw": 0.3346908244680851,
    "MMLU-PRO": 26.076758274231683,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-09-14",
    "Generation": 1,
    "Base Model": "nbeerbower/llama3.1-cc-8B (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-bophades-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-bophades-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-bophades-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-bophades-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-bophades-12B",
    "Model sha": "252a358e099f77a0a28125e00a57aa3a107b3910",
    "Average ‚¨ÜÔ∏è": 24.84743410367204,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.052346504613404,
    "IFEval Raw": 0.6794405510711579,
    "IFEval": 67.94405510711579,
    "BBH Raw": 0.4988471515853883,
    "BBH": 29.543905352144947,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.41778125,
    "MUSR": 12.089322916666667,
    "MMLU-PRO Raw": 0.35006648936170215,
    "MMLU-PRO": 27.785165484633573,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-bophades-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-cc-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-cc-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-cc-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-cc-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-cc-12B",
    "Model sha": "fc32293e0b022d6daef9bfdb0c54d57a5226bf9a",
    "Average ‚¨ÜÔ∏è": 17.07752910499712,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4946224208050107,
    "IFEval Raw": 0.14353249378316202,
    "IFEval": 14.3532493783162,
    "BBH Raw": 0.5399409546487519,
    "BBH": 34.44654701952267,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.44236458333333334,
    "MUSR": 14.262239583333338,
    "MMLU-PRO Raw": 0.3597905585106383,
    "MMLU-PRO": 28.865617612293136,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-09-14",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-cc-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutades-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutades-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutades-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutades-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutades-12B",
    "Model sha": "5689f929808a6165f94ba43f872b944a4bdaaea3",
    "Average ‚¨ÜÔ∏è": 21.000395958546147,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8246200560013106,
    "IFEval Raw": 0.3425189608017837,
    "IFEval": 34.25189608017837,
    "BBH Raw": 0.5407194259684368,
    "BBH": 34.57440821872691,
    "MATH Lvl 5 Raw": 0.11329305135951662,
    "MATH Lvl 5": 11.329305135951662,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.4040416666666667,
    "MUSR": 8.671875000000004,
    "MMLU-PRO Raw": 0.3560505319148936,
    "MMLU-PRO": 28.45005910165484,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutades-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutenberg-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutenberg-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutenberg-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutenberg-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutenberg-12B",
    "Model sha": "6aeb6f769a53eb111839db8f439b614730e39593",
    "Average ‚¨ÜÔ∏è": 20.998978731179033,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5748145702468743,
    "IFEval Raw": 0.350386973231027,
    "IFEval": 35.0386973231027,
    "BBH Raw": 0.5281363707697807,
    "BBH": 32.43387434197657,
    "MATH Lvl 5 Raw": 0.11480362537764352,
    "MATH Lvl 5": 11.480362537764352,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.41706250000000006,
    "MUSR": 10.966145833333336,
    "MMLU-PRO Raw": 0.3562167553191489,
    "MMLU-PRO": 28.46852836879432,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutenberg-12B (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutenberg-12B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutenberg-12B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutenberg-12B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutenberg-12B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutenberg-12B-v2",
    "Model sha": "86bf9c105ff40835132e41699ac1a76ee0e5b683",
    "Average ‚¨ÜÔ∏è": 24.117038870452646,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.8709078021826144,
    "IFEval Raw": 0.6203395878491292,
    "IFEval": 62.033958784912926,
    "BBH Raw": 0.5397203788283472,
    "BBH": 34.73061633928093,
    "MATH Lvl 5 Raw": 0.02492447129909366,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.4286979166666667,
    "MUSR": 13.987239583333333,
    "MMLU-PRO Raw": 0.34990026595744683,
    "MMLU-PRO": 27.76669621749409,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutenberg-12B-v2 (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutenberg-12B-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutenberg-12B-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutenberg-12B-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutenberg-12B-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutenberg-12B-v3",
    "Model sha": "3e1a716281f23280abd72e402139c578faca175a",
    "Average ‚¨ÜÔ∏è": 19.177219082221118,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8353690488650236,
    "IFEval Raw": 0.21827085466562057,
    "IFEval": 21.827085466562057,
    "BBH Raw": 0.544065799051091,
    "BBH": 34.95791456295642,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.3145973154362416,
    "GPQA": 8.612975391498878,
    "MUSR Raw": 0.44503125,
    "MUSR": 14.995572916666662,
    "MMLU-PRO Raw": 0.3644448138297872,
    "MMLU-PRO": 29.38275709219858,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-15",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutenberg-12B-v3 (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutenberg-12B-v4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutenberg-12B-v4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutenberg-12B-v4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutenberg-12B-v4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutenberg-12B-v4",
    "Model sha": "59409afe585ae6945a588c867f879a9d31e571e6",
    "Average ‚¨ÜÔ∏è": 19.750864630385674,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.760461336781201,
    "IFEval Raw": 0.237929804031082,
    "IFEval": 23.7929804031082,
    "BBH Raw": 0.5269028864823667,
    "BBH": 31.97125827358258,
    "MATH Lvl 5 Raw": 0.12084592145015106,
    "MATH Lvl 5": 12.084592145015106,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.4104270833333333,
    "MUSR": 13.203385416666663,
    "MMLU-PRO Raw": 0.3575465425531915,
    "MMLU-PRO": 28.616282505910167,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutenberg-12B-v4 (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-gutenberg2-12B-test_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-gutenberg2-12B-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-gutenberg2-12B-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-gutenberg2-12B-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-gutenberg2-12B-test",
    "Model sha": "10da6150b0bedf8fd59206d72c4c0335ac665df3",
    "Average ‚¨ÜÔ∏è": 20.920227483354918,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.6750269708580445,
    "IFEval Raw": 0.33847192116916447,
    "IFEval": 33.847192116916446,
    "BBH Raw": 0.525477908630255,
    "BBH": 32.04475928596384,
    "MATH Lvl 5 Raw": 0.11329305135951663,
    "MATH Lvl 5": 11.329305135951664,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.4157291666666667,
    "MUSR": 10.966145833333336,
    "MMLU-PRO Raw": 0.35546875,
    "MMLU-PRO": 28.385416666666668,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-25",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-gutenberg2-12B-test (Merge)"
  },
  {
    "eval_name": "nbeerbower_mistral-nemo-wissenschaft-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbeerbower/mistral-nemo-wissenschaft-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbeerbower/mistral-nemo-wissenschaft-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbeerbower__mistral-nemo-wissenschaft-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbeerbower/mistral-nemo-wissenschaft-12B",
    "Model sha": "2480f9924415c72fe00ae9391bb15a6d05c889eb",
    "Average ‚¨ÜÔ∏è": 24.67911015497882,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.429373419444778,
    "IFEval Raw": 0.6520133246452745,
    "IFEval": 65.20133246452745,
    "BBH Raw": 0.5040306120993181,
    "BBH": 29.567999415715217,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.41778125,
    "MUSR": 12.289322916666665,
    "MMLU-PRO Raw": 0.35322473404255317,
    "MMLU-PRO": 28.13608156028369,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-12",
    "Submission Date": "2024-08-30",
    "Generation": 1,
    "Base Model": "nbeerbower/mistral-nemo-wissenschaft-12B (Merge)"
  },
  {
    "eval_name": "nbrahme_IndusQ_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nbrahme/IndusQ\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nbrahme/IndusQ</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nbrahme__IndusQ-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nbrahme/IndusQ",
    "Model sha": "d4224f753e6a2d6e7476752fb927c26c55ec9467",
    "Average ‚¨ÜÔ∏è": 5.623545926817777,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.15061686517706022,
    "IFEval Raw": 0.24397487555242311,
    "IFEval": 24.39748755524231,
    "BBH Raw": 0.30624035198474986,
    "BBH": 3.7470964959740556,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26510067114093966,
    "GPQA": 2.0134228187919545,
    "MUSR Raw": 0.3366354166666667,
    "MUSR": 2.2460937499999996,
    "MMLU-PRO Raw": 0.11203457446808511,
    "MMLU-PRO": 1.337174940898345,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "netcat420_MFANN-Llama3.1-Abliterated-SLERP-TIES-V2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-Llama3.1-Abliterated-SLERP-TIES-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-Llama3.1-Abliterated-SLERP-TIES-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-Llama3.1-Abliterated-SLERP-TIES-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-Llama3.1-Abliterated-SLERP-TIES-V2",
    "Model sha": "0e649dd355ad7d562f9346c96642c24eff35338e",
    "Average ‚¨ÜÔ∏è": 19.213728182261345,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7041129351341759,
    "IFEval Raw": 0.4209796672828096,
    "IFEval": 42.097966728280966,
    "BBH Raw": 0.49237606236472237,
    "BBH": 26.938370096724565,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.37276041666666665,
    "MUSR": 4.328385416666667,
    "MMLU-PRO Raw": 0.35222739361702127,
    "MMLU-PRO": 28.02526595744681,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-09",
    "Generation": 0,
    "Base Model": "netcat420/MFANN-Llama3.1-Abliterated-SLERP-TIES-V2"
  },
  {
    "eval_name": "netcat420_MFANN-Llama3.1-Abliterated-SLERP-V4_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-Llama3.1-Abliterated-SLERP-V4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-Llama3.1-Abliterated-SLERP-V4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-Llama3.1-Abliterated-SLERP-V4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-Llama3.1-Abliterated-SLERP-V4",
    "Model sha": "af160f1cf089ccbcbf00f99b951797a1f3daeb04",
    "Average ‚¨ÜÔ∏è": 19.412059138421036,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7224673074607508,
    "IFEval Raw": 0.41688275996577967,
    "IFEval": 41.68827599657797,
    "BBH Raw": 0.4908971108837563,
    "BBH": 26.706074443441803,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.38209374999999995,
    "MUSR": 5.861718750000001,
    "MMLU-PRO Raw": 0.35164561170212766,
    "MMLU-PRO": 27.960623522458622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-08",
    "Submission Date": "2024-11-09",
    "Generation": 0,
    "Base Model": "netcat420/MFANN-Llama3.1-Abliterated-SLERP-V4"
  },
  {
    "eval_name": "netcat420_MFANN-Llama3.1-Abliterated-Slerp-TIES_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-Llama3.1-Abliterated-Slerp-TIES\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-Llama3.1-Abliterated-Slerp-TIES</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-Llama3.1-Abliterated-Slerp-TIES-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-Llama3.1-Abliterated-Slerp-TIES",
    "Model sha": "dbe0a3b69206c042de2b0a96fc156feeecaa49c7",
    "Average ‚¨ÜÔ∏è": 19.134711811388055,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7733143983972721,
    "IFEval Raw": 0.42934746472692453,
    "IFEval": 42.93474647269245,
    "BBH Raw": 0.49675121796238325,
    "BBH": 27.59982935067644,
    "MATH Lvl 5 Raw": 0.05966767371601209,
    "MATH Lvl 5": 5.966767371601208,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.3686979166666667,
    "MUSR": 4.587239583333335,
    "MMLU-PRO Raw": 0.3531416223404255,
    "MMLU-PRO": 28.126846926713938,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-Llama3.1-Abliterated-Slerp-TIES (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-Llama3.1-Abliterated-Slerp-V3.2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-Llama3.1-Abliterated-Slerp-V3.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-Llama3.1-Abliterated-Slerp-V3.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-Llama3.1-Abliterated-Slerp-V3.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-Llama3.1-Abliterated-Slerp-V3.2",
    "Model sha": "56abb76e65cbf9dc49af662b09894d119d49705a",
    "Average ‚¨ÜÔ∏è": 18.915249675583805,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7414188488584108,
    "IFEval Raw": 0.41281134057633745,
    "IFEval": 41.28113405763375,
    "BBH Raw": 0.49782535474346185,
    "BBH": 27.774394299037056,
    "MATH Lvl 5 Raw": 0.061933534743202415,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.37542708333333336,
    "MUSR": 5.128385416666671,
    "MMLU-PRO Raw": 0.3527260638297872,
    "MMLU-PRO": 28.08067375886525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-Llama3.1-Abliterated-Slerp-V3.2 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-llama3.1-Abliterated-SLERP_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-llama3.1-Abliterated-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-llama3.1-Abliterated-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-llama3.1-Abliterated-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-llama3.1-Abliterated-SLERP",
    "Model sha": "0c7b2916727e6c28bbca2aa613b8247b66905915",
    "Average ‚¨ÜÔ∏è": 13.906809797539301,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7735791810879825,
    "IFEval Raw": 0.25906262051357065,
    "IFEval": 25.906262051357064,
    "BBH Raw": 0.45744999460878283,
    "BBH": 22.28062513418979,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.3809166666666666,
    "MUSR": 5.714583333333336,
    "MMLU-PRO Raw": 0.2928025265957447,
    "MMLU-PRO": 21.42250295508274,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-25",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-llama3.1-Abliterated-SLERP (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-llama3.1-abliterated-SLERP-v3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-llama3.1-abliterated-SLERP-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-llama3.1-abliterated-SLERP-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-llama3.1-abliterated-SLERP-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-llama3.1-abliterated-SLERP-v3",
    "Model sha": "f90a20024060942826302c30860572c227dd4013",
    "Average ‚¨ÜÔ∏è": 18.08002587640519,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7915696524463773,
    "IFEval Raw": 0.37993856301280604,
    "IFEval": 37.9938563012806,
    "BBH Raw": 0.49305765460927126,
    "BBH": 27.1872703942033,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.36603125000000003,
    "MUSR": 3.053906250000002,
    "MMLU-PRO Raw": 0.35305851063829785,
    "MMLU-PRO": 28.117612293144205,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-llama3.1-abliterated-SLERP-v3 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-llama3.1-abliterated-SLERP-v3.1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-llama3.1-abliterated-SLERP-v3.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-llama3.1-abliterated-SLERP-v3.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-llama3.1-abliterated-SLERP-v3.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-llama3.1-abliterated-SLERP-v3.1",
    "Model sha": "6d306eb66466cb8e1456a36f3895890a117e91e4",
    "Average ‚¨ÜÔ∏è": 19.029173646219327,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.749807898283468,
    "IFEval Raw": 0.4201551882338861,
    "IFEval": 42.01551882338861,
    "BBH Raw": 0.492068920606988,
    "BBH": 27.026315532744473,
    "MATH Lvl 5 Raw": 0.07326283987915409,
    "MATH Lvl 5": 7.326283987915409,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3686354166666667,
    "MUSR": 3.846093750000001,
    "MMLU-PRO Raw": 0.3543051861702128,
    "MMLU-PRO": 28.25613179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-llama3.1-abliterated-SLERP-v3.1 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-llama3.1-abliterated-v2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-llama3.1-abliterated-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-llama3.1-abliterated-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-llama3.1-abliterated-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-llama3.1-abliterated-v2",
    "Model sha": "3d0a5d3634726e1a63ac84bee561b346960ca1d7",
    "Average ‚¨ÜÔ∏è": 19.74593509633809,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.824523577087792,
    "IFEval Raw": 0.4429114748866341,
    "IFEval": 44.29114748866341,
    "BBH Raw": 0.4940829733015402,
    "BBH": 27.35361826731554,
    "MATH Lvl 5 Raw": 0.07250755287009063,
    "MATH Lvl 5": 7.250755287009063,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3845416666666666,
    "MUSR": 6.201041666666668,
    "MMLU-PRO Raw": 0.3490691489361702,
    "MMLU-PRO": 27.67434988179669,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-llama3.1-abliterated-v2 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN-phigments-slerp-V2_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN-phigments-slerp-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN-phigments-slerp-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN-phigments-slerp-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN-phigments-slerp-V2",
    "Model sha": "94596dab22ab78f0d2ec00b8e33c8fa98581ad0f",
    "Average ‚¨ÜÔ∏è": 16.004357709615434,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.40812000230946677,
    "IFEval Raw": 0.32316032571355113,
    "IFEval": 32.316032571355116,
    "BBH Raw": 0.48272762171598743,
    "BBH": 26.927491544080876,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.40372916666666664,
    "MUSR": 13.099479166666663,
    "MMLU-PRO Raw": 0.2716921542553192,
    "MMLU-PRO": 19.076906028368796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "netcat420/MFANN-phigments-slerp-V2 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.15_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.15\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.15</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.15-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.15",
    "Model sha": "20dbdfb9154cc2f6d43651fc8cea63a120220dc7",
    "Average ‚¨ÜÔ∏è": 11.811262006601295,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.46713796071127217,
    "IFEval Raw": 0.2012105657433388,
    "IFEval": 20.12105657433388,
    "BBH Raw": 0.453931293669888,
    "BBH": 23.46934667082661,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.3957916666666667,
    "MUSR": 8.773958333333335,
    "MMLU-PRO Raw": 0.24684175531914893,
    "MMLU-PRO": 16.315750591016545,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-04",
    "Submission Date": "2024-07-05",
    "Generation": 0,
    "Base Model": "netcat420/MFANN3bv0.15"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.18_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.18\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.18</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.18-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.18",
    "Model sha": "3e792e3413217b63ea9caa0e8b8595fbeb236a69",
    "Average ‚¨ÜÔ∏è": 12.574347554884428,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4825136794325669,
    "IFEval Raw": 0.22064455644356973,
    "IFEval": 22.064455644356972,
    "BBH Raw": 0.4514366169824164,
    "BBH": 23.07340376774899,
    "MATH Lvl 5 Raw": 0.020392749244712995,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.40236458333333336,
    "MUSR": 10.595572916666669,
    "MMLU-PRO Raw": 0.25,
    "MMLU-PRO": 16.666666666666664,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-25",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "netcat420/MFANN3bv0.18"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.19_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.19\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.19</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.19-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.19",
    "Model sha": "073d42274686f5cb6ef6ff9f6ade24eab198e1f2",
    "Average ‚¨ÜÔ∏è": 12.503371896552709,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.48648825822643327,
    "IFEval Raw": 0.22581528123157665,
    "IFEval": 22.581528123157664,
    "BBH Raw": 0.4515800678058734,
    "BBH": 22.9070546869096,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.40239583333333334,
    "MUSR": 9.899479166666667,
    "MMLU-PRO Raw": 0.25199468085106386,
    "MMLU-PRO": 16.888297872340427,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-04",
    "Submission Date": "2024-08-08",
    "Generation": 0,
    "Base Model": "netcat420/MFANN3bv0.19"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.20_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.20\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.20</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.20-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.20",
    "Model sha": "ac8ba24559cbdb5704d77b602580d911c265fdee",
    "Average ‚¨ÜÔ∏è": 12.383183400006937,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5094182102556662,
    "IFEval Raw": 0.21934578030736224,
    "IFEval": 21.934578030736223,
    "BBH Raw": 0.4493365019423472,
    "BBH": 22.790710795250106,
    "MATH Lvl 5 Raw": 0.015105740181268881,
    "MATH Lvl 5": 1.510574018126888,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.4077291666666667,
    "MUSR": 10.166145833333335,
    "MMLU-PRO Raw": 0.25,
    "MMLU-PRO": 16.666666666666664,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-29",
    "Submission Date": "2024-08-29",
    "Generation": 2,
    "Base Model": "netcat420/MFANN3bv0.19.12 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.21_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.21\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.21</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.21-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.21",
    "Model sha": "8e78416dce916b69247fa03bd587369d0dade5ed",
    "Average ‚¨ÜÔ∏è": 11.703841983980945,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4467861409038123,
    "IFEval Raw": 0.19151850423542865,
    "IFEval": 19.151850423542868,
    "BBH Raw": 0.44700236898039053,
    "BBH": 22.583425716572332,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.37594791666666666,
    "MUSR": 9.82682291666667,
    "MMLU-PRO Raw": 0.23927859042553193,
    "MMLU-PRO": 15.475398936170212,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "netcat420/MFANN3bv0.21 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.22_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.22\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.22</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.22-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.22",
    "Model sha": "20c26f267ebe62ef1da037a5b840a304cb8d740b",
    "Average ‚¨ÜÔ∏è": 11.91662723317865,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3956681264353288,
    "IFEval Raw": 0.1979381374752324,
    "IFEval": 19.793813747523238,
    "BBH Raw": 0.44851095830051274,
    "BBH": 22.49153679693963,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.35213541666666665,
    "MUSR": 10.183593749999996,
    "MMLU-PRO Raw": 0.2517453457446808,
    "MMLU-PRO": 16.860593971631204,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-26",
    "Generation": 0,
    "Base Model": "netcat420/MFANN3bv0.22"
  },
  {
    "eval_name": "netcat420_MFANN3bv0.23_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANN3bv0.23\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANN3bv0.23</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANN3bv0.23-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANN3bv0.23",
    "Model sha": "93eacd43dcb307016e22a4d9f9f8deef49cd9111",
    "Average ‚¨ÜÔ∏è": 11.18367554752539,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.38818373618121066,
    "IFEval Raw": 0.20480768804549704,
    "IFEval": 20.480768804549704,
    "BBH Raw": 0.44954178056127364,
    "BBH": 22.696340563264982,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.3427395833333333,
    "MUSR": 7.042447916666667,
    "MMLU-PRO Raw": 0.2417719414893617,
    "MMLU-PRO": 15.752437943262413,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-07",
    "Generation": 0,
    "Base Model": "netcat420/MFANN3bv0.23"
  },
  {
    "eval_name": "netcat420_MFANNv0.19_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.19\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.19</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.19-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.19",
    "Model sha": "af26a25549b7ad291766c479bebda58f15fbff42",
    "Average ‚¨ÜÔ∏è": 14.187655963423175,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9570787635438418,
    "IFEval Raw": 0.30567449921763146,
    "IFEval": 30.567449921763146,
    "BBH Raw": 0.47313832038755316,
    "BBH": 24.92410586579366,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.35269791666666667,
    "MUSR": 2.720572916666667,
    "MMLU-PRO Raw": 0.24725731382978725,
    "MMLU-PRO": 16.36192375886525,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-27",
    "Generation": 0,
    "Base Model": "netcat420/MFANNv0.19"
  },
  {
    "eval_name": "netcat420_MFANNv0.20_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.20\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.20</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.20-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.20",
    "Model sha": "e612e57c933870b8990ac2bc217c434f3ffc84bd",
    "Average ‚¨ÜÔ∏è": 16.524597395978102,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8678840987838602,
    "IFEval Raw": 0.34786477657061043,
    "IFEval": 34.78647765706104,
    "BBH Raw": 0.4574431878198548,
    "BBH": 22.401696904581673,
    "MATH Lvl 5 Raw": 0.05362537764350453,
    "MATH Lvl 5": 5.362537764350453,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.38739583333333333,
    "MUSR": 6.757812500000003,
    "MMLU-PRO Raw": 0.32022938829787234,
    "MMLU-PRO": 24.469932033096924,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-08-08",
    "Generation": 0,
    "Base Model": "netcat420/MFANNv0.20"
  },
  {
    "eval_name": "netcat420_MFANNv0.21_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.21\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.21</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.21-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.21",
    "Model sha": "8c71d0eb419f54c489fa1ddf55d4bd18a1fb27d8",
    "Average ‚¨ÜÔ∏è": 15.898755134702986,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8794107327163433,
    "IFEval Raw": 0.3233099287667832,
    "IFEval": 32.33099287667832,
    "BBH Raw": 0.45763723048372523,
    "BBH": 22.058431786302453,
    "MATH Lvl 5 Raw": 0.0581570996978852,
    "MATH Lvl 5": 5.81570996978852,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.3993333333333333,
    "MUSR": 8.81666666666667,
    "MMLU-PRO Raw": 0.3031083776595745,
    "MMLU-PRO": 22.567597517730498,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-31",
    "Submission Date": "2024-09-02",
    "Generation": 2,
    "Base Model": "netcat420/MFANNv0.20.12 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANNv0.22.1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.22.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.22.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.22.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.22.1",
    "Model sha": "98108142480b802a3e1bb27e3d47075a4ea3a4f1",
    "Average ‚¨ÜÔ∏è": 15.717729726717332,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.840528673499928,
    "IFEval Raw": 0.3089469274857378,
    "IFEval": 30.894692748573785,
    "BBH Raw": 0.46608928527824584,
    "BBH": 23.602792666118614,
    "MATH Lvl 5 Raw": 0.056646525679758315,
    "MATH Lvl 5": 5.664652567975832,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3753020833333333,
    "MUSR": 4.64609375,
    "MMLU-PRO Raw": 0.33427526595744683,
    "MMLU-PRO": 26.03058510638298,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "netcat420/MFANNv0.22.1 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANNv0.23_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.23\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.23</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.23-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.23",
    "Model sha": "cf7fb44a8c858602d7fcba58adcbd514c7e08ba4",
    "Average ‚¨ÜÔ∏è": 16.652655864272365,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8103797116816797,
    "IFEval Raw": 0.3127435205255389,
    "IFEval": 31.274352052553894,
    "BBH Raw": 0.4898102063834755,
    "BBH": 27.04234546686432,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.3767916666666667,
    "MUSR": 5.498958333333338,
    "MMLU-PRO Raw": 0.33876329787234044,
    "MMLU-PRO": 26.529255319148938,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "netcat420/MFANNv0.23 (Merge)"
  },
  {
    "eval_name": "netcat420_MFANNv0.24_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/netcat420/MFANNv0.24\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">netcat420/MFANNv0.24</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/netcat420__MFANNv0.24-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "netcat420/MFANNv0.24",
    "Model sha": "57ce382fede1adce68bdb95a386255fa363077d7",
    "Average ‚¨ÜÔ∏è": 16.398373723349362,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7439026643676916,
    "IFEval Raw": 0.3162409074588758,
    "IFEval": 31.624090745887578,
    "BBH Raw": 0.479027491915232,
    "BBH": 25.3517249924116,
    "MATH Lvl 5 Raw": 0.06117824773413897,
    "MATH Lvl 5": 6.117824773413897,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.3753958333333333,
    "MUSR": 4.624479166666667,
    "MMLU-PRO Raw": 0.3347739361702128,
    "MMLU-PRO": 26.08599290780142,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-09",
    "Generation": 1,
    "Base Model": "netcat420/MFANNv0.24 (Merge)"
  },
  {
    "eval_name": "newsbang_Homer-7B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/newsbang/Homer-7B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">newsbang/Homer-7B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/newsbang__Homer-7B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "newsbang/Homer-7B-v0.1",
    "Model sha": "c953cc313ef5e5029efd057c0d3809a3b8d1cf9f",
    "Average ‚¨ÜÔ∏è": 31.333865899561875,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6907341953287501,
    "IFEval Raw": 0.6108724850064495,
    "IFEval": 61.087248500644954,
    "BBH Raw": 0.5601389961416444,
    "BBH": 37.30922654202453,
    "MATH Lvl 5 Raw": 0.2824773413897281,
    "MATH Lvl 5": 28.247734138972806,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.43569791666666663,
    "MUSR": 12.79557291666667,
    "MMLU-PRO Raw": 0.4474734042553192,
    "MMLU-PRO": 38.60815602836879,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 0,
    "Base Model": "newsbang/Homer-7B-v0.1"
  },
  {
    "eval_name": "newsbang_Homer-7B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/newsbang/Homer-7B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">newsbang/Homer-7B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/newsbang__Homer-7B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "newsbang/Homer-7B-v0.2",
    "Model sha": "50b4ca941657ed362f5660aed8274a59a6b3fe2d",
    "Average ‚¨ÜÔ∏è": 33.1146630134203,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6745983422890776,
    "IFEval Raw": 0.7493827488840721,
    "IFEval": 74.93827488840721,
    "BBH Raw": 0.5517330182832224,
    "BBH": 36.4034863975643,
    "MATH Lvl 5 Raw": 0.2537764350453172,
    "MATH Lvl 5": 25.377643504531722,
    "GPQA Raw": 0.33221476510067116,
    "GPQA": 10.96196868008949,
    "MUSR Raw": 0.42975,
    "MUSR": 13.118750000000004,
    "MMLU-PRO Raw": 0.4409906914893617,
    "MMLU-PRO": 37.88785460992907,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-15",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "newsbang_Homer-v0.3-Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/newsbang/Homer-v0.3-Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">newsbang/Homer-v0.3-Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/newsbang__Homer-v0.3-Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "newsbang/Homer-v0.3-Qwen2.5-7B",
    "Model sha": "4fa38c6c590d8e9bbf2075b2fa9cc37e75cde5d4",
    "Average ‚¨ÜÔ∏è": 31.088202729238493,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5856034816323977,
    "IFEval Raw": 0.5154013572875525,
    "IFEval": 51.54013572875526,
    "BBH Raw": 0.5480594290467807,
    "BBH": 36.41367722607503,
    "MATH Lvl 5 Raw": 0.29531722054380666,
    "MATH Lvl 5": 29.531722054380666,
    "GPQA Raw": 0.3338926174496644,
    "GPQA": 11.185682326621922,
    "MUSR Raw": 0.47436458333333337,
    "MUSR": 19.462239583333332,
    "MMLU-PRO Raw": 0.445561835106383,
    "MMLU-PRO": 38.39575945626477,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "newsbang_Homer-v0.4-Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/newsbang/Homer-v0.4-Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">newsbang/Homer-v0.4-Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/newsbang__Homer-v0.4-Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "newsbang/Homer-v0.4-Qwen2.5-7B",
    "Model sha": "e5b73b06e63de7f77845463f8a11c93e82befd15",
    "Average ‚¨ÜÔ∏è": 33.91883651932795,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6397204140166718,
    "IFEval Raw": 0.799940823681166,
    "IFEval": 79.9940823681166,
    "BBH Raw": 0.5533099174800821,
    "BBH": 36.6037028382434,
    "MATH Lvl 5 Raw": 0.2764350453172205,
    "MATH Lvl 5": 27.64350453172205,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.4310833333333333,
    "MUSR": 13.185416666666669,
    "MMLU-PRO Raw": 0.4362533244680851,
    "MMLU-PRO": 37.3614804964539,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-11-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "newsbang_Homer-v0.5-Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/newsbang/Homer-v0.5-Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">newsbang/Homer-v0.5-Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/newsbang__Homer-v0.5-Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "newsbang/Homer-v0.5-Qwen2.5-7B",
    "Model sha": "9dc7090b2226f9a2217f593518f734e3246001f9",
    "Average ‚¨ÜÔ∏è": 34.600198999890175,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6725841884959783,
    "IFEval Raw": 0.7880756393037142,
    "IFEval": 78.80756393037142,
    "BBH Raw": 0.5540181073562815,
    "BBH": 36.67808911980736,
    "MATH Lvl 5 Raw": 0.36253776435045315,
    "MATH Lvl 5": 36.25377643504532,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.41930208333333335,
    "MUSR": 11.379427083333335,
    "MMLU-PRO Raw": 0.4369182180851064,
    "MMLU-PRO": 37.43535756501182,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "newsbang/Homer-v0.5-Qwen2.5-7B"
  },
  {
    "eval_name": "nguyentd_FinancialAdvice-Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nguyentd/FinancialAdvice-Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nguyentd/FinancialAdvice-Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nguyentd__FinancialAdvice-Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nguyentd/FinancialAdvice-Qwen2.5-7B",
    "Model sha": "5c3421d5a980d0b2365b0d704ead30c9e534a019",
    "Average ‚¨ÜÔ∏è": 20.93546505358822,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6544450448327062,
    "IFEval Raw": 0.449605934476079,
    "IFEval": 44.960593447607906,
    "BBH Raw": 0.4730934153895792,
    "BBH": 25.630435622160334,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.40248958333333335,
    "MUSR": 9.144531250000002,
    "MMLU-PRO Raw": 0.375249335106383,
    "MMLU-PRO": 30.583259456264773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-21",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "nguyentd/FinancialAdvice-Qwen2.5-7B (Merge)"
  },
  {
    "eval_name": "nhyha_N3N_Delirium-v1_1030_0227_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nhyha/N3N_Delirium-v1_1030_0227\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nhyha/N3N_Delirium-v1_1030_0227</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nhyha__N3N_Delirium-v1_1030_0227-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nhyha/N3N_Delirium-v1_1030_0227",
    "Model sha": "41eabc719bd611e2bd0094b0842df84916a57a46",
    "Average ‚¨ÜÔ∏è": 31.143320415940952,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.131856000028935,
    "IFEval Raw": 0.8022890375315275,
    "IFEval": 80.22890375315274,
    "BBH Raw": 0.5890686677822234,
    "BBH": 40.77504007448568,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.337248322147651,
    "GPQA": 11.633109619686799,
    "MUSR Raw": 0.40981249999999997,
    "MUSR": 9.859895833333335,
    "MMLU-PRO Raw": 0.41497672872340424,
    "MMLU-PRO": 34.99741430260047,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-04",
    "Generation": 2,
    "Base Model": "unsloth/gemma-2-9b-it"
  },
  {
    "eval_name": "nhyha_N3N_Llama-3.1-8B-Instruct_1028_0216_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nhyha/N3N_Llama-3.1-8B-Instruct_1028_0216\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nhyha/N3N_Llama-3.1-8B-Instruct_1028_0216</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nhyha__N3N_Llama-3.1-8B-Instruct_1028_0216-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nhyha/N3N_Llama-3.1-8B-Instruct_1028_0216",
    "Model sha": "d0715a631898112c9c3b729d0334588a2ff636d8",
    "Average ‚¨ÜÔ∏è": 23.403603725573575,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7495062636779843,
    "IFEval Raw": 0.47808259861611635,
    "IFEval": 47.80825986161163,
    "BBH Raw": 0.5053741309920361,
    "BBH": 28.980464124810663,
    "MATH Lvl 5 Raw": 0.16767371601208458,
    "MATH Lvl 5": 16.76737160120846,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.40503125,
    "MUSR": 10.062239583333335,
    "MMLU-PRO Raw": 0.36377992021276595,
    "MMLU-PRO": 29.308880023640665,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-11-04",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "nhyha_N3N_gemma-2-9b-it_20241029_1532_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nhyha/N3N_gemma-2-9b-it_20241029_1532\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nhyha/N3N_gemma-2-9b-it_20241029_1532</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nhyha__N3N_gemma-2-9b-it_20241029_1532-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nhyha/N3N_gemma-2-9b-it_20241029_1532",
    "Model sha": "6cfc55a717961ef206978b577bd74df97efe1455",
    "Average ‚¨ÜÔ∏è": 32.022249259418224,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.394044198507278,
    "IFEval Raw": 0.6751940407008958,
    "IFEval": 67.51940407008958,
    "BBH Raw": 0.5863124381827675,
    "BBH": 40.9866677332497,
    "MATH Lvl 5 Raw": 0.20468277945619334,
    "MATH Lvl 5": 20.468277945619334,
    "GPQA Raw": 0.34060402684563756,
    "GPQA": 12.080536912751676,
    "MUSR Raw": 0.4593541666666667,
    "MUSR": 16.3859375,
    "MMLU-PRO Raw": 0.4122340425531915,
    "MMLU-PRO": 34.692671394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-29",
    "Submission Date": "2024-11-04",
    "Generation": 1,
    "Base Model": "unsloth/gemma-2-9b-it"
  },
  {
    "eval_name": "nhyha_N3N_gemma-2-9b-it_20241110_2026_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nhyha/N3N_gemma-2-9b-it_20241110_2026\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nhyha/N3N_gemma-2-9b-it_20241110_2026</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nhyha__N3N_gemma-2-9b-it_20241110_2026-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nhyha/N3N_gemma-2-9b-it_20241110_2026",
    "Model sha": "2d4c24278ed9d8b42a4035da16a5aea745797441",
    "Average ‚¨ÜÔ∏è": 28.741940791571803,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.5405499103252285,
    "IFEval Raw": 0.6282829558903709,
    "IFEval": 62.82829558903709,
    "BBH Raw": 0.5867149609980419,
    "BBH": 40.944105549057745,
    "MATH Lvl 5 Raw": 0.13821752265861026,
    "MATH Lvl 5": 13.821752265861026,
    "GPQA Raw": 0.33640939597315433,
    "GPQA": 11.521252796420578,
    "MUSR Raw": 0.40730208333333334,
    "MUSR": 9.779427083333337,
    "MMLU-PRO Raw": 0.40201130319148937,
    "MMLU-PRO": 33.55681146572104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "unsloth/gemma-2-9b-it"
  },
  {
    "eval_name": "nhyha_merge_Qwen2.5-7B-Instruct_20241023_0314_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nhyha/merge_Qwen2.5-7B-Instruct_20241023_0314\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nhyha/merge_Qwen2.5-7B-Instruct_20241023_0314</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nhyha__merge_Qwen2.5-7B-Instruct_20241023_0314-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nhyha/merge_Qwen2.5-7B-Instruct_20241023_0314",
    "Model sha": "4d93f65c1f870556f05c77a1ef4f26819d49daf7",
    "Average ‚¨ÜÔ∏è": 29.208864941196907,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6957007877403425,
    "IFEval Raw": 0.5694568190179834,
    "IFEval": 56.94568190179834,
    "BBH Raw": 0.5558529241660143,
    "BBH": 36.36518528982388,
    "MATH Lvl 5 Raw": 0.21978851963746224,
    "MATH Lvl 5": 21.978851963746223,
    "GPQA Raw": 0.3213087248322148,
    "GPQA": 9.507829977628639,
    "MUSR Raw": 0.42506249999999995,
    "MUSR": 11.099479166666669,
    "MMLU-PRO Raw": 0.45420545212765956,
    "MMLU-PRO": 39.35616134751773,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-11-04",
    "Generation": 3,
    "Base Model": "Qwen/Qwen2.5-7B"
  },
  {
    "eval_name": "nidum_Nidum-Limitless-Gemma-2B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nidum/Nidum-Limitless-Gemma-2B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nidum/Nidum-Limitless-Gemma-2B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nidum__Nidum-Limitless-Gemma-2B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nidum/Nidum-Limitless-Gemma-2B",
    "Model sha": "e209e3513d2b34c0e6c433ede26e17604c25cb1a",
    "Average ‚¨ÜÔ∏è": 5.9394218485634305,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3968136960913782,
    "IFEval Raw": 0.24235140538216376,
    "IFEval": 24.235140538216378,
    "BBH Raw": 0.3078801520076317,
    "BBH": 3.4510601516101125,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.37403125,
    "MUSR": 4.120572916666667,
    "MMLU-PRO Raw": 0.11735372340425532,
    "MMLU-PRO": 1.9281914893617011,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-02",
    "Submission Date": "2024-08-07",
    "Generation": 0,
    "Base Model": "nidum/Nidum-Limitless-Gemma-2B"
  },
  {
    "eval_name": "nisten_franqwenstein-35b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nisten/franqwenstein-35b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nisten/franqwenstein-35b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nisten__franqwenstein-35b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nisten/franqwenstein-35b",
    "Model sha": "7180aa73e82945a1d2ae0eb304508e21d57e4c27",
    "Average ‚¨ÜÔ∏è": 35.94192578741262,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.017769702277288,
    "IFEval Raw": 0.37986320740080765,
    "IFEval": 37.986320740080764,
    "BBH Raw": 0.6646579178049268,
    "BBH": 52.227468077653526,
    "MATH Lvl 5 Raw": 0.3028700906344411,
    "MATH Lvl 5": 30.28700906344411,
    "GPQA Raw": 0.4035234899328859,
    "GPQA": 20.46979865771812,
    "MUSR Raw": 0.49402083333333335,
    "MUSR": 22.119270833333328,
    "MMLU-PRO Raw": 0.5730551861702128,
    "MMLU-PRO": 52.56168735224587,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "nisten/franqwenstein-35b (Merge)"
  },
  {
    "eval_name": "nisten_franqwenstein-35b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nisten/franqwenstein-35b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nisten/franqwenstein-35b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nisten__franqwenstein-35b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nisten/franqwenstein-35b",
    "Model sha": "901351a987d664a1cd7f483115a167d3ae5694ec",
    "Average ‚¨ÜÔ∏è": 34.451116831224226,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.328604130163559,
    "IFEval Raw": 0.39135383005979685,
    "IFEval": 39.13538300597969,
    "BBH Raw": 0.6591132598701116,
    "BBH": 51.68027687329707,
    "MATH Lvl 5 Raw": 0.304380664652568,
    "MATH Lvl 5": 30.438066465256803,
    "GPQA Raw": 0.35906040268456374,
    "GPQA": 14.541387024608499,
    "MUSR Raw": 0.4681041666666667,
    "MUSR": 19.6796875,
    "MMLU-PRO Raw": 0.5610871010638298,
    "MMLU-PRO": 51.23190011820331,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "nisten/franqwenstein-35b (Merge)"
  },
  {
    "eval_name": "nlpguy_Mistral-NeMo-Minitron-Upscale-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nlpguy/Mistral-NeMo-Minitron-Upscale-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nlpguy/Mistral-NeMo-Minitron-Upscale-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nlpguy__Mistral-NeMo-Minitron-Upscale-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nlpguy/Mistral-NeMo-Minitron-Upscale-v1",
    "Model sha": "9e6d747cbb81e1f25915a0f42802cbeb85b61c3e",
    "Average ‚¨ÜÔ∏è": 10.876929744617456,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9343057996867397,
    "IFEval Raw": 0.16484040124647048,
    "IFEval": 16.484040124647045,
    "BBH Raw": 0.44679984097967057,
    "BBH": 22.06890968577206,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3803541666666667,
    "MUSR": 4.8442708333333355,
    "MMLU-PRO Raw": 0.2537400265957447,
    "MMLU-PRO": 17.082225177304963,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "nlpguy/Mistral-NeMo-Minitron-Upscale-v1 (Merge)"
  },
  {
    "eval_name": "nlpguy_Mistral-NeMo-Minitron-Upscale-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nlpguy/Mistral-NeMo-Minitron-Upscale-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nlpguy/Mistral-NeMo-Minitron-Upscale-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nlpguy__Mistral-NeMo-Minitron-Upscale-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nlpguy/Mistral-NeMo-Minitron-Upscale-v2",
    "Model sha": "4ac077e496705687fdcbe51f3b915be42e91bf79",
    "Average ‚¨ÜÔ∏è": 8.232151046005278,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9246186706753226,
    "IFEval Raw": 0.15727159492369136,
    "IFEval": 15.727159492369136,
    "BBH Raw": 0.3949668154807224,
    "BBH": 14.382673288078204,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.3790833333333334,
    "MUSR": 5.252083333333334,
    "MMLU-PRO Raw": 0.1926529255319149,
    "MMLU-PRO": 10.2947695035461,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "nlpguy/Mistral-NeMo-Minitron-Upscale-v2 (Merge)"
  },
  {
    "eval_name": "nlpguy_Mistral-NeMo-Minitron-Upscale-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nlpguy/Mistral-NeMo-Minitron-Upscale-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nlpguy/Mistral-NeMo-Minitron-Upscale-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nlpguy__Mistral-NeMo-Minitron-Upscale-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nlpguy/Mistral-NeMo-Minitron-Upscale-v3",
    "Model sha": "6703b09d3d78cc020448ee93c53dc727312bcbaf",
    "Average ‚¨ÜÔ∏è": 5.013437438056034,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.0446685795552275,
    "IFEval Raw": 0.14120976786038822,
    "IFEval": 14.120976786038822,
    "BBH Raw": 0.30524522602918064,
    "BBH": 3.3982664477164874,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.40984375,
    "MUSR": 9.430468750000001,
    "MMLU-PRO Raw": 0.11710438829787234,
    "MMLU-PRO": 1.9004875886524817,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-04",
    "Generation": 1,
    "Base Model": "nlpguy/Mistral-NeMo-Minitron-Upscale-v3 (Merge)"
  },
  {
    "eval_name": "nlpguy_StableProse_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nlpguy/StableProse\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nlpguy/StableProse</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nlpguy__StableProse-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nlpguy/StableProse",
    "Model sha": "4937dc747684705e4b87df27b47eab5429f3a9c1",
    "Average ‚¨ÜÔ∏è": 16.422494931847897,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.794363055741438,
    "IFEval Raw": 0.19723888172271792,
    "IFEval": 19.723888172271792,
    "BBH Raw": 0.5116558625577087,
    "BBH": 30.180202714185956,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.4067083333333333,
    "MUSR": 8.871875000000001,
    "MMLU-PRO Raw": 0.3468251329787234,
    "MMLU-PRO": 27.425014775413715,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-16",
    "Submission Date": "2024-08-17",
    "Generation": 1,
    "Base Model": "nlpguy/StableProse (Merge)"
  },
  {
    "eval_name": "nlpguy_StarFusion-alpha1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nlpguy/StarFusion-alpha1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nlpguy/StarFusion-alpha1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nlpguy__StarFusion-alpha1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nlpguy/StarFusion-alpha1",
    "Model sha": "dccad965a710d7bee001b6387c8307e7c320291e",
    "Average ‚¨ÜÔ∏è": 20.840911669844324,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1744057403627077,
    "IFEval Raw": 0.5660092997690572,
    "IFEval": 56.60092997690572,
    "BBH Raw": 0.4428694115507034,
    "BBH": 21.933181635654744,
    "MATH Lvl 5 Raw": 0.07250755287009064,
    "MATH Lvl 5": 7.250755287009064,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.40810416666666666,
    "MUSR": 8.879687500000001,
    "MMLU-PRO Raw": 0.3190658244680851,
    "MMLU-PRO": 24.34064716312057,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-13",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "nlpguy/StarFusion-alpha1 (Merge)"
  },
  {
    "eval_name": "nothingiisreal_MN-12B-Starcannon-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nothingiisreal/MN-12B-Starcannon-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nothingiisreal/MN-12B-Starcannon-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nothingiisreal__MN-12B-Starcannon-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nothingiisreal/MN-12B-Starcannon-v2",
    "Model sha": "f2ff756e8c32d9107d4f6a3c18c730e3fe0cae88",
    "Average ‚¨ÜÔ∏è": 18.030392502894443,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7226627845551055,
    "IFEval Raw": 0.3925273828995953,
    "IFEval": 39.252738289959524,
    "BBH Raw": 0.5004499888471767,
    "BBH": 28.424782963573875,
    "MATH Lvl 5 Raw": 0.050604229607250764,
    "MATH Lvl 5": 5.060422960725076,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.39781249999999996,
    "MUSR": 7.993229166666668,
    "MMLU-PRO Raw": 0.31283244680851063,
    "MMLU-PRO": 23.648049645390067,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nothingiisreal/MN-12B-Starcannon-v2 (Merge)"
  },
  {
    "eval_name": "nothingiisreal_MN-12B-Starcannon-v3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nothingiisreal/MN-12B-Starcannon-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nothingiisreal/MN-12B-Starcannon-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nothingiisreal__MN-12B-Starcannon-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nothingiisreal/MN-12B-Starcannon-v3",
    "Model sha": "169480b62121c4f070e93a05158545c679712644",
    "Average ‚¨ÜÔ∏è": 18.993413862426948,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7456709308793783,
    "IFEval Raw": 0.38073755413414184,
    "IFEval": 38.07375541341418,
    "BBH Raw": 0.5170553444795719,
    "BBH": 30.873001626388618,
    "MATH Lvl 5 Raw": 0.0687311178247734,
    "MATH Lvl 5": 6.87311178247734,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.40463541666666664,
    "MUSR": 9.846093749999996,
    "MMLU-PRO Raw": 0.32646276595744683,
    "MMLU-PRO": 25.162529550827422,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-03",
    "Generation": 1,
    "Base Model": "nothingiisreal/MN-12B-Starcannon-v3 (Merge)"
  },
  {
    "eval_name": "nvidia_Llama-3.1-Minitron-4B-Depth-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Llama-3.1-Minitron-4B-Depth-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Llama-3.1-Minitron-4B-Depth-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Llama-3.1-Minitron-4B-Depth-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Llama-3.1-Minitron-4B-Depth-Base",
    "Model sha": "40d82bc951b4f39e9c9e11176334250c30975098",
    "Average ‚¨ÜÔ∏è": 11.532169975273654,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 19,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.46769063580522896,
    "IFEval Raw": 0.16069362624502986,
    "IFEval": 16.069362624502986,
    "BBH Raw": 0.4170704193104893,
    "BBH": 19.44410955550794,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.40106250000000004,
    "MUSR": 10.699479166666668,
    "MMLU-PRO Raw": 0.2798371010638298,
    "MMLU-PRO": 19.98190011820331,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-13",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "nvidia/Llama-3.1-Minitron-4B-Depth-Base"
  },
  {
    "eval_name": "nvidia_Llama-3.1-Nemotron-70B-Instruct-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Llama-3.1-Nemotron-70B-Instruct-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Llama-3.1-Nemotron-70B-Instruct-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF",
    "Model sha": "250db5cf2323e04a6d2025a2ca2b94a95c439e88",
    "Average ‚¨ÜÔ∏è": 34.578371538222314,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1700,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 13.628747633601261,
    "IFEval Raw": 0.7380672172059026,
    "IFEval": 73.80672172059025,
    "BBH Raw": 0.6316000668895038,
    "BBH": 47.10953049372728,
    "MATH Lvl 5 Raw": 0.2870090634441088,
    "MATH Lvl 5": 28.700906344410882,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.4327604166666667,
    "MUSR": 13.195052083333335,
    "MMLU-PRO Raw": 0.49185505319148937,
    "MMLU-PRO": 43.53945035460993,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-16",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-70B"
  },
  {
    "eval_name": "nvidia_Minitron-4B-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "NemotronForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Minitron-4B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Minitron-4B-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Minitron-4B-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Minitron-4B-Base",
    "Model sha": "d6321f64412982046a32d761701167e752fedc02",
    "Average ‚¨ÜÔ∏è": 11.939972705176743,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 127,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.1892668131564303,
    "IFEval Raw": 0.2217937295265451,
    "IFEval": 22.17937295265451,
    "BBH Raw": 0.4083876243992497,
    "BBH": 17.215600655061085,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.413375,
    "MUSR": 9.938541666666667,
    "MMLU-PRO Raw": 0.261968085106383,
    "MMLU-PRO": 17.99645390070922,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-19",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "nvidia/Minitron-4B-Base"
  },
  {
    "eval_name": "nvidia_Minitron-8B-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "NemotronForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Minitron-8B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Minitron-8B-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Minitron-8B-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Minitron-8B-Base",
    "Model sha": "70fa5997afc42807f41eebd5d481f040556fdf97",
    "Average ‚¨ÜÔ∏è": 14.178726415431548,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 63,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4125207201663268,
    "IFEval Raw": 0.24242676099416216,
    "IFEval": 24.242676099416215,
    "BBH Raw": 0.43950631883576047,
    "BBH": 22.04079297000523,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.40255208333333337,
    "MUSR": 9.085677083333335,
    "MMLU-PRO Raw": 0.31806848404255317,
    "MMLU-PRO": 24.229831560283685,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-19",
    "Submission Date": "2024-09-25",
    "Generation": 0,
    "Base Model": "nvidia/Minitron-8B-Base"
  },
  {
    "eval_name": "nvidia_Mistral-NeMo-Minitron-8B-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Mistral-NeMo-Minitron-8B-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Mistral-NeMo-Minitron-8B-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Mistral-NeMo-Minitron-8B-Base",
    "Model sha": "cc94637b669b62c4829b1e0c3b9074fecd883b74",
    "Average ‚¨ÜÔ∏è": 17.660161507076435,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 160,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.4040283202450357,
    "IFEval Raw": 0.19456597383830457,
    "IFEval": 19.456597383830456,
    "BBH Raw": 0.5219098090521418,
    "BBH": 31.822015157490153,
    "MATH Lvl 5 Raw": 0.04607250755287008,
    "MATH Lvl 5": 4.607250755287009,
    "GPQA Raw": 0.32550335570469796,
    "GPQA": 10.067114093959727,
    "MUSR Raw": 0.40915625000000005,
    "MUSR": 8.944531250000002,
    "MMLU-PRO Raw": 0.37957114361702127,
    "MMLU-PRO": 31.063460401891252,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-08-19",
    "Submission Date": "2024-08-22",
    "Generation": 0,
    "Base Model": "nvidia/Mistral-NeMo-Minitron-8B-Base"
  },
  {
    "eval_name": "nvidia_Mistral-NeMo-Minitron-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Mistral-NeMo-Minitron-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Mistral-NeMo-Minitron-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Mistral-NeMo-Minitron-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Mistral-NeMo-Minitron-8B-Instruct",
    "Model sha": "27964e305f862f9947f577332a943d7013abc30f",
    "Average ‚¨ÜÔ∏è": 21.722143311104038,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 63,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9938979629252904,
    "IFEval Raw": 0.5003889679384035,
    "IFEval": 50.03889679384034,
    "BBH Raw": 0.5320919605840294,
    "BBH": 34.126491245346166,
    "MATH Lvl 5 Raw": 0.005287009063444108,
    "MATH Lvl 5": 0.5287009063444108,
    "GPQA Raw": 0.287751677852349,
    "GPQA": 5.033557046979867,
    "MUSR Raw": 0.38857291666666666,
    "MUSR": 7.37161458333333,
    "MMLU-PRO Raw": 0.39910239361702127,
    "MMLU-PRO": 33.23359929078014,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-04",
    "Generation": 1,
    "Base Model": "nvidia/Mistral-NeMo-Minitron-8B-Instruct (Merge)"
  },
  {
    "eval_name": "nvidia_Nemotron-Mini-4B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "NemotronForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nvidia/Nemotron-Mini-4B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nvidia/Nemotron-Mini-4B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nvidia__Nemotron-Mini-4B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nvidia/Nemotron-Mini-4B-Instruct",
    "Model sha": "6a417790c444fd65a3da6a5c8821de6afc9654a6",
    "Average ‚¨ÜÔ∏è": 17.93551534108315,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 129,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1173137879976969,
    "IFEval Raw": 0.6668761109411916,
    "IFEval": 66.68761109411916,
    "BBH Raw": 0.3864840798591535,
    "BBH": 14.203825178862052,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3767291666666666,
    "MUSR": 4.624479166666667,
    "MMLU-PRO Raw": 0.26263297872340424,
    "MMLU-PRO": 18.070330969267136,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-10",
    "Submission Date": "2024-09-25",
    "Generation": 1,
    "Base Model": "nvidia/Minitron-4B-Base"
  },
  {
    "eval_name": "nxmwxm_Beast-Soul-new_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/nxmwxm/Beast-Soul-new\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">nxmwxm/Beast-Soul-new</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/nxmwxm__Beast-Soul-new-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "nxmwxm/Beast-Soul-new",
    "Model sha": "dd2ae8a96b7d088eb94a1cfa6ff84c3489e8c010",
    "Average ‚¨ÜÔ∏è": 21.830261529422003,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6570230463653188,
    "IFEval Raw": 0.48687482546310457,
    "IFEval": 48.68748254631046,
    "BBH Raw": 0.5227143628884523,
    "BBH": 33.07275916855207,
    "MATH Lvl 5 Raw": 0.07477341389728097,
    "MATH Lvl 5": 7.477341389728097,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.4459270833333333,
    "MUSR": 14.140885416666668,
    "MMLU-PRO Raw": 0.3101728723404255,
    "MMLU-PRO": 23.352541371158388,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-08-07",
    "Generation": 1,
    "Base Model": "nxmwxm/Beast-Soul-new (Merge)"
  },
  {
    "eval_name": "occiglot_occiglot-7b-es-en-instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/occiglot/occiglot-7b-es-en-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">occiglot/occiglot-7b-es-en-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/occiglot__occiglot-7b-es-en-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "occiglot/occiglot-7b-es-en-instruct",
    "Model sha": "5858f6ee118eef70896f1870fd61052348ff571e",
    "Average ‚¨ÜÔ∏è": 12.39496339099709,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6887381207425713,
    "IFEval Raw": 0.3485141646387142,
    "IFEval": 34.851416463871416,
    "BBH Raw": 0.4110970229781084,
    "BBH": 17.23541035561212,
    "MATH Lvl 5 Raw": 0.020392749244712995,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37375,
    "MUSR": 4.452083333333333,
    "MMLU-PRO Raw": 0.2310505319148936,
    "MMLU-PRO": 14.561170212765957,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-05",
    "Submission Date": "2024-09-02",
    "Generation": 0,
    "Base Model": "occiglot/occiglot-7b-es-en-instruct"
  },
  {
    "eval_name": "olabs-ai_reflection_model_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/olabs-ai/reflection_model\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">olabs-ai/reflection_model</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/olabs-ai__reflection_model-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "olabs-ai/reflection_model",
    "Model sha": "a8b0fc584b10e0110e04f9d21c7f10d24391c1d5",
    "Average ‚¨ÜÔ∏è": 14.01622495148248,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.4075434496217865,
    "IFEval Raw": 0.15986914719610634,
    "IFEval": 15.986914719610633,
    "BBH Raw": 0.4712508645838735,
    "BBH": 25.206881738811884,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.35083333333333333,
    "MUSR": 5.754166666666666,
    "MMLU-PRO Raw": 0.33111702127659576,
    "MMLU-PRO": 25.67966903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-08",
    "Submission Date": "2024-09-08",
    "Generation": 0,
    "Base Model": "olabs-ai/reflection_model"
  },
  {
    "eval_name": "oobabooga_CodeBooga-34B-v0.1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/oobabooga/CodeBooga-34B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">oobabooga/CodeBooga-34B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/oobabooga__CodeBooga-34B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "oobabooga/CodeBooga-34B-v0.1",
    "Model sha": "8a4e1e16ac46333cbd0c17d733d3d70a956071a6",
    "Average ‚¨ÜÔ∏è": 15.095240905583987,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 144,
    "#Params (B)": 33,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.0870035687499255,
    "IFEval Raw": 0.5250180631834643,
    "IFEval": 52.50180631834643,
    "BBH Raw": 0.3427441185661722,
    "BBH": 8.562465862636055,
    "MATH Lvl 5 Raw": 0.005287009063444109,
    "MATH Lvl 5": 0.5287009063444109,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.43102083333333335,
    "MUSR": 12.977604166666671,
    "MMLU-PRO Raw": 0.23595412234042554,
    "MMLU-PRO": 15.106013593380615,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-19",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "oobabooga/CodeBooga-34B-v0.1"
  },
  {
    "eval_name": "oopere_pruned20-llama-1b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/oopere/pruned20-llama-1b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">oopere/pruned20-llama-1b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/oopere__pruned20-llama-1b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "oopere/pruned20-llama-1b",
    "Model sha": "3351c9a062055ce6c16dd2c9f0c229fb5dd7396b",
    "Average ‚¨ÜÔ∏è": 4.863638460422249,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.40147804601965204,
    "IFEval Raw": 0.19936213690784896,
    "IFEval": 19.936213690784896,
    "BBH Raw": 0.30313627830972034,
    "BBH": 3.185394259848986,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.36314583333333333,
    "MUSR": 4.393229166666667,
    "MMLU-PRO Raw": 0.11228390957446809,
    "MMLU-PRO": 1.3648788416075646,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "oopere/pruned20-llama-1b (Merge)"
  },
  {
    "eval_name": "openai-community_gpt2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openai-community/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openai-community/gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openai-community__gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openai-community/gpt2",
    "Model sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
    "Average ‚¨ÜÔ∏è": 6.510807087761722,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2370,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.0859412568146148,
    "IFEval Raw": 0.17925327021192655,
    "IFEval": 17.925327021192658,
    "BBH Raw": 0.3035711244213359,
    "BBH": 2.674981367986987,
    "MATH Lvl 5 Raw": 0.0022658610271903325,
    "MATH Lvl 5": 0.22658610271903326,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.44705208333333335,
    "MUSR": 15.348177083333335,
    "MMLU-PRO Raw": 0.11594082446808511,
    "MMLU-PRO": 1.7712027186761226,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openai-community/gpt2"
  },
  {
    "eval_name": "openai-community_gpt2_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openai-community/gpt2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openai-community/gpt2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openai-community__gpt2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openai-community/gpt2",
    "Model sha": "607a30d783dfa663caf39e06633721c8d4cfcd7e",
    "Average ‚¨ÜÔ∏è": 6.296471067838717,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2370,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.11738689524881103,
    "IFEval Raw": 0.17795449407571912,
    "IFEval": 17.795449407571912,
    "BBH Raw": 0.30165801067653053,
    "BBH": 2.8159113095085133,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.43902083333333336,
    "MUSR": 13.910937500000001,
    "MMLU-PRO Raw": 0.11652260638297872,
    "MMLU-PRO": 1.8358451536643017,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-08-12",
    "Generation": 0,
    "Base Model": "openai-community/gpt2"
  },
  {
    "eval_name": "openai-community_gpt2-large_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openai-community/gpt2-large\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openai-community/gpt2-large</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openai-community__gpt2-large-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openai-community/gpt2-large",
    "Model sha": "32b71b12589c2f8d625668d2335a01cac3249519",
    "Average ‚¨ÜÔ∏è": 5.479590375205572,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 272,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.18046223843161893,
    "IFEval Raw": 0.20478220011790937,
    "IFEval": 20.47822001179094,
    "BBH Raw": 0.30688418760118824,
    "BBH": 3.2537905449787403,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3788645833333333,
    "MUSR": 5.658072916666665,
    "MMLU-PRO Raw": 0.11419547872340426,
    "MMLU-PRO": 1.5772754137115832,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openai-community/gpt2-large"
  },
  {
    "eval_name": "openai-community_gpt2-medium_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openai-community/gpt2-medium\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openai-community/gpt2-medium</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openai-community__gpt2-medium-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openai-community/gpt2-medium",
    "Model sha": "6dcaa7a952f72f9298047fd5137cd6e4f05f41da",
    "Average ‚¨ÜÔ∏è": 5.826811586248101,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 156,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.12106191500059583,
    "IFEval Raw": 0.22084402718121252,
    "IFEval": 22.08440271812125,
    "BBH Raw": 0.3050280232176266,
    "BBH": 2.719972238356244,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.3884479166666666,
    "MUSR": 6.1559895833333345,
    "MMLU-PRO Raw": 0.11818484042553191,
    "MMLU-PRO": 2.020537825059101,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openai-community/gpt2-medium"
  },
  {
    "eval_name": "openai-community_gpt2-xl_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openai-community/gpt2-xl\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openai-community/gpt2-xl</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openai-community__gpt2-xl-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openai-community/gpt2-xl",
    "Model sha": "15ea56dee5df4983c59b2538573817e1667135e2",
    "Average ‚¨ÜÔ∏è": 4.980187627399172,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 311,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.21531369983848483,
    "IFEval Raw": 0.20385798570016445,
    "IFEval": 20.385798570016444,
    "BBH Raw": 0.30085761123260785,
    "BBH": 2.580960647452716,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.37095833333333333,
    "MUSR": 4.036458333333333,
    "MMLU-PRO Raw": 0.11311502659574468,
    "MMLU-PRO": 1.457225177304964,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-03-02",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openai-community/gpt2-xl"
  },
  {
    "eval_name": "openbmb_MiniCPM-S-1B-sft-llama-format_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openbmb/MiniCPM-S-1B-sft-llama-format\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openbmb/MiniCPM-S-1B-sft-llama-format</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openbmb__MiniCPM-S-1B-sft-llama-format-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openbmb/MiniCPM-S-1B-sft-llama-format",
    "Model sha": "7de07f8895c168a7ee01f624f50c44f6966c9735",
    "Average ‚¨ÜÔ∏è": 8.870184531552928,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5400368474711362,
    "IFEval Raw": 0.3328767669782843,
    "IFEval": 33.28767669782843,
    "BBH Raw": 0.30493136322070497,
    "BBH": 3.898455214242885,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.33167708333333334,
    "MUSR": 1.359635416666667,
    "MMLU-PRO Raw": 0.1858377659574468,
    "MMLU-PRO": 9.537529550827422,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-14",
    "Submission Date": "2024-11-19",
    "Generation": 0,
    "Base Model": "openbmb/MiniCPM-S-1B-sft-llama-format"
  },
  {
    "eval_name": "openchat_openchat-3.5-0106_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat-3.5-0106\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat-3.5-0106</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat-3.5-0106-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat-3.5-0106",
    "Model sha": "ff058fda49726ecf4ea53dc1635f917cdb8ba36b",
    "Average ‚¨ÜÔ∏è": 22.658683433075733,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 347,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.354958864336844,
    "IFEval Raw": 0.5951353519771982,
    "IFEval": 59.513535197719825,
    "BBH Raw": 0.46169787083960595,
    "BBH": 24.03871121391158,
    "MATH Lvl 5 Raw": 0.07477341389728098,
    "MATH Lvl 5": 7.477341389728098,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.42543749999999997,
    "MUSR": 11.746354166666668,
    "MMLU-PRO Raw": 0.3291223404255319,
    "MMLU-PRO": 25.458037825059098,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-07",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "openchat_openchat-3.5-1210_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat-3.5-1210\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat-3.5-1210</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat-3.5-1210-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat-3.5-1210",
    "Model sha": "801f5459b7577241500785f11c2b026912badd6e",
    "Average ‚¨ÜÔ∏è": 22.69008525820593,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 275,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5164510603796584,
    "IFEval Raw": 0.603678240402133,
    "IFEval": 60.3678240402133,
    "BBH Raw": 0.4535356846447984,
    "BBH": 23.236296582166464,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4414375,
    "MUSR": 14.279687500000003,
    "MMLU-PRO Raw": 0.3142453457446808,
    "MMLU-PRO": 23.805038416075647,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-12",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "openchat_openchat-3.6-8b-20240522_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat-3.6-8b-20240522\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat-3.6-8b-20240522</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat-3.6-8b-20240522-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat-3.6-8b-20240522",
    "Model sha": "2264eb98558978f708e88ae52afb78e43b832801",
    "Average ‚¨ÜÔ∏è": 22.830377356764014,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 150,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.2673317125059,
    "IFEval Raw": 0.5343355629729118,
    "IFEval": 53.43355629729119,
    "BBH Raw": 0.5338412089001999,
    "BBH": 33.23293691836929,
    "MATH Lvl 5 Raw": 0.08308157099697885,
    "MATH Lvl 5": 8.308157099697885,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.3998541666666667,
    "MUSR": 8.181770833333333,
    "MMLU-PRO Raw": 0.32288896276595747,
    "MMLU-PRO": 24.765440307328607,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-07",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "openchat_openchat_3.5_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat_3.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat_3.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat_3.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat_3.5",
    "Model sha": "0fc98e324280bc4bf5d2c30ecf7b97b84fb8a19b",
    "Average ‚¨ÜÔ∏è": 21.64841522838232,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1118,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5012105911805675,
    "IFEval Raw": 0.5931118321608887,
    "IFEval": 59.31118321608887,
    "BBH Raw": 0.44263196862832893,
    "BBH": 21.58216684769999,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.4228645833333333,
    "MUSR": 11.258072916666668,
    "MMLU-PRO Raw": 0.31532579787234044,
    "MMLU-PRO": 23.92508865248227,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openchat/openchat_3.5"
  },
  {
    "eval_name": "openchat_openchat_v3.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat_v3.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat_v3.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat_v3.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat_v3.2",
    "Model sha": "acc7ce92558681e749678648189812f15c1465fe",
    "Average ‚¨ÜÔ∏è": 13.845733667344602,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 42,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.302455051728013,
    "IFEval Raw": 0.2980558252104416,
    "IFEval": 29.805582521044165,
    "BBH Raw": 0.4330564283474314,
    "BBH": 20.32300299720885,
    "MATH Lvl 5 Raw": 0.013595166163141994,
    "MATH Lvl 5": 1.3595166163141994,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.433625,
    "MUSR": 13.103125,
    "MMLU-PRO Raw": 0.2421875,
    "MMLU-PRO": 15.79861111111111,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-30",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openchat/openchat_v3.2"
  },
  {
    "eval_name": "openchat_openchat_v3.2_super_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/openchat/openchat_v3.2_super\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">openchat/openchat_v3.2_super</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/openchat__openchat_v3.2_super-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "openchat/openchat_v3.2_super",
    "Model sha": "9479cc37d43234a57a33628637d1aca0293d745a",
    "Average ‚¨ÜÔ∏è": 12.848046302494204,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.027693621846664,
    "IFEval Raw": 0.2861906408329898,
    "IFEval": 28.61906408329898,
    "BBH Raw": 0.42212089838803973,
    "BBH": 19.153539587477535,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.41613541666666665,
    "MUSR": 9.916927083333333,
    "MMLU-PRO Raw": 0.24251994680851063,
    "MMLU-PRO": 15.83554964539007,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-04",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "openchat/openchat_v3.2_super"
  },
  {
    "eval_name": "orai-nlp_Llama-eus-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/orai-nlp/Llama-eus-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">orai-nlp/Llama-eus-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/orai-nlp__Llama-eus-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "orai-nlp/Llama-eus-8B",
    "Model sha": "75b5645d222047b517a7a9190922ea1b5382c71f",
    "Average ‚¨ÜÔ∏è": 13.905990007809992,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8692575814647587,
    "IFEval Raw": 0.21612321972366655,
    "IFEval": 21.612321972366654,
    "BBH Raw": 0.4418245490788701,
    "BBH": 20.96137115221118,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.3918854166666667,
    "MUSR": 8.285677083333335,
    "MMLU-PRO Raw": 0.30576795212765956,
    "MMLU-PRO": 22.86310579196217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "paloalma_ECE-TW3-JRGL-V1_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paloalma/ECE-TW3-JRGL-V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paloalma/ECE-TW3-JRGL-V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paloalma__ECE-TW3-JRGL-V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paloalma/ECE-TW3-JRGL-V1",
    "Model sha": "2f08c7ab9db03b1b9f455c7beee6a41e99aa910e",
    "Average ‚¨ÜÔ∏è": 30.223412999710842,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.191694182968674,
    "IFEval Raw": 0.5534947273235016,
    "IFEval": 55.349472732350165,
    "BBH Raw": 0.6283667540784627,
    "BBH": 46.69713905397109,
    "MATH Lvl 5 Raw": 0.13066465256797585,
    "MATH Lvl 5": 13.066465256797585,
    "GPQA Raw": 0.34731543624161076,
    "GPQA": 12.975391498881436,
    "MUSR Raw": 0.46208333333333335,
    "MUSR": 17.460416666666664,
    "MMLU-PRO Raw": 0.422124335106383,
    "MMLU-PRO": 35.791592789598106,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-03",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "paloalma/ECE-TW3-JRGL-V1"
  },
  {
    "eval_name": "paloalma_ECE-TW3-JRGL-V2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paloalma/ECE-TW3-JRGL-V2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paloalma/ECE-TW3-JRGL-V2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paloalma__ECE-TW3-JRGL-V2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paloalma/ECE-TW3-JRGL-V2",
    "Model sha": "f2c15045f1a7a7a34540ab18abcee8a566a74ca6",
    "Average ‚¨ÜÔ∏è": 25.679421947509752,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 12.546249461989335,
    "IFEval Raw": 0.2254894790267601,
    "IFEval": 22.54894790267601,
    "BBH Raw": 0.6030988136029874,
    "BBH": 43.17326773447519,
    "MATH Lvl 5 Raw": 0.17824773413897282,
    "MATH Lvl 5": 17.824773413897283,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.47932291666666665,
    "MUSR": 19.815364583333327,
    "MMLU-PRO Raw": 0.4587765957446808,
    "MMLU-PRO": 39.86406619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-04",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "paloalma/ECE-TW3-JRGL-V2"
  },
  {
    "eval_name": "paloalma_ECE-TW3-JRGL-V5_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paloalma/ECE-TW3-JRGL-V5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paloalma/ECE-TW3-JRGL-V5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paloalma__ECE-TW3-JRGL-V5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paloalma/ECE-TW3-JRGL-V5",
    "Model sha": "4061fa10de22945790cad825f7f4dec96d55b204",
    "Average ‚¨ÜÔ∏è": 29.45428246685488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 23.031124302962095,
    "IFEval Raw": 0.4552509563513699,
    "IFEval": 45.52509563513699,
    "BBH Raw": 0.6024712037668832,
    "BBH": 43.46251365157702,
    "MATH Lvl 5 Raw": 0.18126888217522658,
    "MATH Lvl 5": 18.12688821752266,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.4620520833333333,
    "MUSR": 16.88984375,
    "MMLU-PRO Raw": 0.46476063829787234,
    "MMLU-PRO": 40.528959810874696,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-11",
    "Submission Date": "2024-08-30",
    "Generation": 0,
    "Base Model": "paloalma/ECE-TW3-JRGL-V5"
  },
  {
    "eval_name": "paloalma_Le_Triomphant-ECE-TW3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paloalma/Le_Triomphant-ECE-TW3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paloalma/Le_Triomphant-ECE-TW3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paloalma__Le_Triomphant-ECE-TW3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paloalma/Le_Triomphant-ECE-TW3",
    "Model sha": "f72399253bb3e65c0f55e50461488c098f658a49",
    "Average ‚¨ÜÔ∏è": 31.933353610124517,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 10.4183912289729,
    "IFEval Raw": 0.5402055435134332,
    "IFEval": 54.02055435134332,
    "BBH Raw": 0.6112057897556996,
    "BBH": 44.96329362428286,
    "MATH Lvl 5 Raw": 0.19108761329305138,
    "MATH Lvl 5": 19.10876132930514,
    "GPQA Raw": 0.348993288590604,
    "GPQA": 13.19910514541387,
    "MUSR Raw": 0.4725,
    "MUSR": 18.495833333333334,
    "MMLU-PRO Raw": 0.476313164893617,
    "MMLU-PRO": 41.81257387706855,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-01",
    "Submission Date": "2024-07-25",
    "Generation": 0,
    "Base Model": "paloalma/Le_Triomphant-ECE-TW3"
  },
  {
    "eval_name": "paloalma_TW3-JRGL-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paloalma/TW3-JRGL-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paloalma/TW3-JRGL-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paloalma__TW3-JRGL-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paloalma/TW3-JRGL-v2",
    "Model sha": "aca3f0ba2bfb90038a9e2cd5b486821d4c181b46",
    "Average ‚¨ÜÔ∏è": 32.399598299201564,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 20.896294207346823,
    "IFEval Raw": 0.5316127874040878,
    "IFEval": 53.16127874040878,
    "BBH Raw": 0.6137525505395743,
    "BBH": 45.61110998256794,
    "MATH Lvl 5 Raw": 0.17522658610271905,
    "MATH Lvl 5": 17.522658610271904,
    "GPQA Raw": 0.35906040268456374,
    "GPQA": 14.541387024608499,
    "MUSR Raw": 0.48583333333333334,
    "MUSR": 20.69583333333333,
    "MMLU-PRO Raw": 0.4857878989361702,
    "MMLU-PRO": 42.86532210401891,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-01",
    "Submission Date": "2024-08-29",
    "Generation": 0,
    "Base Model": "paloalma/TW3-JRGL-v2"
  },
  {
    "eval_name": "pankajmathur_Al_Dente_v1_8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/Al_Dente_v1_8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/Al_Dente_v1_8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__Al_Dente_v1_8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/Al_Dente_v1_8b",
    "Model sha": "149d70e04085ecd90510a60f916efc55da1294e7",
    "Average ‚¨ÜÔ∏è": 17.23711848154768,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9085320048952623,
    "IFEval Raw": 0.3693721547715617,
    "IFEval": 36.93721547715617,
    "BBH Raw": 0.48347371404380524,
    "BBH": 27.247898492647995,
    "MATH Lvl 5 Raw": 0.037009063444108765,
    "MATH Lvl 5": 3.7009063444108765,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.3987083333333334,
    "MUSR": 8.271875000000001,
    "MMLU-PRO Raw": 0.2859873670212766,
    "MMLU-PRO": 20.665263002364064,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/Al_Dente_v1_8b"
  },
  {
    "eval_name": "pankajmathur_model_007_13b_v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/model_007_13b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/model_007_13b_v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__model_007_13b_v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/model_007_13b_v2",
    "Model sha": "2c6ddf25cdb134f22e2543121b5a36b41342a9e2",
    "Average ‚¨ÜÔ∏è": 15.868934411081502,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.1817800526593905,
    "IFEval Raw": 0.30564901129004374,
    "IFEval": 30.564901129004376,
    "BBH Raw": 0.4702292766687601,
    "BBH": 25.454420185872465,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.46109375,
    "MUSR": 17.203385416666663,
    "MMLU-PRO Raw": 0.24609375,
    "MMLU-PRO": 16.232638888888886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-08-12",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/model_007_13b_v2"
  },
  {
    "eval_name": "pankajmathur_orca_mini_3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_3b",
    "Model sha": "31e1a7bc3f7ea2f247b432d60036d975b8d590e9",
    "Average ‚¨ÜÔ∏è": 3.0749229598669547,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 158,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5247755745166754,
    "IFEval Raw": 0.07421419611076388,
    "IFEval": 7.421419611076388,
    "BBH Raw": 0.3196070040004752,
    "BBH": 4.6859845437903855,
    "MATH Lvl 5 Raw": 0.00528700906344411,
    "MATH Lvl 5": 0.528700906344411,
    "GPQA Raw": 0.24580536912751677,
    "GPQA": 0.0,
    "MUSR Raw": 0.3349270833333333,
    "MUSR": 4.199218749999999,
    "MMLU-PRO Raw": 0.11452792553191489,
    "MMLU-PRO": 1.6142139479905429,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-06-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_3b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v2_7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v2_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v2_7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v2_7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v2_7b",
    "Model sha": "66d3f32a4a6bca0a2a261f1bdb54d2582028f75f",
    "Average ‚¨ÜÔ∏è": 5.502368522121576,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5925114845474425,
    "IFEval Raw": 0.13578859647956312,
    "IFEval": 13.57885964795631,
    "BBH Raw": 0.35363417847864514,
    "BBH": 10.199953477088151,
    "MATH Lvl 5 Raw": 0.011329305135951664,
    "MATH Lvl 5": 1.1329305135951664,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.35933333333333334,
    "MUSR": 2.0833333333333326,
    "MMLU-PRO Raw": 0.1541722074468085,
    "MMLU-PRO": 6.019134160756501,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-07-03",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v2_7b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v3_13b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v3_13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v3_13b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v3_13b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v3_13b",
    "Model sha": "7d6e567d24ce2f228beaf54e89c17b0e750bfe99",
    "Average ‚¨ÜÔ∏è": 15.016120755880513,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 31,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0971793715207823,
    "IFEval Raw": 0.28966253983873896,
    "IFEval": 28.966253983873898,
    "BBH Raw": 0.4710970361474938,
    "BBH": 25.549482064607844,
    "MATH Lvl 5 Raw": 0.019637462235649546,
    "MATH Lvl 5": 1.9637462235649545,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.45979166666666665,
    "MUSR": 17.107291666666665,
    "MMLU-PRO Raw": 0.23046875,
    "MMLU-PRO": 14.496527777777777,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-08-09",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v3_13b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v3_70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v3_70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v3_70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v3_70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v3_70b",
    "Model sha": "e8e856dfb5c737d1906b50f9e65fd3a4f8d77422",
    "Average ‚¨ÜÔ∏è": 25.29815949268638,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.4065369885751124,
    "IFEval Raw": 0.4014703209705803,
    "IFEval": 40.14703209705803,
    "BBH Raw": 0.5949312065598904,
    "BBH": 42.975787003923045,
    "MATH Lvl 5 Raw": 0.038519637462235655,
    "MATH Lvl 5": 3.8519637462235656,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.5078541666666667,
    "MUSR": 25.11510416666667,
    "MMLU-PRO Raw": 0.3757480053191489,
    "MMLU-PRO": 30.63866725768321,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-08-10",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v3_70b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v3_7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v3_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v3_7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v3_7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v3_7b",
    "Model sha": "6252eb7ca29da8d951ae7d2bca948bf84e04a2b9",
    "Average ‚¨ÜÔ∏è": 13.51814003742416,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 40,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6399501421619029,
    "IFEval Raw": 0.2820937335159599,
    "IFEval": 28.20937335159599,
    "BBH Raw": 0.4095332668279368,
    "BBH": 17.843955571096647,
    "MATH Lvl 5 Raw": 0.003021148036253777,
    "MATH Lvl 5": 0.3021148036253777,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.49823958333333335,
    "MUSR": 22.713281249999998,
    "MMLU-PRO Raw": 0.20836103723404256,
    "MMLU-PRO": 12.04011524822695,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-08-07",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v3_7b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v5_8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v5_8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v5_8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v5_8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v5_8b",
    "Model sha": "f57c84d4cc0b3b74549458c0d38e868bd7fffad1",
    "Average ‚¨ÜÔ∏è": 20.246538396087907,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8789707757835428,
    "IFEval Raw": 0.48060479527653294,
    "IFEval": 48.06047952765329,
    "BBH Raw": 0.5064242853619262,
    "BBH": 29.345795010726494,
    "MATH Lvl 5 Raw": 0.08383685800604229,
    "MATH Lvl 5": 8.38368580060423,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.4000104166666667,
    "MUSR": 7.701302083333336,
    "MMLU-PRO Raw": 0.3075964095744681,
    "MMLU-PRO": 23.066267730496453,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v5_8b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v5_8b_dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v5_8b_dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v5_8b_dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v5_8b_dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v5_8b_dpo",
    "Model sha": "fdc0d0aaa85a58f1abaf2c24ce0ddca10c08f0f1",
    "Average ‚¨ÜÔ∏è": 20.057268423512923,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8166896639067601,
    "IFEval Raw": 0.48964746871633935,
    "IFEval": 48.964746871633935,
    "BBH Raw": 0.5074598658862709,
    "BBH": 29.605372989233754,
    "MATH Lvl 5 Raw": 0.08081570996978851,
    "MATH Lvl 5": 8.08157099697885,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.389375,
    "MUSR": 6.938541666666667,
    "MMLU-PRO Raw": 0.31158577127659576,
    "MMLU-PRO": 23.50953014184397,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-30",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v5_8b_dpo"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v5_8b_orpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v5_8b_orpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v5_8b_orpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v5_8b_orpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v5_8b_orpo",
    "Model sha": "4cdc018043ef439f15bd8a09c4f09c6bc528dfc7",
    "Average ‚¨ÜÔ∏è": 12.96855386656729,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9718497692822918,
    "IFEval Raw": 0.08243239050164675,
    "IFEval": 8.243239050164673,
    "BBH Raw": 0.496374377369289,
    "BBH": 27.877628256858372,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.41312499999999996,
    "MUSR": 8.973958333333334,
    "MMLU-PRO Raw": 0.2947140957446808,
    "MMLU-PRO": 21.634899527186757,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v5_8b_orpo"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v6_8b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v6_8b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v6_8b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v6_8b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v6_8b",
    "Model sha": "e95dc8e4c6b6ca5957b657cc2d905683142eaf3e",
    "Average ‚¨ÜÔ∏è": 1.4133981765593575,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2163678462090854,
    "IFEval Raw": 0.011116060940526692,
    "IFEval": 1.1116060940526693,
    "BBH Raw": 0.30286959112076134,
    "BBH": 3.219809856556432,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.23825503355704697,
    "GPQA": 0.0,
    "MUSR Raw": 0.3554583333333334,
    "MUSR": 2.7656250000000004,
    "MMLU-PRO Raw": 0.1124501329787234,
    "MMLU-PRO": 1.383348108747044,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-02",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v6_8b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v6_8b_dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v6_8b_dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v6_8b_dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v6_8b_dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v6_8b_dpo",
    "Model sha": "ebb11b63839d38e8c03c7ecac012e047fcb2346e",
    "Average ‚¨ÜÔ∏è": 20.392492362112517,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7698236449374195,
    "IFEval Raw": 0.3882564927725103,
    "IFEval": 38.82564927725103,
    "BBH Raw": 0.520280774453148,
    "BBH": 32.47882597428379,
    "MATH Lvl 5 Raw": 0.06117824773413898,
    "MATH Lvl 5": 6.117824773413898,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.40903125,
    "MUSR": 9.262239583333335,
    "MMLU-PRO Raw": 0.359624335106383,
    "MMLU-PRO": 28.847148345153663,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-21",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v6_8b_dpo"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v7_72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v7_72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v7_72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v7_72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v7_72b",
    "Model sha": "447f11912cfa496e32e188a55214043a05760d3a",
    "Average ‚¨ÜÔ∏è": 39.387496349271466,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 14.051707198121015,
    "IFEval Raw": 0.5929622291076566,
    "IFEval": 59.29622291076566,
    "BBH Raw": 0.6842301988001044,
    "BBH": 55.05552307693972,
    "MATH Lvl 5 Raw": 0.283987915407855,
    "MATH Lvl 5": 28.3987915407855,
    "GPQA Raw": 0.3850671140939597,
    "GPQA": 18.008948545861294,
    "MUSR Raw": 0.5070416666666667,
    "MUSR": 24.21354166666666,
    "MMLU-PRO Raw": 0.5621675531914894,
    "MMLU-PRO": 51.35195035460993,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v7_72b"
  },
  {
    "eval_name": "pankajmathur_orca_mini_v7_7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pankajmathur/orca_mini_v7_7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pankajmathur/orca_mini_v7_7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pankajmathur__orca_mini_v7_7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pankajmathur/orca_mini_v7_7b",
    "Model sha": "f5e84ff6ea25fb4585908ea45d1520bac416d803",
    "Average ‚¨ÜÔ∏è": 22.42557778543438,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9251094115145381,
    "IFEval Raw": 0.4387646998851935,
    "IFEval": 43.87646998851935,
    "BBH Raw": 0.5274909601771501,
    "BBH": 33.95043410425148,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.43597916666666664,
    "MUSR": 12.6640625,
    "MMLU-PRO Raw": 0.4167220744680851,
    "MMLU-PRO": 35.19134160756501,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-20",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "pankajmathur/orca_mini_v7_7b"
  },
  {
    "eval_name": "paulml_ECE-ILAB-Q1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/paulml/ECE-ILAB-Q1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">paulml/ECE-ILAB-Q1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/paulml__ECE-ILAB-Q1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "paulml/ECE-ILAB-Q1",
    "Model sha": "393bea0ee85e4c752acd5fd77ce07f577fc13bd9",
    "Average ‚¨ÜÔ∏è": 41.30720139047916,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.415142070664045,
    "IFEval Raw": 0.7864521691334547,
    "IFEval": 78.64521691334548,
    "BBH Raw": 0.6717755530661759,
    "BBH": 53.70222770817057,
    "MATH Lvl 5 Raw": 0.283987915407855,
    "MATH Lvl 5": 28.3987915407855,
    "GPQA Raw": 0.38674496644295303,
    "GPQA": 18.232662192393736,
    "MUSR Raw": 0.46137500000000004,
    "MUSR": 18.805208333333333,
    "MMLU-PRO Raw": 0.550531914893617,
    "MMLU-PRO": 50.05910165484633,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-06",
    "Submission Date": "2024-09-16",
    "Generation": 0,
    "Base Model": "paulml/ECE-ILAB-Q1"
  },
  {
    "eval_name": "pints-ai_1.5-Pints-16K-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pints-ai/1.5-Pints-16K-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pints-ai/1.5-Pints-16K-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pints-ai__1.5-Pints-16K-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pints-ai/1.5-Pints-16K-v0.1",
    "Model sha": "7862a52f250be68fad593f3a4030f00d658ede56",
    "Average ‚¨ÜÔ∏è": 4.150222953923024,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.27993845817722557,
    "IFEval Raw": 0.1635914927946737,
    "IFEval": 16.35914927946737,
    "BBH Raw": 0.3133077677150869,
    "BBH": 3.658292060342125,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.23573825503355705,
    "GPQA": 0.0,
    "MUSR Raw": 0.357875,
    "MUSR": 2.7343749999999996,
    "MMLU-PRO Raw": 0.1118683510638298,
    "MMLU-PRO": 1.3187056737588652,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-09-09",
    "Generation": 0,
    "Base Model": "pints-ai/1.5-Pints-16K-v0.1"
  },
  {
    "eval_name": "pints-ai_1.5-Pints-2K-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pints-ai/1.5-Pints-2K-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pints-ai/1.5-Pints-2K-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pints-ai__1.5-Pints-2K-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pints-ai/1.5-Pints-2K-v0.1",
    "Model sha": "2e865c18669161ebbf5e9ad79ae0502ee0153df0",
    "Average ‚¨ÜÔ∏è": 3.8304416986902328,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.29141651544333597,
    "IFEval Raw": 0.17615593292463996,
    "IFEval": 17.615593292463995,
    "BBH Raw": 0.29801943389750435,
    "BBH": 2.3744704635071923,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.35018749999999993,
    "MUSR": 1.8401041666666658,
    "MMLU-PRO Raw": 0.11037234042553191,
    "MMLU-PRO": 1.1524822695035455,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-09-09",
    "Generation": 0,
    "Base Model": "pints-ai/1.5-Pints-2K-v0.1"
  },
  {
    "eval_name": "piotr25691_thea-3b-25r_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/piotr25691/thea-3b-25r\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">piotr25691/thea-3b-25r</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/piotr25691__thea-3b-25r-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "piotr25691/thea-3b-25r",
    "Model sha": "4661fb3c8b18bdf2059f703c4f69caea24057151",
    "Average ‚¨ÜÔ∏è": 24.021247246270633,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.69050761486917,
    "IFEval Raw": 0.7344202272193336,
    "IFEval": 73.44202272193337,
    "BBH Raw": 0.44844100293649863,
    "BBH": 22.54671082396668,
    "MATH Lvl 5 Raw": 0.1797583081570997,
    "MATH Lvl 5": 17.97583081570997,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.33145833333333335,
    "MUSR": 3.565625000000001,
    "MMLU-PRO Raw": 0.3182347074468085,
    "MMLU-PRO": 24.248300827423165,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-12",
    "Generation": 1,
    "Base Model": "chuanli11/Llama-3.2-3B-Instruct-uncensored"
  },
  {
    "eval_name": "piotr25691_thea-c-3b-25r_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/piotr25691/thea-c-3b-25r\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">piotr25691/thea-c-3b-25r</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/piotr25691__thea-c-3b-25r-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "piotr25691/thea-c-3b-25r",
    "Model sha": "93a2333a84feda26f020bc8fa92f870462dacd89",
    "Average ‚¨ÜÔ∏è": 23.179267392396522,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.662456207120752,
    "IFEval Raw": 0.7401904723910335,
    "IFEval": 74.01904723910336,
    "BBH Raw": 0.4532410175874399,
    "BBH": 22.767850009265825,
    "MATH Lvl 5 Raw": 0.14803625377643506,
    "MATH Lvl 5": 14.803625377643506,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.33148958333333334,
    "MUSR": 1.2695312499999996,
    "MMLU-PRO Raw": 0.3178191489361702,
    "MMLU-PRO": 24.202127659574465,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "piotr25691_thea-rp-3b-25r_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/piotr25691/thea-rp-3b-25r\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">piotr25691/thea-rp-3b-25r</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/piotr25691__thea-rp-3b-25r-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "piotr25691/thea-rp-3b-25r",
    "Model sha": "ed4c338e07356f1657cf4d08b768ff866bbf0a68",
    "Average ‚¨ÜÔ∏è": 21.73208913785183,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6584533497575447,
    "IFEval Raw": 0.6577835698169745,
    "IFEval": 65.77835698169746,
    "BBH Raw": 0.4390291036559586,
    "BBH": 20.007380927568594,
    "MATH Lvl 5 Raw": 0.1253776435045317,
    "MATH Lvl 5": 12.53776435045317,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.381875,
    "MUSR": 5.934375000000003,
    "MMLU-PRO Raw": 0.30601728723404253,
    "MMLU-PRO": 22.89080969267139,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-13",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "SicariusSicariiStuff/Impish_LLAMA_3B"
  },
  {
    "eval_name": "postbot_gpt2-medium-emailgen_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/postbot/gpt2-medium-emailgen\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">postbot/gpt2-medium-emailgen</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/postbot__gpt2-medium-emailgen-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "postbot/gpt2-medium-emailgen",
    "Model sha": "a0299eb6760126e3bd04d2f10cd166c4563f82d2",
    "Average ‚¨ÜÔ∏è": 4.743048119298616,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.07818635378573768,
    "IFEval Raw": 0.1492030035860406,
    "IFEval": 14.92030035860406,
    "BBH Raw": 0.31304286003933807,
    "BBH": 3.673700346196318,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3911145833333333,
    "MUSR": 6.889322916666669,
    "MMLU-PRO Raw": 0.1146941489361702,
    "MMLU-PRO": 1.6326832151300221,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2022-09-29",
    "Submission Date": "2024-11-17",
    "Generation": 0,
    "Base Model": "postbot/gpt2-medium-emailgen"
  },
  {
    "eval_name": "prince-canuma_Ministral-8B-Instruct-2410-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/prince-canuma/Ministral-8B-Instruct-2410-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">prince-canuma/Ministral-8B-Instruct-2410-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/prince-canuma__Ministral-8B-Instruct-2410-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "prince-canuma/Ministral-8B-Instruct-2410-HF",
    "Model sha": "e0a14d7a6a8a1d1e5bef1a77a42e86e8bcae0ee7",
    "Average ‚¨ÜÔ∏è": 21.68029702649504,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0169346078640138,
    "IFEval Raw": 0.5911636679565775,
    "IFEval": 59.11636679565775,
    "BBH Raw": 0.4585611339334732,
    "BBH": 23.778464927274356,
    "MATH Lvl 5 Raw": 0.06797583081570996,
    "MATH Lvl 5": 6.797583081570996,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.41375,
    "MUSR": 10.718750000000005,
    "MMLU-PRO Raw": 0.32978723404255317,
    "MMLU-PRO": 25.53191489361702,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "prince-canuma/Ministral-8B-Instruct-2410-HF (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-8B-ProLong-512k-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-512k-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-8B-ProLong-512k-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-8B-ProLong-512k-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-8B-ProLong-512k-Base",
    "Model sha": "51a333f7c99f5052377154b76909dfe63ff7ab83",
    "Average ‚¨ÜÔ∏è": 21.679044932010054,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8786637911145913,
    "IFEval Raw": 0.5322123077877808,
    "IFEval": 53.221230778778086,
    "BBH Raw": 0.5033213133882991,
    "BBH": 29.847246369144035,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.4222708333333333,
    "MUSR": 12.683854166666663,
    "MMLU-PRO Raw": 0.33294547872340424,
    "MMLU-PRO": 25.882830969267133,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "princeton-nlp/Llama-3-8B-ProLong-512k-Base (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-8B-ProLong-512k-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-512k-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-8B-ProLong-512k-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-8B-ProLong-512k-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-8B-ProLong-512k-Instruct",
    "Model sha": "eae0626e8597575215276c2b248720f731bc50b8",
    "Average ‚¨ÜÔ∏è": 21.942343537569432,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.344706085397087,
    "IFEval Raw": 0.5508218194390884,
    "IFEval": 55.08218194390883,
    "BBH Raw": 0.5028310716285619,
    "BBH": 29.151153442911763,
    "MATH Lvl 5 Raw": 0.052870090634441085,
    "MATH Lvl 5": 5.287009063444108,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.42664583333333334,
    "MUSR": 12.530729166666662,
    "MMLU-PRO Raw": 0.32313829787234044,
    "MMLU-PRO": 24.793144208037827,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "princeton-nlp/Llama-3-8B-ProLong-512k-Instruct (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-8B-ProLong-512k-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-512k-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-8B-ProLong-512k-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-8B-ProLong-512k-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-8B-ProLong-512k-Instruct",
    "Model sha": "bf92e493b7b0ef1db0242bfa97f1d8f92be02e9c",
    "Average ‚¨ÜÔ∏è": 19.317530638140124,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.724373552397904,
    "IFEval Raw": 0.3977734632996006,
    "IFEval": 39.77734632996006,
    "BBH Raw": 0.49830327201612584,
    "BBH": 28.669218583844156,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.425,
    "MUSR": 12.091666666666663,
    "MMLU-PRO Raw": 0.3246343085106383,
    "MMLU-PRO": 24.959367612293143,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-22",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "princeton-nlp/Llama-3-8B-ProLong-512k-Instruct (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-8B-ProLong-64k-Base_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-64k-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-8B-ProLong-64k-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-8B-ProLong-64k-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-8B-ProLong-64k-Base",
    "Model sha": "97994d6918f80162a893e22d5e7bba586551f941",
    "Average ‚¨ÜÔ∏è": 21.601845831882653,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8224611007545528,
    "IFEval Raw": 0.5200722970606879,
    "IFEval": 52.007229706068784,
    "BBH Raw": 0.49271325981523906,
    "BBH": 28.687899000980178,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.4340520833333333,
    "MUSR": 14.62317708333333,
    "MMLU-PRO Raw": 0.3347739361702128,
    "MMLU-PRO": 26.08599290780142,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-22",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "princeton-nlp/Llama-3-8B-ProLong-64k-Base (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-8B-ProLong-64k-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-8B-ProLong-64k-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-8B-ProLong-64k-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-8B-ProLong-64k-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-8B-ProLong-64k-Instruct",
    "Model sha": "fe55aed18544c5744239e473bb0d3aa0151776d3",
    "Average ‚¨ÜÔ∏è": 22.970639332138102,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.656687326449919,
    "IFEval Raw": 0.5563172382611471,
    "IFEval": 55.63172382611471,
    "BBH Raw": 0.5083040804243396,
    "BBH": 30.089572165686842,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.2953020134228188,
    "GPQA": 6.040268456375841,
    "MUSR Raw": 0.43969791666666663,
    "MUSR": 14.595572916666667,
    "MMLU-PRO Raw": 0.32746010638297873,
    "MMLU-PRO": 25.2733451536643,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-21",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "princeton-nlp/Llama-3-8B-ProLong-64k-Instruct (Merge)"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT",
    "Model sha": "b622b7d814aa03aa722328bf88feaf1ad480b7fb",
    "Average ‚¨ÜÔ∏è": 15.437201039593333,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7882860795334263,
    "IFEval Raw": 0.30964618323825227,
    "IFEval": 30.964618323825228,
    "BBH Raw": 0.4547838615765894,
    "BBH": 24.388576484696546,
    "MATH Lvl 5 Raw": 0.03096676737160121,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.39495833333333336,
    "MUSR": 8.036458333333336,
    "MMLU-PRO Raw": 0.29496343085106386,
    "MMLU-PRO": 21.66260342789598,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-CPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-CPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-CPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-CPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-CPO",
    "Model sha": "536ce7e7beb35175c48538fe46e7e9e100f228c9",
    "Average ‚¨ÜÔ∏è": 15.878260608735323,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9678460554423564,
    "IFEval Raw": 0.37034623687371726,
    "IFEval": 37.03462368737173,
    "BBH Raw": 0.4594875922440002,
    "BBH": 25.474648628373444,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3608541666666667,
    "MUSR": 2.5734375000000025,
    "MMLU-PRO Raw": 0.2976230053191489,
    "MMLU-PRO": 21.95811170212766,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-CPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-DPO",
    "Model sha": "3f5ec47c9beffb37cfbdcd837e76a336a9b1e651",
    "Average ‚¨ÜÔ∏è": 18.162221126395032,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9263398665907964,
    "IFEval Raw": 0.41111251479407973,
    "IFEval": 41.11125147940797,
    "BBH Raw": 0.46658506064913546,
    "BBH": 26.001873821480995,
    "MATH Lvl 5 Raw": 0.02870090634441088,
    "MATH Lvl 5": 2.870090634441088,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.38673958333333336,
    "MUSR": 7.842447916666668,
    "MMLU-PRO Raw": 0.3078457446808511,
    "MMLU-PRO": 23.093971631205672,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-DPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-IPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-IPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-IPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-IPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-IPO",
    "Model sha": "85055cc4b9c707e0bd1239d20d1f62927a7a54c3",
    "Average ‚¨ÜÔ∏è": 18.281889183492527,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9321908470587108,
    "IFEval Raw": 0.4486562321307464,
    "IFEval": 44.86562321307464,
    "BBH Raw": 0.4690068582318399,
    "BBH": 25.705433288023944,
    "MATH Lvl 5 Raw": 0.012839879154078552,
    "MATH Lvl 5": 1.2839879154078553,
    "GPQA Raw": 0.2978187919463087,
    "GPQA": 6.375838926174497,
    "MUSR Raw": 0.3919479166666667,
    "MUSR": 7.960156250000001,
    "MMLU-PRO Raw": 0.3115026595744681,
    "MMLU-PRO": 23.50029550827423,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-IPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-KTO",
    "Model sha": "49a8c2e5ccc7a28ed7bbedf093e352015fc1eb9b",
    "Average ‚¨ÜÔ∏è": 17.9648580856736,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8618519357390183,
    "IFEval Raw": 0.4522533544329047,
    "IFEval": 45.225335443290476,
    "BBH Raw": 0.4692852292721417,
    "BBH": 25.55523001299593,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.3841979166666667,
    "MUSR": 5.5914062499999995,
    "MMLU-PRO Raw": 0.3054355053191489,
    "MMLU-PRO": 22.826167257683213,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-KTO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-ORPO",
    "Model sha": "54d58402e0168faff6503e41621ad6c8274a310a",
    "Average ‚¨ÜÔ∏è": 19.1927971889142,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9065632232864196,
    "IFEval Raw": 0.45165383404921167,
    "IFEval": 45.16538340492117,
    "BBH Raw": 0.47340573024653915,
    "BBH": 26.48589369385502,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.3706770833333333,
    "MUSR": 7.634635416666668,
    "MMLU-PRO Raw": 0.30826130319148937,
    "MMLU-PRO": 23.140144799054376,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-ORPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-RDPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-RDPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-RDPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-RDPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-RDPO",
    "Model sha": "b41a964c2135ba34dcc6fa7edf76b6b9ea656949",
    "Average ‚¨ÜÔ∏è": 18.815011194547868,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9024353781364343,
    "IFEval Raw": 0.4480068440626427,
    "IFEval": 44.80068440626427,
    "BBH Raw": 0.46620140448752295,
    "BBH": 25.526521127200017,
    "MATH Lvl 5 Raw": 0.03776435045317221,
    "MATH Lvl 5": 3.776435045317221,
    "GPQA Raw": 0.3062080536912752,
    "GPQA": 7.494407158836691,
    "MUSR Raw": 0.4027395833333334,
    "MUSR": 8.909114583333336,
    "MMLU-PRO Raw": 0.30144614361702127,
    "MMLU-PRO": 22.382904846335695,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-RDPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-RRHF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-RRHF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-RRHF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-RRHF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-RRHF",
    "Model sha": "aea8c04b3940cebd1f8296a2c76914f0ce70c276",
    "Average ‚¨ÜÔ∏è": 16.081314404469236,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9514687231888692,
    "IFEval Raw": 0.3357247658435174,
    "IFEval": 33.57247658435174,
    "BBH Raw": 0.4520360167602379,
    "BBH": 23.659142323042403,
    "MATH Lvl 5 Raw": 0.03323262839879155,
    "MATH Lvl 5": 3.323262839879155,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.37222916666666667,
    "MUSR": 7.561979166666668,
    "MMLU-PRO Raw": 0.2888962765957447,
    "MMLU-PRO": 20.988475177304963,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-RRHF"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-SLiC-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-SLiC-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-SLiC-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-SLiC-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-SLiC-HF",
    "Model sha": "325092c1eddffc3ca7157be1ff9958128e5753ef",
    "Average ‚¨ÜÔ∏è": 19.73052537424587,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9604710642075931,
    "IFEval Raw": 0.4890479483326463,
    "IFEval": 48.90479483326463,
    "BBH Raw": 0.4704075127777334,
    "BBH": 26.37396261839453,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.40909375000000003,
    "MUSR": 10.270052083333335,
    "MMLU-PRO Raw": 0.30634973404255317,
    "MMLU-PRO": 22.92774822695035,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-SLiC-HF"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Base-8B-SFT-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Base-8B-SFT-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Base-8B-SFT-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Base-8B-SFT-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Base-8B-SFT-SimPO",
    "Model sha": "0a6e518b13b67abe8433bce3f7beee9beb74a794",
    "Average ‚¨ÜÔ∏è": 19.279456083611223,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8615650260322562,
    "IFEval Raw": 0.4685401401614383,
    "IFEval": 46.854014016143836,
    "BBH Raw": 0.47412507033960827,
    "BBH": 26.39594961870209,
    "MATH Lvl 5 Raw": 0.02039274924471299,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.41268750000000004,
    "MUSR": 11.852604166666666,
    "MMLU-PRO Raw": 0.31050531914893614,
    "MMLU-PRO": 23.389479905437348,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-24",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Base-8B-SFT-SimPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-CPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-CPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-CPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-CPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-CPO",
    "Model sha": "d4645ae4c3b99892f1c59f60a77330be35567835",
    "Average ‚¨ÜÔ∏è": 23.910959611683296,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7394162955537005,
    "IFEval Raw": 0.7292993701157373,
    "IFEval": 72.92993701157374,
    "BBH Raw": 0.4998793158888361,
    "BBH": 28.604298572618475,
    "MATH Lvl 5 Raw": 0.09365558912386708,
    "MATH Lvl 5": 9.365558912386708,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.35139583333333335,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.36519281914893614,
    "MMLU-PRO": 29.465868794326234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-CPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-CPO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-CPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-CPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-CPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-CPO-v0.2",
    "Model sha": "5ed83728712693437bd547f4cd32923ac4e1172d",
    "Average ‚¨ÜÔ∏è": 24.821014388873724,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7728855456988353,
    "IFEval Raw": 0.7505817896514582,
    "IFEval": 75.05817896514581,
    "BBH Raw": 0.5026669871217129,
    "BBH": 29.086406714200834,
    "MATH Lvl 5 Raw": 0.1042296072507553,
    "MATH Lvl 5": 10.42296072507553,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.36190625000000004,
    "MUSR": 2.8382812499999996,
    "MMLU-PRO Raw": 0.37059507978723405,
    "MMLU-PRO": 30.06611997635934,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-CPO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-DPO",
    "Model sha": "0afbf4c012ec7507f61c554999151b95a3651db3",
    "Average ‚¨ÜÔ∏è": 22.66742401601125,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5648537840381018,
    "IFEval Raw": 0.6757436934001781,
    "IFEval": 67.57436934001782,
    "BBH Raw": 0.4991303079139502,
    "BBH": 28.50739167799402,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.37381250000000005,
    "MUSR": 3.9265625000000015,
    "MMLU-PRO Raw": 0.36652260638297873,
    "MMLU-PRO": 29.613622931442084,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-DPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-DPO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-DPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2",
    "Model sha": "d06275e02abbeaf29d911a3c0cf22922dcca6b0b",
    "Average ‚¨ÜÔ∏è": 24.718026665279236,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6028798290914141,
    "IFEval Raw": 0.7208063493752133,
    "IFEval": 72.08063493752132,
    "BBH Raw": 0.505620320855615,
    "BBH": 28.939587046939987,
    "MATH Lvl 5 Raw": 0.06042296072507554,
    "MATH Lvl 5": 6.042296072507554,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.3844479166666666,
    "MUSR": 5.555989583333333,
    "MMLU-PRO Raw": 0.37691156914893614,
    "MMLU-PRO": 30.767952127659566,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-DPO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-KTO",
    "Model sha": "e697908201cbab01e0ca54088bb8cd2fd99b4574",
    "Average ‚¨ÜÔ∏è": 22.78964072446439,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6025171322260297,
    "IFEval Raw": 0.6864098370102439,
    "IFEval": 68.6409837010244,
    "BBH Raw": 0.4981903187457697,
    "BBH": 28.64965788695442,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.36984374999999997,
    "MUSR": 3.630468749999999,
    "MMLU-PRO Raw": 0.35987367021276595,
    "MMLU-PRO": 28.874852245862886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-KTO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-KTO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-KTO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-KTO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-KTO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-KTO-v0.2",
    "Model sha": "477d33ea62ed57a0429517170612aa1df21c78d6",
    "Average ‚¨ÜÔ∏è": 24.344687156912922,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6305363966824757,
    "IFEval Raw": 0.7290245437660962,
    "IFEval": 72.90245437660963,
    "BBH Raw": 0.5079766897761946,
    "BBH": 29.648405523209778,
    "MATH Lvl 5 Raw": 0.08081570996978851,
    "MATH Lvl 5": 8.08157099697885,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.37775,
    "MUSR": 4.4520833333333325,
    "MMLU-PRO Raw": 0.3667719414893617,
    "MMLU-PRO": 29.6413268321513,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-KTO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-ORPO",
    "Model sha": "4bb3ffcf9ede48cb01a10bf3223eb41b59aa3fef",
    "Average ‚¨ÜÔ∏è": 23.53447504508262,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6239041153562161,
    "IFEval Raw": 0.712813113649561,
    "IFEval": 71.2813113649561,
    "BBH Raw": 0.5001206199104097,
    "BBH": 28.839356158957287,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.35018750000000004,
    "MUSR": 3.240104166666668,
    "MMLU-PRO Raw": 0.36461103723404253,
    "MMLU-PRO": 29.40122635933806,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-ORPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-ORPO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-ORPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-ORPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-ORPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-ORPO-v0.2",
    "Model sha": "3ea5c542a3d8d61f6afb6cdbef5972a501ddf759",
    "Average ‚¨ÜÔ∏è": 25.852851896932588,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5942324942157904,
    "IFEval Raw": 0.7633213207622442,
    "IFEval": 76.33213207622441,
    "BBH Raw": 0.507835231782556,
    "BBH": 29.60483732707141,
    "MATH Lvl 5 Raw": 0.09516616314199396,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.37796874999999996,
    "MUSR": 4.8460937500000005,
    "MMLU-PRO Raw": 0.37308843085106386,
    "MMLU-PRO": 30.343158983451534,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-ORPO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-RDPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-RDPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-RDPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-RDPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-RDPO",
    "Model sha": "9497ca226a68981f42df2e5b3a4a1a2ea702a942",
    "Average ‚¨ÜÔ∏è": 22.584116934438033,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5662499674735774,
    "IFEval Raw": 0.6660017642078574,
    "IFEval": 66.60017642078574,
    "BBH Raw": 0.5033626077797596,
    "BBH": 29.032479102136296,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.3752083333333333,
    "MUSR": 4.201041666666666,
    "MMLU-PRO Raw": 0.36070478723404253,
    "MMLU-PRO": 28.967198581560282,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-RDPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-RDPO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-RDPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-RDPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-RDPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-RDPO-v0.2",
    "Model sha": "4e5bc9779cba3a2f615379d3f8ef1bbb3ea487f7",
    "Average ‚¨ÜÔ∏è": 24.42799546082195,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5579481941063958,
    "IFEval Raw": 0.7076922565459647,
    "IFEval": 70.76922565459647,
    "BBH Raw": 0.5049218189829557,
    "BBH": 28.85427665062166,
    "MATH Lvl 5 Raw": 0.05060422960725076,
    "MATH Lvl 5": 5.0604229607250755,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.3804479166666666,
    "MUSR": 5.355989583333333,
    "MMLU-PRO Raw": 0.37741023936170215,
    "MMLU-PRO": 30.82335992907802,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-RDPO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-RRHF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-RRHF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-RRHF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-RRHF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-RRHF",
    "Model sha": "73561d9b0fd42b94250246f8d794251fe9f9d2e9",
    "Average ‚¨ÜÔ∏è": 24.05931787265854,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6392157227940654,
    "IFEval Raw": 0.7274509412802475,
    "IFEval": 72.74509412802476,
    "BBH Raw": 0.49105468765647214,
    "BBH": 27.21648494751436,
    "MATH Lvl 5 Raw": 0.09516616314199394,
    "MATH Lvl 5": 9.516616314199394,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3475520833333334,
    "MUSR": 1.47734375,
    "MMLU-PRO Raw": 0.36436170212765956,
    "MMLU-PRO": 29.373522458628837,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-RRHF"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-RRHF-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-RRHF-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-RRHF-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-RRHF-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-RRHF-v0.2",
    "Model sha": "81191fbb214d17f0a4fec247da5d648f4cb61ef1",
    "Average ‚¨ÜÔ∏è": 23.75375059997253,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5058734967410842,
    "IFEval Raw": 0.712488419615509,
    "IFEval": 71.24884196155091,
    "BBH Raw": 0.49838952572927536,
    "BBH": 28.498723991187727,
    "MATH Lvl 5 Raw": 0.08761329305135951,
    "MATH Lvl 5": 8.76132930513595,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.37378125,
    "MUSR": 5.089322916666668,
    "MMLU-PRO Raw": 0.3482380319148936,
    "MMLU-PRO": 27.582003546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-RRHF-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-SLiC-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SLiC-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-SLiC-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-SLiC-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-SLiC-HF",
    "Model sha": "7e9001f6f4fe940c363bb7ea1814d33c79b21737",
    "Average ‚¨ÜÔ∏è": 25.056381748983814,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7251922683253157,
    "IFEval Raw": 0.7399655137258031,
    "IFEval": 73.99655137258031,
    "BBH Raw": 0.5029422936734547,
    "BBH": 29.211612180239623,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.3722916666666667,
    "MUSR": 5.369791666666667,
    "MMLU-PRO Raw": 0.35846077127659576,
    "MMLU-PRO": 28.717863475177303,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-SLiC-HF"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-SLiC-HF-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SLiC-HF-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-SLiC-HF-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-SLiC-HF-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-SLiC-HF-v0.2",
    "Model sha": "1821cc42189d8dab9e157c31b223dc60fc037c2d",
    "Average ‚¨ÜÔ∏è": 23.728355019948566,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5212392670154666,
    "IFEval Raw": 0.7109646848140712,
    "IFEval": 71.09646848140711,
    "BBH Raw": 0.49838952572927536,
    "BBH": 28.498723991187727,
    "MATH Lvl 5 Raw": 0.08761329305135951,
    "MATH Lvl 5": 8.76132930513595,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.37378125,
    "MUSR": 5.089322916666668,
    "MMLU-PRO Raw": 0.3482380319148936,
    "MMLU-PRO": 27.582003546099287,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-SLiC-HF-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-SimPO",
    "Model sha": "f700cb6afb4509b10dea43ab72bb0e260e166be4",
    "Average ‚¨ÜÔ∏è": 22.657116024857118,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 55,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5333456023212843,
    "IFEval Raw": 0.6503898544750152,
    "IFEval": 65.03898544750152,
    "BBH Raw": 0.48446848524905367,
    "BBH": 26.709132779658223,
    "MATH Lvl 5 Raw": 0.025679758308157108,
    "MATH Lvl 5": 2.567975830815711,
    "GPQA Raw": 0.2936241610738255,
    "GPQA": 5.8165548098433995,
    "MUSR Raw": 0.39483333333333337,
    "MUSR": 8.154166666666669,
    "MMLU-PRO Raw": 0.3489029255319149,
    "MMLU-PRO": 27.655880614657207,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-SimPO"
  },
  {
    "eval_name": "princeton-nlp_Llama-3-Instruct-8B-SimPO-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Llama-3-Instruct-8B-SimPO-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2",
    "Model sha": "9ac0fbee445e7755e50520e9881d67588b4b854c",
    "Average ‚¨ÜÔ∏è": 24.47460110863511,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5799816078244477,
    "IFEval Raw": 0.6808645505037745,
    "IFEval": 68.08645505037744,
    "BBH Raw": 0.503833834044343,
    "BBH": 29.214021710829382,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.3988020833333334,
    "MUSR": 7.8502604166666705,
    "MMLU-PRO Raw": 0.36220079787234044,
    "MMLU-PRO": 29.1334219858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-09-28",
    "Generation": 0,
    "Base Model": "princeton-nlp/Llama-3-Instruct-8B-SimPO-v0.2"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-CPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-CPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-CPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-CPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-CPO",
    "Model sha": "7f67394668b94a9ddfb64daff8976b48b135d96c",
    "Average ‚¨ÜÔ∏è": 17.37379376007735,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8097687829360695,
    "IFEval Raw": 0.46549267055856236,
    "IFEval": 46.549267055856234,
    "BBH Raw": 0.43821512506663574,
    "BBH": 21.857696499882195,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.4070833333333333,
    "MUSR": 9.252083333333333,
    "MMLU-PRO Raw": 0.26512632978723405,
    "MMLU-PRO": 18.34736997635934,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-CPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-DPO",
    "Model sha": "17134fd80cfbf3980353967a30dc6f450f18f78f",
    "Average ‚¨ÜÔ∏è": 16.23632494136771,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6676196704743197,
    "IFEval Raw": 0.44033830237104216,
    "IFEval": 44.03383023710421,
    "BBH Raw": 0.43501123979612694,
    "BBH": 20.79098038827006,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.41222916666666665,
    "MUSR": 9.628645833333332,
    "MMLU-PRO Raw": 0.26454454787234044,
    "MMLU-PRO": 18.28272754137116,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-DPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-IPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-IPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-IPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-IPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-IPO",
    "Model sha": "eea781724e4d2ab8bdda7c13526f042de4cfae41",
    "Average ‚¨ÜÔ∏è": 17.21042759741096,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6673344587639496,
    "IFEval Raw": 0.48295300912689443,
    "IFEval": 48.29530091268944,
    "BBH Raw": 0.4458024605899282,
    "BBH": 23.70349052130433,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.37762500000000004,
    "MUSR": 4.836458333333334,
    "MMLU-PRO Raw": 0.2791722074468085,
    "MMLU-PRO": 19.908023049645386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-IPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-KTO",
    "Model sha": "02148bb9241b0f4bb0c75e93893eed005abe25e8",
    "Average ‚¨ÜÔ∏è": 18.96263981716781,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.666016525476527,
    "IFEval Raw": 0.478481540091402,
    "IFEval": 47.848154009140195,
    "BBH Raw": 0.44764334464528677,
    "BBH": 23.10764227790982,
    "MATH Lvl 5 Raw": 0.03625377643504533,
    "MATH Lvl 5": 3.6253776435045326,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.43678124999999995,
    "MUSR": 13.030989583333335,
    "MMLU-PRO Raw": 0.28715093085106386,
    "MMLU-PRO": 20.794547872340427,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-KTO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-RDPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-RDPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-RDPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-RDPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-RDPO",
    "Model sha": "2a63a6d9e1978c99444e440371268f7c2b7e0375",
    "Average ‚¨ÜÔ∏è": 16.46575739308115,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6625052174402891,
    "IFEval Raw": 0.46064663980460735,
    "IFEval": 46.064663980460736,
    "BBH Raw": 0.44395328626924213,
    "BBH": 22.98200980704625,
    "MATH Lvl 5 Raw": 0.02039274924471299,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.3579375,
    "MUSR": 4.275520833333334,
    "MMLU-PRO Raw": 0.27767619680851063,
    "MMLU-PRO": 19.741799645390067,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-RDPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-RRHF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-RRHF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-RRHF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-RRHF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-RRHF",
    "Model sha": "0d5861072e9d01f420451bf6a5b108bc8d3a76bc",
    "Average ‚¨ÜÔ∏è": 16.194612659830515,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6690011367211723,
    "IFEval Raw": 0.44066299640509404,
    "IFEval": 44.0662996405094,
    "BBH Raw": 0.42805937403716016,
    "BBH": 19.59883081662414,
    "MATH Lvl 5 Raw": 0.0256797583081571,
    "MATH Lvl 5": 2.56797583081571,
    "GPQA Raw": 0.2902684563758389,
    "GPQA": 5.369127516778524,
    "MUSR Raw": 0.4186770833333333,
    "MUSR": 10.034635416666665,
    "MMLU-PRO Raw": 0.23977726063829788,
    "MMLU-PRO": 15.530806737588652,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-RRHF"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-SLiC-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-SLiC-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-SLiC-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-SLiC-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-SLiC-HF",
    "Model sha": "65d2cc49ad05258da3d982b39682c7f672f5e4ab",
    "Average ‚¨ÜÔ∏è": 18.955533214031515,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.668441955801904,
    "IFEval Raw": 0.5127284494031392,
    "IFEval": 51.27284494031392,
    "BBH Raw": 0.44223991890402176,
    "BBH": 22.304722895019296,
    "MATH Lvl 5 Raw": 0.032477341389728104,
    "MATH Lvl 5": 3.2477341389728105,
    "GPQA Raw": 0.29194630872483224,
    "GPQA": 5.592841163310966,
    "MUSR Raw": 0.42608333333333337,
    "MUSR": 11.527083333333332,
    "MMLU-PRO Raw": 0.2780917553191489,
    "MMLU-PRO": 19.78797281323877,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-SLiC-HF"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Base-SFT-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Base-SFT-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Base-SFT-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Base-SFT-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Base-SFT-SimPO",
    "Model sha": "9d9e8b8de4f673d45bc826efc4a1444f9d480222",
    "Average ‚¨ÜÔ∏è": 16.8935452731778,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6357063932920375,
    "IFEval Raw": 0.47006387496287627,
    "IFEval": 47.00638749628763,
    "BBH Raw": 0.4398050727924064,
    "BBH": 22.33288648076616,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.39706250000000004,
    "MUSR": 8.0328125,
    "MMLU-PRO Raw": 0.27019614361702127,
    "MMLU-PRO": 18.910682624113473,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Base-SFT-SimPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-CPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-CPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-CPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-CPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-CPO",
    "Model sha": "32492f8e5588f06005689ac944c2ea39c394c28e",
    "Average ‚¨ÜÔ∏è": 15.565535434141871,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.645922259585959,
    "IFEval Raw": 0.4203047912871182,
    "IFEval": 42.03047912871182,
    "BBH Raw": 0.406922267565148,
    "BBH": 17.248538100586853,
    "MATH Lvl 5 Raw": 0.021903323262839884,
    "MATH Lvl 5": 2.1903323262839884,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.41784375,
    "MUSR": 10.89713541666667,
    "MMLU-PRO Raw": 0.2701130319148936,
    "MMLU-PRO": 18.901447990543733,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-CPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-DPO",
    "Model sha": "5e96cff70d8db87cf17c616429c17c8dc9352543",
    "Average ‚¨ÜÔ∏è": 16.549607396377137,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6052665886001811,
    "IFEval Raw": 0.517624347841505,
    "IFEval": 51.7624347841505,
    "BBH Raw": 0.4060358459697702,
    "BBH": 16.875389341982814,
    "MATH Lvl 5 Raw": 0.03021148036253777,
    "MATH Lvl 5": 3.021148036253777,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3833333333333333,
    "MUSR": 5.750000000000001,
    "MMLU-PRO Raw": 0.2748503989361702,
    "MMLU-PRO": 19.42782210401891,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-DPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-IPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-IPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-IPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-IPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-IPO",
    "Model sha": "32ad99c6e7231bbe8ebd9d24b28e084c60848558",
    "Average ‚¨ÜÔ∏è": 17.707096374684916,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6257476715781332,
    "IFEval Raw": 0.4929198969844457,
    "IFEval": 49.29198969844457,
    "BBH Raw": 0.4322183023180588,
    "BBH": 20.094109548877523,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.43241666666666667,
    "MUSR": 12.785416666666668,
    "MMLU-PRO Raw": 0.2707779255319149,
    "MMLU-PRO": 18.975325059101653,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-IPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-KTO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-KTO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-KTO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-KTO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-KTO",
    "Model sha": "834422e5b9b9eee6aac2f8d4822b925a6574d628",
    "Average ‚¨ÜÔ∏è": 16.664827429190535,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6033781298318931,
    "IFEval Raw": 0.4907966417993147,
    "IFEval": 49.07966417993147,
    "BBH Raw": 0.4139586477181159,
    "BBH": 17.81264785919903,
    "MATH Lvl 5 Raw": 0.024169184290030215,
    "MATH Lvl 5": 2.4169184290030215,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.3952708333333333,
    "MUSR": 7.408854166666667,
    "MMLU-PRO Raw": 0.28125,
    "MMLU-PRO": 20.138888888888886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-KTO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-ORPO",
    "Model sha": "69c0481f4100629a49ae73f760ddbb61d8e98e48",
    "Average ‚¨ÜÔ∏è": 16.050528759167427,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6242966238404694,
    "IFEval Raw": 0.4719621714827768,
    "IFEval": 47.19621714827768,
    "BBH Raw": 0.41040615756566107,
    "BBH": 18.038372836612158,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3912395833333333,
    "MUSR": 6.638281250000001,
    "MMLU-PRO Raw": 0.2662067819148936,
    "MMLU-PRO": 18.467420212765955,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-ORPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-RDPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-RDPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-RDPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-RDPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-RDPO",
    "Model sha": "23ec6ab4f996134eb15c19322dabb34d7332d7cd",
    "Average ‚¨ÜÔ∏è": 16.420490569691292,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6106155591198511,
    "IFEval Raw": 0.4887232542985944,
    "IFEval": 48.872325429859444,
    "BBH Raw": 0.40501479745073615,
    "BBH": 17.04838760964466,
    "MATH Lvl 5 Raw": 0.024169184290030215,
    "MATH Lvl 5": 2.4169184290030215,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.3873333333333333,
    "MUSR": 6.416666666666669,
    "MMLU-PRO Raw": 0.27767619680851063,
    "MMLU-PRO": 19.741799645390067,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-RDPO"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-RRHF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-RRHF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-RRHF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-RRHF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-RRHF",
    "Model sha": "493d3ceb571232fe3b2f55c0bf78692760f4fc7e",
    "Average ‚¨ÜÔ∏è": 16.82908340292271,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5877512961659127,
    "IFEval Raw": 0.49601723427173233,
    "IFEval": 49.60172342717323,
    "BBH Raw": 0.41897663476657404,
    "BBH": 19.20655206374787,
    "MATH Lvl 5 Raw": 0.024169184290030215,
    "MATH Lvl 5": 2.4169184290030215,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.397875,
    "MUSR": 7.934375000000003,
    "MMLU-PRO Raw": 0.26512632978723405,
    "MMLU-PRO": 18.34736997635934,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-07",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-RRHF"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-SLiC-HF_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-SLiC-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-SLiC-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-SLiC-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-SLiC-HF",
    "Model sha": "3d08c8b7c3e73beb2a3264848f17246b74c3d162",
    "Average ‚¨ÜÔ∏è": 16.376555520375778,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6224528546377669,
    "IFEval Raw": 0.5115294086357531,
    "IFEval": 51.15294086357531,
    "BBH Raw": 0.4040013641288438,
    "BBH": 16.653429432655862,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.39130208333333333,
    "MUSR": 6.712760416666666,
    "MMLU-PRO Raw": 0.27152593085106386,
    "MMLU-PRO": 19.058436761229316,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-06",
    "Submission Date": "2024-10-16",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-SLiC-HF"
  },
  {
    "eval_name": "princeton-nlp_Mistral-7B-Instruct-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Mistral-7B-Instruct-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Mistral-7B-Instruct-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Mistral-7B-Instruct-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Mistral-7B-Instruct-SimPO",
    "Model sha": "03191ee1e60d21a698d11a515703a037073724f8",
    "Average ‚¨ÜÔ∏è": 17.569551449088625,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.570562459649707,
    "IFEval Raw": 0.4686897432146704,
    "IFEval": 46.868974321467036,
    "BBH Raw": 0.4507226157033399,
    "BBH": 22.38227741589404,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.40978125,
    "MUSR": 9.755989583333333,
    "MMLU-PRO Raw": 0.2796708776595745,
    "MMLU-PRO": 19.963430851063833,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-24",
    "Submission Date": "2024-09-21",
    "Generation": 0,
    "Base Model": "princeton-nlp/Mistral-7B-Instruct-SimPO"
  },
  {
    "eval_name": "princeton-nlp_Sheared-LLaMA-1.3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Sheared-LLaMA-1.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Sheared-LLaMA-1.3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Sheared-LLaMA-1.3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Sheared-LLaMA-1.3B",
    "Model sha": "a4b76938edbf571ea7d7d9904861cbdca08809b4",
    "Average ‚¨ÜÔ∏è": 5.505396871233472,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 91,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3546000820737492,
    "IFEval Raw": 0.2197702097102355,
    "IFEval": 21.97702097102355,
    "BBH Raw": 0.31970467392464424,
    "BBH": 4.744629874421679,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.23993288590604026,
    "GPQA": 0.0,
    "MUSR Raw": 0.3713020833333333,
    "MUSR": 3.5794270833333326,
    "MMLU-PRO Raw": 0.11710438829787234,
    "MMLU-PRO": 1.9004875886524817,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-10",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "princeton-nlp/Sheared-LLaMA-1.3B"
  },
  {
    "eval_name": "princeton-nlp_Sheared-LLaMA-2.7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/Sheared-LLaMA-2.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/Sheared-LLaMA-2.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__Sheared-LLaMA-2.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/Sheared-LLaMA-2.7B",
    "Model sha": "2f157a0306b75d37694ae05f6a4067220254d540",
    "Average ‚¨ÜÔ∏è": 6.324627009658596,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 60,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.47004976391081105,
    "IFEval Raw": 0.24165214962964932,
    "IFEval": 24.16521496296493,
    "BBH Raw": 0.32586855691245953,
    "BBH": 5.655521329938437,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.3567291666666667,
    "MUSR": 2.091145833333335,
    "MMLU-PRO Raw": 0.11868351063829788,
    "MMLU-PRO": 2.0759456264775418,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-10",
    "Submission Date": "2024-07-29",
    "Generation": 0,
    "Base Model": "princeton-nlp/Sheared-LLaMA-2.7B"
  },
  {
    "eval_name": "princeton-nlp_gemma-2-9b-it-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/gemma-2-9b-it-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/gemma-2-9b-it-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__gemma-2-9b-it-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/gemma-2-9b-it-DPO",
    "Model sha": "f646c99fc3aa7afc7b22c3c7115fd03a40fc1d22",
    "Average ‚¨ÜÔ∏è": 19.434034541190076,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.89062697102829,
    "IFEval Raw": 0.27687203287277756,
    "IFEval": 27.687203287277757,
    "BBH Raw": 0.5941444682956648,
    "BBH": 41.59365445538448,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.38203125,
    "MUSR": 5.653906250000001,
    "MMLU-PRO Raw": 0.3723404255319149,
    "MMLU-PRO": 30.26004728132387,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-09-19",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "princeton-nlp_gemma-2-9b-it-SimPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/princeton-nlp/gemma-2-9b-it-SimPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">princeton-nlp/gemma-2-9b-it-SimPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/princeton-nlp__gemma-2-9b-it-SimPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "princeton-nlp/gemma-2-9b-it-SimPO",
    "Model sha": "8c87091f412e3aa6f74f66bd86c57fb81cbc3fde",
    "Average ‚¨ÜÔ∏è": 21.161651627569338,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 124,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7690037242540337,
    "IFEval Raw": 0.3206857803960159,
    "IFEval": 32.06857803960159,
    "BBH Raw": 0.5839179923162123,
    "BBH": 40.093429916371655,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.33557046979865773,
    "GPQA": 11.409395973154364,
    "MUSR Raw": 0.41232291666666665,
    "MUSR": 10.340364583333338,
    "MMLU-PRO Raw": 0.39752327127659576,
    "MMLU-PRO": 33.05814125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-16",
    "Submission Date": "2024-08-10",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "pszemraj_Llama-3-6.3b-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/Llama-3-6.3b-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/Llama-3-6.3b-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pszemraj__Llama-3-6.3b-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pszemraj/Llama-3-6.3b-v0.1",
    "Model sha": "7000b39346162f95f19aa4ca3975242db61902d7",
    "Average ‚¨ÜÔ∏è": 10.33395420311163,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8144632872297608,
    "IFEval Raw": 0.10438968603305895,
    "IFEval": 10.438968603305895,
    "BBH Raw": 0.41968070468284147,
    "BBH": 18.67999639960586,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3908333333333333,
    "MUSR": 6.154166666666666,
    "MMLU-PRO Raw": 0.2839926861702128,
    "MMLU-PRO": 20.44363179669031,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-17",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "pszemraj_Mistral-v0.3-6B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/pszemraj/Mistral-v0.3-6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">pszemraj/Mistral-v0.3-6B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/pszemraj__Mistral-v0.3-6B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "pszemraj/Mistral-v0.3-6B",
    "Model sha": "ae11a699012b83996361f04808f4d45debf3b01c",
    "Average ‚¨ÜÔ∏è": 10.0468505270085,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 5,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5305394539336015,
    "IFEval Raw": 0.2453744952282167,
    "IFEval": 24.537449522821667,
    "BBH Raw": 0.3774050646438491,
    "BBH": 13.515091344549441,
    "MATH Lvl 5 Raw": 0.00906344410876133,
    "MATH Lvl 5": 0.906344410876133,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.39077083333333335,
    "MUSR": 6.613020833333334,
    "MMLU-PRO Raw": 0.2142619680851064,
    "MMLU-PRO": 12.695774231678486,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-25",
    "Submission Date": "2024-06-26",
    "Generation": 2,
    "Base Model": "pszemraj/Mistral-7B-v0.3-prune6 (Merge)"
  },
  {
    "eval_name": "qingy2019_LLaMa_3.2_3B_Catalysts_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/qingy2019/LLaMa_3.2_3B_Catalysts\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">qingy2019/LLaMa_3.2_3B_Catalysts</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/qingy2019__LLaMa_3.2_3B_Catalysts-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "qingy2019/LLaMa_3.2_3B_Catalysts",
    "Model sha": "3f4a318114beb37f32a2c143cbd68b6d15d18164",
    "Average ‚¨ÜÔ∏è": 19.628815881381005,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6498338267277844,
    "IFEval Raw": 0.499239794855428,
    "IFEval": 49.9239794855428,
    "BBH Raw": 0.44681268798954793,
    "BBH": 21.345400954820075,
    "MATH Lvl 5 Raw": 0.11102719033232629,
    "MATH Lvl 5": 11.10271903323263,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.37877083333333333,
    "MUSR": 7.946354166666668,
    "MMLU-PRO Raw": 0.30078125,
    "MMLU-PRO": 22.309027777777775,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-19",
    "Submission Date": "2024-10-29",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "qq8933_OpenLongCoT-Base-Gemma2-2B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/qq8933/OpenLongCoT-Base-Gemma2-2B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">qq8933/OpenLongCoT-Base-Gemma2-2B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/qq8933__OpenLongCoT-Base-Gemma2-2B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "qq8933/OpenLongCoT-Base-Gemma2-2B",
    "Model sha": "39e5bc941f107ac28142c802aecfd257cc47c1bb",
    "Average ‚¨ÜÔ∏è": 5.082910297196927,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6584866670975984,
    "IFEval Raw": 0.1965141380426158,
    "IFEval": 19.65141380426158,
    "BBH Raw": 0.3106362870893106,
    "BBH": 3.546298466806123,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.32225,
    "MUSR": 2.114583333333334,
    "MMLU-PRO Raw": 0.1315658244680851,
    "MMLU-PRO": 3.507313829787232,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-11-12",
    "Generation": 2,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "rasyosef_Mistral-NeMo-Minitron-8B-Chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rasyosef/Mistral-NeMo-Minitron-8B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rasyosef/Mistral-NeMo-Minitron-8B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rasyosef__Mistral-NeMo-Minitron-8B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rasyosef/Mistral-NeMo-Minitron-8B-Chat",
    "Model sha": "cede47eac8a4e65aa27567d3f087c28185b537d9",
    "Average ‚¨ÜÔ∏è": 17.230945642276307,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4763979510576521,
    "IFEval Raw": 0.4451843331249973,
    "IFEval": 44.518433312499724,
    "BBH Raw": 0.47594353379058535,
    "BBH": 26.036695387358723,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.4304270833333333,
    "MUSR": 12.936718749999997,
    "MMLU-PRO Raw": 0.2403590425531915,
    "MMLU-PRO": 15.595449172576831,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-08-26",
    "Generation": 1,
    "Base Model": "nvidia/Mistral-NeMo-Minitron-8B-Base"
  },
  {
    "eval_name": "rasyosef_Phi-1_5-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rasyosef/Phi-1_5-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rasyosef/Phi-1_5-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rasyosef__Phi-1_5-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rasyosef/Phi-1_5-Instruct-v0.1",
    "Model sha": "f4c405ee4bff5dc1a69383f3fe682342c9c87c77",
    "Average ‚¨ÜÔ∏è": 6.638161761630591,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.29502193496014384,
    "IFEval Raw": 0.24022815019703275,
    "IFEval": 24.022815019703277,
    "BBH Raw": 0.3117898107092894,
    "BBH": 4.820243721122045,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.34215625,
    "MUSR": 3.4028645833333346,
    "MMLU-PRO Raw": 0.15616688829787234,
    "MMLU-PRO": 6.240765366430259,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-07-25",
    "Generation": 1,
    "Base Model": "microsoft/phi-1_5"
  },
  {
    "eval_name": "rasyosef_phi-2-instruct-apo_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rasyosef/phi-2-instruct-apo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rasyosef/phi-2-instruct-apo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rasyosef__phi-2-instruct-apo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rasyosef/phi-2-instruct-apo",
    "Model sha": "2d3722d6db77a8c844a50dd32ddc4278fdc89e1f",
    "Average ‚¨ÜÔ∏è": 12.043527849960943,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4950649054145608,
    "IFEval Raw": 0.31459194936102874,
    "IFEval": 31.459194936102875,
    "BBH Raw": 0.44450964630048634,
    "BBH": 21.672437586715603,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.33421875,
    "MUSR": 3.6106770833333353,
    "MMLU-PRO Raw": 0.21550864361702127,
    "MMLU-PRO": 12.834293735224584,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-17",
    "Generation": 1,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "rasyosef_phi-2-instruct-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rasyosef/phi-2-instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rasyosef/phi-2-instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rasyosef__phi-2-instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rasyosef/phi-2-instruct-v0.1",
    "Model sha": "29aeb3ccf7c79e0169a038fbd0deaf9772a9fefd",
    "Average ‚¨ÜÔ∏è": 14.218631101919177,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.49272607630116594,
    "IFEval Raw": 0.3681476260765879,
    "IFEval": 36.81476260765879,
    "BBH Raw": 0.47261184292654473,
    "BBH": 26.35880186790661,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3523541666666667,
    "MUSR": 5.044270833333334,
    "MMLU-PRO Raw": 0.22465093085106383,
    "MMLU-PRO": 13.850103427895979,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-09",
    "Submission Date": "2024-08-10",
    "Generation": 1,
    "Base Model": "microsoft/phi-2"
  },
  {
    "eval_name": "realtreetune_rho-1b-sft-MATH_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/realtreetune/rho-1b-sft-MATH\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">realtreetune/rho-1b-sft-MATH</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/realtreetune__rho-1b-sft-MATH-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "realtreetune/rho-1b-sft-MATH",
    "Model sha": "b5f93df6af679a860caac9a9598e0f70c326b4fb",
    "Average ‚¨ÜÔ∏è": 5.355177434087309,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.2781341125746011,
    "IFEval Raw": 0.212101668018635,
    "IFEval": 21.210166801863497,
    "BBH Raw": 0.3144153389594046,
    "BBH": 4.19762318329166,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.34584375,
    "MUSR": 2.897135416666666,
    "MMLU-PRO Raw": 0.11170212765957446,
    "MMLU-PRO": 1.300236406619384,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-06",
    "Submission Date": "2024-10-05",
    "Generation": 1,
    "Base Model": "realtreetune/rho-1b-sft-MATH (Merge)"
  },
  {
    "eval_name": "recoilme_Gemma-2-Ataraxy-Gemmasutra-9B-slerp_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__Gemma-2-Ataraxy-Gemmasutra-9B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp",
    "Model sha": "9048af8616bc62b6efab2bc1bc77ba53c5dfed79",
    "Average ‚¨ÜÔ∏è": 29.873991757143997,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.114373144767299,
    "IFEval Raw": 0.7648949232480928,
    "IFEval": 76.48949232480928,
    "BBH Raw": 0.597438766061506,
    "BBH": 42.25120987807252,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.4244791666666667,
    "MUSR": 12.39322916666667,
    "MMLU-PRO Raw": 0.4207114361702128,
    "MMLU-PRO": 35.63460401891253,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-12",
    "Generation": 0,
    "Base Model": "recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp"
  },
  {
    "eval_name": "recoilme_Gemma-2-Ataraxy-Gemmasutra-9B-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__Gemma-2-Ataraxy-Gemmasutra-9B-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp",
    "Model sha": "5a4f7299d9f8ea5faad2b1edc68b7bf634dac40b",
    "Average ‚¨ÜÔ∏è": 23.205618260897882,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9698275311503877,
    "IFEval Raw": 0.28536505361330156,
    "IFEval": 28.536505361330157,
    "BBH Raw": 0.5983926033872208,
    "BBH": 42.703797634497924,
    "MATH Lvl 5 Raw": 0.058157099697885205,
    "MATH Lvl 5": 5.815709969788521,
    "GPQA Raw": 0.3296979865771812,
    "GPQA": 10.626398210290827,
    "MUSR Raw": 0.46065625,
    "MUSR": 16.415364583333332,
    "MMLU-PRO Raw": 0.4162234042553192,
    "MMLU-PRO": 35.13593380614657,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "recoilme/Gemma-2-Ataraxy-Gemmasutra-9B-slerp"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.1_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.1",
    "Model sha": "6dc0997046db4e9932f87d338ecdc2a4158abbda",
    "Average ‚¨ÜÔ∏è": 29.60274564940404,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9248087162364753,
    "IFEval Raw": 0.751506004069203,
    "IFEval": 75.1506004069203,
    "BBH Raw": 0.5995309756292291,
    "BBH": 42.321861031477475,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.3389261744966443,
    "GPQA": 11.85682326621924,
    "MUSR Raw": 0.41914583333333333,
    "MUSR": 11.526562500000002,
    "MMLU-PRO Raw": 0.4158909574468085,
    "MMLU-PRO": 35.09899527186761,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-18",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.2_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.2",
    "Model sha": "483116e575fb3a56de25243b14d715c58fe127bc",
    "Average ‚¨ÜÔ∏è": 30.048864030373213,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9140857936630222,
    "IFEval Raw": 0.7591745457608035,
    "IFEval": 75.91745457608036,
    "BBH Raw": 0.6025964285724085,
    "BBH": 43.02796904930725,
    "MATH Lvl 5 Raw": 0.052870090634441085,
    "MATH Lvl 5": 5.287009063444108,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.409875,
    "MUSR": 10.401041666666664,
    "MMLU-PRO Raw": 0.41630651595744683,
    "MMLU-PRO": 35.145168439716315,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-18",
    "Generation": 0,
    "Base Model": "recoilme/recoilme-gemma-2-9B-v0.2"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.2",
    "Model sha": "483116e575fb3a56de25243b14d715c58fe127bc",
    "Average ‚¨ÜÔ∏è": 23.674734523330642,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9467844206686786,
    "IFEval Raw": 0.2746989100032359,
    "IFEval": 27.469891000323585,
    "BBH Raw": 0.6030832642626502,
    "BBH": 43.56058143461737,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.33053691275167785,
    "GPQA": 10.738255033557047,
    "MUSR Raw": 0.46859375,
    "MUSR": 17.807552083333327,
    "MMLU-PRO Raw": 0.4122340425531915,
    "MMLU-PRO": 34.692671394799056,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "recoilme/recoilme-gemma-2-9B-v0.2"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.3_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.3",
    "Model sha": "772cab46d9d22cbcc3c574d193021803ce5c444c",
    "Average ‚¨ÜÔ∏è": 30.2074720895527,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8766372136690954,
    "IFEval Raw": 0.743937197746424,
    "IFEval": 74.39371977464239,
    "BBH Raw": 0.5992527878628748,
    "BBH": 42.026279212829245,
    "MATH Lvl 5 Raw": 0.08761329305135952,
    "MATH Lvl 5": 8.761329305135952,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4203854166666667,
    "MUSR": 12.081510416666665,
    "MMLU-PRO Raw": 0.4072473404255319,
    "MMLU-PRO": 34.13859338061466,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-18",
    "Generation": 0,
    "Base Model": "recoilme/recoilme-gemma-2-9B-v0.3"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.3_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.3",
    "Model sha": "76c8fb761660e6eb237c91bb6e6761ee36266bba",
    "Average ‚¨ÜÔ∏è": 30.111638321259665,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.555349708552554,
    "IFEval Raw": 0.57607592299543,
    "IFEval": 57.60759229954299,
    "BBH Raw": 0.6019827101058847,
    "BBH": 43.326868296283614,
    "MATH Lvl 5 Raw": 0.1729607250755287,
    "MATH Lvl 5": 17.29607250755287,
    "GPQA Raw": 0.337248322147651,
    "GPQA": 11.633109619686799,
    "MUSR Raw": 0.46322916666666664,
    "MUSR": 17.03697916666666,
    "MMLU-PRO Raw": 0.4039228723404255,
    "MMLU-PRO": 33.76920803782505,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-27",
    "Generation": 0,
    "Base Model": "recoilme/recoilme-gemma-2-9B-v0.3"
  },
  {
    "eval_name": "recoilme_recoilme-gemma-2-9B-v0.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/recoilme/recoilme-gemma-2-9B-v0.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">recoilme/recoilme-gemma-2-9B-v0.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/recoilme__recoilme-gemma-2-9B-v0.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "recoilme/recoilme-gemma-2-9B-v0.4",
    "Model sha": "2691f2cc8d80072f15d78cb7ae72831e1a12139e",
    "Average ‚¨ÜÔ∏è": 24.100363216853946,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9189102422083795,
    "IFEval Raw": 0.2561891337207498,
    "IFEval": 25.618913372074985,
    "BBH Raw": 0.5967285833554881,
    "BBH": 42.44248167542507,
    "MATH Lvl 5 Raw": 0.0823262839879154,
    "MATH Lvl 5": 8.23262839879154,
    "GPQA Raw": 0.34060402684563756,
    "GPQA": 12.080536912751676,
    "MUSR Raw": 0.4726875,
    "MUSR": 18.3859375,
    "MMLU-PRO Raw": 0.4405751329787234,
    "MMLU-PRO": 37.841681442080386,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-18",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "recoilme/recoilme-gemma-2-9B-v0.4"
  },
  {
    "eval_name": "refuelai_Llama-3-Refueled_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/refuelai/Llama-3-Refueled\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">refuelai/Llama-3-Refueled</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/refuelai__Llama-3-Refueled-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "refuelai/Llama-3-Refueled",
    "Model sha": "ff6d1c3ba37b31d4af421951c2300f2256fb3691",
    "Average ‚¨ÜÔ∏è": 22.803804801742604,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 188,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8759858333778655,
    "IFEval Raw": 0.4619952836252255,
    "IFEval": 46.19952836252256,
    "BBH Raw": 0.5870766201705051,
    "BBH": 41.721971003391026,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.44540625,
    "MUSR": 14.642447916666667,
    "MMLU-PRO Raw": 0.30950797872340424,
    "MMLU-PRO": 23.278664302600472,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-03",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "refuelai/Llama-3-Refueled"
  },
  {
    "eval_name": "rhplus0831_maid-yuzu-v7_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rhplus0831/maid-yuzu-v7\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rhplus0831/maid-yuzu-v7</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rhplus0831__maid-yuzu-v7-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rhplus0831/maid-yuzu-v7",
    "Model sha": "a0bd8c707bb80024778da4a0d057917faa53d2f6",
    "Average ‚¨ÜÔ∏è": 24.481930124524308,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 46,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.104285323181495,
    "IFEval Raw": 0.6462430794735745,
    "IFEval": 64.62430794735747,
    "BBH Raw": 0.480491692312673,
    "BBH": 26.8198371046094,
    "MATH Lvl 5 Raw": 0.09516616314199394,
    "MATH Lvl 5": 9.516616314199394,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.41362499999999996,
    "MUSR": 9.769791666666666,
    "MMLU-PRO Raw": 0.35397273936170215,
    "MMLU-PRO": 28.219193262411352,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-09",
    "Submission Date": "2024-09-08",
    "Generation": 1,
    "Base Model": "rhplus0831/maid-yuzu-v7 (Merge)"
  },
  {
    "eval_name": "rhymes-ai_Aria_bfloat16",
    "Precision": "bfloat16",
    "Type": "üå∏ multimodal",
    "T": "üå∏",
    "Weight type": "Original",
    "Architecture": "AriaForConditionalGeneration",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rhymes-ai/Aria\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rhymes-ai/Aria</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rhymes-ai__Aria-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rhymes-ai/Aria",
    "Model sha": "5cc2703b3afd585f232ec5027e9c039a2001bcec",
    "Average ‚¨ÜÔ∏è": 28.35405120572536,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 584,
    "#Params (B)": 25,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 7.750709623214252,
    "IFEval Raw": 0.4773079872516035,
    "IFEval": 47.730798725160355,
    "BBH Raw": 0.5695312446413633,
    "BBH": 39.28149335481041,
    "MATH Lvl 5 Raw": 0.1623867069486405,
    "MATH Lvl 5": 16.238670694864048,
    "GPQA Raw": 0.3624161073825503,
    "GPQA": 14.988814317673373,
    "MUSR Raw": 0.43375,
    "MUSR": 14.05208333333333,
    "MMLU-PRO Raw": 0.44049202127659576,
    "MMLU-PRO": 37.83244680851063,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-10-10",
    "Generation": 0,
    "Base Model": "rhymes-ai/Aria"
  },
  {
    "eval_name": "rhysjones_phi-2-orange-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "PhiForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rhysjones/phi-2-orange-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rhysjones/phi-2-orange-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rhysjones__phi-2-orange-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rhysjones/phi-2-orange-v2",
    "Model sha": "f4085189114accfb65225deb8fbdf15767b7ee56",
    "Average ‚¨ÜÔ∏è": 14.644426788213977,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4709493454041584,
    "IFEval Raw": 0.3669740732367895,
    "IFEval": 36.697407323678945,
    "BBH Raw": 0.4770220109816213,
    "BBH": 25.60654883732465,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3629583333333333,
    "MUSR": 6.969791666666668,
    "MMLU-PRO Raw": 0.25324135638297873,
    "MMLU-PRO": 17.026817375886523,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-04",
    "Submission Date": "2024-06-28",
    "Generation": 0,
    "Base Model": "rhysjones/phi-2-orange-v2"
  },
  {
    "eval_name": "riaz_FineLlama-3.1-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/riaz/FineLlama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">riaz/FineLlama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/riaz__FineLlama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "riaz/FineLlama-3.1-8B",
    "Model sha": "c4d8f16eb446910edce0c1afd0e6d5f3b06e2e7d",
    "Average ‚¨ÜÔ∏è": 17.610295593029527,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9210915805150138,
    "IFEval Raw": 0.43734070045257695,
    "IFEval": 43.734070045257695,
    "BBH Raw": 0.45857296498013483,
    "BBH": 24.14877809167861,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.3762916666666667,
    "MUSR": 7.76979166666667,
    "MMLU-PRO Raw": 0.29637632978723405,
    "MMLU-PRO": 21.81959219858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-12",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "riaz_FineLlama-3.1-8B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/riaz/FineLlama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">riaz/FineLlama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/riaz__FineLlama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "riaz/FineLlama-3.1-8B",
    "Model sha": "c4d8f16eb446910edce0c1afd0e6d5f3b06e2e7d",
    "Average ‚¨ÜÔ∏è": 17.14751095671923,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9019979013274062,
    "IFEval Raw": 0.413660199382084,
    "IFEval": 41.366019938208396,
    "BBH Raw": 0.456451981676995,
    "BBH": 23.773389590539722,
    "MATH Lvl 5 Raw": 0.045317220543806644,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.37762500000000004,
    "MUSR": 7.76979166666667,
    "MMLU-PRO Raw": 0.29778922872340424,
    "MMLU-PRO": 21.976580969267136,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-12",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "rmdhirr_Gluon-8B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rmdhirr/Gluon-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rmdhirr/Gluon-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rmdhirr__Gluon-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rmdhirr/Gluon-8B",
    "Model sha": "cc949908c60ab7f696e133714222d6cab156e493",
    "Average ‚¨ÜÔ∏è": 23.951786710939242,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9030782884458364,
    "IFEval Raw": 0.5052848663767692,
    "IFEval": 50.52848663767692,
    "BBH Raw": 0.5153305292144984,
    "BBH": 30.34224724618852,
    "MATH Lvl 5 Raw": 0.14274924471299094,
    "MATH Lvl 5": 14.274924471299094,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.4038854166666667,
    "MUSR": 9.085677083333335,
    "MMLU-PRO Raw": 0.38081781914893614,
    "MMLU-PRO": 31.201979905437344,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-14",
    "Generation": 1,
    "Base Model": "rmdhirr/Gluon-8B (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-0.5b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-0.5b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-0.5b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-0.5b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-0.5b",
    "Model sha": "aae2e55548c8090ce357c64ca78e8b9ef6baf118",
    "Average ‚¨ÜÔ∏è": 8.718749803539854,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6457068119892789,
    "IFEval Raw": 0.28466690603155187,
    "IFEval": 28.466690603155183,
    "BBH Raw": 0.32936751831436256,
    "BBH": 8.412218566269734,
    "MATH Lvl 5 Raw": 0.027945619335347432,
    "MATH Lvl 5": 2.794561933534743,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.32358333333333333,
    "MUSR": 0.7812499999999996,
    "MMLU-PRO Raw": 0.18658577127659576,
    "MMLU-PRO": 9.620641252955084,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-0.5b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-1.5b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-1.5b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-1.5b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-1.5b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-1.5b",
    "Model sha": "1f634da015ed671efe7dc574bc2a1954f5b2cc93",
    "Average ‚¨ÜÔ∏è": 16.165564144912796,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7403579961950006,
    "IFEval Raw": 0.3402461025634206,
    "IFEval": 34.02461025634206,
    "BBH Raw": 0.4256703145864387,
    "BBH": 18.711343783972325,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.4185520833333333,
    "MUSR": 10.352343750000001,
    "MMLU-PRO Raw": 0.2922207446808511,
    "MMLU-PRO": 21.35786052009456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-1.5b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-14b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-14b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-14b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-14b",
    "Model sha": "834ddb1712ae6d1b232b2d5b26be658d90d23e43",
    "Average ‚¨ÜÔ∏è": 34.730059643743424,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.1826998616106565,
    "IFEval Raw": 0.5840447789642593,
    "IFEval": 58.40447789642593,
    "BBH Raw": 0.6481086261669653,
    "BBH": 49.38690027144481,
    "MATH Lvl 5 Raw": 0.1691842900302115,
    "MATH Lvl 5": 16.91842900302115,
    "GPQA Raw": 0.3716442953020134,
    "GPQA": 16.21923937360179,
    "MUSR Raw": 0.4717291666666667,
    "MUSR": 18.832812499999992,
    "MMLU-PRO Raw": 0.5375664893617021,
    "MMLU-PRO": 48.6184988179669,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-14b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-32b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-32b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-32b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-32b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-32b",
    "Model sha": "234abe4b494dbe83ba805b791f74feb33462a33d",
    "Average ‚¨ÜÔ∏è": 44.57420016656997,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 25,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 17.912689723519435,
    "IFEval Raw": 0.6826631116548536,
    "IFEval": 68.26631116548536,
    "BBH Raw": 0.7045537070859799,
    "BBH": 58.261894086787414,
    "MATH Lvl 5 Raw": 0.4199395770392749,
    "MATH Lvl 5": 41.99395770392749,
    "GPQA Raw": 0.39681208053691275,
    "GPQA": 19.574944071588366,
    "MUSR Raw": 0.5034166666666667,
    "MUSR": 24.727083333333336,
    "MMLU-PRO Raw": 0.5915890957446809,
    "MMLU-PRO": 54.621010638297875,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-32b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-3b",
    "Model sha": "26601a8da5afce3b5959d91bdd0faaab6df8bf95",
    "Average ‚¨ÜÔ∏è": 22.183111356949308,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.005794477102998,
    "IFEval Raw": 0.5342358276040905,
    "IFEval": 53.42358276040905,
    "BBH Raw": 0.4808896246368473,
    "BBH": 27.213596951125698,
    "MATH Lvl 5 Raw": 0.055135951661631426,
    "MATH Lvl 5": 5.513595166163142,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4041666666666666,
    "MUSR": 8.554166666666667,
    "MMLU-PRO Raw": 0.37608045212765956,
    "MMLU-PRO": 30.67560579196217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-3b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-72b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-72b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-72b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-72b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-72b",
    "Model sha": "5260f182e7859e13d515c4cb3926ac85ad057504",
    "Average ‚¨ÜÔ∏è": 45.9092456451536,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 23,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 16.03394562589673,
    "IFEval Raw": 0.715535889218385,
    "IFEval": 71.5535889218385,
    "BBH Raw": 0.7229589065788488,
    "BBH": 61.26714504573664,
    "MATH Lvl 5 Raw": 0.506797583081571,
    "MATH Lvl 5": 50.6797583081571,
    "GPQA Raw": 0.39848993288590606,
    "GPQA": 19.798657718120808,
    "MUSR Raw": 0.4599166666666667,
    "MUSR": 17.322916666666668,
    "MMLU-PRO Raw": 0.593500664893617,
    "MMLU-PRO": 54.83340721040189,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-72b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5-Qwen-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5-Qwen-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5-Qwen-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5-Qwen-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5-Qwen-7b",
    "Model sha": "dbd819e8f765181f774cb5b79812d081669eb302",
    "Average ‚¨ÜÔ∏è": 31.112348349566087,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3170840554561158,
    "IFEval Raw": 0.6237117514860571,
    "IFEval": 62.3711751486057,
    "BBH Raw": 0.5543885046903589,
    "BBH": 36.372350414300634,
    "MATH Lvl 5 Raw": 0.28323262839879154,
    "MATH Lvl 5": 28.323262839879153,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.42909375,
    "MUSR": 12.003385416666669,
    "MMLU-PRO Raw": 0.4468916223404255,
    "MMLU-PRO": 38.54351359338061,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5-Qwen-7b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5.1-Qwen-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5.1-Qwen-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5.1-Qwen-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5.1-Qwen-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5.1-Qwen-3b",
    "Model sha": "a3305ce148f4273ab334052ab47d3aebb51d104c",
    "Average ‚¨ÜÔ∏è": 13.357124972053427,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9292438685729816,
    "IFEval Raw": 0.2595125378440316,
    "IFEval": 25.95125378440316,
    "BBH Raw": 0.3884043024656656,
    "BBH": 14.881409184451359,
    "MATH Lvl 5 Raw": 0.09138972809667675,
    "MATH Lvl 5": 9.138972809667676,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.39911458333333333,
    "MUSR": 7.822656250000001,
    "MMLU-PRO Raw": 0.27194148936170215,
    "MMLU-PRO": 19.104609929078016,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5.1-Qwen-3b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.5.1-Qwen-3b_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.5.1-Qwen-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.5.1-Qwen-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.5.1-Qwen-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.5.1-Qwen-3b",
    "Model sha": "b65848c13b31f5b9d5d953df95d504d195082a3b",
    "Average ‚¨ÜÔ∏è": 13.130246819292033,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9540306505200438,
    "IFEval Raw": 0.2566401592219755,
    "IFEval": 25.664015922197546,
    "BBH Raw": 0.39000839740376536,
    "BBH": 15.057744482096084,
    "MATH Lvl 5 Raw": 0.09214501510574018,
    "MATH Lvl 5": 9.214501510574017,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.39911458333333333,
    "MUSR": 7.822656250000001,
    "MMLU-PRO Raw": 0.27410239361702127,
    "MMLU-PRO": 19.344710401891252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-11-14",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.5.1-Qwen-3b (Merge)"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.6-Nemotron-70b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.6-Nemotron-70b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.6-Nemotron-70b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.6-Nemotron-70b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.6-Nemotron-70b",
    "Model sha": "951c9cdf68d6e679c78625d1a1f396eb71cdf746",
    "Average ‚¨ÜÔ∏è": 41.93364237578902,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 11.950774466853911,
    "IFEval Raw": 0.7526551771521784,
    "IFEval": 75.26551771521784,
    "BBH Raw": 0.6937699482580332,
    "BBH": 55.80557342514651,
    "MATH Lvl 5 Raw": 0.3323262839879154,
    "MATH Lvl 5": 33.23262839879154,
    "GPQA Raw": 0.40604026845637586,
    "GPQA": 20.805369127516784,
    "MUSR Raw": 0.46686458333333336,
    "MUSR": 18.391406250000003,
    "MMLU-PRO Raw": 0.5329122340425532,
    "MMLU-PRO": 48.101359338061464,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-10-17",
    "Generation": 0,
    "Base Model": "rombodawg/Rombos-LLM-V2.6-Nemotron-70b"
  },
  {
    "eval_name": "rombodawg_Rombos-LLM-V2.6-Qwen-14b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/Rombos-LLM-V2.6-Qwen-14b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/Rombos-LLM-V2.6-Qwen-14b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__Rombos-LLM-V2.6-Qwen-14b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/Rombos-LLM-V2.6-Qwen-14b",
    "Model sha": "887910d75a1837b8b8c7c3e50a257517d286ec60",
    "Average ‚¨ÜÔ∏è": 36.35349519748817,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 44,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.1799289557718433,
    "IFEval Raw": 0.5214464288088938,
    "IFEval": 52.14464288088938,
    "BBH Raw": 0.6482048156327189,
    "BBH": 49.21778416393897,
    "MATH Lvl 5 Raw": 0.3164652567975831,
    "MATH Lvl 5": 31.64652567975831,
    "GPQA Raw": 0.3775167785234899,
    "GPQA": 17.00223713646532,
    "MUSR Raw": 0.47675,
    "MUSR": 19.26041666666666,
    "MMLU-PRO Raw": 0.5396442819148937,
    "MMLU-PRO": 48.84936465721041,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-13",
    "Generation": 1,
    "Base Model": "rombodawg/Rombos-LLM-V2.6-Qwen-14b (Merge)"
  },
  {
    "eval_name": "rombodawg_rombos_Replete-Coder-Instruct-8b-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/rombos_Replete-Coder-Instruct-8b-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/rombos_Replete-Coder-Instruct-8b-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__rombos_Replete-Coder-Instruct-8b-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/rombos_Replete-Coder-Instruct-8b-Merged",
    "Model sha": "85ad1fb943d73866ba5c8dcfe4a4f2cbfba12d4d",
    "Average ‚¨ÜÔ∏è": 16.433823987631932,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9641281933170567,
    "IFEval Raw": 0.5387571643239937,
    "IFEval": 53.87571643239937,
    "BBH Raw": 0.4461693860075828,
    "BBH": 21.937706578272657,
    "MATH Lvl 5 Raw": 0.07779456193353475,
    "MATH Lvl 5": 7.779456193353475,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.36603125,
    "MUSR": 3.4539062499999993,
    "MMLU-PRO Raw": 0.18085106382978725,
    "MMLU-PRO": 8.983451536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-14",
    "Generation": 0,
    "Base Model": "rombodawg/rombos_Replete-Coder-Instruct-8b-Merged"
  },
  {
    "eval_name": "rombodawg_rombos_Replete-Coder-Llama3-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rombodawg/rombos_Replete-Coder-Llama3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rombodawg/rombos_Replete-Coder-Llama3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rombodawg__rombos_Replete-Coder-Llama3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rombodawg/rombos_Replete-Coder-Llama3-8B",
    "Model sha": "938a45789cf94821ef6b12c98dc76622a0fa936a",
    "Average ‚¨ÜÔ∏è": 11.832563502872832,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2056020933418046,
    "IFEval Raw": 0.4714125187834945,
    "IFEval": 47.14125187834945,
    "BBH Raw": 0.32762771025266835,
    "BBH": 7.087845117845117,
    "MATH Lvl 5 Raw": 0.03096676737160121,
    "MATH Lvl 5": 3.096676737160121,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.39663541666666663,
    "MUSR": 7.712760416666666,
    "MMLU-PRO Raw": 0.13347739361702127,
    "MMLU-PRO": 3.719710401891251,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-06",
    "Submission Date": "2024-10-14",
    "Generation": 0,
    "Base Model": "rombodawg/rombos_Replete-Coder-Llama3-8B"
  },
  {
    "eval_name": "rwitz_go-bruins-v2_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/rwitz/go-bruins-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">rwitz/go-bruins-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/rwitz__go-bruins-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "rwitz/go-bruins-v2",
    "Model sha": "6d9e57d3a36dbad364ec77ca642873d9fc7fd61c",
    "Average ‚¨ÜÔ∏è": 15.421379081655521,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6378197207812667,
    "IFEval Raw": 0.40958877999264176,
    "IFEval": 40.958877999264175,
    "BBH Raw": 0.37988446841089685,
    "BBH": 12.69326018768569,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.2625838926174497,
    "GPQA": 1.6778523489932917,
    "MUSR Raw": 0.41375,
    "MUSR": 10.985416666666664,
    "MMLU-PRO Raw": 0.2760970744680851,
    "MMLU-PRO": 19.566341607565015,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "saishf_Fimbulvetr-Kuro-Lotus-10.7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/saishf/Fimbulvetr-Kuro-Lotus-10.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">saishf/Fimbulvetr-Kuro-Lotus-10.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/saishf__Fimbulvetr-Kuro-Lotus-10.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "saishf/Fimbulvetr-Kuro-Lotus-10.7B",
    "Model sha": "ec1288fd8c06ac408a2a7e503ea62ac300e474e1",
    "Average ‚¨ÜÔ∏è": 20.023285083744465,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8091685781283036,
    "IFEval Raw": 0.49394384677101205,
    "IFEval": 49.39438467710121,
    "BBH Raw": 0.4342316286386943,
    "BBH": 19.90882095261725,
    "MATH Lvl 5 Raw": 0.014350453172205437,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.4445104166666667,
    "MUSR": 16.03046875,
    "MMLU-PRO Raw": 0.33892952127659576,
    "MMLU-PRO": 26.547724586288417,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-13",
    "Submission Date": "2024-07-09",
    "Generation": 1,
    "Base Model": "saishf/Fimbulvetr-Kuro-Lotus-10.7B (Merge)"
  },
  {
    "eval_name": "sakhan10_quantized_open_llama_3b_v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sakhan10/quantized_open_llama_3b_v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sakhan10/quantized_open_llama_3b_v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sakhan10__quantized_open_llama_3b_v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sakhan10/quantized_open_llama_3b_v2",
    "Model sha": "e8d51ad5204806edf9c2eeb8c56139a440a70265",
    "Average ‚¨ÜÔ∏è": 5.142500028294101,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3927003736037606,
    "IFEval Raw": 0.18722212618075595,
    "IFEval": 18.722212618075595,
    "BBH Raw": 0.3019800780121471,
    "BBH": 2.805733273363854,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.3681666666666667,
    "MUSR": 4.687499999999999,
    "MMLU-PRO Raw": 0.10954122340425532,
    "MMLU-PRO": 1.0601359338061456,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-23",
    "Submission Date": "2024-08-28",
    "Generation": 1,
    "Base Model": "openlm-research/open_llama_3b_v2"
  },
  {
    "eval_name": "saltlux_luxia-21.4b-alignment-v1.0_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/saltlux/luxia-21.4b-alignment-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">saltlux/luxia-21.4b-alignment-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/saltlux__luxia-21.4b-alignment-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "saltlux/luxia-21.4b-alignment-v1.0",
    "Model sha": "87d5673e6d9f60462f195e9414a0bf6874c89ceb",
    "Average ‚¨ÜÔ∏è": 22.925873030438265,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 21,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7440474604009988,
    "IFEval Raw": 0.36929679915956326,
    "IFEval": 36.92967991595633,
    "BBH Raw": 0.6373342606775594,
    "BBH": 48.02111296160791,
    "MATH Lvl 5 Raw": 0.06570996978851963,
    "MATH Lvl 5": 6.570996978851963,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.43284374999999997,
    "MUSR": 12.50546875,
    "MMLU-PRO Raw": 0.34034242021276595,
    "MMLU-PRO": 26.70471335697399,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-12",
    "Submission Date": "2024-06-29",
    "Generation": 0,
    "Base Model": "saltlux/luxia-21.4b-alignment-v1.0"
  },
  {
    "eval_name": "saltlux_luxia-21.4b-alignment-v1.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/saltlux/luxia-21.4b-alignment-v1.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">saltlux/luxia-21.4b-alignment-v1.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/saltlux__luxia-21.4b-alignment-v1.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "saltlux/luxia-21.4b-alignment-v1.2",
    "Model sha": "eed12b5574fa49cc81e57a88aff24c08c13721c0",
    "Average ‚¨ÜÔ∏è": 23.435191845518442,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 8,
    "#Params (B)": 21,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.045925776234354,
    "IFEval Raw": 0.41153694419695297,
    "IFEval": 41.1536944196953,
    "BBH Raw": 0.6371180708112368,
    "BBH": 47.76916471884749,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.4458958333333334,
    "MUSR": 14.903645833333329,
    "MMLU-PRO Raw": 0.34732380319148937,
    "MMLU-PRO": 27.480422576832154,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-27",
    "Submission Date": "2024-07-30",
    "Generation": 0,
    "Base Model": "saltlux/luxia-21.4b-alignment-v1.2"
  },
  {
    "eval_name": "sam-paech_Darkest-muse-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sam-paech/Darkest-muse-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sam-paech/Darkest-muse-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sam-paech__Darkest-muse-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sam-paech/Darkest-muse-v1",
    "Model sha": "55f6ba0218e9615d18a76f244a874b941f8c434f",
    "Average ‚¨ÜÔ∏è": 31.810869013554015,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2069472661116345,
    "IFEval Raw": 0.7344202272193336,
    "IFEval": 73.44202272193337,
    "BBH Raw": 0.5968439530708949,
    "BBH": 42.61173126837064,
    "MATH Lvl 5 Raw": 0.1163141993957704,
    "MATH Lvl 5": 11.63141993957704,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.4502083333333333,
    "MUSR": 15.276041666666671,
    "MMLU-PRO Raw": 0.4183843085106383,
    "MMLU-PRO": 35.37603427895981,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "sam-paech/Darkest-muse-v1 (Merge)"
  },
  {
    "eval_name": "sam-paech_Delirium-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sam-paech/Delirium-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sam-paech/Delirium-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sam-paech__Delirium-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sam-paech/Delirium-v1",
    "Model sha": "98dc2dad47af405013c0584d752504ca448bd8eb",
    "Average ‚¨ÜÔ∏è": 31.732318132305014,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3955012459141973,
    "IFEval Raw": 0.7207564816908026,
    "IFEval": 72.07564816908027,
    "BBH Raw": 0.5962113834521733,
    "BBH": 42.31507908993327,
    "MATH Lvl 5 Raw": 0.12915407854984895,
    "MATH Lvl 5": 12.915407854984895,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.45144791666666667,
    "MUSR": 15.230989583333338,
    "MMLU-PRO Raw": 0.4189660904255319,
    "MMLU-PRO": 35.44067671394799,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-17",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "unsloth/gemma-2-9b-it"
  },
  {
    "eval_name": "sam-paech_Quill-v1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sam-paech/Quill-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sam-paech/Quill-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sam-paech__Quill-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sam-paech/Quill-v1",
    "Model sha": "3cab1cac9d3de0d25b48ea86b4533aa220231f20",
    "Average ‚¨ÜÔ∏è": 31.503020867691873,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 9,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3134691067192557,
    "IFEval Raw": 0.712213593265868,
    "IFEval": 71.22135932658679,
    "BBH Raw": 0.5969226347989487,
    "BBH": 42.59766913903588,
    "MATH Lvl 5 Raw": 0.11858006042296072,
    "MATH Lvl 5": 11.858006042296072,
    "GPQA Raw": 0.33976510067114096,
    "GPQA": 11.968680089485462,
    "MUSR Raw": 0.45547916666666666,
    "MUSR": 16.13489583333333,
    "MMLU-PRO Raw": 0.4171376329787234,
    "MMLU-PRO": 35.237514775413715,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-20",
    "Submission Date": "2024-10-26",
    "Generation": 1,
    "Base Model": "sam-paech/Quill-v1 (Merge)"
  },
  {
    "eval_name": "schnapss_testmerge-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/schnapss/testmerge-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">schnapss/testmerge-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/schnapss__testmerge-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "schnapss/testmerge-7b",
    "Model sha": "ff84f5b87ba51db9622b1c553c076533890a8f50",
    "Average ‚¨ÜÔ∏è": 20.913446084822322,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.4701547174775827,
    "IFEval Raw": 0.39222817679313116,
    "IFEval": 39.22281767931311,
    "BBH Raw": 0.5187478405637375,
    "BBH": 32.63816624149671,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.4685625,
    "MUSR": 17.703645833333333,
    "MMLU-PRO Raw": 0.30601728723404253,
    "MMLU-PRO": 22.89080969267139,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "schnapss/testmerge-7b (Merge)"
  },
  {
    "eval_name": "sci-m-wang_Mistral-7B-Instruct-sa-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sci-m-wang/Mistral-7B-Instruct-sa-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sci-m-wang/Mistral-7B-Instruct-sa-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sci-m-wang__Mistral-7B-Instruct-sa-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sci-m-wang/Mistral-7B-Instruct-sa-v0.1",
    "Model sha": "2dcff66eac0c01dc50e4c41eea959968232187fe",
    "Average ‚¨ÜÔ∏è": 12.200064286998193,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7650823093917314,
    "IFEval Raw": 0.4335186194851882,
    "IFEval": 43.35186194851882,
    "BBH Raw": 0.32727821561411724,
    "BBH": 5.7436460774299505,
    "MATH Lvl 5 Raw": 0.010574018126888218,
    "MATH Lvl 5": 1.0574018126888218,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.38999999999999996,
    "MUSR": 6.6833333333333345,
    "MMLU-PRO Raw": 0.2362034574468085,
    "MMLU-PRO": 15.133717494089835,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-27",
    "Generation": 2,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "sci-m-wang_Phi-3-mini-4k-instruct-sa-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sci-m-wang/Phi-3-mini-4k-instruct-sa-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sci-m-wang/Phi-3-mini-4k-instruct-sa-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sci-m-wang__Phi-3-mini-4k-instruct-sa-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sci-m-wang/Phi-3-mini-4k-instruct-sa-v0.1",
    "Model sha": "5a516f86087853f9d560c95eb9209c1d4ed9ff69",
    "Average ‚¨ÜÔ∏è": 25.773792049151695,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2805026750601647,
    "IFEval Raw": 0.5020623057930734,
    "IFEval": 50.206230579307345,
    "BBH Raw": 0.5502038722383045,
    "BBH": 36.605419148768114,
    "MATH Lvl 5 Raw": 0.14501510574018128,
    "MATH Lvl 5": 14.501510574018129,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.40730208333333334,
    "MUSR": 9.64609375,
    "MMLU-PRO Raw": 0.39852061170212766,
    "MMLU-PRO": 33.16895685579196,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-01",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "microsoft/Phi-3-mini-4k-instruct"
  },
  {
    "eval_name": "sci-m-wang_deepseek-llm-7b-chat-sa-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sci-m-wang/deepseek-llm-7b-chat-sa-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sci-m-wang/deepseek-llm-7b-chat-sa-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sci-m-wang__deepseek-llm-7b-chat-sa-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sci-m-wang/deepseek-llm-7b-chat-sa-v0.1",
    "Model sha": "afbda8b347ec881666061fa67447046fc5164ec8",
    "Average ‚¨ÜÔ∏è": 13.119932983260716,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9915740024811909,
    "IFEval Raw": 0.4035935761557113,
    "IFEval": 40.35935761557113,
    "BBH Raw": 0.37177200995276305,
    "BBH": 12.05197465522808,
    "MATH Lvl 5 Raw": 0.02114803625377644,
    "MATH Lvl 5": 2.114803625377644,
    "GPQA Raw": 0.25671140939597314,
    "GPQA": 0.8948545861297527,
    "MUSR Raw": 0.4173125,
    "MUSR": 9.864062499999998,
    "MMLU-PRO Raw": 0.22091090425531915,
    "MMLU-PRO": 13.434544917257682,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-31",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "deepseek-ai/deepseek-llm-7b-chat"
  },
  {
    "eval_name": "senseable_WestLake-7B-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/senseable/WestLake-7B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">senseable/WestLake-7B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/senseable__WestLake-7B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "senseable/WestLake-7B-v2",
    "Model sha": "41625004c47628837678859753b94c50c82f3bec",
    "Average ‚¨ÜÔ∏è": 16.33259389480185,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 109,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6310112912933874,
    "IFEval Raw": 0.4418620371724801,
    "IFEval": 44.18620371724801,
    "BBH Raw": 0.4073276290688943,
    "BBH": 17.858141685089326,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.39371874999999995,
    "MUSR": 7.481510416666669,
    "MMLU-PRO Raw": 0.27642952127659576,
    "MMLU-PRO": 19.60328014184397,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-22",
    "Submission Date": "2024-07-23",
    "Generation": 0,
    "Base Model": "senseable/WestLake-7B-v2"
  },
  {
    "eval_name": "sequelbox_Llama3.1-8B-MOTH_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sequelbox/Llama3.1-8B-MOTH\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sequelbox/Llama3.1-8B-MOTH</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sequelbox__Llama3.1-8B-MOTH-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sequelbox/Llama3.1-8B-MOTH",
    "Model sha": "8db363e36b1efc9015ab14648e68bcfba9e8d8a0",
    "Average ‚¨ÜÔ∏è": 20.68544620644089,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9544795262274066,
    "IFEval Raw": 0.5244938984117696,
    "IFEval": 52.449389841176966,
    "BBH Raw": 0.490246673015408,
    "BBH": 27.916332245365282,
    "MATH Lvl 5 Raw": 0.11253776435045316,
    "MATH Lvl 5": 11.253776435045317,
    "GPQA Raw": 0.2684563758389262,
    "GPQA": 2.460850111856823,
    "MUSR Raw": 0.3689166666666666,
    "MUSR": 4.047916666666668,
    "MMLU-PRO Raw": 0.3338597074468085,
    "MMLU-PRO": 25.984411938534276,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-01",
    "Submission Date": "2024-09-19",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "sequelbox_Llama3.1-8B-PlumChat_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sequelbox/Llama3.1-8B-PlumChat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sequelbox/Llama3.1-8B-PlumChat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sequelbox__Llama3.1-8B-PlumChat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sequelbox/Llama3.1-8B-PlumChat",
    "Model sha": "1afdc9856591f573e4fcb52dba19a9d8da631e0b",
    "Average ‚¨ÜÔ∏è": 13.151789511615036,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9892568936162274,
    "IFEval Raw": 0.42427647530773904,
    "IFEval": 42.427647530773896,
    "BBH Raw": 0.3873291395699702,
    "BBH": 13.935991387298124,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.3754583333333333,
    "MUSR": 4.765625000000001,
    "MMLU-PRO Raw": 0.21268284574468085,
    "MMLU-PRO": 12.520316193853429,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "sequelbox/Llama3.1-8B-PlumChat (Merge)"
  },
  {
    "eval_name": "sequelbox_Llama3.1-8B-PlumCode_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sequelbox/Llama3.1-8B-PlumCode\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sequelbox/Llama3.1-8B-PlumCode</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sequelbox__Llama3.1-8B-PlumCode-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sequelbox/Llama3.1-8B-PlumCode",
    "Model sha": "171cd599d574000607491f08e6cf7b7eb199e33d",
    "Average ‚¨ÜÔ∏è": 9.811411782603258,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8906762478975032,
    "IFEval Raw": 0.20448299401144518,
    "IFEval": 20.448299401144517,
    "BBH Raw": 0.3368086861425416,
    "BBH": 8.502927271642019,
    "MATH Lvl 5 Raw": 0.026435045317220546,
    "MATH Lvl 5": 2.6435045317220545,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.37734375000000003,
    "MUSR": 8.967968750000002,
    "MMLU-PRO Raw": 0.23354388297872342,
    "MMLU-PRO": 14.838209219858156,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "sequelbox/Llama3.1-8B-PlumCode (Merge)"
  },
  {
    "eval_name": "sequelbox_Llama3.1-8B-PlumMath_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sequelbox/Llama3.1-8B-PlumMath\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sequelbox/Llama3.1-8B-PlumMath</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sequelbox__Llama3.1-8B-PlumMath-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sequelbox/Llama3.1-8B-PlumMath",
    "Model sha": "b857c30a626f7c020fcba89df7bece4bb7381ac2",
    "Average ‚¨ÜÔ∏è": 13.88633260724132,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8687721882465117,
    "IFEval Raw": 0.224241678745728,
    "IFEval": 22.424167874572802,
    "BBH Raw": 0.40323023090048143,
    "BBH": 16.44658382894578,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.3179530201342282,
    "GPQA": 9.060402684563762,
    "MUSR Raw": 0.39185416666666667,
    "MUSR": 8.981770833333334,
    "MMLU-PRO Raw": 0.29753989361702127,
    "MMLU-PRO": 21.948877068557916,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "sequelbox/Llama3.1-8B-PlumMath (Merge)"
  },
  {
    "eval_name": "sequelbox_gemma-2-9B-MOTH_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sequelbox/gemma-2-9B-MOTH\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sequelbox/gemma-2-9B-MOTH</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sequelbox__gemma-2-9B-MOTH-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sequelbox/gemma-2-9B-MOTH",
    "Model sha": "8dff98ab82ba0087706afa0d6c69874a45548212",
    "Average ‚¨ÜÔ∏è": 4.5533242317995155,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.0279489430746143,
    "IFEval Raw": 0.20588150551647405,
    "IFEval": 20.588150551647406,
    "BBH Raw": 0.30797000521562534,
    "BBH": 3.2122172300496232,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3409479166666667,
    "MUSR": 0.6184895833333329,
    "MMLU-PRO Raw": 0.11402925531914894,
    "MMLU-PRO": 1.5588061465721037,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-10",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "sethuiyer_Qwen2.5-7B-Anvita_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sethuiyer/Qwen2.5-7B-Anvita\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sethuiyer/Qwen2.5-7B-Anvita</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sethuiyer__Qwen2.5-7B-Anvita-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sethuiyer/Qwen2.5-7B-Anvita",
    "Model sha": "dc6f8ca6507cc282938e70b23b02c1a3db7b7ddc",
    "Average ‚¨ÜÔ∏è": 29.18083946237797,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0801233122330587,
    "IFEval Raw": 0.6480416406246536,
    "IFEval": 64.80416406246536,
    "BBH Raw": 0.5465860266784314,
    "BBH": 35.482447523885746,
    "MATH Lvl 5 Raw": 0.15861027190332327,
    "MATH Lvl 5": 15.861027190332328,
    "GPQA Raw": 0.3271812080536913,
    "GPQA": 10.290827740492169,
    "MUSR Raw": 0.43365625,
    "MUSR": 13.473697916666671,
    "MMLU-PRO Raw": 0.4165558510638298,
    "MMLU-PRO": 35.172872340425535,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-27",
    "Generation": 1,
    "Base Model": "sethuiyer/Qwen2.5-7B-Anvita (Merge)"
  },
  {
    "eval_name": "shadowml_BeagSake-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shadowml/BeagSake-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shadowml/BeagSake-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shadowml__BeagSake-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shadowml/BeagSake-7B",
    "Model sha": "b7a3b25a188a4608fd05fc4247ddd504c1f529d1",
    "Average ‚¨ÜÔ∏è": 19.063697813257686,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.880127952207298,
    "IFEval Raw": 0.5215960318621258,
    "IFEval": 52.15960318621258,
    "BBH Raw": 0.47110342371098474,
    "BBH": 25.19294464311316,
    "MATH Lvl 5 Raw": 0.054380664652567974,
    "MATH Lvl 5": 5.438066465256798,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.41235416666666663,
    "MUSR": 9.844270833333333,
    "MMLU-PRO Raw": 0.25847739361702127,
    "MMLU-PRO": 17.608599290780138,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-31",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "shadowml/BeagSake-7B (Merge)"
  },
  {
    "eval_name": "shadowml_Mixolar-4x7b_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shadowml/Mixolar-4x7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shadowml/Mixolar-4x7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shadowml__Mixolar-4x7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shadowml/Mixolar-4x7b",
    "Model sha": "bb793526b063765e9861cad8834160fb0945e66d",
    "Average ‚¨ÜÔ∏è": 19.28341153049504,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 36,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.354727700325131,
    "IFEval Raw": 0.3893303102434873,
    "IFEval": 38.93303102434873,
    "BBH Raw": 0.5215949876221495,
    "BBH": 32.728963576299655,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.29278523489932884,
    "GPQA": 5.7046979865771785,
    "MUSR Raw": 0.42575,
    "MUSR": 12.718749999999995,
    "MMLU-PRO Raw": 0.33053523936170215,
    "MMLU-PRO": 25.615026595744684,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "shadowml/Mixolar-4x7b"
  },
  {
    "eval_name": "shastraai_Shastra-LLAMA2-Math-Commonsense-SFT_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shastraai/Shastra-LLAMA2-Math-Commonsense-SFT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shastraai/Shastra-LLAMA2-Math-Commonsense-SFT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shastraai__Shastra-LLAMA2-Math-Commonsense-SFT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shastraai/Shastra-LLAMA2-Math-Commonsense-SFT",
    "Model sha": "97a578246d4edecb5fde3dae262a64e4ec9f489a",
    "Average ‚¨ÜÔ∏è": 10.503347194602945,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7640422361022917,
    "IFEval Raw": 0.3041507644161935,
    "IFEval": 30.415076441619348,
    "BBH Raw": 0.384316753625765,
    "BBH": 13.659523241343651,
    "MATH Lvl 5 Raw": 0.01812688821752266,
    "MATH Lvl 5": 1.812688821752266,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3604479166666667,
    "MUSR": 4.8226562500000005,
    "MMLU-PRO Raw": 0.19971742021276595,
    "MMLU-PRO": 11.079713356973993,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-10-27",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "shivam9980_NEPALI-LLM_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shivam9980/NEPALI-LLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shivam9980/NEPALI-LLM</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shivam9980__NEPALI-LLM-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shivam9980/NEPALI-LLM",
    "Model sha": "5fe146065b53bfd6d8e242cffbe9176bc245551d",
    "Average ‚¨ÜÔ∏è": 6.892789049241789,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 9.628949415951807,
    "IFEval Raw": 0.041666112581284324,
    "IFEval": 4.166611258128433,
    "BBH Raw": 0.3828457133787513,
    "BBH": 13.12524427731519,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.41219791666666666,
    "MUSR": 9.991406250000002,
    "MMLU-PRO Raw": 0.2064494680851064,
    "MMLU-PRO": 11.827718676122931,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "unsloth/gemma-2-9b-bnb-4bit"
  },
  {
    "eval_name": "shivam9980_mistral-7b-news-cnn-merged_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shivam9980/mistral-7b-news-cnn-merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shivam9980/mistral-7b-news-cnn-merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shivam9980__mistral-7b-news-cnn-merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shivam9980/mistral-7b-news-cnn-merged",
    "Model sha": "a0d7029cb00c122843aef3d7ad61d514de334ea3",
    "Average ‚¨ÜÔ∏è": 17.120747422684502,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5940926654709744,
    "IFEval Raw": 0.4634192830578421,
    "IFEval": 46.34192830578421,
    "BBH Raw": 0.3635484854246454,
    "BBH": 11.146535574656042,
    "MATH Lvl 5 Raw": 0.01435045317220544,
    "MATH Lvl 5": 1.435045317220544,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.45226041666666666,
    "MUSR": 15.665885416666669,
    "MMLU-PRO Raw": 0.28274601063829785,
    "MMLU-PRO": 20.305112293144205,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-18",
    "Submission Date": "2024-09-12",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-instruct-v0.2-bnb-4bit"
  },
  {
    "eval_name": "shyamieee_Padma-v7.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/shyamieee/Padma-v7.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">shyamieee/Padma-v7.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/shyamieee__Padma-v7.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "shyamieee/Padma-v7.0",
    "Model sha": "caf70bd6e2f819cc6a18dda8516f2cbdc101fdde",
    "Average ‚¨ÜÔ∏è": 19.75621841010717,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5898989777495254,
    "IFEval Raw": 0.3841097177710696,
    "IFEval": 38.410971777106965,
    "BBH Raw": 0.5118785631761485,
    "BBH": 31.657520764874246,
    "MATH Lvl 5 Raw": 0.07024169184290031,
    "MATH Lvl 5": 7.024169184290032,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.43855208333333334,
    "MUSR": 14.085677083333332,
    "MMLU-PRO Raw": 0.3029421542553192,
    "MMLU-PRO": 22.549128250591018,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "shyamieee/Padma-v7.0 (Merge)"
  },
  {
    "eval_name": "silma-ai_SILMA-9B-Instruct-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/silma-ai/SILMA-9B-Instruct-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">silma-ai/SILMA-9B-Instruct-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/silma-ai__SILMA-9B-Instruct-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "silma-ai/SILMA-9B-Instruct-v1.0",
    "Model sha": "25d7b116ab3fb9f97417a297f8df4a7e34e7de68",
    "Average ‚¨ÜÔ∏è": 24.369441925704603,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 48,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2459989142928811,
    "IFEval Raw": 0.5841943820174914,
    "IFEval": 58.419438201749145,
    "BBH Raw": 0.5219015032853501,
    "BBH": 30.71300262979214,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3053691275167785,
    "GPQA": 7.38255033557047,
    "MUSR Raw": 0.46369791666666665,
    "MUSR": 17.262239583333336,
    "MMLU-PRO Raw": 0.39195478723404253,
    "MMLU-PRO": 32.439420803782504,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-17",
    "Submission Date": "2024-11-12",
    "Generation": 0,
    "Base Model": "silma-ai/SILMA-9B-Instruct-v1.0"
  },
  {
    "eval_name": "skymizer_Llama2-7b-sft-chat-custom-template-dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/skymizer/Llama2-7b-sft-chat-custom-template-dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">skymizer/Llama2-7b-sft-chat-custom-template-dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/skymizer__Llama2-7b-sft-chat-custom-template-dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "skymizer/Llama2-7b-sft-chat-custom-template-dpo",
    "Model sha": "22302ebd8c551a5f302fcb8366cc61fdeedf0e00",
    "Average ‚¨ÜÔ∏è": 10.090195714675467,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6164704416741893,
    "IFEval Raw": 0.2352823840742563,
    "IFEval": 23.528238407425633,
    "BBH Raw": 0.36884662302661564,
    "BBH": 11.238865074478818,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.23909395973154363,
    "GPQA": 0.0,
    "MUSR Raw": 0.44286458333333334,
    "MUSR": 14.124739583333332,
    "MMLU-PRO Raw": 0.19464760638297873,
    "MMLU-PRO": 10.516400709219859,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-11",
    "Submission Date": "2024-07-01",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "sonthenguyen_ft-unsloth-zephyr-sft-bnb-4bit-20241014-161415_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-161415\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-161415</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__ft-unsloth-zephyr-sft-bnb-4bit-20241014-161415-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-161415",
    "Model sha": "467eff1ac1c3395c130929bbe1f34a8194715e7c",
    "Average ‚¨ÜÔ∏è": 8.826874025525804,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.627711564696026,
    "IFEval Raw": 0.28933784580468713,
    "IFEval": 28.93378458046871,
    "BBH Raw": 0.38041816886828617,
    "BBH": 12.789212309485556,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.24664429530201343,
    "GPQA": 0.0,
    "MUSR Raw": 0.3860625,
    "MUSR": 6.024479166666666,
    "MMLU-PRO Raw": 0.14012632978723405,
    "MMLU-PRO": 4.4584810874704495,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "unsloth/zephyr-sft-bnb-4bit"
  },
  {
    "eval_name": "sonthenguyen_ft-unsloth-zephyr-sft-bnb-4bit-20241014-164205_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-164205\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-164205</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__ft-unsloth-zephyr-sft-bnb-4bit-20241014-164205-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-164205",
    "Model sha": "467eff1ac1c3395c130929bbe1f34a8194715e7c",
    "Average ‚¨ÜÔ∏è": 12.818811383335003,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.588998148262559,
    "IFEval Raw": 0.3199377651298555,
    "IFEval": 31.993776512985548,
    "BBH Raw": 0.39586243698929185,
    "BBH": 16.710725154148115,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.4271770833333333,
    "MUSR": 12.097135416666667,
    "MMLU-PRO Raw": 0.21243351063829788,
    "MMLU-PRO": 12.492612293144207,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "unsloth/zephyr-sft-bnb-4bit"
  },
  {
    "eval_name": "sonthenguyen_ft-unsloth-zephyr-sft-bnb-4bit-20241014-170522_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-170522\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-170522</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__ft-unsloth-zephyr-sft-bnb-4bit-20241014-170522-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/ft-unsloth-zephyr-sft-bnb-4bit-20241014-170522",
    "Model sha": "467eff1ac1c3395c130929bbe1f34a8194715e7c",
    "Average ‚¨ÜÔ∏è": 13.43709720554555,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.614698039103553,
    "IFEval Raw": 0.37644117607946914,
    "IFEval": 37.64411760794691,
    "BBH Raw": 0.3828367247244511,
    "BBH": 14.138281987896178,
    "MATH Lvl 5 Raw": 0.009818731117824773,
    "MATH Lvl 5": 0.9818731117824773,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.4404166666666667,
    "MUSR": 14.11875,
    "MMLU-PRO Raw": 0.20553523936170212,
    "MMLU-PRO": 11.726137706855791,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "unsloth/zephyr-sft-bnb-4bit"
  },
  {
    "eval_name": "sonthenguyen_zephyr-sft-bnb-4bit-DPO-mtbc-213steps_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbc-213steps\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbc-213steps</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__zephyr-sft-bnb-4bit-DPO-mtbc-213steps-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbc-213steps",
    "Model sha": "4ae2af48b6ac53f14e153b91309624100ae3d7c2",
    "Average ‚¨ÜÔ∏è": 15.790852050868637,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6988102693798343,
    "IFEval Raw": 0.4275489035758454,
    "IFEval": 42.75489035758454,
    "BBH Raw": 0.4197290890050172,
    "BBH": 19.669907319611504,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.40863541666666664,
    "MUSR": 9.579427083333336,
    "MMLU-PRO Raw": 0.27086103723404253,
    "MMLU-PRO": 18.98455969267139,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbc-213steps"
  },
  {
    "eval_name": "sonthenguyen_zephyr-sft-bnb-4bit-DPO-mtbo-180steps_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbo-180steps\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbo-180steps</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__zephyr-sft-bnb-4bit-DPO-mtbo-180steps-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbo-180steps",
    "Model sha": "0393baf362e29cf51867596fb64746b5edafa6ed",
    "Average ‚¨ÜÔ∏è": 15.55201205955274,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6756848192040504,
    "IFEval Raw": 0.40871443325930756,
    "IFEval": 40.871443325930755,
    "BBH Raw": 0.4322585223071556,
    "BBH": 21.35140303187909,
    "MATH Lvl 5 Raw": 0.020392749244712995,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.38851041666666664,
    "MUSR": 6.1638020833333345,
    "MMLU-PRO Raw": 0.27476728723404253,
    "MMLU-PRO": 19.418587470449168,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbo-180steps"
  },
  {
    "eval_name": "sonthenguyen_zephyr-sft-bnb-4bit-DPO-mtbr-180steps_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbr-180steps\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbr-180steps</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sonthenguyen__zephyr-sft-bnb-4bit-DPO-mtbr-180steps-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbr-180steps",
    "Model sha": "c4ee848caf14649f9260166653d4cdb30bcfc52a",
    "Average ‚¨ÜÔ∏è": 16.475407146329456,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6842248406153846,
    "IFEval Raw": 0.4032190144372487,
    "IFEval": 40.32190144372487,
    "BBH Raw": 0.43053552565190517,
    "BBH": 21.21356840671135,
    "MATH Lvl 5 Raw": 0.024924471299093656,
    "MATH Lvl 5": 2.492447129909366,
    "GPQA Raw": 0.2802013422818792,
    "GPQA": 4.026845637583895,
    "MUSR Raw": 0.42575,
    "MUSR": 11.785416666666665,
    "MMLU-PRO Raw": 0.2711103723404255,
    "MMLU-PRO": 19.01226359338061,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-02",
    "Submission Date": "2024-10-03",
    "Generation": 0,
    "Base Model": "sonthenguyen/zephyr-sft-bnb-4bit-DPO-mtbr-180steps"
  },
  {
    "eval_name": "sophosympatheia_Midnight-Miqu-70B-v1.5_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sophosympatheia/Midnight-Miqu-70B-v1.5\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sophosympatheia/Midnight-Miqu-70B-v1.5</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sophosympatheia__Midnight-Miqu-70B-v1.5-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sophosympatheia/Midnight-Miqu-70B-v1.5",
    "Model sha": "f6062ca8ccba38ce91eef16f85138e279160b9b9",
    "Average ‚¨ÜÔ∏è": 25.222319653302844,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 155,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.45296708318094,
    "IFEval Raw": 0.6118465671086051,
    "IFEval": 61.18465671086051,
    "BBH Raw": 0.5606228371685053,
    "BBH": 38.541461590145985,
    "MATH Lvl 5 Raw": 0.02416918429003021,
    "MATH Lvl 5": 2.416918429003021,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.42441666666666666,
    "MUSR": 11.652083333333332,
    "MMLU-PRO Raw": 0.38248005319148937,
    "MMLU-PRO": 31.386672576832154,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-11",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "sophosympatheia/Midnight-Miqu-70B-v1.5 (Merge)"
  },
  {
    "eval_name": "speakleash_Bielik-11B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/speakleash/Bielik-11B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">speakleash/Bielik-11B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/speakleash__Bielik-11B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "speakleash/Bielik-11B-v2",
    "Model sha": "a620588280793e605d1e0b125fe2a663030206ab",
    "Average ‚¨ÜÔ∏è": 15.913540466018453,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.918733033632083,
    "IFEval Raw": 0.23810489501190177,
    "IFEval": 23.810489501190176,
    "BBH Raw": 0.49308409091594996,
    "BBH": 27.817906537862154,
    "MATH Lvl 5 Raw": 0.07401812688821753,
    "MATH Lvl 5": 7.401812688821753,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.39244791666666673,
    "MUSR": 7.555989583333338,
    "MMLU-PRO Raw": 0.3137466755319149,
    "MMLU-PRO": 23.749630614657207,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-10-16",
    "Generation": 0,
    "Base Model": "speakleash/Bielik-11B-v2"
  },
  {
    "eval_name": "speakleash_Bielik-11B-v2.0-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/speakleash/Bielik-11B-v2.0-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">speakleash/Bielik-11B-v2.0-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/speakleash__Bielik-11B-v2.0-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "speakleash/Bielik-11B-v2.0-Instruct",
    "Model sha": "e4721e2af1152bad2e077c34375911a28aa1b8dc",
    "Average ‚¨ÜÔ∏è": 24.4219930239921,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8884246061646348,
    "IFEval Raw": 0.5252430218486948,
    "IFEval": 52.524302184869484,
    "BBH Raw": 0.5361579931173499,
    "BBH": 33.774676263963016,
    "MATH Lvl 5 Raw": 0.1042296072507553,
    "MATH Lvl 5": 10.42296072507553,
    "GPQA Raw": 0.31711409395973156,
    "GPQA": 8.948545861297541,
    "MUSR Raw": 0.4467083333333333,
    "MUSR": 14.738541666666668,
    "MMLU-PRO Raw": 0.3351063829787234,
    "MMLU-PRO": 26.12293144208038,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "speakleash/Bielik-11B-v2"
  },
  {
    "eval_name": "speakleash_Bielik-11B-v2.1-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/speakleash/Bielik-11B-v2.1-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">speakleash/Bielik-11B-v2.1-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/speakleash__Bielik-11B-v2.1-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "speakleash/Bielik-11B-v2.1-Instruct",
    "Model sha": "c91776047eb235f51238a9e42f80f19e3ed114e7",
    "Average ‚¨ÜÔ∏è": 22.854263857567442,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3056231161333907,
    "IFEval Raw": 0.5089817240477489,
    "IFEval": 50.89817240477488,
    "BBH Raw": 0.5530119844151298,
    "BBH": 36.29005304442506,
    "MATH Lvl 5 Raw": 0.006042296072507552,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.337248322147651,
    "GPQA": 11.633109619686799,
    "MUSR Raw": 0.4185208333333333,
    "MUSR": 10.515104166666669,
    "MMLU-PRO Raw": 0.34466422872340424,
    "MMLU-PRO": 27.184914302600472,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "speakleash/Bielik-11B-v2"
  },
  {
    "eval_name": "speakleash_Bielik-11B-v2.2-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/speakleash/Bielik-11B-v2.2-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">speakleash/Bielik-11B-v2.2-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/speakleash__Bielik-11B-v2.2-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "speakleash/Bielik-11B-v2.2-Instruct",
    "Model sha": "b5502dab61fcc5e087e72c8a120057dea78082ad",
    "Average ‚¨ÜÔ∏è": 24.76930780622726,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 57,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4609252683619047,
    "IFEval Raw": 0.5551935531057595,
    "IFEval": 55.519355310575946,
    "BBH Raw": 0.5596561190863629,
    "BBH": 36.95804119871526,
    "MATH Lvl 5 Raw": 0.0755287009063444,
    "MATH Lvl 5": 7.552870090634441,
    "GPQA Raw": 0.3313758389261745,
    "GPQA": 10.850111856823268,
    "MUSR Raw": 0.41712499999999997,
    "MUSR": 10.107291666666667,
    "MMLU-PRO Raw": 0.3486535904255319,
    "MMLU-PRO": 27.628176713947987,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-26",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "speakleash/Bielik-11B-v2"
  },
  {
    "eval_name": "speakleash_Bielik-11B-v2.3-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/speakleash/Bielik-11B-v2.3-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">speakleash/Bielik-11B-v2.3-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/speakleash__Bielik-11B-v2.3-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "speakleash/Bielik-11B-v2.3-Instruct",
    "Model sha": "7494fdc4d648707ea12b908d40b0ae708989b329",
    "Average ‚¨ÜÔ∏è": 26.191144076712487,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 27,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9061419757283573,
    "IFEval Raw": 0.558290890393046,
    "IFEval": 55.829089039304606,
    "BBH Raw": 0.5662699020280031,
    "BBH": 38.062787893588194,
    "MATH Lvl 5 Raw": 0.08006042296072508,
    "MATH Lvl 5": 8.006042296072508,
    "GPQA Raw": 0.34060402684563756,
    "GPQA": 12.080536912751676,
    "MUSR Raw": 0.4518229166666667,
    "MUSR": 16.011197916666664,
    "MMLU-PRO Raw": 0.34441489361702127,
    "MMLU-PRO": 27.157210401891252,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-30",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "speakleash/Bielik-11B-v2.3-Instruct (Merge)"
  },
  {
    "eval_name": "spmurrayzzz_Mistral-Syndicate-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spmurrayzzz/Mistral-Syndicate-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spmurrayzzz/Mistral-Syndicate-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spmurrayzzz__Mistral-Syndicate-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spmurrayzzz/Mistral-Syndicate-7B",
    "Model sha": "c74379dd6055ef4a70339b105ea315cebec23d24",
    "Average ‚¨ÜÔ∏è": 13.899524613123079,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5798591062792636,
    "IFEval Raw": 0.249595517670891,
    "IFEval": 24.9595517670891,
    "BBH Raw": 0.42450570755678535,
    "BBH": 20.50625197041595,
    "MATH Lvl 5 Raw": 0.027190332326283987,
    "MATH Lvl 5": 2.719033232628399,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.43855208333333334,
    "MUSR": 13.619010416666669,
    "MMLU-PRO Raw": 0.2631316489361702,
    "MMLU-PRO": 18.125738770685576,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-06-27",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "spow12_ChatWaifu_12B_v2.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spow12/ChatWaifu_12B_v2.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spow12/ChatWaifu_12B_v2.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spow12__ChatWaifu_12B_v2.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spow12/ChatWaifu_12B_v2.0",
    "Model sha": "1fb38700b2e2a66d4ff32636817df76285cea5f1",
    "Average ‚¨ÜÔ∏è": 21.77857568626986,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.4262147423877196,
    "IFEval Raw": 0.47675833455232114,
    "IFEval": 47.67583345523211,
    "BBH Raw": 0.5207681738205238,
    "BBH": 31.165239578024238,
    "MATH Lvl 5 Raw": 0.058912386706948656,
    "MATH Lvl 5": 5.891238670694865,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.44317708333333333,
    "MUSR": 15.83046875,
    "MMLU-PRO Raw": 0.33876329787234044,
    "MMLU-PRO": 26.529255319148938,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-10",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "spow12/ChatWaifu_12B_v2.0 (Merge)"
  },
  {
    "eval_name": "spow12_ChatWaifu_22B_v2.0_preview_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spow12/ChatWaifu_22B_v2.0_preview\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spow12/ChatWaifu_22B_v2.0_preview</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spow12__ChatWaifu_22B_v2.0_preview-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spow12/ChatWaifu_22B_v2.0_preview",
    "Model sha": "36af7ec06bc85405e8641986ad45c6d21353b114",
    "Average ‚¨ÜÔ∏è": 29.407500037348317,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4942038315265374,
    "IFEval Raw": 0.6744947849483814,
    "IFEval": 67.44947849483815,
    "BBH Raw": 0.6170153091362338,
    "BBH": 45.48829424136917,
    "MATH Lvl 5 Raw": 0.18051359516616317,
    "MATH Lvl 5": 18.051359516616316,
    "GPQA Raw": 0.31543624161073824,
    "GPQA": 8.7248322147651,
    "MUSR Raw": 0.3685416666666667,
    "MUSR": 3.5343750000000003,
    "MMLU-PRO Raw": 0.39876994680851063,
    "MMLU-PRO": 33.19666075650118,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-23",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "spow12/ChatWaifu_22B_v2.0_preview (Merge)"
  },
  {
    "eval_name": "spow12_ChatWaifu_v1.4_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spow12/ChatWaifu_v1.4\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spow12/ChatWaifu_v1.4</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spow12__ChatWaifu_v1.4-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spow12/ChatWaifu_v1.4",
    "Model sha": "c5b2b30a8e9fa23722b6e30aa2ca1dab7fe1c2b5",
    "Average ‚¨ÜÔ∏è": 25.37944307784449,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 14,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4421426879942196,
    "IFEval Raw": 0.5690567693719332,
    "IFEval": 56.90567693719332,
    "BBH Raw": 0.5176247229970669,
    "BBH": 31.63055380047582,
    "MATH Lvl 5 Raw": 0.08610271903323262,
    "MATH Lvl 5": 8.610271903323262,
    "GPQA Raw": 0.3070469798657718,
    "GPQA": 7.606263982102905,
    "MUSR Raw": 0.47433333333333333,
    "MUSR": 20.025000000000002,
    "MMLU-PRO Raw": 0.3474900265957447,
    "MMLU-PRO": 27.498891843971627,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-03",
    "Submission Date": "2024-09-05",
    "Generation": 1,
    "Base Model": "spow12/ChatWaifu_v1.4 (Merge)"
  },
  {
    "eval_name": "spow12_ChatWaifu_v2.0_22B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spow12/ChatWaifu_v2.0_22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spow12/ChatWaifu_v2.0_22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spow12__ChatWaifu_v2.0_22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spow12/ChatWaifu_v2.0_22B",
    "Model sha": "54771319920ed791ba3f0262b036f37a92b880f2",
    "Average ‚¨ÜÔ∏è": 28.838097623831434,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.7398346907612154,
    "IFEval Raw": 0.6510891102275296,
    "IFEval": 65.10891102275296,
    "BBH Raw": 0.592630190761292,
    "BBH": 42.28622796334265,
    "MATH Lvl 5 Raw": 0.18580060422960726,
    "MATH Lvl 5": 18.580060422960727,
    "GPQA Raw": 0.32466442953020136,
    "GPQA": 9.955257270693513,
    "MUSR Raw": 0.3841979166666667,
    "MUSR": 5.5914062499999995,
    "MMLU-PRO Raw": 0.3835605053191489,
    "MMLU-PRO": 31.506722813238763,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "spow12/ChatWaifu_v2.0_22B (Merge)"
  },
  {
    "eval_name": "spow12_ChatWaifu_v2.0_22B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/spow12/ChatWaifu_v2.0_22B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">spow12/ChatWaifu_v2.0_22B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/spow12__ChatWaifu_v2.0_22B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "spow12/ChatWaifu_v2.0_22B",
    "Model sha": "a6e7c206d9af77d3f85faf0ce4a711d62815b2ab",
    "Average ‚¨ÜÔ∏è": 28.86865918746933,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3958601155432835,
    "IFEval Raw": 0.6517384982956334,
    "IFEval": 65.17384982956334,
    "BBH Raw": 0.5908050619550995,
    "BBH": 42.01979809251511,
    "MATH Lvl 5 Raw": 0.1933534743202417,
    "MATH Lvl 5": 19.335347432024168,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.3841979166666667,
    "MUSR": 5.5914062499999995,
    "MMLU-PRO Raw": 0.3812333776595745,
    "MMLU-PRO": 31.24815307328605,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "spow12/ChatWaifu_v2.0_22B (Merge)"
  },
  {
    "eval_name": "ssmits_Qwen2.5-95B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ssmits/Qwen2.5-95B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ssmits/Qwen2.5-95B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ssmits__Qwen2.5-95B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ssmits/Qwen2.5-95B-Instruct",
    "Model sha": "9c0e7df57a4fcf4d364efd916a0fc0abdd2d20a3",
    "Average ‚¨ÜÔ∏è": 37.44012498837456,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 94,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 19.233494802814402,
    "IFEval Raw": 0.8431051831363006,
    "IFEval": 84.31051831363006,
    "BBH Raw": 0.7037799697488242,
    "BBH": 58.530351322851054,
    "MATH Lvl 5 Raw": 0.06117824773413897,
    "MATH Lvl 5": 6.117824773413897,
    "GPQA Raw": 0.3640939597315436,
    "GPQA": 15.212527964205815,
    "MUSR Raw": 0.4283854166666667,
    "MUSR": 13.61484375,
    "MMLU-PRO Raw": 0.5216921542553191,
    "MMLU-PRO": 46.85468380614657,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "ssmits/Qwen2.5-95B-Instruct (Merge)"
  },
  {
    "eval_name": "stabilityai_StableBeluga2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/StableBeluga2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/StableBeluga2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__StableBeluga2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/StableBeluga2",
    "Model sha": "cb47d3db70ea3ddc2cabdeb358c303b328f65900",
    "Average ‚¨ÜÔ∏è": 22.682841793144064,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 884,
    "#Params (B)": 68,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 6.254673656044013,
    "IFEval Raw": 0.37871403431783224,
    "IFEval": 37.87140343178322,
    "BBH Raw": 0.5824128134553807,
    "BBH": 41.26326112722379,
    "MATH Lvl 5 Raw": 0.03625377643504532,
    "MATH Lvl 5": 3.625377643504532,
    "GPQA Raw": 0.3162751677852349,
    "GPQA": 8.83668903803132,
    "MUSR Raw": 0.47296875,
    "MUSR": 18.654427083333335,
    "MMLU-PRO Raw": 0.3326130319148936,
    "MMLU-PRO": 25.845892434988176,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-20",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "stabilityai/StableBeluga2"
  },
  {
    "eval_name": "stabilityai_stablelm-2-12b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-2-12b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-2-12b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-2-12b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-2-12b",
    "Model sha": "fead13ddbf4492970666650c3cd6f85f485411ec",
    "Average ‚¨ÜÔ∏è": 13.935722477068603,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 116,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.473279224261559,
    "IFEval Raw": 0.1569214129620518,
    "IFEval": 15.69214129620518,
    "BBH Raw": 0.4508654171114765,
    "BBH": 22.685797482043984,
    "MATH Lvl 5 Raw": 0.0392749244712991,
    "MATH Lvl 5": 3.92749244712991,
    "GPQA Raw": 0.2785234899328859,
    "GPQA": 3.8031319910514525,
    "MUSR Raw": 0.44788541666666665,
    "MUSR": 14.485677083333334,
    "MMLU-PRO Raw": 0.3071808510638298,
    "MMLU-PRO": 23.020094562647756,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-03-21",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-2-12b"
  },
  {
    "eval_name": "stabilityai_stablelm-2-12b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-2-12b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-2-12b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-2-12b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-2-12b-chat",
    "Model sha": "b6b62cd451b84e848514c00fafa66d9ead9297c5",
    "Average ‚¨ÜÔ∏è": 16.249477114737257,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 86,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0880966859368317,
    "IFEval Raw": 0.4081647805600252,
    "IFEval": 40.81647805600252,
    "BBH Raw": 0.4672024731282805,
    "BBH": 25.253697090812636,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.3914270833333333,
    "MUSR": 7.7283854166666694,
    "MMLU-PRO Raw": 0.2734375,
    "MMLU-PRO": 19.270833333333332,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-04",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-2-12b-chat"
  },
  {
    "eval_name": "stabilityai_stablelm-2-1_6b_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-2-1_6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-2-1_6b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-2-1_6b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-2-1_6b",
    "Model sha": "8879812cccd176fbbe9ceb747b815bcc7d6499f8",
    "Average ‚¨ÜÔ∏è": 5.216126538850885,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 186,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5498718276757216,
    "IFEval Raw": 0.11570521771122844,
    "IFEval": 11.570521771122843,
    "BBH Raw": 0.338457720511071,
    "BBH": 8.632695204968835,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.2483221476510067,
    "GPQA": 0.0,
    "MUSR Raw": 0.38819791666666664,
    "MUSR": 5.791406249999999,
    "MMLU-PRO Raw": 0.1463597074468085,
    "MMLU-PRO": 5.1510786052009445,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-18",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-2-1_6b"
  },
  {
    "eval_name": "stabilityai_stablelm-2-1_6b-chat_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-2-1_6b-chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-2-1_6b-chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-2-1_6b-chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-2-1_6b-chat",
    "Model sha": "f3fe67057c2789ae1bb1fe42b038da99840d4f13",
    "Average ‚¨ÜÔ∏è": 8.640774589382056,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4954265085188646,
    "IFEval Raw": 0.30599919325168334,
    "IFEval": 30.59991932516833,
    "BBH Raw": 0.3390172395486522,
    "BBH": 7.493378297410634,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.35796875,
    "MUSR": 5.712760416666669,
    "MMLU-PRO Raw": 0.16215093085106383,
    "MMLU-PRO": 6.905658983451536,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-04-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-2-1_6b-chat"
  },
  {
    "eval_name": "stabilityai_stablelm-2-zephyr-1_6b_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-2-zephyr-1_6b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-2-zephyr-1_6b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-2-zephyr-1_6b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-2-zephyr-1_6b",
    "Model sha": "2f275b1127d59fc31e4f7c7426d528768ada9ea4",
    "Average ‚¨ÜÔ∏è": 9.281933956173114,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 181,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4730886627409424,
    "IFEval Raw": 0.32793100085550786,
    "IFEval": 32.79310008555078,
    "BBH Raw": 0.3351608706280727,
    "BBH": 6.708710147938231,
    "MATH Lvl 5 Raw": 0.022658610271903325,
    "MATH Lvl 5": 2.2658610271903323,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.3511458333333333,
    "MUSR": 5.993229166666668,
    "MMLU-PRO Raw": 0.17137632978723405,
    "MMLU-PRO": 7.930703309692672,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-01-19",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-2-zephyr-1_6b"
  },
  {
    "eval_name": "stabilityai_stablelm-3b-4e1t_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-3b-4e1t\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-3b-4e1t</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-3b-4e1t-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-3b-4e1t",
    "Model sha": "fa4a6a92fca83c3b4223a3c9bf792887090ebfba",
    "Average ‚¨ÜÔ∏è": 7.2632507075969786,
    "Hub License": "cc-by-sa-4.0",
    "Hub ‚ù§Ô∏è": 309,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.43426512639316794,
    "IFEval Raw": 0.22031986240951784,
    "IFEval": 22.031986240951785,
    "BBH Raw": 0.3504211415826912,
    "BBH": 9.013070349546279,
    "MATH Lvl 5 Raw": 0.006797583081570998,
    "MATH Lvl 5": 0.6797583081570998,
    "GPQA Raw": 0.23741610738255034,
    "GPQA": 0.0,
    "MUSR Raw": 0.37778124999999996,
    "MUSR": 4.422656249999999,
    "MMLU-PRO Raw": 0.1668882978723404,
    "MMLU-PRO": 7.432033096926712,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-29",
    "Submission Date": "2024-08-10",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-3b-4e1t"
  },
  {
    "eval_name": "stabilityai_stablelm-zephyr-3b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "StableLmForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/stabilityai/stablelm-zephyr-3b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">stabilityai/stablelm-zephyr-3b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/stabilityai__stablelm-zephyr-3b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "stabilityai/stablelm-zephyr-3b",
    "Model sha": "a14f62d95754d96aea2be6e24c0f6966636797b9",
    "Average ‚¨ÜÔ∏è": 12.356618845485963,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 248,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.38402363149593016,
    "IFEval Raw": 0.36832271705740766,
    "IFEval": 36.832271705740766,
    "BBH Raw": 0.3866361442837871,
    "BBH": 14.7591192080273,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.23909395973154363,
    "GPQA": 0.0,
    "MUSR Raw": 0.4183020833333333,
    "MUSR": 9.787760416666666,
    "MMLU-PRO Raw": 0.17677859042553193,
    "MMLU-PRO": 8.530954491725769,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-11-21",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "stabilityai/stablelm-zephyr-3b"
  },
  {
    "eval_name": "sthenno-com_miscii-14b-1028_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sthenno-com/miscii-14b-1028\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sthenno-com/miscii-14b-1028</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sthenno-com__miscii-14b-1028-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sthenno-com/miscii-14b-1028",
    "Model sha": "a60c866621ee35d04e84cf366e972f2466d617b1",
    "Average ‚¨ÜÔ∏è": 35.0544159891224,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 10,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.5337282890093384,
    "IFEval Raw": 0.8236711924360696,
    "IFEval": 82.36711924360696,
    "BBH Raw": 0.64483340535341,
    "BBH": 49.262667655574724,
    "MATH Lvl 5 Raw": 0.0634441087613293,
    "MATH Lvl 5": 6.3444108761329305,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.41815625,
    "MUSR": 12.002864583333329,
    "MMLU-PRO Raw": 0.5152925531914894,
    "MMLU-PRO": 46.1436170212766,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "sthenno-com/miscii-14b-1028 (Merge)"
  },
  {
    "eval_name": "suayptalha_Komodo-Llama-3.2-3B-v2-fp16_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/suayptalha/Komodo-Llama-3.2-3B-v2-fp16\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">suayptalha/Komodo-Llama-3.2-3B-v2-fp16</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/suayptalha__Komodo-Llama-3.2-3B-v2-fp16-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "suayptalha/Komodo-Llama-3.2-3B-v2-fp16",
    "Model sha": "1ff4b55d952597429c249ca71dc08b823eba17c0",
    "Average ‚¨ÜÔ∏è": 19.587262051056502,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5980647635972424,
    "IFEval Raw": 0.6340532010620709,
    "IFEval": 63.405320106207085,
    "BBH Raw": 0.43549964909074995,
    "BBH": 20.204328973550357,
    "MATH Lvl 5 Raw": 0.06268882175226587,
    "MATH Lvl 5": 6.268882175226587,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.34057291666666667,
    "MUSR": 3.3716145833333333,
    "MMLU-PRO Raw": 0.28523936170212766,
    "MMLU-PRO": 20.582151300236408,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-19",
    "Submission Date": "2024-11-19",
    "Generation": 1,
    "Base Model": "suayptalha/Komodo-Llama-3.2-3B-v2-fp16 (Merge)"
  },
  {
    "eval_name": "suayptalha_Rombos-2.5-T.E-8.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/suayptalha/Rombos-2.5-T.E-8.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">suayptalha/Rombos-2.5-T.E-8.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/suayptalha__Rombos-2.5-T.E-8.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "suayptalha/Rombos-2.5-T.E-8.1",
    "Model sha": "c0ee2950b07377e1d0e01fc013a0f200b0306ea2",
    "Average ‚¨ÜÔ∏è": 27.335178928774166,
    "Hub License": "cc-by-nc-sa-4.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6860155613579194,
    "IFEval Raw": 0.6925047762159957,
    "IFEval": 69.25047762159957,
    "BBH Raw": 0.5514641249478369,
    "BBH": 36.499861205880386,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.311241610738255,
    "GPQA": 8.165548098434002,
    "MUSR Raw": 0.41663541666666665,
    "MUSR": 10.979427083333336,
    "MMLU-PRO Raw": 0.4445644946808511,
    "MMLU-PRO": 38.2849438534279,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-16",
    "Generation": 1,
    "Base Model": "suayptalha/Rombos-2.5-T.E-8.1 (Merge)"
  },
  {
    "eval_name": "sumink_ftgpt_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPT2LMHeadModel",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sumink/ftgpt\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sumink/ftgpt</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sumink__ftgpt-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sumink/ftgpt",
    "Model sha": "fea7c59fff2443a73a7fd11a78b1d80eb5f0c4e6",
    "Average ‚¨ÜÔ∏è": 3.951784139825086,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.052817519539640305,
    "IFEval Raw": 0.0787100449030794,
    "IFEval": 7.871004490307939,
    "BBH Raw": 0.29190853217047663,
    "BBH": 1.9312767142279632,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.41384375,
    "MUSR": 10.097135416666667,
    "MMLU-PRO Raw": 0.1171875,
    "MMLU-PRO": 1.9097222222222217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-20",
    "Generation": 0,
    "Base Model": "sumink/ftgpt"
  },
  {
    "eval_name": "sunbaby_BrainCog-8B-0.1-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/sunbaby/BrainCog-8B-0.1-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">sunbaby/BrainCog-8B-0.1-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/sunbaby__BrainCog-8B-0.1-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "sunbaby/BrainCog-8B-0.1-Instruct",
    "Model sha": "6c03cb7af723c7f7785df9eee5d5838247619bee",
    "Average ‚¨ÜÔ∏è": 18.040753529707544,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8345543950885596,
    "IFEval Raw": 0.4253004250943053,
    "IFEval": 42.53004250943053,
    "BBH Raw": 0.46182179983247446,
    "BBH": 24.283467839476657,
    "MATH Lvl 5 Raw": 0.07628398791540786,
    "MATH Lvl 5": 7.628398791540786,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.36559375,
    "MUSR": 6.332552083333334,
    "MMLU-PRO Raw": 0.28582114361702127,
    "MMLU-PRO": 20.646793735224584,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-31",
    "Submission Date": "2024-08-27",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B"
  },
  {
    "eval_name": "swap-uniba_LLaMAntino-3-ANITA-8B-Inst-DPO-ITA_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/swap-uniba__LLaMAntino-3-ANITA-8B-Inst-DPO-ITA-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "swap-uniba/LLaMAntino-3-ANITA-8B-Inst-DPO-ITA",
    "Model sha": "2b6e46e4c9d341dc8bf8350a167492c880116b66",
    "Average ‚¨ÜÔ∏è": 21.752024381882908,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8166388113842887,
    "IFEval Raw": 0.4815046299374548,
    "IFEval": 48.15046299374548,
    "BBH Raw": 0.4935698792285044,
    "BBH": 27.990827697552746,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.43873958333333335,
    "MUSR": 13.242447916666665,
    "MMLU-PRO Raw": 0.3723404255319149,
    "MMLU-PRO": 30.26004728132387,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-29",
    "Submission Date": "2024-10-25",
    "Generation": 1,
    "Base Model": "meta-llama/Meta-Llama-3-8B-Instruct"
  },
  {
    "eval_name": "talha2001_Beast-Soul-new_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/talha2001/Beast-Soul-new\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">talha2001/Beast-Soul-new</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/talha2001__Beast-Soul-new-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "talha2001/Beast-Soul-new",
    "Model sha": "e6cf8caa60264a3005df2ff4b9d967f684519d4b",
    "Average ‚¨ÜÔ∏è": 21.804865949398035,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6428829778333867,
    "IFEval Raw": 0.4853510906616666,
    "IFEval": 48.53510906616666,
    "BBH Raw": 0.5227143628884523,
    "BBH": 33.07275916855207,
    "MATH Lvl 5 Raw": 0.07477341389728097,
    "MATH Lvl 5": 7.477341389728097,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.4459270833333333,
    "MUSR": 14.140885416666668,
    "MMLU-PRO Raw": 0.3101728723404255,
    "MMLU-PRO": 23.352541371158388,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-08-07",
    "Generation": 1,
    "Base Model": "talha2001/Beast-Soul-new (Merge)"
  },
  {
    "eval_name": "tangledgroup_tangled-llama-pints-1.5b-v0.1-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tangledgroup/tangled-llama-pints-1.5b-v0.1-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tangledgroup/tangled-llama-pints-1.5b-v0.1-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tangledgroup__tangled-llama-pints-1.5b-v0.1-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tangledgroup/tangled-llama-pints-1.5b-v0.1-instruct",
    "Model sha": "3e1429f20007740877c51e44ed63b870a57a2e17",
    "Average ‚¨ÜÔ∏è": 4.190264372734615,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.2954340058447049,
    "IFEval Raw": 0.15090182936829835,
    "IFEval": 15.090182936829834,
    "BBH Raw": 0.31434444692284963,
    "BBH": 3.8421954101765152,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.23993288590604026,
    "GPQA": 0.0,
    "MUSR Raw": 0.37613541666666667,
    "MUSR": 4.850260416666665,
    "MMLU-PRO Raw": 0.11087101063829788,
    "MMLU-PRO": 1.2078900709219857,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-08-29",
    "Generation": 1,
    "Base Model": "pints-ai/1.5-Pints-16K-v0.1"
  },
  {
    "eval_name": "tangledgroup_tangled-llama-pints-1.5b-v0.2-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tangledgroup/tangled-llama-pints-1.5b-v0.2-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tangledgroup/tangled-llama-pints-1.5b-v0.2-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tangledgroup__tangled-llama-pints-1.5b-v0.2-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tangledgroup/tangled-llama-pints-1.5b-v0.2-instruct",
    "Model sha": "5c229e26f3ab3d0f0f613ed242f3f0f57c930155",
    "Average ‚¨ÜÔ∏è": 4.657740221045246,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.2978114572213741,
    "IFEval Raw": 0.1724092075692496,
    "IFEval": 17.24092075692496,
    "BBH Raw": 0.3158349391752727,
    "BBH": 4.0802054869970155,
    "MATH Lvl 5 Raw": 0.007552870090634441,
    "MATH Lvl 5": 0.755287009063444,
    "GPQA Raw": 0.24161073825503357,
    "GPQA": 0.0,
    "MUSR Raw": 0.3642916666666667,
    "MUSR": 4.569791666666666,
    "MMLU-PRO Raw": 0.11170212765957446,
    "MMLU-PRO": 1.300236406619384,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-15",
    "Generation": 1,
    "Base Model": "pints-ai/1.5-Pints-16K-v0.1"
  },
  {
    "eval_name": "tanliboy_lambda-gemma-2-9b-dpo_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tanliboy/lambda-gemma-2-9b-dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tanliboy/lambda-gemma-2-9b-dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tanliboy__lambda-gemma-2-9b-dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tanliboy/lambda-gemma-2-9b-dpo",
    "Model sha": "b141471308bc41ffe15180a6668c735396c3949b",
    "Average ‚¨ÜÔ∏è": 21.336889814787824,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.2415872481931163,
    "IFEval Raw": 0.45008023156336296,
    "IFEval": 45.0080231563363,
    "BBH Raw": 0.547172399190412,
    "BBH": 35.554545346782085,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.40165625,
    "MUSR": 7.940364583333334,
    "MMLU-PRO Raw": 0.379155585106383,
    "MMLU-PRO": 31.017287234042552,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-09-18",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "tanliboy_lambda-gemma-2-9b-dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tanliboy/lambda-gemma-2-9b-dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tanliboy/lambda-gemma-2-9b-dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tanliboy__lambda-gemma-2-9b-dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tanliboy/lambda-gemma-2-9b-dpo",
    "Model sha": "b141471308bc41ffe15180a6668c735396c3949b",
    "Average ‚¨ÜÔ∏è": 16.97010860262216,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.9035758929213187,
    "IFEval Raw": 0.18292463995531855,
    "IFEval": 18.292463995531854,
    "BBH Raw": 0.5487911206515993,
    "BBH": 35.73966330720827,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.40562499999999996,
    "MUSR": 8.569791666666665,
    "MMLU-PRO Raw": 0.3804853723404255,
    "MMLU-PRO": 31.165041371158388,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-09-18",
    "Generation": 2,
    "Base Model": "google/gemma-2-9b"
  },
  {
    "eval_name": "tanliboy_lambda-qwen2.5-14b-dpo-test_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tanliboy/lambda-qwen2.5-14b-dpo-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tanliboy/lambda-qwen2.5-14b-dpo-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tanliboy__lambda-qwen2.5-14b-dpo-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tanliboy/lambda-qwen2.5-14b-dpo-test",
    "Model sha": "96607eea3c67f14f73e576580610dba7530c5dd9",
    "Average ‚¨ÜÔ∏è": 33.51619236741214,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8007432909656687,
    "IFEval Raw": 0.8231215397367873,
    "IFEval": 82.31215397367873,
    "BBH Raw": 0.6393505282981286,
    "BBH": 48.454439828605324,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3624161073825503,
    "GPQA": 14.988814317673373,
    "MUSR Raw": 0.42603125000000003,
    "MUSR": 12.587239583333336,
    "MMLU-PRO Raw": 0.4847905585106383,
    "MMLU-PRO": 42.75450650118203,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-20",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "tanliboy_lambda-qwen2.5-32b-dpo-test_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tanliboy/lambda-qwen2.5-32b-dpo-test\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tanliboy/lambda-qwen2.5-32b-dpo-test</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tanliboy__lambda-qwen2.5-32b-dpo-test-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tanliboy/lambda-qwen2.5-32b-dpo-test",
    "Model sha": "675b60d6e859455a6139e6e284bbe1844b8ddf46",
    "Average ‚¨ÜÔ∏è": 35.753394199467664,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 32,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 5.499303446572655,
    "IFEval Raw": 0.8083839767372794,
    "IFEval": 80.83839767372794,
    "BBH Raw": 0.6763904009446838,
    "BBH": 54.40796058706255,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3565436241610738,
    "GPQA": 14.205816554809845,
    "MUSR Raw": 0.42742708333333335,
    "MUSR": 13.328385416666663,
    "MMLU-PRO Raw": 0.565658244680851,
    "MMLU-PRO": 51.739804964539005,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-30",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-32B"
  },
  {
    "eval_name": "teknium_CollectiveCognition-v1.1-Mistral-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/teknium/CollectiveCognition-v1.1-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">teknium/CollectiveCognition-v1.1-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/teknium__CollectiveCognition-v1.1-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "teknium/CollectiveCognition-v1.1-Mistral-7B",
    "Model sha": "5f57f70ec99450c70da2540e94dd7fd67be4b23c",
    "Average ‚¨ÜÔ∏è": 14.268985169299853,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 78,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.42931806128098543,
    "IFEval Raw": 0.27904626391308396,
    "IFEval": 27.904626391308398,
    "BBH Raw": 0.4493426704276236,
    "BBH": 23.47613361696592,
    "MATH Lvl 5 Raw": 0.03172205438066465,
    "MATH Lvl 5": 3.1722054380664653,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.3869270833333333,
    "MUSR": 5.732552083333332,
    "MMLU-PRO Raw": 0.28366023936170215,
    "MMLU-PRO": 20.40669326241135,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-04",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "teknium_OpenHermes-13B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/teknium/OpenHermes-13B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">teknium/OpenHermes-13B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/teknium__OpenHermes-13B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "teknium/OpenHermes-13B",
    "Model sha": "bcad6fff9f8591e091d2d57356a3f102197e8c5f",
    "Average ‚¨ÜÔ∏è": 12.16967620818891,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 54,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 31.119116699481676,
    "IFEval Raw": 0.2668065178171696,
    "IFEval": 26.680651781716954,
    "BBH Raw": 0.42064384521911524,
    "BBH": 18.21332824040884,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2726510067114094,
    "GPQA": 3.0201342281879207,
    "MUSR Raw": 0.4042604166666666,
    "MUSR": 8.532552083333334,
    "MMLU-PRO Raw": 0.23894614361702127,
    "MMLU-PRO": 15.438460401891252,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-06",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "NousResearch/Llama-2-13b-hf"
  },
  {
    "eval_name": "teknium_OpenHermes-2-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/teknium/OpenHermes-2-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">teknium/OpenHermes-2-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/teknium__OpenHermes-2-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "teknium/OpenHermes-2-Mistral-7B",
    "Model sha": "4c6e34123b140ce773a8433cae5410949289102c",
    "Average ‚¨ÜÔ∏è": 21.415299888727333,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 255,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.47502983508094826,
    "IFEval Raw": 0.5286151854856226,
    "IFEval": 52.86151854856226,
    "BBH Raw": 0.4947516371878204,
    "BBH": 29.251839211223313,
    "MATH Lvl 5 Raw": 0.043806646525679754,
    "MATH Lvl 5": 4.380664652567975,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.45197916666666665,
    "MUSR": 16.064062499999995,
    "MMLU-PRO Raw": 0.2931349734042553,
    "MMLU-PRO": 21.4594414893617,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-12",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "teknium_OpenHermes-2.5-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">teknium/OpenHermes-2.5-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/teknium__OpenHermes-2.5-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "teknium/OpenHermes-2.5-Mistral-7B",
    "Model sha": "24c0bea14d53e6f67f1fbe2eca5bfe7cae389b33",
    "Average ‚¨ÜÔ∏è": 21.266836560152168,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 815,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4727833003105659,
    "IFEval Raw": 0.5571417173100706,
    "IFEval": 55.71417173100706,
    "BBH Raw": 0.4870013259924984,
    "BBH": 27.770026367807578,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.4241979166666667,
    "MUSR": 12.058072916666669,
    "MMLU-PRO Raw": 0.3054355053191489,
    "MMLU-PRO": 22.826167257683213,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-10-29",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "mistralai/Mistral-7B-v0.1"
  },
  {
    "eval_name": "teknium_OpenHermes-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/teknium/OpenHermes-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">teknium/OpenHermes-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/teknium__OpenHermes-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "teknium/OpenHermes-7B",
    "Model sha": "9f55d6eb15f1edd52ee1fd863a220aa682e78a00",
    "Average ‚¨ÜÔ∏è": 9.481131901453509,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 13,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.483089752614603,
    "IFEval Raw": 0.1812513021006485,
    "IFEval": 18.12513021006485,
    "BBH Raw": 0.362033648602934,
    "BBH": 12.08139546207365,
    "MATH Lvl 5 Raw": 0.01057401812688822,
    "MATH Lvl 5": 1.057401812688822,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.4323854166666667,
    "MUSR": 12.681510416666667,
    "MMLU-PRO Raw": 0.19331781914893617,
    "MMLU-PRO": 10.368646572104018,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-09-14",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "NousResearch/Llama-2-7b-hf"
  },
  {
    "eval_name": "tensoropera_Fox-1-1.6B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tensoropera/Fox-1-1.6B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tensoropera/Fox-1-1.6B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tensoropera__Fox-1-1.6B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tensoropera/Fox-1-1.6B",
    "Model sha": "6389dde4d7e52aa1200ad954c565f03c7fdcf8db",
    "Average ‚¨ÜÔ∏è": 7.739189414804567,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 30,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3428202981994517,
    "IFEval Raw": 0.27659831469390106,
    "IFEval": 27.659831469390106,
    "BBH Raw": 0.3307369914593792,
    "BBH": 7.399760932518088,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.35498958333333336,
    "MUSR": 3.873697916666666,
    "MMLU-PRO Raw": 0.1371343085106383,
    "MMLU-PRO": 4.12603427895981,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-13",
    "Submission Date": "2024-06-29",
    "Generation": 0,
    "Base Model": "tensoropera/Fox-1-1.6B"
  },
  {
    "eval_name": "tenyx_Llama3-TenyxChat-70B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tenyx/Llama3-TenyxChat-70B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tenyx/Llama3-TenyxChat-70B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tenyx__Llama3-TenyxChat-70B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tenyx/Llama3-TenyxChat-70B",
    "Model sha": "a85d31e3af8fcc847cc9169f1144cf02f5351fab",
    "Average ‚¨ÜÔ∏è": 36.87224824370192,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 63,
    "#Params (B)": 70,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.367006518049083,
    "IFEval Raw": 0.8087086707713311,
    "IFEval": 80.87086707713311,
    "BBH Raw": 0.6511486901811531,
    "BBH": 49.61562001611543,
    "MATH Lvl 5 Raw": 0.24622356495468278,
    "MATH Lvl 5": 24.622356495468278,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.42603125000000003,
    "MUSR": 12.520572916666667,
    "MMLU-PRO Raw": 0.5210272606382979,
    "MMLU-PRO": 46.780806737588655,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-08-04",
    "Generation": 0,
    "Base Model": "tenyx/Llama3-TenyxChat-70B"
  },
  {
    "eval_name": "theprint_Boptruth-Agatha-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/Boptruth-Agatha-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/Boptruth-Agatha-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__Boptruth-Agatha-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/Boptruth-Agatha-7B",
    "Model sha": "ef7c7570be29a58f4a8358a6d4c75f59a5282191",
    "Average ‚¨ÜÔ∏è": 17.449440022593617,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3881051588060305,
    "IFEval Raw": 0.312418826491487,
    "IFEval": 31.241882649148703,
    "BBH Raw": 0.4983936045348778,
    "BBH": 29.286422282807493,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.42766666666666664,
    "MUSR": 11.758333333333335,
    "MMLU-PRO Raw": 0.28607047872340424,
    "MMLU-PRO": 20.674497635933804,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-11",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "theprint/Boptruth-Agatha-7B"
  },
  {
    "eval_name": "theprint_CleverBoi-7B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/CleverBoi-7B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/CleverBoi-7B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__CleverBoi-7B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/CleverBoi-7B-v2",
    "Model sha": "1d82629c1e6778cf8568b532a3c09b668805b15a",
    "Average ‚¨ÜÔ∏è": 15.032974273301008,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5223975334715987,
    "IFEval Raw": 0.21699756645700075,
    "IFEval": 21.699756645700074,
    "BBH Raw": 0.45317253321634526,
    "BBH": 23.44418148733149,
    "MATH Lvl 5 Raw": 0.02265861027190333,
    "MATH Lvl 5": 2.2658610271903328,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.46953125,
    "MUSR": 18.65807291666667,
    "MMLU-PRO Raw": 0.27086103723404253,
    "MMLU-PRO": 18.98455969267139,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-v0.3-bnb-4bit"
  },
  {
    "eval_name": "theprint_CleverBoi-7B-v3_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/CleverBoi-7B-v3\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/CleverBoi-7B-v3</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__CleverBoi-7B-v3-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/CleverBoi-7B-v3",
    "Model sha": "1d82629c1e6778cf8568b532a3c09b668805b15a",
    "Average ‚¨ÜÔ∏è": 13.589762491249099,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.602889817897953,
    "IFEval Raw": 0.23823011830831084,
    "IFEval": 23.823011830831085,
    "BBH Raw": 0.4414430902840938,
    "BBH": 21.93674717909172,
    "MATH Lvl 5 Raw": 0.033987915407854986,
    "MATH Lvl 5": 3.3987915407854987,
    "GPQA Raw": 0.26593959731543626,
    "GPQA": 2.1252796420581683,
    "MUSR Raw": 0.4071770833333333,
    "MUSR": 9.497135416666667,
    "MMLU-PRO Raw": 0.28681848404255317,
    "MMLU-PRO": 20.75760933806146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-14",
    "Submission Date": "2024-09-22",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-v0.3-bnb-4bit"
  },
  {
    "eval_name": "theprint_CleverBoi-Llama-3.1-8B-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/CleverBoi-Llama-3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/CleverBoi-Llama-3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__CleverBoi-Llama-3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/CleverBoi-Llama-3.1-8B-Instruct",
    "Model sha": "3514c510ea4ba4d650522f467d4d0cef7de4a43c",
    "Average ‚¨ÜÔ∏è": 13.605339337761052,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8702225951405291,
    "IFEval Raw": 0.16816269719898758,
    "IFEval": 16.816269719898756,
    "BBH Raw": 0.4559618469185147,
    "BBH": 24.048603081139294,
    "MATH Lvl 5 Raw": 0.02719033232628399,
    "MATH Lvl 5": 2.7190332326283992,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.40143750000000006,
    "MUSR": 8.279687500000003,
    "MMLU-PRO Raw": 0.30751329787234044,
    "MMLU-PRO": 23.057033096926716,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-27",
    "Submission Date": "2024-09-13",
    "Generation": 2,
    "Base Model": "unsloth/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "theprint_CleverBoi-Llama-3.1-8B-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/CleverBoi-Llama-3.1-8B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/CleverBoi-Llama-3.1-8B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__CleverBoi-Llama-3.1-8B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/CleverBoi-Llama-3.1-8B-v2",
    "Model sha": "a8b0fc584b10e0110e04f9d21c7f10d24391c1d5",
    "Average ‚¨ÜÔ∏è": 14.09523510262281,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.521379276092503,
    "IFEval Raw": 0.19613957632415324,
    "IFEval": 19.613957632415325,
    "BBH Raw": 0.46678160110644784,
    "BBH": 24.132844977310707,
    "MATH Lvl 5 Raw": 0.04984894259818732,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.2860738255033557,
    "GPQA": 4.809843400447425,
    "MUSR Raw": 0.37346875,
    "MUSR": 6.716927083333334,
    "MMLU-PRO Raw": 0.31881648936170215,
    "MMLU-PRO": 24.31294326241135,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-15",
    "Submission Date": "2024-09-22",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "theprint_CleverBoi-Nemo-12B-v2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/CleverBoi-Nemo-12B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/CleverBoi-Nemo-12B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__CleverBoi-Nemo-12B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/CleverBoi-Nemo-12B-v2",
    "Model sha": "cd1f9ee1c484f857bb0e5ae6aac37dc434911f10",
    "Average ‚¨ÜÔ∏è": 17.682159672297907,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.505513278926229,
    "IFEval Raw": 0.2045827293802666,
    "IFEval": 20.45827293802666,
    "BBH Raw": 0.5241085887165254,
    "BBH": 31.65269522562221,
    "MATH Lvl 5 Raw": 0.09290030211480363,
    "MATH Lvl 5": 9.290030211480364,
    "GPQA Raw": 0.313758389261745,
    "GPQA": 8.501118568232664,
    "MUSR Raw": 0.4186770833333333,
    "MUSR": 11.434635416666666,
    "MMLU-PRO Raw": 0.3228058510638298,
    "MMLU-PRO": 24.756205673758867,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-24",
    "Generation": 1,
    "Base Model": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"
  },
  {
    "eval_name": "theprint_Code-Llama-Bagel-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/Code-Llama-Bagel-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/Code-Llama-Bagel-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__Code-Llama-Bagel-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/Code-Llama-Bagel-8B",
    "Model sha": "7fa415f3f758ab7930d7e1df27b2d16207513125",
    "Average ‚¨ÜÔ∏è": 14.526782048766131,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8180467028392016,
    "IFEval Raw": 0.2529676813078188,
    "IFEval": 25.29676813078188,
    "BBH Raw": 0.46974200049001086,
    "BBH": 25.33815455229531,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3679791666666667,
    "MUSR": 7.530729166666667,
    "MMLU-PRO Raw": 0.28216422872340424,
    "MMLU-PRO": 20.240469858156025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-21",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "theprint/Code-Llama-Bagel-8B (Merge)"
  },
  {
    "eval_name": "theprint_Llama-3.2-3B-VanRossum_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/Llama-3.2-3B-VanRossum\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/Llama-3.2-3B-VanRossum</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__Llama-3.2-3B-VanRossum-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/Llama-3.2-3B-VanRossum",
    "Model sha": "7048abecd492a1f5d53981cb175431ec01bbced0",
    "Average ‚¨ÜÔ∏è": 17.521868377821338,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8545883752961583,
    "IFEval Raw": 0.4782820693537591,
    "IFEval": 47.82820693537591,
    "BBH Raw": 0.42787418229776697,
    "BBH": 19.366361887075996,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.2676174496644295,
    "GPQA": 2.348993288590602,
    "MUSR Raw": 0.3441666666666667,
    "MUSR": 6.554166666666667,
    "MMLU-PRO Raw": 0.27701130319148937,
    "MMLU-PRO": 19.66792257683215,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-14",
    "Submission Date": "2024-11-14",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "theprint_ReWiz-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-7B",
    "Model sha": "d9f28e67d52181d1478e7788e3edf252f5bf32a8",
    "Average ‚¨ÜÔ∏è": 17.598218896455336,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.4454056830758402,
    "IFEval Raw": 0.40479261692309737,
    "IFEval": 40.479261692309734,
    "BBH Raw": 0.4564215411912313,
    "BBH": 23.50442985462492,
    "MATH Lvl 5 Raw": 0.029456193353474325,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.46115625,
    "MUSR": 16.744531249999998,
    "MMLU-PRO Raw": 0.2670378989361702,
    "MMLU-PRO": 18.559766548463354,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-08",
    "Submission Date": "2024-10-08",
    "Generation": 1,
    "Base Model": "unsloth/mistral-7b-instruct-v0.3-bnb-4bit"
  },
  {
    "eval_name": "theprint_ReWiz-Llama-3.1-8B-v2_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-Llama-3.1-8B-v2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-Llama-3.1-8B-v2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-Llama-3.1-8B-v2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-Llama-3.1-8B-v2",
    "Model sha": "a8b0fc584b10e0110e04f9d21c7f10d24391c1d5",
    "Average ‚¨ÜÔ∏è": 15.68192591041191,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 9,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.3273951613364714,
    "IFEval Raw": 0.2373059038905659,
    "IFEval": 23.73059038905659,
    "BBH Raw": 0.46324275457450953,
    "BBH": 23.773287089432596,
    "MATH Lvl 5 Raw": 0.045317220543806644,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.3028523489932886,
    "GPQA": 7.046979865771815,
    "MUSR Raw": 0.381375,
    "MUSR": 9.33854166666667,
    "MMLU-PRO Raw": 0.3310339095744681,
    "MMLU-PRO": 25.67043439716312,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-02",
    "Submission Date": "2024-11-03",
    "Generation": 2,
    "Base Model": "meta-llama/Meta-Llama-3.1-8B"
  },
  {
    "eval_name": "theprint_ReWiz-Llama-3.2-3B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-Llama-3.2-3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-Llama-3.2-3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-Llama-3.2-3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-Llama-3.2-3B",
    "Model sha": "e6aed95ad8f104f105b8423cd5f87c75705a828c",
    "Average ‚¨ÜÔ∏è": 17.984844344047133,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3097201117737385,
    "IFEval Raw": 0.4648931501748693,
    "IFEval": 46.48931501748693,
    "BBH Raw": 0.4343257577815292,
    "BBH": 19.293728122396512,
    "MATH Lvl 5 Raw": 0.09743202416918428,
    "MATH Lvl 5": 9.743202416918429,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.361375,
    "MUSR": 6.938541666666667,
    "MMLU-PRO Raw": 0.28873005319148937,
    "MMLU-PRO": 20.970005910165483,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "theprint/ReWiz-Llama-3.2-3B (Merge)"
  },
  {
    "eval_name": "theprint_ReWiz-Nemo-12B-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-Nemo-12B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-Nemo-12B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-Nemo-12B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-Nemo-12B-Instruct",
    "Model sha": "6f8ea24f8d19b48850d68bef1b5c50837d37761b",
    "Average ‚¨ÜÔ∏è": 15.631853381940678,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.170029697649757,
    "IFEval Raw": 0.10623811486854878,
    "IFEval": 10.623811486854878,
    "BBH Raw": 0.5092407647626753,
    "BBH": 29.926389365821837,
    "MATH Lvl 5 Raw": 0.07175226586102719,
    "MATH Lvl 5": 7.175226586102719,
    "GPQA Raw": 0.3238255033557047,
    "GPQA": 9.843400447427292,
    "MUSR Raw": 0.4095625,
    "MUSR": 10.22864583333333,
    "MMLU-PRO Raw": 0.33394281914893614,
    "MMLU-PRO": 25.993646572104012,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-31",
    "Submission Date": "2024-11-02",
    "Generation": 1,
    "Base Model": "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"
  },
  {
    "eval_name": "theprint_ReWiz-Qwen-2.5-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-Qwen-2.5-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-Qwen-2.5-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-Qwen-2.5-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-Qwen-2.5-14B",
    "Model sha": "e5524628f15c30d7542427c53a565e6e2d3ff760",
    "Average ‚¨ÜÔ∏è": 29.641502386603474,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 16,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 5.9282663560452935,
    "IFEval Raw": 0.27854647889821227,
    "IFEval": 27.85464788982123,
    "BBH Raw": 0.6179492756426455,
    "BBH": 44.86187336352475,
    "MATH Lvl 5 Raw": 0.2688821752265861,
    "MATH Lvl 5": 26.888217522658607,
    "GPQA Raw": 0.3800335570469799,
    "GPQA": 17.337807606263986,
    "MUSR Raw": 0.45389583333333333,
    "MUSR": 15.436979166666667,
    "MMLU-PRO Raw": 0.5092253989361702,
    "MMLU-PRO": 45.46948877068559,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-10",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-14B"
  },
  {
    "eval_name": "theprint_ReWiz-Worldbuilder-7B_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/ReWiz-Worldbuilder-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/ReWiz-Worldbuilder-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__ReWiz-Worldbuilder-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/ReWiz-Worldbuilder-7B",
    "Model sha": "e88c715097d824f115f59a97e612d662ffb1031f",
    "Average ‚¨ÜÔ∏è": 15.664818750813359,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6108671734325988,
    "IFEval Raw": 0.25101951710350756,
    "IFEval": 25.101951710350757,
    "BBH Raw": 0.46361558385510165,
    "BBH": 25.07634729001636,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.45725,
    "MUSR": 16.389583333333334,
    "MMLU-PRO Raw": 0.297124335106383,
    "MMLU-PRO": 21.90270390070922,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-28",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "theprint/ReWiz-Worldbuilder-7B (Merge)"
  },
  {
    "eval_name": "theprint_RuDolph-Hermes-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/RuDolph-Hermes-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/RuDolph-Hermes-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__RuDolph-Hermes-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/RuDolph-Hermes-7B",
    "Model sha": "e07aea56963bbfe5c6753d1056566a56acc30d4a",
    "Average ‚¨ÜÔ∏è": 19.024424971411335,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5020672197937716,
    "IFEval Raw": 0.3604292167005767,
    "IFEval": 36.042921670057666,
    "BBH Raw": 0.5052928613425586,
    "BBH": 30.709648163100884,
    "MATH Lvl 5 Raw": 0.05060422960725076,
    "MATH Lvl 5": 5.0604229607250755,
    "GPQA Raw": 0.31208053691275167,
    "GPQA": 8.277404921700223,
    "MUSR Raw": 0.4226145833333333,
    "MUSR": 11.026822916666669,
    "MMLU-PRO Raw": 0.30726396276595747,
    "MMLU-PRO": 23.029329196217496,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-10",
    "Submission Date": "2024-11-10",
    "Generation": 1,
    "Base Model": "theprint/RuDolph-Hermes-7B (Merge)"
  },
  {
    "eval_name": "theprint_WorldBuilder-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/WorldBuilder-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/WorldBuilder-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__WorldBuilder-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/WorldBuilder-12B",
    "Model sha": "20cfd0e98fb2628b00867147b2c6f423d27f3561",
    "Average ‚¨ÜÔ∏è": 14.377937291631477,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.8312751025995966,
    "IFEval Raw": 0.13743755457741016,
    "IFEval": 13.743755457741015,
    "BBH Raw": 0.5010100641541125,
    "BBH": 29.277996282041656,
    "MATH Lvl 5 Raw": 0.03625377643504532,
    "MATH Lvl 5": 3.625377643504532,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4066458333333334,
    "MUSR": 8.997395833333337,
    "MMLU-PRO Raw": 0.31923204787234044,
    "MMLU-PRO": 24.35911643026005,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "unsloth/mistral-nemo-base-2407-bnb-4bit"
  },
  {
    "eval_name": "theprint_phi-3-mini-4k-python_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/theprint/phi-3-mini-4k-python\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">theprint/phi-3-mini-4k-python</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/theprint__phi-3-mini-4k-python-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "theprint/phi-3-mini-4k-python",
    "Model sha": "81453e5718775630581ab9950e6c0ccf0d7a4177",
    "Average ‚¨ÜÔ∏è": 17.564492625487873,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3755509270872666,
    "IFEval Raw": 0.24087753826513653,
    "IFEval": 24.08775382651365,
    "BBH Raw": 0.493759004635898,
    "BBH": 28.44601616578647,
    "MATH Lvl 5 Raw": 0.09516616314199396,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.3921666666666666,
    "MUSR": 9.22083333333333,
    "MMLU-PRO Raw": 0.35771276595744683,
    "MMLU-PRO": 28.63475177304965,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-03",
    "Submission Date": "2024-09-13",
    "Generation": 1,
    "Base Model": "unsloth/Phi-3-mini-4k-instruct-bnb-4bit"
  },
  {
    "eval_name": "thomas-yanxin_XinYuan-Qwen2-1_5B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/thomas-yanxin/XinYuan-Qwen2-1_5B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">thomas-yanxin/XinYuan-Qwen2-1_5B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/thomas-yanxin__XinYuan-Qwen2-1_5B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "thomas-yanxin/XinYuan-Qwen2-1_5B",
    "Model sha": "a01b362887832bea08d686737861ac3d5b437a32",
    "Average ‚¨ÜÔ∏è": 11.515091263493494,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.352364069538753,
    "IFEval Raw": 0.2985556102253133,
    "IFEval": 29.855561022531326,
    "BBH Raw": 0.3635491993150823,
    "BBH": 12.125579560037659,
    "MATH Lvl 5 Raw": 0.06722054380664653,
    "MATH Lvl 5": 6.7220543806646536,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.36339583333333336,
    "MUSR": 2.6244791666666685,
    "MMLU-PRO Raw": 0.23570478723404256,
    "MMLU-PRO": 15.078309692671397,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-25",
    "Submission Date": "2024-09-04",
    "Generation": 1,
    "Base Model": "Removed"
  },
  {
    "eval_name": "thomas-yanxin_XinYuan-Qwen2-7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/thomas-yanxin/XinYuan-Qwen2-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">thomas-yanxin/XinYuan-Qwen2-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/thomas-yanxin__XinYuan-Qwen2-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "thomas-yanxin/XinYuan-Qwen2-7B",
    "Model sha": "c62d83eee2f4812ac17fc17d307f4aa1a77c5359",
    "Average ‚¨ÜÔ∏è": 22.217713671682052,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.2761544395927573,
    "IFEval Raw": 0.44376033369238066,
    "IFEval": 44.376033369238066,
    "BBH Raw": 0.4936629157238895,
    "BBH": 28.40148852275863,
    "MATH Lvl 5 Raw": 0.13293051359516617,
    "MATH Lvl 5": 13.293051359516618,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.40581249999999996,
    "MUSR": 9.259895833333337,
    "MMLU-PRO Raw": 0.3924534574468085,
    "MMLU-PRO": 32.494828605200944,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-21",
    "Submission Date": "2024-09-03",
    "Generation": 0,
    "Base Model": "thomas-yanxin/XinYuan-Qwen2-7B"
  },
  {
    "eval_name": "thomas-yanxin_XinYuan-Qwen2-7B-0917_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/thomas-yanxin/XinYuan-Qwen2-7B-0917\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">thomas-yanxin/XinYuan-Qwen2-7B-0917</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/thomas-yanxin__XinYuan-Qwen2-7B-0917-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "thomas-yanxin/XinYuan-Qwen2-7B-0917",
    "Model sha": "6cee1b155fca9ae1f558f434953dfdadb9596af0",
    "Average ‚¨ÜÔ∏è": 22.721616532140345,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4855644936356307,
    "IFEval Raw": 0.37191983935956596,
    "IFEval": 37.19198393595659,
    "BBH Raw": 0.5169215573786009,
    "BBH": 32.61993813582105,
    "MATH Lvl 5 Raw": 0.08836858006042297,
    "MATH Lvl 5": 8.836858006042297,
    "GPQA Raw": 0.30956375838926176,
    "GPQA": 7.941834451901568,
    "MUSR Raw": 0.4401041666666667,
    "MUSR": 13.6796875,
    "MMLU-PRO Raw": 0.4245345744680851,
    "MMLU-PRO": 36.059397163120565,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-17",
    "Generation": 0,
    "Base Model": "thomas-yanxin/XinYuan-Qwen2-7B-0917"
  },
  {
    "eval_name": "thomas-yanxin_XinYuan-Qwen2.5-7B-0917_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/thomas-yanxin/XinYuan-Qwen2.5-7B-0917\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">thomas-yanxin/XinYuan-Qwen2.5-7B-0917</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/thomas-yanxin__XinYuan-Qwen2.5-7B-0917-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "thomas-yanxin/XinYuan-Qwen2.5-7B-0917",
    "Model sha": "bbbeafd1003c4d5e13f09b7223671957384b961a",
    "Average ‚¨ÜÔ∏è": 18.175036981235262,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9712252333507589,
    "IFEval Raw": 0.35770644113175265,
    "IFEval": 35.770644113175265,
    "BBH Raw": 0.5184106116987492,
    "BBH": 33.43966927024198,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.3675520833333333,
    "MUSR": 3.677343750000001,
    "MMLU-PRO Raw": 0.38821476063829785,
    "MMLU-PRO": 32.023862293144205,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-17",
    "Submission Date": "2024-09-24",
    "Generation": 0,
    "Base Model": "thomas-yanxin/XinYuan-Qwen2.5-7B-0917"
  },
  {
    "eval_name": "tiiuae_falcon-11B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "FalconForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-11B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-11B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-11B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-11B",
    "Model sha": "066e3bf4e2d9aaeefa129af0a6d39727d27816b3",
    "Average ‚¨ÜÔ∏è": 13.814138235727041,
    "Hub License": "unknown",
    "Hub ‚ù§Ô∏è": 212,
    "#Params (B)": 11,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0828709914176635,
    "IFEval Raw": 0.3261324397044287,
    "IFEval": 32.613243970442866,
    "BBH Raw": 0.43916370355493844,
    "BBH": 21.937999462890275,
    "MATH Lvl 5 Raw": 0.0256797583081571,
    "MATH Lvl 5": 2.56797583081571,
    "GPQA Raw": 0.2709731543624161,
    "GPQA": 2.796420581655479,
    "MUSR Raw": 0.39864583333333337,
    "MUSR": 7.530729166666667,
    "MMLU-PRO Raw": 0.23894614361702127,
    "MMLU-PRO": 15.438460401891252,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-05-09",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-11B"
  },
  {
    "eval_name": "tiiuae_falcon-40b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "FalconForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-40b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-40b",
    "Model sha": "4a70170c215b36a3cce4b4253f6d0612bb7d4146",
    "Average ‚¨ÜÔ∏è": 11.363540111846916,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2417,
    "#Params (B)": 40,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 21.793583901434186,
    "IFEval Raw": 0.24964538535530173,
    "IFEval": 24.964538535530174,
    "BBH Raw": 0.4018532495595801,
    "BBH": 16.583304730312175,
    "MATH Lvl 5 Raw": 0.015861027190332326,
    "MATH Lvl 5": 1.5861027190332326,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.36314583333333333,
    "MUSR": 5.193229166666668,
    "MMLU-PRO Raw": 0.25049867021276595,
    "MMLU-PRO": 16.722074468085104,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-24",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-40b"
  },
  {
    "eval_name": "tiiuae_falcon-40b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "FalconForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-40b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-40b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-40b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-40b-instruct",
    "Model sha": "ecb78d97ac356d098e79f0db222c9ce7c5d9ee5f",
    "Average ‚¨ÜÔ∏è": 10.434154314827852,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1172,
    "#Params (B)": 40,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 19.733245487176763,
    "IFEval Raw": 0.24544874266945038,
    "IFEval": 24.54487426694504,
    "BBH Raw": 0.40538675151591974,
    "BBH": 17.220114203264526,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.37622916666666667,
    "MUSR": 5.161979166666666,
    "MMLU-PRO Raw": 0.2261469414893617,
    "MMLU-PRO": 14.016326832151298,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-25",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-40b-instruct"
  },
  {
    "eval_name": "tiiuae_falcon-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "FalconForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-7b",
    "Model sha": "898df1396f35e447d5fe44e0a3ccaaaa69f30d36",
    "Average ‚¨ÜÔ∏è": 5.110504136230859,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1078,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7858412374207651,
    "IFEval Raw": 0.182051401392749,
    "IFEval": 18.205140139274903,
    "BBH Raw": 0.32852446117322215,
    "BBH": 5.963936911876051,
    "MATH Lvl 5 Raw": 0.006042296072507554,
    "MATH Lvl 5": 0.6042296072507554,
    "GPQA Raw": 0.24496644295302014,
    "GPQA": 0.0,
    "MUSR Raw": 0.37784375,
    "MUSR": 4.497135416666667,
    "MMLU-PRO Raw": 0.11253324468085106,
    "MMLU-PRO": 1.392582742316784,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-04-24",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-7b"
  },
  {
    "eval_name": "tiiuae_falcon-7b-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "FalconForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-7b-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-7b-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-7b-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-7b-instruct",
    "Model sha": "cf4b3c42ce2fdfe24f753f0f0d179202fea59c99",
    "Average ‚¨ÜÔ∏è": 5.015868974143408,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 922,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7662145256058875,
    "IFEval Raw": 0.19688869976107837,
    "IFEval": 19.68886997610784,
    "BBH Raw": 0.32034221512355765,
    "BBH": 4.8231784606744315,
    "MATH Lvl 5 Raw": 0.006042296072507553,
    "MATH Lvl 5": 0.6042296072507553,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.3633645833333334,
    "MUSR": 3.2539062500000004,
    "MMLU-PRO Raw": 0.1155252659574468,
    "MMLU-PRO": 1.725029550827422,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-04-25",
    "Submission Date": "2024-06-09",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-7b-instruct"
  },
  {
    "eval_name": "tiiuae_falcon-mamba-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "FalconMambaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tiiuae/falcon-mamba-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tiiuae/falcon-mamba-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tiiuae__falcon-mamba-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tiiuae/falcon-mamba-7b",
    "Model sha": "5337fd73f19847e111ba2291f3f0e1617b90c37d",
    "Average ‚¨ÜÔ∏è": 15.116297443522598,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 216,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 3.6104079174982933,
    "IFEval Raw": 0.3335760227307987,
    "IFEval": 33.35760227307987,
    "BBH Raw": 0.4284854988604366,
    "BBH": 19.876877803543437,
    "MATH Lvl 5 Raw": 0.040785498489425975,
    "MATH Lvl 5": 4.078549848942598,
    "GPQA Raw": 0.3104026845637584,
    "GPQA": 8.05369127516779,
    "MUSR Raw": 0.42103124999999997,
    "MUSR": 10.862239583333334,
    "MMLU-PRO Raw": 0.23021941489361702,
    "MMLU-PRO": 14.468823877068557,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-07-17",
    "Submission Date": "2024-07-23",
    "Generation": 0,
    "Base Model": "tiiuae/falcon-mamba-7b"
  },
  {
    "eval_name": "tklohj_WindyFloLLM_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tklohj/WindyFloLLM\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tklohj/WindyFloLLM</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tklohj__WindyFloLLM-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tklohj/WindyFloLLM",
    "Model sha": "21f4241ab3f091d1d309e9076a8d8e3f014908a8",
    "Average ‚¨ÜÔ∏è": 14.205891053135085,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0985120930498795,
    "IFEval Raw": 0.26685638550158025,
    "IFEval": 26.685638550158025,
    "BBH Raw": 0.4636616007058791,
    "BBH": 24.39876319785054,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.4253125,
    "MUSR": 11.864062500000003,
    "MMLU-PRO Raw": 0.25814494680851063,
    "MMLU-PRO": 17.57166075650118,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-30",
    "Submission Date": "2024-07-10",
    "Generation": 1,
    "Base Model": "tklohj/WindyFloLLM (Merge)"
  },
  {
    "eval_name": "togethercomputer_GPT-JT-6B-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTJForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/GPT-JT-6B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/GPT-JT-6B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__GPT-JT-6B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/GPT-JT-6B-v1",
    "Model sha": "f34aa35f906895602c1f86f5685e598afdea8051",
    "Average ‚¨ÜÔ∏è": 6.827354360467209,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 301,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 37.95881068085606,
    "IFEval Raw": 0.20610646418170453,
    "IFEval": 20.610646418170454,
    "BBH Raw": 0.33026609127426704,
    "BBH": 7.318523965141613,
    "MATH Lvl 5 Raw": 0.007552870090634441,
    "MATH Lvl 5": 0.755287009063444,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.37365625,
    "MUSR": 3.873697916666666,
    "MMLU-PRO Raw": 0.16256648936170212,
    "MMLU-PRO": 6.951832151300234,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2022-11-24",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/GPT-JT-6B-v1"
  },
  {
    "eval_name": "togethercomputer_GPT-NeoXT-Chat-Base-20B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/GPT-NeoXT-Chat-Base-20B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/GPT-NeoXT-Chat-Base-20B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__GPT-NeoXT-Chat-Base-20B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/GPT-NeoXT-Chat-Base-20B",
    "Model sha": "d386708e84d862a65f7d2b4989f64750cb657227",
    "Average ‚¨ÜÔ∏è": 4.964061821264274,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 695,
    "#Params (B)": 20,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.9835879877218745,
    "IFEval Raw": 0.18297561581049393,
    "IFEval": 18.297561581049393,
    "BBH Raw": 0.33209702572173033,
    "BBH": 6.830794983137852,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.3460625,
    "MUSR": 1.7578124999999993,
    "MMLU-PRO Raw": 0.11452792553191489,
    "MMLU-PRO": 1.6142139479905429,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-03-03",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/GPT-NeoXT-Chat-Base-20B"
  },
  {
    "eval_name": "togethercomputer_LLaMA-2-7B-32K_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/LLaMA-2-7B-32K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/LLaMA-2-7B-32K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__LLaMA-2-7B-32K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/LLaMA-2-7B-32K",
    "Model sha": "46c24bb5aef59722fa7aa6d75e832afd1d64b980",
    "Average ‚¨ÜÔ∏è": 6.737010933052859,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 533,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5845727731386725,
    "IFEval Raw": 0.18649738250065384,
    "IFEval": 18.649738250065383,
    "BBH Raw": 0.33995175217301715,
    "BBH": 8.089984229889549,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.25,
    "GPQA": 0.0,
    "MUSR Raw": 0.3753645833333333,
    "MUSR": 4.320572916666666,
    "MMLU-PRO Raw": 0.17677859042553193,
    "MMLU-PRO": 8.530954491725769,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-07-26",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/LLaMA-2-7B-32K"
  },
  {
    "eval_name": "togethercomputer_Llama-2-7B-32K-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/Llama-2-7B-32K-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/Llama-2-7B-32K-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__Llama-2-7B-32K-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/Llama-2-7B-32K-Instruct",
    "Model sha": "d27380af003252f5eb0d218e104938b4e673e3f3",
    "Average ‚¨ÜÔ∏è": 8.208189718518014,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 159,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5899093632595604,
    "IFEval Raw": 0.2130003945087922,
    "IFEval": 21.300039450879225,
    "BBH Raw": 0.34434724239927544,
    "BBH": 8.563469919446954,
    "MATH Lvl 5 Raw": 0.012839879154078549,
    "MATH Lvl 5": 1.2839879154078548,
    "GPQA Raw": 0.2516778523489933,
    "GPQA": 0.22371364653244186,
    "MUSR Raw": 0.40559375,
    "MUSR": 9.199218750000002,
    "MMLU-PRO Raw": 0.17810837765957446,
    "MMLU-PRO": 8.678708628841607,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-08-08",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/Llama-2-7B-32K-Instruct"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-7B-Base_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Base\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Base</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-7B-Base-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-7B-Base",
    "Model sha": "78f7e482443971f4873ba3239f0ac810a367833b",
    "Average ‚¨ÜÔ∏è": 5.486285593128926,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 94,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.220607163326697,
    "IFEval Raw": 0.20822971936683554,
    "IFEval": 20.822971936683555,
    "BBH Raw": 0.31948898765013445,
    "BBH": 5.0872422729164315,
    "MATH Lvl 5 Raw": 0.011329305135951663,
    "MATH Lvl 5": 1.1329305135951662,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.36199999999999993,
    "MUSR": 3.0166666666666657,
    "MMLU-PRO Raw": 0.1196808510638298,
    "MMLU-PRO": 2.186761229314421,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-04",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-7B-Base"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-7B-Chat_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-7B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-7B-Chat",
    "Model sha": "47b94a739e2f3164b438501c8684acc5d5acc146",
    "Average ‚¨ÜÔ∏è": 3.962783773521173,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 92,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2193361066814834,
    "IFEval Raw": 0.1557977278066641,
    "IFEval": 15.57977278066641,
    "BBH Raw": 0.3175449328457368,
    "BBH": 4.502173664381199,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.2525167785234899,
    "GPQA": 0.33557046979865535,
    "MUSR Raw": 0.3447604166666667,
    "MUSR": 1.8617187499999996,
    "MMLU-PRO Raw": 0.11211768617021277,
    "MMLU-PRO": 1.3464095744680846,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-04",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-7B-Chat"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-7B-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-7B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-7B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-7B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-7B-Instruct",
    "Model sha": "7f36397b9985a3f981cdb618f8fec1c565ca5927",
    "Average ‚¨ÜÔ∏è": 6.356020558176529,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 104,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.181118670326691,
    "IFEval Raw": 0.2055069437980115,
    "IFEval": 20.55069437980115,
    "BBH Raw": 0.337743947089799,
    "BBH": 7.9054164937041635,
    "MATH Lvl 5 Raw": 0.015105740181268881,
    "MATH Lvl 5": 1.510574018126888,
    "GPQA Raw": 0.25083892617449666,
    "GPQA": 0.11185682326622093,
    "MUSR Raw": 0.3685104166666666,
    "MUSR": 5.03046875,
    "MMLU-PRO Raw": 0.1272440159574468,
    "MMLU-PRO": 3.027112884160755,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-05",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-7B-Instruct"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-Base-3B-v1_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Base-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Base-3B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-Base-3B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-Base-3B-v1",
    "Model sha": "094fbdd0c911feb485ce55de1952ab2e75277e1e",
    "Average ‚¨ÜÔ∏è": 5.4455616837478376,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 90,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7761018456495044,
    "IFEval Raw": 0.22936253584932426,
    "IFEval": 22.936253584932423,
    "BBH Raw": 0.3060403878987615,
    "BBH": 3.518607767474259,
    "MATH Lvl 5 Raw": 0.009818731117824773,
    "MATH Lvl 5": 0.9818731117824773,
    "GPQA Raw": 0.24328859060402686,
    "GPQA": 0.0,
    "MUSR Raw": 0.37387499999999996,
    "MUSR": 4.001041666666667,
    "MMLU-PRO Raw": 0.11112034574468085,
    "MMLU-PRO": 1.2355939716312052,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-04",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-Base-3B-v1"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-Chat-3B-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Chat-3B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-Chat-3B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-Chat-3B-v1",
    "Model sha": "f0e0995eba801096ed04cb87931d96a8316871af",
    "Average ‚¨ÜÔ∏è": 4.748118992215374,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 152,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7749089583727328,
    "IFEval Raw": 0.16521496296493304,
    "IFEval": 16.521496296493304,
    "BBH Raw": 0.32166937119202416,
    "BBH": 5.164727927050627,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.24412751677852348,
    "GPQA": 0.0,
    "MUSR Raw": 0.3684479166666667,
    "MUSR": 5.089322916666668,
    "MMLU-PRO Raw": 0.11269946808510638,
    "MMLU-PRO": 1.4110520094562635,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-05",
    "Submission Date": "2024-06-13",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-Chat-3B-v1"
  },
  {
    "eval_name": "togethercomputer_RedPajama-INCITE-Instruct-3B-v1_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "GPTNeoXForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/togethercomputer/RedPajama-INCITE-Instruct-3B-v1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">togethercomputer/RedPajama-INCITE-Instruct-3B-v1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/togethercomputer__RedPajama-INCITE-Instruct-3B-v1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
    "Model sha": "0c66778ee09a036886741707733620b91057909a",
    "Average ‚¨ÜÔ∏è": 5.676526625585194,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 93,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7606710347098407,
    "IFEval Raw": 0.2124263620526869,
    "IFEval": 21.24263620526869,
    "BBH Raw": 0.3146017752057237,
    "BBH": 4.510786368926982,
    "MATH Lvl 5 Raw": 0.006797583081570997,
    "MATH Lvl 5": 0.6797583081570997,
    "GPQA Raw": 0.24748322147651006,
    "GPQA": 0.0,
    "MUSR Raw": 0.38860416666666664,
    "MUSR": 6.408854166666669,
    "MMLU-PRO Raw": 0.11095412234042554,
    "MMLU-PRO": 1.2171247044917257,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-05-05",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "togethercomputer/RedPajama-INCITE-Instruct-3B-v1"
  },
  {
    "eval_name": "tokyotech-llm_Llama-3-Swallow-8B-Instruct-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/tokyotech-llm__Llama-3-Swallow-8B-Instruct-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1",
    "Model sha": "1fae784584dd03680b72dd4de7eefbc5b7cabcd5",
    "Average ‚¨ÜÔ∏è": 22.30738546745772,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8581101707530224,
    "IFEval Raw": 0.5507719517546776,
    "IFEval": 55.077195175467764,
    "BBH Raw": 0.5009389976232003,
    "BBH": 29.267966131617708,
    "MATH Lvl 5 Raw": 0.07250755287009064,
    "MATH Lvl 5": 7.250755287009064,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.43569791666666663,
    "MUSR": 13.795572916666664,
    "MMLU-PRO Raw": 0.3087599734042553,
    "MMLU-PRO": 23.195552600472812,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-09-12",
    "Generation": 0,
    "Base Model": "tokyotech-llm/Llama-3-Swallow-8B-Instruct-v0.1"
  },
  {
    "eval_name": "upstage_SOLAR-10.7B-Instruct-v1.0_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-Instruct-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/upstage__SOLAR-10.7B-Instruct-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "upstage/SOLAR-10.7B-Instruct-v1.0",
    "Model sha": "c08c25ed66414a878fe0401a3596d536c083606c",
    "Average ‚¨ÜÔ∏è": 19.628255331894646,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 614,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7827757856385875,
    "IFEval Raw": 0.4736609972650345,
    "IFEval": 47.36609972650345,
    "BBH Raw": 0.5162494941446991,
    "BBH": 31.872401888002116,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.3899375,
    "MUSR": 6.942187500000002,
    "MMLU-PRO Raw": 0.31382978723404253,
    "MMLU-PRO": 23.758865248226947,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-12",
    "Submission Date": "2024-06-12",
    "Generation": 1,
    "Base Model": "upstage/SOLAR-10.7B-Instruct-v1.0 (Merge)"
  },
  {
    "eval_name": "upstage_SOLAR-10.7B-v1.0_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/upstage/SOLAR-10.7B-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/SOLAR-10.7B-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/upstage__SOLAR-10.7B-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "upstage/SOLAR-10.7B-v1.0",
    "Model sha": "a45090b8e56bdc2b8e32e46b3cd782fc0bea1fa5",
    "Average ‚¨ÜÔ∏è": 4.916447886280902,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 291,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.5191940183802333,
    "IFEval Raw": 0.17158472852032608,
    "IFEval": 17.158472852032606,
    "BBH Raw": 0.2998351737549512,
    "BBH": 2.1471627638186876,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3681979166666667,
    "MUSR": 4.524739583333332,
    "MMLU-PRO Raw": 0.11685505319148937,
    "MMLU-PRO": 1.8727836879432622,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2023-12-12",
    "Submission Date": "2024-06-12",
    "Generation": 0,
    "Base Model": "upstage/SOLAR-10.7B-v1.0"
  },
  {
    "eval_name": "upstage_solar-pro-preview-instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "SolarForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/upstage/solar-pro-preview-instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">upstage/solar-pro-preview-instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/upstage__solar-pro-preview-instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "upstage/solar-pro-preview-instruct",
    "Model sha": "b4db141b5fb08b23f8bc323bc34e2cff3e9675f8",
    "Average ‚¨ÜÔ∏è": 39.900890514079926,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 427,
    "#Params (B)": 22,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.7417631529200974,
    "IFEval Raw": 0.8415814483348626,
    "IFEval": 84.15814483348626,
    "BBH Raw": 0.6816843051379534,
    "BBH": 54.82235099983529,
    "MATH Lvl 5 Raw": 0.21827794561933536,
    "MATH Lvl 5": 21.827794561933537,
    "GPQA Raw": 0.37080536912751677,
    "GPQA": 16.10738255033557,
    "MUSR Raw": 0.44165625000000003,
    "MUSR": 15.007031249999997,
    "MMLU-PRO Raw": 0.52734375,
    "MMLU-PRO": 47.48263888888889,
    "Maintainer's Highlight": true,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-11",
    "Generation": 0,
    "Base Model": "upstage/solar-pro-preview-instruct"
  },
  {
    "eval_name": "uukuguy_speechless-code-mistral-7b-v1.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-code-mistral-7b-v1.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-code-mistral-7b-v1.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-code-mistral-7b-v1.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-code-mistral-7b-v1.0",
    "Model sha": "1862e0a712efc6002112e9c1235a197d58419b37",
    "Average ‚¨ÜÔ∏è": 18.09188675578363,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 18,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6463983393033655,
    "IFEval Raw": 0.36652415590632853,
    "IFEval": 36.652415590632856,
    "BBH Raw": 0.4571712887094195,
    "BBH": 24.091412067845624,
    "MATH Lvl 5 Raw": 0.04607250755287009,
    "MATH Lvl 5": 4.607250755287009,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.45017708333333334,
    "MUSR": 14.772135416666666,
    "MMLU-PRO Raw": 0.3145777925531915,
    "MMLU-PRO": 23.841976950354614,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-10",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-code-mistral-7b-v1.0"
  },
  {
    "eval_name": "uukuguy_speechless-codellama-34b-v2.0_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-codellama-34b-v2.0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-codellama-34b-v2.0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-codellama-34b-v2.0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-codellama-34b-v2.0",
    "Model sha": "419bc42a254102d6a5486a1a854068e912c4047c",
    "Average ‚¨ÜÔ∏è": 17.209357596769955,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 34,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.9912541922771037,
    "IFEval Raw": 0.46042168113937687,
    "IFEval": 46.042168113937684,
    "BBH Raw": 0.4813126697444618,
    "BBH": 25.993293267840638,
    "MATH Lvl 5 Raw": 0.04305135951661632,
    "MATH Lvl 5": 4.305135951661631,
    "GPQA Raw": 0.2692953020134229,
    "GPQA": 2.572706935123052,
    "MUSR Raw": 0.37870833333333337,
    "MUSR": 7.205208333333334,
    "MMLU-PRO Raw": 0.25423869680851063,
    "MMLU-PRO": 17.137632978723403,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-04",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-codellama-34b-v2.0"
  },
  {
    "eval_name": "uukuguy_speechless-coder-ds-6.7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-coder-ds-6.7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-coder-ds-6.7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-coder-ds-6.7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-coder-ds-6.7b",
    "Model sha": "c813a5268c6dfe267a720ad3b51773f1ab0feb59",
    "Average ‚¨ÜÔ∏è": 9.63932330169255,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 5,
    "#Params (B)": 6,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7886035058969163,
    "IFEval Raw": 0.25046986440422525,
    "IFEval": 25.046986440422522,
    "BBH Raw": 0.4036373344669979,
    "BBH": 15.897457343156352,
    "MATH Lvl 5 Raw": 0.01661631419939577,
    "MATH Lvl 5": 1.6616314199395772,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.3819375,
    "MUSR": 5.3421875000000005,
    "MMLU-PRO Raw": 0.171875,
    "MMLU-PRO": 7.986111111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-30",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-coder-ds-6.7b"
  },
  {
    "eval_name": "uukuguy_speechless-instruct-mistral-7b-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-instruct-mistral-7b-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-instruct-mistral-7b-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-instruct-mistral-7b-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-instruct-mistral-7b-v0.2",
    "Model sha": "87a4d214f7d028d61c3dc013a7410b3c34a24072",
    "Average ‚¨ÜÔ∏è": 18.018596667256222,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6176204823045223,
    "IFEval Raw": 0.3261324397044287,
    "IFEval": 32.613243970442866,
    "BBH Raw": 0.4606667950681749,
    "BBH": 24.558747365322688,
    "MATH Lvl 5 Raw": 0.04380664652567976,
    "MATH Lvl 5": 4.380664652567976,
    "GPQA Raw": 0.28187919463087246,
    "GPQA": 4.250559284116329,
    "MUSR Raw": 0.4901770833333334,
    "MUSR": 21.172135416666666,
    "MMLU-PRO Raw": 0.2902260638297872,
    "MMLU-PRO": 21.136229314420802,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-22",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-instruct-mistral-7b-v0.2"
  },
  {
    "eval_name": "uukuguy_speechless-llama2-hermes-orca-platypus-wizardlm-13b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-llama2-hermes-orca-platypus-wizardlm-13b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b",
    "Model sha": "954cc87b0ed5fa280126de546daf648861031512",
    "Average ‚¨ÜÔ∏è": 18.60089096059006,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 32,
    "#Params (B)": 13,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9795243328943037,
    "IFEval Raw": 0.45617517076911485,
    "IFEval": 45.61751707691148,
    "BBH Raw": 0.48455373040676664,
    "BBH": 26.79172729423422,
    "MATH Lvl 5 Raw": 0.014350453172205438,
    "MATH Lvl 5": 1.4350453172205437,
    "GPQA Raw": 0.2701342281879195,
    "GPQA": 2.684563758389265,
    "MUSR Raw": 0.4655,
    "MUSR": 17.754166666666666,
    "MMLU-PRO Raw": 0.25590093085106386,
    "MMLU-PRO": 17.322325650118206,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-09-01",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-llama2-hermes-orca-platypus-wizardlm-13b"
  },
  {
    "eval_name": "uukuguy_speechless-mistral-dolphin-orca-platypus-samantha-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-mistral-dolphin-orca-platypus-samantha-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b",
    "Model sha": "b1de043468a15198b55a6509293a4ee585139043",
    "Average ‚¨ÜÔ∏è": 18.340089485864258,
    "Hub License": "llama2",
    "Hub ‚ù§Ô∏è": 17,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6557187249634665,
    "IFEval Raw": 0.37002154283966543,
    "IFEval": 37.00215428396654,
    "BBH Raw": 0.4982774952761688,
    "BBH": 29.65312947574292,
    "MATH Lvl 5 Raw": 0.02945619335347432,
    "MATH Lvl 5": 2.9456193353474323,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.43613541666666666,
    "MUSR": 13.850260416666663,
    "MMLU-PRO Raw": 0.2990359042553192,
    "MMLU-PRO": 22.11510047281324,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-10-13",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-mistral-dolphin-orca-platypus-samantha-7b"
  },
  {
    "eval_name": "uukuguy_speechless-zephyr-code-functionary-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/uukuguy/speechless-zephyr-code-functionary-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">uukuguy/speechless-zephyr-code-functionary-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/uukuguy__speechless-zephyr-code-functionary-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "uukuguy/speechless-zephyr-code-functionary-7b",
    "Model sha": "d66fc775ece679966e352195c42444e9c70af7fa",
    "Average ‚¨ÜÔ∏è": 16.36012940579845,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6339996265773694,
    "IFEval Raw": 0.2695791610704043,
    "IFEval": 26.957916107040433,
    "BBH Raw": 0.46642753957194555,
    "BBH": 25.983622785908505,
    "MATH Lvl 5 Raw": 0.03625377643504532,
    "MATH Lvl 5": 3.625377643504532,
    "GPQA Raw": 0.30033557046979864,
    "GPQA": 6.711409395973152,
    "MUSR Raw": 0.4267708333333333,
    "MUSR": 11.613020833333332,
    "MMLU-PRO Raw": 0.3094248670212766,
    "MMLU-PRO": 23.269429669030732,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-23",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "uukuguy/speechless-zephyr-code-functionary-7b"
  },
  {
    "eval_name": "v000000_L3.1-Niitorm-8B-DPO-t0.0001_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/v000000/L3.1-Niitorm-8B-DPO-t0.0001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">v000000/L3.1-Niitorm-8B-DPO-t0.0001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/v000000__L3.1-Niitorm-8B-DPO-t0.0001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "v000000/L3.1-Niitorm-8B-DPO-t0.0001",
    "Model sha": "a34150b5f63de4bc83d79b1de127faff3750289f",
    "Average ‚¨ÜÔ∏è": 28.10064235417701,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8781090796981162,
    "IFEval Raw": 0.7688666072687137,
    "IFEval": 76.88666072687137,
    "BBH Raw": 0.5134234526726582,
    "BBH": 30.51317301580421,
    "MATH Lvl 5 Raw": 0.16163141993957705,
    "MATH Lvl 5": 16.163141993957705,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.3879791666666667,
    "MUSR": 7.2640625000000005,
    "MMLU-PRO Raw": 0.38663563829787234,
    "MMLU-PRO": 31.848404255319146,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-19",
    "Submission Date": "2024-09-19",
    "Generation": 1,
    "Base Model": "v000000/L3.1-Niitorm-8B-DPO-t0.0001 (Merge)"
  },
  {
    "eval_name": "v000000_L3.1-Storniitova-8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/v000000/L3.1-Storniitova-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">v000000/L3.1-Storniitova-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/v000000__L3.1-Storniitova-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "v000000/L3.1-Storniitova-8B",
    "Model sha": "05b126857f43d1b1383e50f8c97d214ceb199723",
    "Average ‚¨ÜÔ∏è": 28.28170680403471,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8135399997505502,
    "IFEval Raw": 0.7816560060639104,
    "IFEval": 78.16560060639105,
    "BBH Raw": 0.5151452004311876,
    "BBH": 30.810993185589904,
    "MATH Lvl 5 Raw": 0.14652567975830819,
    "MATH Lvl 5": 14.652567975830818,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.4028958333333333,
    "MUSR": 9.961979166666664,
    "MMLU-PRO Raw": 0.37757646276595747,
    "MMLU-PRO": 30.841829196217496,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-12",
    "Submission Date": "2024-09-18",
    "Generation": 1,
    "Base Model": "v000000/L3.1-Storniitova-8B (Merge)"
  },
  {
    "eval_name": "v000000_Qwen2.5-14B-Gutenberg-1e-Delta_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/v000000/Qwen2.5-14B-Gutenberg-1e-Delta\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">v000000/Qwen2.5-14B-Gutenberg-1e-Delta</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/v000000__Qwen2.5-14B-Gutenberg-1e-Delta-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "v000000/Qwen2.5-14B-Gutenberg-1e-Delta",
    "Model sha": "f624854b4380e01322e752ce4daadd49ac86580f",
    "Average ‚¨ÜÔ∏è": 32.105096397160736,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.8023865071901506,
    "IFEval Raw": 0.8045120280854798,
    "IFEval": 80.45120280854799,
    "BBH Raw": 0.639849930188539,
    "BBH": 48.6166718794722,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3288590604026846,
    "GPQA": 10.514541387024611,
    "MUSR Raw": 0.40730208333333334,
    "MUSR": 9.379427083333335,
    "MMLU-PRO Raw": 0.4930186170212766,
    "MMLU-PRO": 43.668735224586285,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "v000000/Qwen2.5-14B-Gutenberg-1e-Delta (Merge)"
  },
  {
    "eval_name": "v000000_Qwen2.5-14B-Gutenberg-Instruct-Slerpeno_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/v000000/Qwen2.5-14B-Gutenberg-Instruct-Slerpeno\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">v000000/Qwen2.5-14B-Gutenberg-Instruct-Slerpeno</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/v000000__Qwen2.5-14B-Gutenberg-Instruct-Slerpeno-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "v000000/Qwen2.5-14B-Gutenberg-Instruct-Slerpeno",
    "Model sha": "1069abb4c25855e67ffaefa08a0befbb376e7ca7",
    "Average ‚¨ÜÔ∏è": 33.665022590688345,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 4,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 2.2317902269561314,
    "IFEval Raw": 0.48547631395807567,
    "IFEval": 48.54763139580757,
    "BBH Raw": 0.651078668009944,
    "BBH": 49.73940007466614,
    "MATH Lvl 5 Raw": 0.2137462235649547,
    "MATH Lvl 5": 21.374622356495472,
    "GPQA Raw": 0.3640939597315436,
    "GPQA": 15.212527964205815,
    "MUSR Raw": 0.4690625,
    "MUSR": 18.432812499999994,
    "MMLU-PRO Raw": 0.5381482712765957,
    "MMLU-PRO": 48.68314125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-28",
    "Generation": 1,
    "Base Model": "v000000/Qwen2.5-14B-Gutenberg-Instruct-Slerpeno (Merge)"
  },
  {
    "eval_name": "v000000_Qwen2.5-Lumen-14B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/v000000/Qwen2.5-Lumen-14B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">v000000/Qwen2.5-Lumen-14B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/v000000__Qwen2.5-Lumen-14B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "v000000/Qwen2.5-Lumen-14B",
    "Model sha": "fbb1d184ed01dac52d307737893ebb6b0ace444c",
    "Average ‚¨ÜÔ∏è": 32.20028820833853,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 14,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.836692543968627,
    "IFEval Raw": 0.8063604569209697,
    "IFEval": 80.63604569209697,
    "BBH Raw": 0.6390809511149668,
    "BBH": 48.50786084405761,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.32802013422818793,
    "GPQA": 10.402684563758392,
    "MUSR Raw": 0.41139583333333335,
    "MUSR": 10.291145833333337,
    "MMLU-PRO Raw": 0.49027593085106386,
    "MMLU-PRO": 43.363992316784866,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-20",
    "Submission Date": "2024-09-20",
    "Generation": 1,
    "Base Model": "v000000/Qwen2.5-Lumen-14B (Merge)"
  },
  {
    "eval_name": "vhab10_Llama-3.1-8B-Base-Instruct-SLERP_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vhab10/Llama-3.1-8B-Base-Instruct-SLERP\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vhab10/Llama-3.1-8B-Base-Instruct-SLERP</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vhab10__Llama-3.1-8B-Base-Instruct-SLERP-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vhab10/Llama-3.1-8B-Base-Instruct-SLERP",
    "Model sha": "eccb4bde0dc91f586954109ecdce7c94f47e2625",
    "Average ‚¨ÜÔ∏è": 19.24961731431654,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8067207811139262,
    "IFEval Raw": 0.290711977552893,
    "IFEval": 29.0711977552893,
    "BBH Raw": 0.5057443268070797,
    "BBH": 29.926041623092612,
    "MATH Lvl 5 Raw": 0.11858006042296071,
    "MATH Lvl 5": 11.85800604229607,
    "GPQA Raw": 0.2961409395973154,
    "GPQA": 6.152125279642054,
    "MUSR Raw": 0.40106250000000004,
    "MUSR": 9.366145833333334,
    "MMLU-PRO Raw": 0.3621176861702128,
    "MMLU-PRO": 29.12418735224587,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-29",
    "Generation": 1,
    "Base Model": "vhab10/Llama-3.1-8B-Base-Instruct-SLERP (Merge)"
  },
  {
    "eval_name": "vhab10_llama-3-8b-merged-linear_float16",
    "Precision": "float16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vhab10/llama-3-8b-merged-linear\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vhab10/llama-3-8b-merged-linear</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vhab10__llama-3-8b-merged-linear-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vhab10/llama-3-8b-merged-linear",
    "Model sha": "c37e7671b5ccfadbf3065fa5b48af05cd4f13292",
    "Average ‚¨ÜÔ∏è": 23.91136833689406,
    "Hub License": "mit",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3049433344587256,
    "IFEval Raw": 0.5916634529714491,
    "IFEval": 59.166345297144915,
    "BBH Raw": 0.49370937443498536,
    "BBH": 27.816051327740798,
    "MATH Lvl 5 Raw": 0.08157099697885198,
    "MATH Lvl 5": 8.157099697885197,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.4190520833333333,
    "MUSR": 11.681510416666667,
    "MMLU-PRO Raw": 0.37042885638297873,
    "MMLU-PRO": 30.04765070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-26",
    "Submission Date": "2024-09-26",
    "Generation": 1,
    "Base Model": "vhab10/llama-3-8b-merged-linear (Merge)"
  },
  {
    "eval_name": "vicgalle_CarbonBeagle-11B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/CarbonBeagle-11B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/CarbonBeagle-11B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__CarbonBeagle-11B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/CarbonBeagle-11B",
    "Model sha": "3fe9bf5327606d013b182fed17a472f5f043759b",
    "Average ‚¨ÜÔ∏è": 22.470185912589034,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9153787011169693,
    "IFEval Raw": 0.5415298075772285,
    "IFEval": 54.152980757722844,
    "BBH Raw": 0.5293652486530874,
    "BBH": 33.06060419684841,
    "MATH Lvl 5 Raw": 0.06193353474320242,
    "MATH Lvl 5": 6.193353474320242,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.40203125,
    "MUSR": 9.187239583333339,
    "MMLU-PRO Raw": 0.32762632978723405,
    "MMLU-PRO": 25.29181442080378,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-21",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "vicgalle/CarbonBeagle-11B (Merge)"
  },
  {
    "eval_name": "vicgalle_CarbonBeagle-11B-truthy_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/CarbonBeagle-11B-truthy\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/CarbonBeagle-11B-truthy</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__CarbonBeagle-11B-truthy-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/CarbonBeagle-11B-truthy",
    "Model sha": "476cd2a6d938bddb38dfbeb4cb21e3e34303413d",
    "Average ‚¨ÜÔ∏è": 21.3577268557097,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9072733098483965,
    "IFEval Raw": 0.5212214701436633,
    "IFEval": 52.12214701436632,
    "BBH Raw": 0.5348420085288232,
    "BBH": 33.98837559181831,
    "MATH Lvl 5 Raw": 0.0513595166163142,
    "MATH Lvl 5": 5.13595166163142,
    "GPQA Raw": 0.29949664429530204,
    "GPQA": 6.599552572706939,
    "MUSR Raw": 0.37396874999999996,
    "MUSR": 4.112760416666666,
    "MMLU-PRO Raw": 0.335688164893617,
    "MMLU-PRO": 26.18757387706856,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-10",
    "Submission Date": "2024-07-13",
    "Generation": 0,
    "Base Model": "vicgalle/CarbonBeagle-11B-truthy"
  },
  {
    "eval_name": "vicgalle_Configurable-Hermes-2-Pro-Llama-3-8B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Configurable-Hermes-2-Pro-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Configurable-Hermes-2-Pro-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Configurable-Hermes-2-Pro-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Configurable-Hermes-2-Pro-Llama-3-8B",
    "Model sha": "3cb5792509966a963645be24fdbeb2e7dc6cac15",
    "Average ‚¨ÜÔ∏è": 22.35195448537733,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7489272856866994,
    "IFEval Raw": 0.5762510139762497,
    "IFEval": 57.62510139762497,
    "BBH Raw": 0.5054841203275775,
    "BBH": 30.50962474895478,
    "MATH Lvl 5 Raw": 0.06344410876132932,
    "MATH Lvl 5": 6.344410876132931,
    "GPQA Raw": 0.29697986577181207,
    "GPQA": 6.263982102908276,
    "MUSR Raw": 0.4183645833333333,
    "MUSR": 10.062239583333334,
    "MMLU-PRO Raw": 0.3097573138297872,
    "MMLU-PRO": 23.30636820330969,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-02",
    "Submission Date": "2024-07-24",
    "Generation": 2,
    "Base Model": "NousResearch/Meta-Llama-3-8B"
  },
  {
    "eval_name": "vicgalle_Configurable-Llama-3.1-8B-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Configurable-Llama-3.1-8B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Configurable-Llama-3.1-8B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Configurable-Llama-3.1-8B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Configurable-Llama-3.1-8B-Instruct",
    "Model sha": "133b3ab1a5385ff9b3d17da2addfe3fc1fd6f733",
    "Average ‚¨ÜÔ∏è": 28.01011138792457,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 11,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7966095539710131,
    "IFEval Raw": 0.8312399987588488,
    "IFEval": 83.12399987588486,
    "BBH Raw": 0.5044756225072481,
    "BBH": 29.661397892084384,
    "MATH Lvl 5 Raw": 0.1729607250755287,
    "MATH Lvl 5": 17.29607250755287,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.3845416666666666,
    "MUSR": 5.934375,
    "MMLU-PRO Raw": 0.3592087765957447,
    "MMLU-PRO": 28.800975177304966,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-24",
    "Submission Date": "2024-08-05",
    "Generation": 0,
    "Base Model": "vicgalle/Configurable-Llama-3.1-8B-Instruct"
  },
  {
    "eval_name": "vicgalle_Configurable-Yi-1.5-9B-Chat_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Configurable-Yi-1.5-9B-Chat\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Configurable-Yi-1.5-9B-Chat</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Configurable-Yi-1.5-9B-Chat-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Configurable-Yi-1.5-9B-Chat",
    "Model sha": "992cb2232caae78eff6a836b2e0642f7cbf6018e",
    "Average ‚¨ÜÔ∏è": 23.972567172321703,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9419088118283393,
    "IFEval Raw": 0.43234506664538974,
    "IFEval": 43.234506664538976,
    "BBH Raw": 0.5452196737175008,
    "BBH": 35.33444508462291,
    "MATH Lvl 5 Raw": 0.07326283987915407,
    "MATH Lvl 5": 7.3262839879154065,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.42711458333333335,
    "MUSR": 12.022656249999999,
    "MMLU-PRO Raw": 0.4015126329787234,
    "MMLU-PRO": 33.5014036643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-12",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "vicgalle/Configurable-Yi-1.5-9B-Chat"
  },
  {
    "eval_name": "vicgalle_ConfigurableBeagle-11B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/ConfigurableBeagle-11B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/ConfigurableBeagle-11B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__ConfigurableBeagle-11B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/ConfigurableBeagle-11B",
    "Model sha": "bbc16dbf94b8e8a99bb3e2ada6755faf9c2990dd",
    "Average ‚¨ÜÔ∏è": 22.63554412021797,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8798565227060466,
    "IFEval Raw": 0.5834452585805663,
    "IFEval": 58.34452585805663,
    "BBH Raw": 0.5286592318626696,
    "BBH": 32.392022902811185,
    "MATH Lvl 5 Raw": 0.04380664652567977,
    "MATH Lvl 5": 4.380664652567977,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.39530208333333333,
    "MUSR": 7.379427083333333,
    "MMLU-PRO Raw": 0.33743351063829785,
    "MMLU-PRO": 26.381501182033094,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-17",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "vicgalle/ConfigurableBeagle-11B"
  },
  {
    "eval_name": "vicgalle_ConfigurableHermes-7B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/ConfigurableHermes-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/ConfigurableHermes-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__ConfigurableHermes-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/ConfigurableHermes-7B",
    "Model sha": "1333a88eaf6591836b2d9825d1eaec7260f336c9",
    "Average ‚¨ÜÔ∏è": 19.536295414907375,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6172818813605503,
    "IFEval Raw": 0.5410798902467675,
    "IFEval": 54.10798902467674,
    "BBH Raw": 0.4572969627830424,
    "BBH": 23.158164380406475,
    "MATH Lvl 5 Raw": 0.047583081570996985,
    "MATH Lvl 5": 4.758308157099698,
    "GPQA Raw": 0.27684563758389263,
    "GPQA": 3.5794183445190177,
    "MUSR Raw": 0.4056875,
    "MUSR": 9.110937500000004,
    "MMLU-PRO Raw": 0.3025265957446808,
    "MMLU-PRO": 22.50295508274231,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-17",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "vicgalle/ConfigurableHermes-7B"
  },
  {
    "eval_name": "vicgalle_ConfigurableSOLAR-10.7B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/ConfigurableSOLAR-10.7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/ConfigurableSOLAR-10.7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__ConfigurableSOLAR-10.7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/ConfigurableSOLAR-10.7B",
    "Model sha": "9d9baad88ea9dbaa61881f15e4f0d16e931033b4",
    "Average ‚¨ÜÔ∏è": 19.04569592182013,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6776813147755817,
    "IFEval Raw": 0.5099558061499045,
    "IFEval": 50.995580614990445,
    "BBH Raw": 0.48668100977360457,
    "BBH": 27.450950141666922,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.38047916666666665,
    "MUSR": 5.193229166666667,
    "MMLU-PRO Raw": 0.31732047872340424,
    "MMLU-PRO": 24.146719858156025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-10",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "vicgalle/ConfigurableSOLAR-10.7B"
  },
  {
    "eval_name": "vicgalle_Humanish-RP-Llama-3.1-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Humanish-RP-Llama-3.1-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Humanish-RP-Llama-3.1-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Humanish-RP-Llama-3.1-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Humanish-RP-Llama-3.1-8B",
    "Model sha": "d27aa731db1d390a8d17b0a4565c9231ee5ae8b9",
    "Average ‚¨ÜÔ∏è": 25.347670753782026,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7534505380054982,
    "IFEval Raw": 0.6669259786256023,
    "IFEval": 66.69259786256023,
    "BBH Raw": 0.5100385476143247,
    "BBH": 29.958560315236678,
    "MATH Lvl 5 Raw": 0.14728096676737162,
    "MATH Lvl 5": 14.728096676737163,
    "GPQA Raw": 0.28691275167785235,
    "GPQA": 4.921700223713646,
    "MUSR Raw": 0.39520833333333333,
    "MUSR": 8.26770833333333,
    "MMLU-PRO Raw": 0.34765625,
    "MMLU-PRO": 27.51736111111111,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-03",
    "Submission Date": "2024-08-03",
    "Generation": 0,
    "Base Model": "vicgalle/Humanish-RP-Llama-3.1-8B"
  },
  {
    "eval_name": "vicgalle_Merge-Mistral-Prometheus-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Merge-Mistral-Prometheus-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Merge-Mistral-Prometheus-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Merge-Mistral-Prometheus-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Merge-Mistral-Prometheus-7B",
    "Model sha": "a7083581b508ce83c74f9267f07024bd462e7161",
    "Average ‚¨ÜÔ∏è": 16.57405423077369,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6303555751281607,
    "IFEval Raw": 0.48480143796238423,
    "IFEval": 48.48014379623842,
    "BBH Raw": 0.420139773821292,
    "BBH": 18.41040626692948,
    "MATH Lvl 5 Raw": 0.017371601208459216,
    "MATH Lvl 5": 1.7371601208459215,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.41,
    "MUSR": 9.950000000000003,
    "MMLU-PRO Raw": 0.2716921542553192,
    "MMLU-PRO": 19.076906028368796,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "vicgalle/Merge-Mistral-Prometheus-7B (Merge)"
  },
  {
    "eval_name": "vicgalle_Merge-Mixtral-Prometheus-8x7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Merge-Mixtral-Prometheus-8x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Merge-Mixtral-Prometheus-8x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Merge-Mixtral-Prometheus-8x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Merge-Mixtral-Prometheus-8x7B",
    "Model sha": "ba53ee5b52a81e56b01e919c069a0d045cfd4e83",
    "Average ‚¨ÜÔ∏è": 24.794157759798424,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.6740091491375724,
    "IFEval Raw": 0.5744025851407598,
    "IFEval": 57.44025851407598,
    "BBH Raw": 0.5351498071096573,
    "BBH": 34.65142126614313,
    "MATH Lvl 5 Raw": 0.09441087613293052,
    "MATH Lvl 5": 9.441087613293051,
    "GPQA Raw": 0.3087248322147651,
    "GPQA": 7.829977628635347,
    "MUSR Raw": 0.40975,
    "MUSR": 9.585416666666667,
    "MMLU-PRO Raw": 0.3683510638297872,
    "MMLU-PRO": 29.81678486997636,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-04",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "vicgalle/Merge-Mixtral-Prometheus-8x7B (Merge)"
  },
  {
    "eval_name": "vicgalle_Roleplay-Llama-3-8B_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vicgalle/Roleplay-Llama-3-8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vicgalle/Roleplay-Llama-3-8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vicgalle__Roleplay-Llama-3-8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vicgalle/Roleplay-Llama-3-8B",
    "Model sha": "57297eb57dcc2c116f061d9dda341094203da01b",
    "Average ‚¨ÜÔ∏è": 24.083123520237592,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 36,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.126158513891887,
    "IFEval Raw": 0.7320221456845614,
    "IFEval": 73.20221456845613,
    "BBH Raw": 0.5012318206922323,
    "BBH": 28.554603909240623,
    "MATH Lvl 5 Raw": 0.09516616314199396,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3528854166666666,
    "MUSR": 1.6773437499999992,
    "MMLU-PRO Raw": 0.370844414893617,
    "MMLU-PRO": 30.093823877068555,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-19",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "vicgalle/Roleplay-Llama-3-8B"
  },
  {
    "eval_name": "vihangd_smart-dan-sft-v0.1_4bit",
    "Precision": "4bit",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vihangd/smart-dan-sft-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vihangd/smart-dan-sft-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vihangd__smart-dan-sft-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vihangd/smart-dan-sft-v0.1",
    "Model sha": "924b4a09153d4061fa9d58f03b10cd7cde7e3084",
    "Average ‚¨ÜÔ∏è": 3.7830957201075477,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.3610246676774613,
    "IFEval Raw": 0.15764615664215392,
    "IFEval": 15.764615664215393,
    "BBH Raw": 0.30617689187138886,
    "BBH": 3.1255992643495936,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2550335570469799,
    "GPQA": 0.6711409395973182,
    "MUSR Raw": 0.35018750000000004,
    "MUSR": 1.1067708333333328,
    "MMLU-PRO Raw": 0.11419547872340426,
    "MMLU-PRO": 1.5772754137115832,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-09",
    "Submission Date": "2024-08-20",
    "Generation": 0,
    "Base Model": "vihangd/smart-dan-sft-v0.1"
  },
  {
    "eval_name": "vonjack_MobileLLM-125M-HF_float16",
    "Precision": "float16",
    "Type": "üü¢ pretrained",
    "T": "üü¢",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/MobileLLM-125M-HF\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/MobileLLM-125M-HF</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__MobileLLM-125M-HF-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/MobileLLM-125M-HF",
    "Model sha": "7664f5e1b91faa04fac545f64db84c26316c7e63",
    "Average ‚¨ÜÔ∏è": 5.4646467904200335,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.17181135840405803,
    "IFEval Raw": 0.21072753627042912,
    "IFEval": 21.072753627042914,
    "BBH Raw": 0.30272988561565645,
    "BBH": 3.146583712799116,
    "MATH Lvl 5 Raw": 0.003021148036253776,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.37818749999999995,
    "MUSR": 5.106770833333335,
    "MMLU-PRO Raw": 0.1163563829787234,
    "MMLU-PRO": 1.8173758865248217,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 0,
    "Base Model": "vonjack/MobileLLM-125M-HF"
  },
  {
    "eval_name": "vonjack_Phi-3.5-mini-instruct-hermes-fc-json_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Adapter",
    "Architecture": "?",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/Phi-3.5-mini-instruct-hermes-fc-json\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/Phi-3.5-mini-instruct-hermes-fc-json</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__Phi-3.5-mini-instruct-hermes-fc-json-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/Phi-3.5-mini-instruct-hermes-fc-json",
    "Model sha": "4cacfb35723647d408f0414886d0dfe67404a14f",
    "Average ‚¨ÜÔ∏è": 4.516525076259892,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 4,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.2851885770493952,
    "IFEval Raw": 0.14158432957885078,
    "IFEval": 14.15843295788508,
    "BBH Raw": 0.29747555432824196,
    "BBH": 2.390836087243887,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.25419463087248323,
    "GPQA": 0.5592841163310973,
    "MUSR Raw": 0.40413541666666664,
    "MUSR": 8.450260416666667,
    "MMLU-PRO Raw": 0.11386303191489362,
    "MMLU-PRO": 1.540336879432624,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-05",
    "Submission Date": "2024-11-05",
    "Generation": 1,
    "Base Model": "vonjack/Phi-3.5-mini-instruct-hermes-fc-json (Merge)"
  },
  {
    "eval_name": "vonjack_Qwen2.5-Coder-0.5B-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/Qwen2.5-Coder-0.5B-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/Qwen2.5-Coder-0.5B-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__Qwen2.5-Coder-0.5B-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/Qwen2.5-Coder-0.5B-Merged",
    "Model sha": "38e4789c0fc5fad359de2f7bafdb65c3ae26b95b",
    "Average ‚¨ÜÔ∏è": 6.350287428834207,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.49677872395470457,
    "IFEval Raw": 0.30997087727230416,
    "IFEval": 30.997087727230415,
    "BBH Raw": 0.3076017752057237,
    "BBH": 3.5887384386437557,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.33034375,
    "MUSR": 0.8263020833333333,
    "MMLU-PRO Raw": 0.12017952127659574,
    "MMLU-PRO": 2.24216903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-19",
    "Submission Date": "2024-11-19",
    "Generation": 1,
    "Base Model": "vonjack/Qwen2.5-Coder-0.5B-Merged (Merge)"
  },
  {
    "eval_name": "vonjack_SmolLM2-1.7B-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/SmolLM2-1.7B-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/SmolLM2-1.7B-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__SmolLM2-1.7B-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/SmolLM2-1.7B-Merged",
    "Model sha": "232d54a335220b0d83d6036f6d8df3971d3e79bb",
    "Average ‚¨ÜÔ∏è": 11.944702869086635,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.31132726743045247,
    "IFEval Raw": 0.36979658417443495,
    "IFEval": 36.979658417443495,
    "BBH Raw": 0.3586553457965105,
    "BBH": 10.766530256983183,
    "MATH Lvl 5 Raw": 0.045317220543806644,
    "MATH Lvl 5": 4.531722054380665,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.34079166666666666,
    "MUSR": 3.832291666666668,
    "MMLU-PRO Raw": 0.2047872340425532,
    "MMLU-PRO": 11.643026004728133,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-18",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "vonjack/SmolLM2-1.7B-Merged (Merge)"
  },
  {
    "eval_name": "vonjack_SmolLM2-135M-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/SmolLM2-135M-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/SmolLM2-135M-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__SmolLM2-135M-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/SmolLM2-135M-Merged",
    "Model sha": "a1700ca913a87ad713edfe57a2030a9d7c088970",
    "Average ‚¨ÜÔ∏è": 5.733960494198002,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.3455102656079413,
    "IFEval Raw": 0.24829674153468353,
    "IFEval": 24.829674153468353,
    "BBH Raw": 0.3099931265410582,
    "BBH": 4.587041236226676,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.23825503355704697,
    "GPQA": 0.0,
    "MUSR Raw": 0.36618749999999994,
    "MUSR": 3.4401041666666674,
    "MMLU-PRO Raw": 0.11120345744680851,
    "MMLU-PRO": 1.2448286052009452,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "vonjack/SmolLM2-135M-Merged (Merge)"
  },
  {
    "eval_name": "vonjack_SmolLM2-360M-Merged_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/vonjack/SmolLM2-360M-Merged\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">vonjack/SmolLM2-360M-Merged</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/vonjack__SmolLM2-360M-Merged-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "vonjack/SmolLM2-360M-Merged",
    "Model sha": "32bceedf56b29a4a9fdd459a36fbc7fae5e274c8",
    "Average ‚¨ÜÔ∏è": 7.130731072162045,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 0,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.38574186108316194,
    "IFEval Raw": 0.32058715319795916,
    "IFEval": 32.05871531979592,
    "BBH Raw": 0.31548533684955926,
    "BBH": 4.741734006734007,
    "MATH Lvl 5 Raw": 0.0075528700906344415,
    "MATH Lvl 5": 0.7552870090634441,
    "GPQA Raw": 0.2558724832214765,
    "GPQA": 0.7829977628635317,
    "MUSR Raw": 0.3527291666666667,
    "MUSR": 3.357812500000001,
    "MMLU-PRO Raw": 0.10979055851063829,
    "MMLU-PRO": 1.087839834515365,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-15",
    "Submission Date": "2024-11-15",
    "Generation": 1,
    "Base Model": "vonjack/SmolLM2-360M-Merged (Merge)"
  },
  {
    "eval_name": "w4r10ck_SOLAR-10.7B-Instruct-v1.0-uncensored_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/w4r10ck__SOLAR-10.7B-Instruct-v1.0-uncensored-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored",
    "Model sha": "baa7b3899e85af4b2f02b01fd93f203872140d27",
    "Average ‚¨ÜÔ∏è": 20.577180762918037,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 30,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8019711090078706,
    "IFEval Raw": 0.38840609582574237,
    "IFEval": 38.84060958257423,
    "BBH Raw": 0.5301525050503222,
    "BBH": 33.858639234912964,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.29446308724832215,
    "GPQA": 5.92841163310962,
    "MUSR Raw": 0.4639479166666667,
    "MUSR": 18.493489583333332,
    "MMLU-PRO Raw": 0.3343583776595745,
    "MMLU-PRO": 26.03981973995272,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2023-12-14",
    "Submission Date": "2024-10-11",
    "Generation": 0,
    "Base Model": "w4r10ck/SOLAR-10.7B-Instruct-v1.0-uncensored"
  },
  {
    "eval_name": "wannaphong_KhanomTanLLM-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/wannaphong/KhanomTanLLM-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">wannaphong/KhanomTanLLM-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/wannaphong__KhanomTanLLM-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "wannaphong/KhanomTanLLM-Instruct",
    "Model sha": "351239c92c0ff3304d1dd98fdf4ac054a8c1acc3",
    "Average ‚¨ÜÔ∏è": 4.61787445145566,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.4017306328341432,
    "IFEval Raw": 0.16211762567764643,
    "IFEval": 16.211762567764644,
    "BBH Raw": 0.30931233392513263,
    "BBH": 3.9448660598049243,
    "MATH Lvl 5 Raw": 0.0015105740181268884,
    "MATH Lvl 5": 0.15105740181268884,
    "GPQA Raw": 0.2634228187919463,
    "GPQA": 1.7897091722595053,
    "MUSR Raw": 0.37006249999999996,
    "MUSR": 4.291145833333334,
    "MMLU-PRO Raw": 0.1118683510638298,
    "MMLU-PRO": 1.3187056737588652,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-24",
    "Submission Date": "2024-08-29",
    "Generation": 0,
    "Base Model": "wannaphong/KhanomTanLLM-Instruct"
  },
  {
    "eval_name": "waqasali1707_Beast-Soul-new_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/waqasali1707/Beast-Soul-new\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">waqasali1707/Beast-Soul-new</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/waqasali1707__Beast-Soul-new-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "waqasali1707/Beast-Soul-new",
    "Model sha": "a23d68c4556d91a129de3f8fd8b9e0ff0890f4cc",
    "Average ‚¨ÜÔ∏è": 22.10838822353402,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.6368880654542547,
    "IFEval Raw": 0.5029865202108184,
    "IFEval": 50.298652021081836,
    "BBH Raw": 0.522494907014536,
    "BBH": 33.044262388969805,
    "MATH Lvl 5 Raw": 0.0702416918429003,
    "MATH Lvl 5": 7.02416918429003,
    "GPQA Raw": 0.2827181208053691,
    "GPQA": 4.36241610738255,
    "MUSR Raw": 0.4485625,
    "MUSR": 14.503645833333335,
    "MMLU-PRO Raw": 0.3107546542553192,
    "MMLU-PRO": 23.417183806146575,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-07",
    "Submission Date": "2024-08-07",
    "Generation": 1,
    "Base Model": "waqasali1707/Beast-Soul-new (Merge)"
  },
  {
    "eval_name": "wave-on-discord_qwent-7b_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/wave-on-discord/qwent-7b\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">wave-on-discord/qwent-7b</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/wave-on-discord__qwent-7b-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "wave-on-discord/qwent-7b",
    "Model sha": "40000e76d2a4d0ad054aff9fe873c5beb0e4925e",
    "Average ‚¨ÜÔ∏è": 8.734092515058288,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3234959385291836,
    "IFEval Raw": 0.20148539209297997,
    "IFEval": 20.148539209297997,
    "BBH Raw": 0.4228103286118343,
    "BBH": 18.066398100675865,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.2651006711409396,
    "GPQA": 2.0134228187919474,
    "MUSR Raw": 0.38165625000000003,
    "MUSR": 5.473697916666668,
    "MMLU-PRO Raw": 0.16032247340425532,
    "MMLU-PRO": 6.702497044917257,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 1,
    "Base Model": "wave-on-discord/qwent-7b (Merge)"
  },
  {
    "eval_name": "win10_ArliAI-RPMax-v1.3-merge-13.3B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/ArliAI-RPMax-v1.3-merge-13.3B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/ArliAI-RPMax-v1.3-merge-13.3B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__ArliAI-RPMax-v1.3-merge-13.3B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/ArliAI-RPMax-v1.3-merge-13.3B",
    "Model sha": "4d3ed351827f1afc1652e13aafeb1eae79b8f562",
    "Average ‚¨ÜÔ∏è": 16.456100825403436,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 13,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.4513051890140358,
    "IFEval Raw": 0.3038260703821416,
    "IFEval": 30.38260703821416,
    "BBH Raw": 0.4581388671914119,
    "BBH": 23.029799582073213,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.4325104166666667,
    "MUSR": 14.16380208333333,
    "MMLU-PRO Raw": 0.31998005319148937,
    "MMLU-PRO": 24.442228132387704,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-16",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "win10/ArliAI-RPMax-v1.3-merge-13.3B (Merge)"
  },
  {
    "eval_name": "win10_Breeze-13B-32k-Instruct-v1_0_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/Breeze-13B-32k-Instruct-v1_0\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/Breeze-13B-32k-Instruct-v1_0</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__Breeze-13B-32k-Instruct-v1_0-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/Breeze-13B-32k-Instruct-v1_0",
    "Model sha": "220c957cf5d9c534a4ef75c11a18221c461de40a",
    "Average ‚¨ÜÔ∏è": 15.411205960587205,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.448811021429436,
    "IFEval Raw": 0.35843118481185476,
    "IFEval": 35.84311848118548,
    "BBH Raw": 0.46112304746712934,
    "BBH": 25.258698638977545,
    "MATH Lvl 5 Raw": 0.009818731117824773,
    "MATH Lvl 5": 0.9818731117824773,
    "GPQA Raw": 0.26426174496644295,
    "GPQA": 1.9015659955257262,
    "MUSR Raw": 0.42019791666666667,
    "MUSR": 11.058072916666667,
    "MMLU-PRO Raw": 0.2568151595744681,
    "MMLU-PRO": 17.423906619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "win10/Breeze-13B-32k-Instruct-v1_0"
  },
  {
    "eval_name": "win10_EVA-Norns-Qwen2.5-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/EVA-Norns-Qwen2.5-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/EVA-Norns-Qwen2.5-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__EVA-Norns-Qwen2.5-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/EVA-Norns-Qwen2.5-v0.1",
    "Model sha": "90c3ca66e700b4a7d2c509634f9b9748a2e4c3ab",
    "Average ‚¨ÜÔ∏è": 24.657872246584844,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6566609072578895,
    "IFEval Raw": 0.6219630580193884,
    "IFEval": 62.19630580193885,
    "BBH Raw": 0.507240838017382,
    "BBH": 30.060941501467855,
    "MATH Lvl 5 Raw": 0.15483383685800603,
    "MATH Lvl 5": 15.483383685800604,
    "GPQA Raw": 0.28523489932885904,
    "GPQA": 4.697986577181204,
    "MUSR Raw": 0.40451041666666665,
    "MUSR": 8.563802083333334,
    "MMLU-PRO Raw": 0.3425033244680851,
    "MMLU-PRO": 26.944813829787233,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "win10/EVA-Norns-Qwen2.5-v0.1 (Merge)"
  },
  {
    "eval_name": "win10_Llama-3.2-3B-Instruct-24-9-29_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/Llama-3.2-3B-Instruct-24-9-29\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/Llama-3.2-3B-Instruct-24-9-29</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__Llama-3.2-3B-Instruct-24-9-29-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/Llama-3.2-3B-Instruct-24-9-29",
    "Model sha": "4defb10e2415111abb873d695dd40c387c1d6d57",
    "Average ‚¨ÜÔ∏è": 23.929169389157675,
    "Hub License": "llama3.2",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7136056242441963,
    "IFEval Raw": 0.7332211864519476,
    "IFEval": 73.32211864519475,
    "BBH Raw": 0.4614234982167829,
    "BBH": 24.196425775209622,
    "MATH Lvl 5 Raw": 0.1661631419939577,
    "MATH Lvl 5": 16.61631419939577,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.35552083333333334,
    "MUSR": 1.4401041666666685,
    "MMLU-PRO Raw": 0.3228058510638298,
    "MMLU-PRO": 24.756205673758867,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-11",
    "Generation": 2,
    "Base Model": "meta-llama/Llama-3.2-3B-Instruct"
  },
  {
    "eval_name": "win10_Norns-Qwen2.5-12B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/Norns-Qwen2.5-12B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/Norns-Qwen2.5-12B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__Norns-Qwen2.5-12B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/Norns-Qwen2.5-12B",
    "Model sha": "464793295c8633a95e6faedad24dfa8f0fd35663",
    "Average ‚¨ÜÔ∏è": 16.386375297002896,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 12,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6229719714068847,
    "IFEval Raw": 0.48969733640074997,
    "IFEval": 48.969733640075,
    "BBH Raw": 0.46189201103923744,
    "BBH": 23.769257476895735,
    "MATH Lvl 5 Raw": 0.004531722054380665,
    "MATH Lvl 5": 0.4531722054380665,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.3554895833333333,
    "MUSR": 2.2028645833333327,
    "MMLU-PRO Raw": 0.2660405585106383,
    "MMLU-PRO": 18.44895094562648,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-17",
    "Generation": 1,
    "Base Model": "win10/Norns-Qwen2.5-12B (Merge)"
  },
  {
    "eval_name": "win10_Norns-Qwen2.5-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/Norns-Qwen2.5-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/Norns-Qwen2.5-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__Norns-Qwen2.5-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/Norns-Qwen2.5-7B",
    "Model sha": "148d9156f734a8050812892879cf13d1ca01f137",
    "Average ‚¨ÜÔ∏è": 24.59327692124154,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6499135921420335,
    "IFEval Raw": 0.6122211288270678,
    "IFEval": 61.22211288270678,
    "BBH Raw": 0.5072887832228614,
    "BBH": 30.250415044309648,
    "MATH Lvl 5 Raw": 0.1555891238670695,
    "MATH Lvl 5": 15.55891238670695,
    "GPQA Raw": 0.28439597315436244,
    "GPQA": 4.5861297539149914,
    "MUSR Raw": 0.40847916666666667,
    "MUSR": 9.1265625,
    "MMLU-PRO Raw": 0.34133976063829785,
    "MMLU-PRO": 26.81552895981087,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-17",
    "Submission Date": "2024-11-18",
    "Generation": 1,
    "Base Model": "win10/Norns-Qwen2.5-7B (Merge)"
  },
  {
    "eval_name": "win10_llama3-13.45b-Instruct_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/win10/llama3-13.45b-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">win10/llama3-13.45b-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/win10__llama3-13.45b-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "win10/llama3-13.45b-Instruct",
    "Model sha": "94cc0f415e355c6d3d47168a6ff5239ca586904a",
    "Average ‚¨ÜÔ∏è": 17.277281515838737,
    "Hub License": "llama3",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 13,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.136534644599343,
    "IFEval Raw": 0.4144348107465968,
    "IFEval": 41.44348107465968,
    "BBH Raw": 0.486541523346714,
    "BBH": 26.67569043948038,
    "MATH Lvl 5 Raw": 0.020392749244712995,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.38476041666666666,
    "MUSR": 6.3283854166666655,
    "MMLU-PRO Raw": 0.3345246010638298,
    "MMLU-PRO": 26.0582890070922,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-09",
    "Submission Date": "2024-06-26",
    "Generation": 1,
    "Base Model": "win10/llama3-13.45b-Instruct (Merge)"
  },
  {
    "eval_name": "winglian_Llama-3-8b-64k-PoSE_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/winglian/Llama-3-8b-64k-PoSE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/Llama-3-8b-64k-PoSE</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/winglian__Llama-3-8b-64k-PoSE-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "winglian/Llama-3-8b-64k-PoSE",
    "Model sha": "5481d9b74a3ec5a95789673e194c8ff86e2bc2bc",
    "Average ‚¨ÜÔ∏è": 11.00473816557232,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 74,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9110212160163924,
    "IFEval Raw": 0.28569085581811815,
    "IFEval": 28.569085581811816,
    "BBH Raw": 0.37021796005121793,
    "BBH": 13.30731679540503,
    "MATH Lvl 5 Raw": 0.03323262839879154,
    "MATH Lvl 5": 3.3232628398791544,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.33955208333333337,
    "MUSR": 3.077343750000001,
    "MMLU-PRO Raw": 0.2466755319148936,
    "MMLU-PRO": 16.297281323877066,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-24",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "winglian/Llama-3-8b-64k-PoSE"
  },
  {
    "eval_name": "winglian_llama-3-8b-256k-PoSE_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/winglian/llama-3-8b-256k-PoSE\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">winglian/llama-3-8b-256k-PoSE</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/winglian__llama-3-8b-256k-PoSE-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "winglian/llama-3-8b-256k-PoSE",
    "Model sha": "93e7b0b6433c96583ffcef3bc47203e6fdcbbe8b",
    "Average ‚¨ÜÔ∏è": 6.557715228218817,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 42,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.050722810825721,
    "IFEval Raw": 0.2909114482905358,
    "IFEval": 29.091144829053576,
    "BBH Raw": 0.3156583397739859,
    "BBH": 5.502848923020156,
    "MATH Lvl 5 Raw": 0.015105740181268885,
    "MATH Lvl 5": 1.5105740181268885,
    "GPQA Raw": 0.2575503355704698,
    "GPQA": 1.0067114093959737,
    "MUSR Raw": 0.33155208333333336,
    "MUSR": 0.9440104166666662,
    "MMLU-PRO Raw": 0.1116190159574468,
    "MMLU-PRO": 1.2910017730496441,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-06-26",
    "Generation": 0,
    "Base Model": "winglian/llama-3-8b-256k-PoSE"
  },
  {
    "eval_name": "xMaulana_FinMatcha-3B-Instruct_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xMaulana/FinMatcha-3B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xMaulana/FinMatcha-3B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xMaulana__FinMatcha-3B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xMaulana/FinMatcha-3B-Instruct",
    "Model sha": "be2c0c04fc4dc3fb93631e3c663721da92fea8fc",
    "Average ‚¨ÜÔ∏è": 24.01624315506544,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 3,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.577035290617962,
    "IFEval Raw": 0.7548283000217202,
    "IFEval": 75.48283000217202,
    "BBH Raw": 0.453555265188897,
    "BBH": 23.191022969435476,
    "MATH Lvl 5 Raw": 0.13595166163141997,
    "MATH Lvl 5": 13.595166163141997,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.36333333333333334,
    "MUSR": 5.016666666666668,
    "MMLU-PRO Raw": 0.3181515957446808,
    "MMLU-PRO": 24.23906619385342,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "xMaulana/FinMatcha-3B-Instruct (Merge)"
  },
  {
    "eval_name": "xinchen9_Llama3.1_8B_Instruct_CoT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xinchen9/Llama3.1_8B_Instruct_CoT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xinchen9/Llama3.1_8B_Instruct_CoT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xinchen9__Llama3.1_8B_Instruct_CoT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xinchen9/Llama3.1_8B_Instruct_CoT",
    "Model sha": "cab1b33ddff08de11c5daea8ae079d126d503d8b",
    "Average ‚¨ÜÔ∏è": 16.190743429600833,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8565516381748084,
    "IFEval Raw": 0.2973565694579272,
    "IFEval": 29.73565694579272,
    "BBH Raw": 0.4398206147249642,
    "BBH": 21.14286611806116,
    "MATH Lvl 5 Raw": 0.05287009063444109,
    "MATH Lvl 5": 5.287009063444109,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.43706249999999996,
    "MUSR": 13.166145833333337,
    "MMLU-PRO Raw": 0.2878989361702128,
    "MMLU-PRO": 20.877659574468087,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-16",
    "Submission Date": "2024-09-19",
    "Generation": 0,
    "Base Model": "xinchen9/Llama3.1_8B_Instruct_CoT"
  },
  {
    "eval_name": "xinchen9_Llama3.1_CoT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xinchen9/Llama3.1_CoT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xinchen9/Llama3.1_CoT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xinchen9__Llama3.1_CoT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xinchen9/Llama3.1_CoT",
    "Model sha": "3cb467f51a59ff163bb942fcde3ef60573c12b79",
    "Average ‚¨ÜÔ∏è": 13.351283385198409,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9500988441643737,
    "IFEval Raw": 0.22461624046419057,
    "IFEval": 22.461624046419058,
    "BBH Raw": 0.43410143664277245,
    "BBH": 19.899123541883053,
    "MATH Lvl 5 Raw": 0.015105740181268883,
    "MATH Lvl 5": 1.5105740181268883,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.43045833333333333,
    "MUSR": 11.773958333333333,
    "MMLU-PRO Raw": 0.2738530585106383,
    "MMLU-PRO": 19.317006501182032,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-04",
    "Submission Date": "2024-09-06",
    "Generation": 0,
    "Base Model": "xinchen9/Llama3.1_CoT"
  },
  {
    "eval_name": "xinchen9_Llama3.1_CoT_V1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xinchen9/Llama3.1_CoT_V1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xinchen9/Llama3.1_CoT_V1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xinchen9__Llama3.1_CoT_V1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xinchen9/Llama3.1_CoT_V1",
    "Model sha": "c5ed4b8bfc364ebae1843af14799818551f5251f",
    "Average ‚¨ÜÔ∏è": 14.39494693888178,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8734619209061871,
    "IFEval Raw": 0.2452991396162183,
    "IFEval": 24.52991396162183,
    "BBH Raw": 0.4376001847280673,
    "BBH": 20.166003338515427,
    "MATH Lvl 5 Raw": 0.01283987915407855,
    "MATH Lvl 5": 1.283987915407855,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.45721875,
    "MUSR": 16.419010416666666,
    "MMLU-PRO Raw": 0.2805019946808511,
    "MMLU-PRO": 20.05577718676123,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-06",
    "Submission Date": "2024-09-07",
    "Generation": 0,
    "Base Model": "xinchen9/Llama3.1_CoT_V1"
  },
  {
    "eval_name": "xinchen9_Mistral-7B-CoT_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xinchen9/Mistral-7B-CoT\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xinchen9/Mistral-7B-CoT</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xinchen9__Mistral-7B-CoT-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xinchen9/Mistral-7B-CoT",
    "Model sha": "9a3c8103dac20d5497d1b8fc041bb5125ff4dc00",
    "Average ‚¨ÜÔ∏è": 11.202954659357488,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.8886892507260218,
    "IFEval Raw": 0.2798707429620075,
    "IFEval": 27.987074296200745,
    "BBH Raw": 0.38726762098069667,
    "BBH": 14.80619341451162,
    "MATH Lvl 5 Raw": 0.01963746223564955,
    "MATH Lvl 5": 1.963746223564955,
    "GPQA Raw": 0.24916107382550334,
    "GPQA": 0.0,
    "MUSR Raw": 0.3994270833333333,
    "MUSR": 8.195052083333335,
    "MMLU-PRO Raw": 0.2283909574468085,
    "MMLU-PRO": 14.265661938534278,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-09",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "xinchen9/Mistral-7B-CoT"
  },
  {
    "eval_name": "xinchen9_llama3-b8-ft-dis_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xinchen9/llama3-b8-ft-dis\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xinchen9/llama3-b8-ft-dis</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xinchen9__llama3-b8-ft-dis-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xinchen9/llama3-b8-ft-dis",
    "Model sha": "e4da730f28f79543262de37908943c35f8df81fe",
    "Average ‚¨ÜÔ∏è": 13.897963286655186,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.0623267804278298,
    "IFEval Raw": 0.154598687039278,
    "IFEval": 15.459868703927802,
    "BBH Raw": 0.4625789691224553,
    "BBH": 24.72745698442778,
    "MATH Lvl 5 Raw": 0.03474320241691843,
    "MATH Lvl 5": 3.474320241691843,
    "GPQA Raw": 0.31291946308724833,
    "GPQA": 8.389261744966444,
    "MUSR Raw": 0.365375,
    "MUSR": 6.405208333333333,
    "MMLU-PRO Raw": 0.3243849734042553,
    "MMLU-PRO": 24.931663711583923,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-06-28",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "xinchen9/llama3-b8-ft-dis"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table",
    "Model sha": "c083d6796f54f66b4cec2261657a02801c761093",
    "Average ‚¨ÜÔ∏è": 22.421029113566533,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6242307315240399,
    "IFEval Raw": 0.6374752323834094,
    "IFEval": 63.74752323834093,
    "BBH Raw": 0.4912273915261041,
    "BBH": 27.42282120153998,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.38199999999999995,
    "MUSR": 5.483333333333334,
    "MMLU-PRO Raw": 0.3686003989361702,
    "MMLU-PRO": 29.844488770685572,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_2b-table"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table",
    "Model sha": "5416d34b5243559914a377ee9d95ce4830bf8dba",
    "Average ‚¨ÜÔ∏è": 24.502405108853072,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7502635961344971,
    "IFEval Raw": 0.7274509412802475,
    "IFEval": 72.74509412802476,
    "BBH Raw": 0.5056858683165713,
    "BBH": 29.398353220629613,
    "MATH Lvl 5 Raw": 0.08459214501510574,
    "MATH Lvl 5": 8.459214501510575,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.38190624999999995,
    "MUSR": 5.104947916666667,
    "MMLU-PRO Raw": 0.3696808510638298,
    "MMLU-PRO": 29.964539007092196,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_bt_8b-table"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table",
    "Model sha": "235204157d7fac0d64fa609d5aee3cebb49ccd11",
    "Average ‚¨ÜÔ∏è": 22.236353504365166,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6717413529123942,
    "IFEval Raw": 0.6568593553992297,
    "IFEval": 65.68593553992297,
    "BBH Raw": 0.49518319163897667,
    "BBH": 27.695199510550037,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.35939583333333336,
    "MUSR": 2.291145833333333,
    "MMLU-PRO Raw": 0.37017952127659576,
    "MMLU-PRO": 30.019946808510632,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_2b-table"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table",
    "Model sha": "9db00cbbba84453b18956fcc76f264f94a205955",
    "Average ‚¨ÜÔ∏è": 22.935265315443363,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.719228295791037,
    "IFEval Raw": 0.6620799478716473,
    "IFEval": 66.20799478716472,
    "BBH Raw": 0.500449109241973,
    "BBH": 28.508587310114308,
    "MATH Lvl 5 Raw": 0.07779456193353473,
    "MATH Lvl 5": 7.779456193353473,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3805416666666666,
    "MUSR": 5.001041666666667,
    "MMLU-PRO Raw": 0.3599567819148936,
    "MMLU-PRO": 28.88408687943262,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-Iter2_gp_8b-table"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001",
    "Model sha": "1062757826de031a4ae82277e6e737e19e82e514",
    "Average ‚¨ÜÔ∏è": 21.84548120927315,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6150025014431778,
    "IFEval Raw": 0.6042278931014153,
    "IFEval": 60.42278931014154,
    "BBH Raw": 0.4936062924421171,
    "BBH": 27.61371406788812,
    "MATH Lvl 5 Raw": 0.0649546827794562,
    "MATH Lvl 5": 6.495468277945619,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3793333333333333,
    "MUSR": 5.216666666666668,
    "MMLU-PRO Raw": 0.370844414893617,
    "MMLU-PRO": 30.093823877068555,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_2b-table-0.001"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002",
    "Model sha": "e5d2f179b4a7bd851dcf2b7db6358b13001bf1af",
    "Average ‚¨ÜÔ∏è": 23.938825237280103,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8414684783175977,
    "IFEval Raw": 0.7131876753680235,
    "IFEval": 71.31876753680235,
    "BBH Raw": 0.4996376240562969,
    "BBH": 28.574878539626724,
    "MATH Lvl 5 Raw": 0.06948640483383686,
    "MATH Lvl 5": 6.948640483383686,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.3872083333333333,
    "MUSR": 6.067708333333335,
    "MMLU-PRO Raw": 0.3664394946808511,
    "MMLU-PRO": 29.604388297872337,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_bt_8b-table-0.002"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001",
    "Model sha": "0e319ad47ed2b2636b72d07ee9b32657e1e50412",
    "Average ‚¨ÜÔ∏è": 21.224623586067224,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.679841397373017,
    "IFEval Raw": 0.594710922574325,
    "IFEval": 59.4710922574325,
    "BBH Raw": 0.48992211803775065,
    "BBH": 26.943904089240508,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.35809374999999993,
    "MUSR": 2.328385416666666,
    "MMLU-PRO Raw": 0.37042885638297873,
    "MMLU-PRO": 30.04765070921986,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_2b-table-0.001"
  },
  {
    "eval_name": "xkp24_Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xkp24__Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002",
    "Model sha": "0877f2458ea667edcf9213383df41294c788190f",
    "Average ‚¨ÜÔ∏è": 22.693580348529768,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7691192613552272,
    "IFEval Raw": 0.6453188650558297,
    "IFEval": 64.53188650558296,
    "BBH Raw": 0.4951075713814987,
    "BBH": 28.046977965255564,
    "MATH Lvl 5 Raw": 0.06797583081570997,
    "MATH Lvl 5": 6.797583081570997,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.393875,
    "MUSR": 7.334375000000001,
    "MMLU-PRO Raw": 0.3529753989361702,
    "MMLU-PRO": 28.10837765957446,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-30",
    "Submission Date": "2024-10-01",
    "Generation": 0,
    "Base Model": "xkp24/Llama-3-8B-Instruct-SPPO-score-Iter2_gp_8b-table-0.002"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table",
    "Model sha": "d2b87100e5ba3215fddbd308bb17b7bf12fe6c9e",
    "Average ‚¨ÜÔ∏è": 21.01777971289453,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9864303649997264,
    "IFEval Raw": 0.575601625908146,
    "IFEval": 57.5601625908146,
    "BBH Raw": 0.4901206199104098,
    "BBH": 26.866404089240515,
    "MATH Lvl 5 Raw": 0.07930513595166164,
    "MATH Lvl 5": 7.930513595166164,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.36596874999999995,
    "MUSR": 2.9794270833333325,
    "MMLU-PRO Raw": 0.36585771276595747,
    "MMLU-PRO": 29.539745862884164,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_2b-table"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table",
    "Model sha": "19a48ccf5ea463afbbbc61d650b8fb63ff2d94c7",
    "Average ‚¨ÜÔ∏è": 23.969225572574327,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5901530881421527,
    "IFEval Raw": 0.7034457461757027,
    "IFEval": 70.34457461757027,
    "BBH Raw": 0.5091868512191421,
    "BBH": 29.73123940180749,
    "MATH Lvl 5 Raw": 0.08685800604229607,
    "MATH Lvl 5": 8.685800604229607,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37390624999999994,
    "MUSR": 3.904947916666666,
    "MMLU-PRO Raw": 0.3692652925531915,
    "MMLU-PRO": 29.918365839243506,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_bt_8b-table"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table",
    "Model sha": "0fe230b3432fb2b0f89942d7926291a4dbeb2820",
    "Average ‚¨ÜÔ∏è": 21.781466243597034,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6655214350619397,
    "IFEval Raw": 0.6023794642659255,
    "IFEval": 60.23794642659256,
    "BBH Raw": 0.49695315361511977,
    "BBH": 27.892403263090213,
    "MATH Lvl 5 Raw": 0.08610271903323263,
    "MATH Lvl 5": 8.610271903323262,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.36736458333333327,
    "MUSR": 3.187239583333333,
    "MMLU-PRO Raw": 0.3657746010638298,
    "MMLU-PRO": 29.53051122931442,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_2b-table"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table",
    "Model sha": "d1e19da1029f2d4d45de015754bc52dcb1ea5570",
    "Average ‚¨ÜÔ∏è": 23.059714466802834,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5884191255459703,
    "IFEval Raw": 0.6620300801872365,
    "IFEval": 66.20300801872366,
    "BBH Raw": 0.49999369392208165,
    "BBH": 28.43982384277912,
    "MATH Lvl 5 Raw": 0.08308157099697887,
    "MATH Lvl 5": 8.308157099697887,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.38181249999999994,
    "MUSR": 5.1265624999999995,
    "MMLU-PRO Raw": 0.3614527925531915,
    "MMLU-PRO": 29.050310283687946,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-Iter3_gp_8b-table"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001",
    "Model sha": "a478aa202c59773eba615ae37feb4cc750757695",
    "Average ‚¨ÜÔ∏è": 20.36405214940248,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5864428984648447,
    "IFEval Raw": 0.5336363072203975,
    "IFEval": 53.363630722039744,
    "BBH Raw": 0.49148727192613517,
    "BBH": 27.145373836403248,
    "MATH Lvl 5 Raw": 0.06570996978851963,
    "MATH Lvl 5": 6.570996978851963,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37796874999999996,
    "MUSR": 4.712760416666668,
    "MMLU-PRO Raw": 0.3624501329787234,
    "MMLU-PRO": 29.161125886524825,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_2b-table-0.001"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002",
    "Model sha": "8ef9ef7e2bf522e707a7b090af55f2ec1eafd4b9",
    "Average ‚¨ÜÔ∏è": 23.261322326940746,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8694737921494468,
    "IFEval Raw": 0.6851609285584471,
    "IFEval": 68.5160928558447,
    "BBH Raw": 0.507516320435292,
    "BBH": 29.740550305634912,
    "MATH Lvl 5 Raw": 0.05438066465256798,
    "MATH Lvl 5": 5.4380664652567985,
    "GPQA Raw": 0.25838926174496646,
    "GPQA": 1.1185682326621946,
    "MUSR Raw": 0.3831770833333333,
    "MUSR": 5.630468750000001,
    "MMLU-PRO Raw": 0.3621176861702128,
    "MMLU-PRO": 29.12418735224587,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_bt_8b-table-0.002"
  },
  {
    "eval_name": "xukp20_Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001",
    "Model sha": "86673872245ad902f8d466bdc20edae9c115b965",
    "Average ‚¨ÜÔ∏è": 20.03216916745218,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6750938218418726,
    "IFEval Raw": 0.5482242671666733,
    "IFEval": 54.82242671666733,
    "BBH Raw": 0.48871746894288526,
    "BBH": 26.839803365680336,
    "MATH Lvl 5 Raw": 0.044561933534743206,
    "MATH Lvl 5": 4.456193353474321,
    "GPQA Raw": 0.2609060402684564,
    "GPQA": 1.4541387024608499,
    "MUSR Raw": 0.3632708333333334,
    "MUSR": 2.9421874999999997,
    "MMLU-PRO Raw": 0.36710438829787234,
    "MMLU-PRO": 29.678265366430256,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-28",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "xukp20/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_2b-table-0.001"
  },
  {
    "eval_name": "xukp20_llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xukp20/llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xukp20/llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xukp20__llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xukp20/llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table",
    "Model sha": "abb3afe2b0398b24ed823b0124c8a72d354487bd",
    "Average ‚¨ÜÔ∏è": 23.498954773173068,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.3793418013730123,
    "IFEval Raw": 0.6909311737301471,
    "IFEval": 69.0931173730147,
    "BBH Raw": 0.4978456981516493,
    "BBH": 28.11988708608538,
    "MATH Lvl 5 Raw": 0.09290030211480362,
    "MATH Lvl 5": 9.290030211480362,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3673333333333333,
    "MUSR": 3.0833333333333326,
    "MMLU-PRO Raw": 0.37159242021276595,
    "MMLU-PRO": 30.17693557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 0,
    "Base Model": "xukp20/llama-3-8b-instruct-sppo-iter1-gp-2b-tau01-table"
  },
  {
    "eval_name": "xxx777xxxASD_L3.1-ClaudeMaid-4x8B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/xxx777xxxASD/L3.1-ClaudeMaid-4x8B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">xxx777xxxASD/L3.1-ClaudeMaid-4x8B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/xxx777xxxASD__L3.1-ClaudeMaid-4x8B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "xxx777xxxASD/L3.1-ClaudeMaid-4x8B",
    "Model sha": "2a98d9cb91c7aa775acbf5bfe7bb91beb2faf682",
    "Average ‚¨ÜÔ∏è": 26.190882979161234,
    "Hub License": "llama3.1",
    "Hub ‚ù§Ô∏è": 7,
    "#Params (B)": 24,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.376184569149158,
    "IFEval Raw": 0.6696487541944263,
    "IFEval": 66.96487541944263,
    "BBH Raw": 0.5070848048063867,
    "BBH": 29.437347820739546,
    "MATH Lvl 5 Raw": 0.1283987915407855,
    "MATH Lvl 5": 12.83987915407855,
    "GPQA Raw": 0.2911073825503356,
    "GPQA": 5.480984340044745,
    "MUSR Raw": 0.42893749999999997,
    "MUSR": 13.750520833333331,
    "MMLU-PRO Raw": 0.35804521276595747,
    "MMLU-PRO": 28.671690307328607,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-07-27",
    "Submission Date": "2024-07-28",
    "Generation": 0,
    "Base Model": "xxx777xxxASD/L3.1-ClaudeMaid-4x8B"
  },
  {
    "eval_name": "yam-peleg_Hebrew-Gemma-11B-Instruct_float16",
    "Precision": "float16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "GemmaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yam-peleg/Hebrew-Gemma-11B-Instruct\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yam-peleg/Hebrew-Gemma-11B-Instruct</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yam-peleg__Hebrew-Gemma-11B-Instruct-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yam-peleg/Hebrew-Gemma-11B-Instruct",
    "Model sha": "a40259d1efbcac4829ed44d3b589716f615ed362",
    "Average ‚¨ÜÔ∏è": 13.919762723869878,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 22,
    "#Params (B)": 10,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.937267172630614,
    "IFEval Raw": 0.30207737691547315,
    "IFEval": 30.207737691547315,
    "BBH Raw": 0.40357843109818686,
    "BBH": 16.86274051283721,
    "MATH Lvl 5 Raw": 0.05740181268882176,
    "MATH Lvl 5": 5.740181268882176,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.4088541666666667,
    "MUSR": 9.973437500000003,
    "MMLU-PRO Raw": 0.25540226063829785,
    "MMLU-PRO": 17.266917848699762,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-03-06",
    "Submission Date": "2024-07-31",
    "Generation": 0,
    "Base Model": "yam-peleg/Hebrew-Gemma-11B-Instruct"
  },
  {
    "eval_name": "yam-peleg_Hebrew-Mistral-7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yam-peleg/Hebrew-Mistral-7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yam-peleg/Hebrew-Mistral-7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yam-peleg__Hebrew-Mistral-7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yam-peleg/Hebrew-Mistral-7B",
    "Model sha": "3d32134b5959492fd7efbbf16395352594bc89f7",
    "Average ‚¨ÜÔ∏è": 13.302117179699644,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 62,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.3992812273120876,
    "IFEval Raw": 0.23283443485507344,
    "IFEval": 23.28344348550734,
    "BBH Raw": 0.43340366992362034,
    "BBH": 20.176940422218426,
    "MATH Lvl 5 Raw": 0.04984894259818731,
    "MATH Lvl 5": 4.984894259818732,
    "GPQA Raw": 0.27936241610738255,
    "GPQA": 3.9149888143176734,
    "MUSR Raw": 0.39765625,
    "MUSR": 7.673697916666668,
    "MMLU-PRO Raw": 0.27800864361702127,
    "MMLU-PRO": 19.778738179669027,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-04-26",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "yam-peleg/Hebrew-Mistral-7B"
  },
  {
    "eval_name": "yam-peleg_Hebrew-Mistral-7B-200K_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yam-peleg/Hebrew-Mistral-7B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yam-peleg/Hebrew-Mistral-7B-200K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yam-peleg__Hebrew-Mistral-7B-200K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yam-peleg/Hebrew-Mistral-7B-200K",
    "Model sha": "7b51c7b31e3d9e29ea964c579a45233cfad255fe",
    "Average ‚¨ÜÔ∏è": 10.64429135893812,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.7353117338467755,
    "IFEval Raw": 0.1855731680829089,
    "IFEval": 18.557316808290892,
    "BBH Raw": 0.4149272793394017,
    "BBH": 17.49360317518456,
    "MATH Lvl 5 Raw": 0.023413897280966767,
    "MATH Lvl 5": 2.3413897280966767,
    "GPQA Raw": 0.276006711409396,
    "GPQA": 3.467561521252797,
    "MUSR Raw": 0.3764791666666667,
    "MUSR": 4.5265625000000025,
    "MMLU-PRO Raw": 0.25731382978723405,
    "MMLU-PRO": 17.47931442080378,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-05",
    "Submission Date": "2024-07-11",
    "Generation": 0,
    "Base Model": "yam-peleg/Hebrew-Mistral-7B-200K"
  },
  {
    "eval_name": "yam-peleg_Hebrew-Mistral-7B-200K_bfloat16",
    "Precision": "bfloat16",
    "Type": "üü© continuously pretrained",
    "T": "üü©",
    "Weight type": "Original",
    "Architecture": "MistralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yam-peleg/Hebrew-Mistral-7B-200K\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yam-peleg/Hebrew-Mistral-7B-200K</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yam-peleg__Hebrew-Mistral-7B-200K-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yam-peleg/Hebrew-Mistral-7B-200K",
    "Model sha": "7b51c7b31e3d9e29ea964c579a45233cfad255fe",
    "Average ‚¨ÜÔ∏è": 8.235611674051803,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 15,
    "#Params (B)": 7,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6844942774660865,
    "IFEval Raw": 0.17698041197356346,
    "IFEval": 17.698041197356346,
    "BBH Raw": 0.3410500846818921,
    "BBH": 7.671323719331375,
    "MATH Lvl 5 Raw": 0.02190332326283988,
    "MATH Lvl 5": 2.190332326283988,
    "GPQA Raw": 0.2533557046979866,
    "GPQA": 0.44742729306487633,
    "MUSR Raw": 0.37399999999999994,
    "MUSR": 4.416666666666667,
    "MMLU-PRO Raw": 0.2529089095744681,
    "MMLU-PRO": 16.989878841607567,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-05-05",
    "Submission Date": "2024-08-06",
    "Generation": 0,
    "Base Model": "yam-peleg/Hebrew-Mistral-7B-200K"
  },
  {
    "eval_name": "ycros_BagelMIsteryTour-v2-8x7B_float16",
    "Precision": "float16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ycros/BagelMIsteryTour-v2-8x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ycros__BagelMIsteryTour-v2-8x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ycros/BagelMIsteryTour-v2-8x7B",
    "Model sha": "98a8b319707be3dab1659594da69a37ed8f8c148",
    "Average ‚¨ÜÔ∏è": 24.258614269254906,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.64913185802464,
    "IFEval Raw": 0.599431730031871,
    "IFEval": 59.9431730031871,
    "BBH Raw": 0.515923595752544,
    "BBH": 31.69928662894613,
    "MATH Lvl 5 Raw": 0.07854984894259819,
    "MATH Lvl 5": 7.854984894259818,
    "GPQA Raw": 0.30453020134228187,
    "GPQA": 7.270693512304249,
    "MUSR Raw": 0.4202916666666667,
    "MUSR": 11.303125000000001,
    "MMLU-PRO Raw": 0.34732380319148937,
    "MMLU-PRO": 27.480422576832154,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-19",
    "Submission Date": "2024-06-28",
    "Generation": 1,
    "Base Model": "ycros/BagelMIsteryTour-v2-8x7B (Merge)"
  },
  {
    "eval_name": "ycros_BagelMIsteryTour-v2-8x7B_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ycros/BagelMIsteryTour-v2-8x7B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ycros/BagelMIsteryTour-v2-8x7B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ycros__BagelMIsteryTour-v2-8x7B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ycros/BagelMIsteryTour-v2-8x7B",
    "Model sha": "98a8b319707be3dab1659594da69a37ed8f8c148",
    "Average ‚¨ÜÔ∏è": 24.72480237405757,
    "Hub License": "cc-by-nc-4.0",
    "Hub ‚ù§Ô∏è": 16,
    "#Params (B)": 46,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.6193365321460234,
    "IFEval Raw": 0.6262095683896506,
    "IFEval": 62.62095683896506,
    "BBH Raw": 0.5141943573573103,
    "BBH": 31.366123015915477,
    "MATH Lvl 5 Raw": 0.08761329305135951,
    "MATH Lvl 5": 8.76132930513595,
    "GPQA Raw": 0.30788590604026844,
    "GPQA": 7.718120805369126,
    "MUSR Raw": 0.41375,
    "MUSR": 10.31875,
    "MMLU-PRO Raw": 0.3480718085106383,
    "MMLU-PRO": 27.56353427895981,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-01-19",
    "Submission Date": "2024-08-04",
    "Generation": 1,
    "Base Model": "ycros/BagelMIsteryTour-v2-8x7B (Merge)"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table",
    "Model sha": "97b2d0e790a6fcdf39c34a2043f0818368c7dcb3",
    "Average ‚¨ÜÔ∏è": 22.974570612630092,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6182530222970236,
    "IFEval Raw": 0.6708976626462231,
    "IFEval": 67.08976626462231,
    "BBH Raw": 0.49866134349131935,
    "BBH": 28.170106538118223,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37269791666666663,
    "MUSR": 3.85390625,
    "MMLU-PRO Raw": 0.37159242021276595,
    "MMLU-PRO": 30.17693557919622,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_2b-table"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table",
    "Model sha": "e8786291c206d5cd1b01d29466e3b397278f4e2b",
    "Average ‚¨ÜÔ∏è": 24.877776348887362,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6406628367373487,
    "IFEval Raw": 0.7332710541363582,
    "IFEval": 73.32710541363582,
    "BBH Raw": 0.5080359954971677,
    "BBH": 29.308127928492553,
    "MATH Lvl 5 Raw": 0.09743202416918428,
    "MATH Lvl 5": 9.743202416918429,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.38060416666666663,
    "MUSR": 5.008854166666667,
    "MMLU-PRO Raw": 0.3748337765957447,
    "MMLU-PRO": 30.537086288416077,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_bt_8b-table"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table",
    "Model sha": "0d9cb29aa87b0c17ed011ffbc83803f3f6dd18e7",
    "Average ‚¨ÜÔ∏è": 23.168113557883544,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6795544547648138,
    "IFEval Raw": 0.6784664689690023,
    "IFEval": 67.84664689690022,
    "BBH Raw": 0.49412091896520455,
    "BBH": 27.469588233937547,
    "MATH Lvl 5 Raw": 0.09516616314199397,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3646666666666667,
    "MUSR": 2.7500000000000018,
    "MMLU-PRO Raw": 0.37175864361702127,
    "MMLU-PRO": 30.1954048463357,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_2b-table"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table",
    "Model sha": "7a326a956e6169b287a04ef93cdc0342a0f3311a",
    "Average ‚¨ÜÔ∏è": 24.00167654994105,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6481841267166133,
    "IFEval Raw": 0.7131876753680235,
    "IFEval": 71.31876753680235,
    "BBH Raw": 0.5025359954971677,
    "BBH": 28.604424224788854,
    "MATH Lvl 5 Raw": 0.09365558912386707,
    "MATH Lvl 5": 9.365558912386707,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.3713333333333333,
    "MUSR": 3.683333333333333,
    "MMLU-PRO Raw": 0.36826795212765956,
    "MMLU-PRO": 29.807550236406616,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-Iter1_gp_8b-table"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001",
    "Model sha": "e5c8baadbf6ce17b344596ad42bd3546f66e253e",
    "Average ‚¨ÜÔ∏è": 22.3648668008077,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.5822351629701248,
    "IFEval Raw": 0.6495653754260917,
    "IFEval": 64.95653754260917,
    "BBH Raw": 0.4979459532536201,
    "BBH": 28.09919885125768,
    "MATH Lvl 5 Raw": 0.04833836858006042,
    "MATH Lvl 5": 4.833836858006042,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37796874999999996,
    "MUSR": 4.8460937500000005,
    "MMLU-PRO Raw": 0.37200797872340424,
    "MMLU-PRO": 30.223108747044915,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_2b-table-0.001"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002",
    "Model sha": "064e237b850151938caf171a4c8c7e34c93e580e",
    "Average ‚¨ÜÔ∏è": 24.319539143511975,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6060215135466479,
    "IFEval Raw": 0.7196073086078272,
    "IFEval": 71.96073086078272,
    "BBH Raw": 0.5045147424411157,
    "BBH": 28.785910542437534,
    "MATH Lvl 5 Raw": 0.0785498489425982,
    "MATH Lvl 5": 7.85498489425982,
    "GPQA Raw": 0.2600671140939597,
    "GPQA": 1.342281879194629,
    "MUSR Raw": 0.3831458333333333,
    "MUSR": 5.593229166666667,
    "MMLU-PRO Raw": 0.3734208776595745,
    "MMLU-PRO": 30.38009751773049,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_bt_8b-table-0.002"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001",
    "Model sha": "b685b90063258e05f8b4930fdbce2e565f13f620",
    "Average ‚¨ÜÔ∏è": 22.38483694563696,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6490922181425386,
    "IFEval Raw": 0.6504397221594258,
    "IFEval": 65.04397221594259,
    "BBH Raw": 0.49578758563187125,
    "BBH": 27.82525272195498,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.36603125,
    "MUSR": 2.853906249999999,
    "MMLU-PRO Raw": 0.3702626329787234,
    "MMLU-PRO": 30.029181442080382,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_2b-table-0.001"
  },
  {
    "eval_name": "yfzp_Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yfzp__Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002",
    "Model sha": "5ab3f2cfc96bdda3b5a629ab4a81adf7394ba90a",
    "Average ‚¨ÜÔ∏è": 23.52252221618673,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6076899508204758,
    "IFEval Raw": 0.7015973173402128,
    "IFEval": 70.15973173402128,
    "BBH Raw": 0.4991547169583548,
    "BBH": 28.12061516996449,
    "MATH Lvl 5 Raw": 0.07326283987915409,
    "MATH Lvl 5": 7.326283987915409,
    "GPQA Raw": 0.25922818791946306,
    "GPQA": 1.230425055928408,
    "MUSR Raw": 0.37790624999999994,
    "MUSR": 4.6382812499999995,
    "MMLU-PRO Raw": 0.366938164893617,
    "MMLU-PRO": 29.659796099290777,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-29",
    "Submission Date": "2024-09-29",
    "Generation": 0,
    "Base Model": "yfzp/Llama-3-8B-Instruct-SPPO-score-Iter1_gp_8b-table-0.002"
  },
  {
    "eval_name": "yifAI_Llama-3-8B-Instruct-SPPO-score-Iter3_gp_8b-table-0.002_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yifAI/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_8b-table-0.002\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yifAI/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_8b-table-0.002</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yifAI__Llama-3-8B-Instruct-SPPO-score-Iter3_gp_8b-table-0.002-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yifAI/Llama-3-8B-Instruct-SPPO-score-Iter3_gp_8b-table-0.002",
    "Model sha": "7a046b74179225d6055dd8aa601b5234f817b1e5",
    "Average ‚¨ÜÔ∏è": 22.624782298675466,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.6720156795047371,
    "IFEval Raw": 0.6489658550423987,
    "IFEval": 64.89658550423985,
    "BBH Raw": 0.49145217071254876,
    "BBH": 27.28106392287093,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.38987499999999997,
    "MUSR": 7.134375000000002,
    "MMLU-PRO Raw": 0.3519780585106383,
    "MMLU-PRO": 27.997562056737586,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "Removed"
  },
  {
    "eval_name": "ylalain_ECE-PRYMMAL-YL-1B-SLERP-V8_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ylalain/ECE-PRYMMAL-YL-1B-SLERP-V8\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ylalain/ECE-PRYMMAL-YL-1B-SLERP-V8</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ylalain__ECE-PRYMMAL-YL-1B-SLERP-V8-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ylalain/ECE-PRYMMAL-YL-1B-SLERP-V8",
    "Model sha": "2c00dbc74e55d42fbc8b08f474fb9568f820edb9",
    "Average ‚¨ÜÔ∏è": 9.604138800037871,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 1,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.5484277459241534,
    "IFEval Raw": 0.15052726764983576,
    "IFEval": 15.052726764983579,
    "BBH Raw": 0.3975573100103517,
    "BBH": 15.175392374828268,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28942953020134227,
    "GPQA": 5.257270693512303,
    "MUSR Raw": 0.3874583333333333,
    "MUSR": 6.765625,
    "MMLU-PRO Raw": 0.23836436170212766,
    "MMLU-PRO": 15.373817966903072,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-13",
    "Submission Date": "2024-11-13",
    "Generation": 0,
    "Base Model": "ylalain/ECE-PRYMMAL-YL-1B-SLERP-V8"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-ORPO-jpn-it-abliterated-18_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-ORPO-jpn-it-abliterated-18-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18",
    "Model sha": "aed2a9061ffa21beaec0d617a9605e160136aab4",
    "Average ‚¨ÜÔ∏è": 14.63378065380544,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.200401649006539,
    "IFEval Raw": 0.4630945890237902,
    "IFEval": 46.30945890237902,
    "BBH Raw": 0.4052902505118913,
    "BBH": 16.30199203988385,
    "MATH Lvl 5 Raw": 0.0037764350453172208,
    "MATH Lvl 5": 0.3776435045317221,
    "GPQA Raw": 0.28859060402684567,
    "GPQA": 5.145413870246088,
    "MUSR Raw": 0.3754270833333333,
    "MUSR": 4.728385416666668,
    "MMLU-PRO Raw": 0.23445811170212766,
    "MMLU-PRO": 14.939790189125294,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-16",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-ORPO-jpn-it-abliterated-18-merge_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18-merge\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18-merge</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-ORPO-jpn-it-abliterated-18-merge-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-ORPO-jpn-it-abliterated-18-merge",
    "Model sha": "b72be0a7879f0d82cb2024cfc1d02c370ce3efe8",
    "Average ‚¨ÜÔ∏è": 15.737662679744027,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9879904264816353,
    "IFEval Raw": 0.5218209905273563,
    "IFEval": 52.18209905273564,
    "BBH Raw": 0.414688942270627,
    "BBH": 17.34833699622109,
    "MATH Lvl 5 Raw": 0.008308157099697885,
    "MATH Lvl 5": 0.8308157099697886,
    "GPQA Raw": 0.2835570469798658,
    "GPQA": 4.4742729306487705,
    "MUSR Raw": 0.35139583333333335,
    "MUSR": 3.357812500000001,
    "MMLU-PRO Raw": 0.24609375,
    "MMLU-PRO": 16.232638888888886,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-30",
    "Submission Date": "2024-11-16",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-17_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-17\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-17</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-17-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-17",
    "Model sha": "e6f82b93dae0b8207aa3252ab4157182e2610787",
    "Average ‚¨ÜÔ∏è": 15.002982229394513,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.104509456852725,
    "IFEval Raw": 0.5081572449988254,
    "IFEval": 50.815724499882535,
    "BBH Raw": 0.40762664531580056,
    "BBH": 16.23474918432881,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27181208053691275,
    "GPQA": 2.9082774049216997,
    "MUSR Raw": 0.37006249999999996,
    "MUSR": 3.891145833333333,
    "MMLU-PRO Raw": 0.2455119680851064,
    "MMLU-PRO": 16.16799645390071,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-18",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-17-18-24_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-17-18-24\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-17-18-24</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-17-18-24-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-17-18-24",
    "Model sha": "38f56fcb99bd64278a1d90dd23aea527036329a0",
    "Average ‚¨ÜÔ∏è": 14.019764718656953,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.7048586085816617,
    "IFEval Raw": 0.505484337114412,
    "IFEval": 50.54843371144119,
    "BBH Raw": 0.38123590457353557,
    "BBH": 13.114728218255616,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.28104026845637586,
    "GPQA": 4.138702460850116,
    "MUSR Raw": 0.35015625,
    "MUSR": 2.0695312500000003,
    "MMLU-PRO Raw": 0.2282247340425532,
    "MMLU-PRO": 14.247192671394798,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-17-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-17-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO",
    "Model sha": "531b2e2043285cb40cd0433f5ad43441f8ac6b6c",
    "Average ‚¨ÜÔ∏è": 14.516850808390743,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 9.681597491499097,
    "IFEval Raw": 0.47478468242042227,
    "IFEval": 47.478468242042226,
    "BBH Raw": 0.38979797271028965,
    "BBH": 14.389413087436518,
    "MATH Lvl 5 Raw": 0.04229607250755287,
    "MATH Lvl 5": 4.229607250755287,
    "GPQA Raw": 0.27432885906040266,
    "GPQA": 3.243847874720355,
    "MUSR Raw": 0.37676041666666665,
    "MUSR": 4.5283854166666675,
    "MMLU-PRO Raw": 0.21908244680851063,
    "MMLU-PRO": 13.231382978723403,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-18",
    "Submission Date": "2024-10-27",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-17-ORPO-alpaca_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO-alpaca\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO-alpaca</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-17-ORPO-alpaca-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-17-ORPO-alpaca",
    "Model sha": "5503b5e892be463fa4b1d265b8ba9ba4304af012",
    "Average ‚¨ÜÔ∏è": 12.001731227528522,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.1846663769000478,
    "IFEval Raw": 0.30647349033896726,
    "IFEval": 30.647349033896724,
    "BBH Raw": 0.40715971926711275,
    "BBH": 16.922412033306475,
    "MATH Lvl 5 Raw": 0.0007552870090634441,
    "MATH Lvl 5": 0.0755287009063444,
    "GPQA Raw": 0.26929530201342283,
    "GPQA": 2.572706935123044,
    "MUSR Raw": 0.39691666666666664,
    "MUSR": 7.9145833333333355,
    "MMLU-PRO Raw": 0.2249002659574468,
    "MMLU-PRO": 13.877807328605199,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-27",
    "Submission Date": "2024-10-27",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-18_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-18\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-18</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-18-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-18",
    "Model sha": "c50b85f9b60b444f85fe230b8d77fcbc7b18ef91",
    "Average ‚¨ÜÔ∏è": 15.503245492497465,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.0526635707489849,
    "IFEval Raw": 0.5175246124726836,
    "IFEval": 51.75246124726836,
    "BBH Raw": 0.4132188791645781,
    "BBH": 17.143414938177205,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27348993288590606,
    "GPQA": 3.1319910514541416,
    "MUSR Raw": 0.37415624999999997,
    "MUSR": 4.269531250000001,
    "MMLU-PRO Raw": 0.25049867021276595,
    "MMLU-PRO": 16.722074468085104,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-18",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-18-ORPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-18-ORPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-18-ORPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-18-ORPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-18-ORPO",
    "Model sha": "b9f41f53827b8a5a600546b41f63023bf84617a3",
    "Average ‚¨ÜÔ∏è": 14.943472204777024,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.6103771280333505,
    "IFEval Raw": 0.47423502972113984,
    "IFEval": 47.42350297211399,
    "BBH Raw": 0.40389353402379324,
    "BBH": 16.538078577820993,
    "MATH Lvl 5 Raw": 0.035498489425981876,
    "MATH Lvl 5": 3.5498489425981874,
    "GPQA Raw": 0.26174496644295303,
    "GPQA": 1.5659955257270708,
    "MUSR Raw": 0.3953333333333333,
    "MUSR": 7.416666666666667,
    "MMLU-PRO Raw": 0.21850066489361702,
    "MMLU-PRO": 13.166740543735225,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "ymcki_gemma-2-2b-jpn-it-abliterated-24_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/ymcki/gemma-2-2b-jpn-it-abliterated-24\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">ymcki/gemma-2-2b-jpn-it-abliterated-24</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/ymcki__gemma-2-2b-jpn-it-abliterated-24-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "ymcki/gemma-2-2b-jpn-it-abliterated-24",
    "Model sha": "06c129ba5261ee88e32035c88f90ca11d835175d",
    "Average ‚¨ÜÔ∏è": 15.60407576414145,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.8104422900230217,
    "IFEval Raw": 0.49786566310722213,
    "IFEval": 49.786566310722215,
    "BBH Raw": 0.41096027770392857,
    "BBH": 16.772590130572933,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.27768456375838924,
    "GPQA": 3.6912751677852316,
    "MUSR Raw": 0.39148958333333334,
    "MUSR": 7.002864583333333,
    "MMLU-PRO Raw": 0.2473404255319149,
    "MMLU-PRO": 16.37115839243499,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-25",
    "Generation": 3,
    "Base Model": "google/gemma-2-2b"
  },
  {
    "eval_name": "yuvraj17_Llama3-8B-SuperNova-Spectrum-Hermes-DPO_bfloat16",
    "Precision": "bfloat16",
    "Type": "üí¨ chat models (RLHF, DPO, IFT, ...)",
    "T": "üí¨",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yuvraj17/Llama3-8B-SuperNova-Spectrum-Hermes-DPO\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yuvraj17/Llama3-8B-SuperNova-Spectrum-Hermes-DPO</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yuvraj17__Llama3-8B-SuperNova-Spectrum-Hermes-DPO-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yuvraj17/Llama3-8B-SuperNova-Spectrum-Hermes-DPO",
    "Model sha": "0da9f780f7dd94ed1e10c8d3e082472ff2922177",
    "Average ‚¨ÜÔ∏è": 18.07557946942494,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9720295103475554,
    "IFEval Raw": 0.4690897928607206,
    "IFEval": 46.90897928607207,
    "BBH Raw": 0.4399870586095269,
    "BBH": 21.238562899271304,
    "MATH Lvl 5 Raw": 0.05589123867069487,
    "MATH Lvl 5": 5.589123867069487,
    "GPQA Raw": 0.30201342281879195,
    "GPQA": 6.935123042505594,
    "MUSR Raw": 0.40121875,
    "MUSR": 9.619010416666669,
    "MMLU-PRO Raw": 0.2634640957446808,
    "MMLU-PRO": 18.162677304964536,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-24",
    "Submission Date": "2024-09-30",
    "Generation": 0,
    "Base Model": "yuvraj17/Llama3-8B-SuperNova-Spectrum-Hermes-DPO"
  },
  {
    "eval_name": "yuvraj17_Llama3-8B-SuperNova-Spectrum-dare_ties_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yuvraj17/Llama3-8B-SuperNova-Spectrum-dare_ties\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yuvraj17/Llama3-8B-SuperNova-Spectrum-dare_ties</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yuvraj17__Llama3-8B-SuperNova-Spectrum-dare_ties-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yuvraj17/Llama3-8B-SuperNova-Spectrum-dare_ties",
    "Model sha": "998d15b32900bc230727c8a7984e005f611723e9",
    "Average ‚¨ÜÔ∏è": 19.134800777531545,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.9141440717323275,
    "IFEval Raw": 0.4012708502329375,
    "IFEval": 40.127085023293745,
    "BBH Raw": 0.4615794426716074,
    "BBH": 23.492187889680057,
    "MATH Lvl 5 Raw": 0.08232628398791542,
    "MATH Lvl 5": 8.232628398791542,
    "GPQA Raw": 0.2751677852348993,
    "GPQA": 3.355704697986576,
    "MUSR Raw": 0.42109375,
    "MUSR": 11.003385416666667,
    "MMLU-PRO Raw": 0.35738031914893614,
    "MMLU-PRO": 28.59781323877068,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "yuvraj17/Llama3-8B-SuperNova-Spectrum-dare_ties (Merge)"
  },
  {
    "eval_name": "yuvraj17_Llama3-8B-abliterated-Spectrum-slerp_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "LlamaForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/yuvraj17/Llama3-8B-abliterated-Spectrum-slerp\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">yuvraj17/Llama3-8B-abliterated-Spectrum-slerp</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/yuvraj17__Llama3-8B-abliterated-Spectrum-slerp-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "yuvraj17/Llama3-8B-abliterated-Spectrum-slerp",
    "Model sha": "28789950975ecf5aac846c3f2c0a5d6841651ee6",
    "Average ‚¨ÜÔ∏è": 17.687551599695436,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 8,
    "Available on the hub": true,
    "Not_Merged": false,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 0.8266602155286076,
    "IFEval Raw": 0.2884878788281759,
    "IFEval": 28.84878788281759,
    "BBH Raw": 0.4977912063897858,
    "BBH": 28.54692976096071,
    "MATH Lvl 5 Raw": 0.05815709969788519,
    "MATH Lvl 5": 5.815709969788519,
    "GPQA Raw": 0.3011744966442953,
    "GPQA": 6.823266219239373,
    "MUSR Raw": 0.39982291666666664,
    "MUSR": 11.011197916666662,
    "MMLU-PRO Raw": 0.32571476063829785,
    "MMLU-PRO": 25.07941784869976,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-09-22",
    "Submission Date": "2024-09-23",
    "Generation": 1,
    "Base Model": "yuvraj17/Llama3-8B-abliterated-Spectrum-slerp (Merge)"
  },
  {
    "eval_name": "zake7749_gemma-2-2b-it-chinese-kyara-dpo_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zake7749/gemma-2-2b-it-chinese-kyara-dpo\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zake7749/gemma-2-2b-it-chinese-kyara-dpo</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zake7749__gemma-2-2b-it-chinese-kyara-dpo-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zake7749/gemma-2-2b-it-chinese-kyara-dpo",
    "Model sha": "bbc011dae0416c1664a0287f3a7a0f9563deac91",
    "Average ‚¨ÜÔ∏è": 19.33458518063949,
    "Hub License": "gemma",
    "Hub ‚ù§Ô∏è": 6,
    "#Params (B)": 2,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 1.2793090717282345,
    "IFEval Raw": 0.5382075116247114,
    "IFEval": 53.82075116247113,
    "BBH Raw": 0.4257464897414603,
    "BBH": 19.061804188812648,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.26677852348993286,
    "GPQA": 2.2371364653243813,
    "MUSR Raw": 0.45756250000000004,
    "MUSR": 16.761979166666666,
    "MMLU-PRO Raw": 0.25731382978723405,
    "MMLU-PRO": 17.47931442080378,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-08-18",
    "Submission Date": "2024-10-17",
    "Generation": 1,
    "Base Model": "zake7749/gemma-2-2b-it-chinese-kyara-dpo (Merge)"
  },
  {
    "eval_name": "zelk12_Gemma-2-TM-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/Gemma-2-TM-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/Gemma-2-TM-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__Gemma-2-TM-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/Gemma-2-TM-9B",
    "Model sha": "42366d605e6bdad354a5632547e37d34d300ff7a",
    "Average ‚¨ÜÔ∏è": 30.151929070322968,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9678926729505282,
    "IFEval Raw": 0.8044621604010691,
    "IFEval": 80.44621604010692,
    "BBH Raw": 0.5986592993557701,
    "BBH": 42.04949103777593,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.41523958333333333,
    "MUSR": 11.238281249999995,
    "MMLU-PRO Raw": 0.40882646276595747,
    "MMLU-PRO": 34.314051418439725,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-06",
    "Submission Date": "2024-11-06",
    "Generation": 1,
    "Base Model": "zelk12/Gemma-2-TM-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT-Gen1-gemma-2-9B",
    "Model sha": "b78f8883614cbbdf182ebb4acf8a8c124bc782ae",
    "Average ‚¨ÜÔ∏è": 33.04135620447669,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.362746413139062,
    "IFEval Raw": 0.7886252920029965,
    "IFEval": 78.86252920029965,
    "BBH Raw": 0.6099997385328262,
    "BBH": 44.01124668886745,
    "MATH Lvl 5 Raw": 0.1336858006042296,
    "MATH Lvl 5": 13.36858006042296,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.4216875,
    "MUSR": 11.577604166666667,
    "MMLU-PRO Raw": 0.4380817819148936,
    "MMLU-PRO": 37.56464243498817,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-23",
    "Generation": 1,
    "Base Model": "zelk12/MT-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT-Gen2-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT-Gen2-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT-Gen2-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT-Gen2-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT-Gen2-gemma-2-9B",
    "Model sha": "c723f8b9b7334fddd1eb8b6e5230b76fb18139a5",
    "Average ‚¨ÜÔ∏è": 33.644494917916724,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9894476247543253,
    "IFEval Raw": 0.7907485471881275,
    "IFEval": 79.07485471881274,
    "BBH Raw": 0.6100494662695,
    "BBH": 44.10778153097642,
    "MATH Lvl 5 Raw": 0.1487915407854985,
    "MATH Lvl 5": 14.879154078549849,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.4322916666666667,
    "MUSR": 13.303125000000003,
    "MMLU-PRO Raw": 0.4387466755319149,
    "MMLU-PRO": 37.63851950354609,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-10",
    "Submission Date": "2024-11-10",
    "Generation": 1,
    "Base Model": "zelk12/MT-Gen2-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT-Merge-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT-Merge-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT-Merge-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT-Merge-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT-Merge-gemma-2-9B",
    "Model sha": "f4c3b001bc8692bcbbd7005b6f8db048e651aa46",
    "Average ‚¨ÜÔ∏è": 33.39320826035015,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.2190558370084803,
    "IFEval Raw": 0.8035379459833243,
    "IFEval": 80.35379459833243,
    "BBH Raw": 0.6118379158679297,
    "BBH": 44.32084182103274,
    "MATH Lvl 5 Raw": 0.1314199395770393,
    "MATH Lvl 5": 13.14199395770393,
    "GPQA Raw": 0.34815436241610737,
    "GPQA": 13.087248322147648,
    "MUSR Raw": 0.425625,
    "MUSR": 12.103125,
    "MMLU-PRO Raw": 0.43617021276595747,
    "MMLU-PRO": 37.35224586288417,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-22",
    "Submission Date": "2024-10-22",
    "Generation": 1,
    "Base Model": "zelk12/MT-Merge-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT-Merge1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT-Merge1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT-Merge1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT-Merge1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT-Merge1-gemma-2-9B",
    "Model sha": "71bb4577c877715f3f6646a224b184544639c856",
    "Average ‚¨ÜÔ∏è": 33.130535769514076,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 4.036662135142767,
    "IFEval Raw": 0.7886252920029965,
    "IFEval": 78.86252920029965,
    "BBH Raw": 0.6099997385328262,
    "BBH": 44.05824559954283,
    "MATH Lvl 5 Raw": 0.1268882175226586,
    "MATH Lvl 5": 12.688821752265861,
    "GPQA Raw": 0.35151006711409394,
    "GPQA": 13.534675615212524,
    "MUSR Raw": 0.4243854166666667,
    "MUSR": 12.148177083333335,
    "MMLU-PRO Raw": 0.43741688829787234,
    "MMLU-PRO": 37.49076536643025,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-07",
    "Submission Date": "2024-11-07",
    "Generation": 1,
    "Base Model": "zelk12/MT-Merge1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT-gemma-2-9B",
    "Model sha": "24e1f894517b86dd866c1a5999ced4a5924dcd90",
    "Average ‚¨ÜÔ∏è": 30.239611687439066,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.023398909368739,
    "IFEval Raw": 0.7968434863938794,
    "IFEval": 79.68434863938793,
    "BBH Raw": 0.6063604478633632,
    "BBH": 43.32424255563143,
    "MATH Lvl 5 Raw": 0.0030211480362537764,
    "MATH Lvl 5": 0.3021148036253776,
    "GPQA Raw": 0.34563758389261745,
    "GPQA": 12.751677852348994,
    "MUSR Raw": 0.40711458333333334,
    "MUSR": 9.555989583333337,
    "MMLU-PRO Raw": 0.42237367021276595,
    "MMLU-PRO": 35.819296690307326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-11",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "zelk12/MT-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT1-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT1-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT1-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT1-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT1-Gen1-gemma-2-9B",
    "Model sha": "939ac6c12059a18fc1117cdb3861f46816eff2fb",
    "Average ‚¨ÜÔ∏è": 33.2322593015476,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.3624847194588017,
    "IFEval Raw": 0.7974430067775724,
    "IFEval": 79.74430067775725,
    "BBH Raw": 0.6117787046647335,
    "BBH": 44.27328174531427,
    "MATH Lvl 5 Raw": 0.12235649546827794,
    "MATH Lvl 5": 12.235649546827794,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.43095833333333333,
    "MUSR": 13.103125000000004,
    "MMLU-PRO Raw": 0.43758311170212766,
    "MMLU-PRO": 37.50923463356973,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-23",
    "Submission Date": "2024-10-24",
    "Generation": 1,
    "Base Model": "zelk12/MT1-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT1-Gen2-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT1-Gen2-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT1-Gen2-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT1-Gen2-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT1-Gen2-gemma-2-9B",
    "Model sha": "aeaca7dc7d50a425a5d3c38d7c4a7daf1c772ad4",
    "Average ‚¨ÜÔ∏è": 33.14239831327872,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9959951518146926,
    "IFEval Raw": 0.7983672211953173,
    "IFEval": 79.83672211953174,
    "BBH Raw": 0.6095989894691557,
    "BBH": 43.91919055805058,
    "MATH Lvl 5 Raw": 0.11329305135951662,
    "MATH Lvl 5": 11.329305135951662,
    "GPQA Raw": 0.3523489932885906,
    "GPQA": 13.646532438478745,
    "MUSR Raw": 0.42835416666666665,
    "MUSR": 12.844270833333335,
    "MMLU-PRO Raw": 0.43550531914893614,
    "MMLU-PRO": 37.278368794326234,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-11",
    "Submission Date": "2024-11-11",
    "Generation": 1,
    "Base Model": "zelk12/MT1-Gen2-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT1-gemma-2-9B",
    "Model sha": "3a5e77518ca9c3c8ea2edac4c03bc220ee91f3ed",
    "Average ‚¨ÜÔ∏è": 33.63382917709331,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.3457193902416043,
    "IFEval Raw": 0.7946703635243377,
    "IFEval": 79.46703635243378,
    "BBH Raw": 0.6108745950756924,
    "BBH": 44.16152621661877,
    "MATH Lvl 5 Raw": 0.14954682779456194,
    "MATH Lvl 5": 14.954682779456194,
    "GPQA Raw": 0.34563758389261745,
    "GPQA": 12.751677852348994,
    "MUSR Raw": 0.43222916666666666,
    "MUSR": 13.161979166666669,
    "MMLU-PRO Raw": 0.4357546542553192,
    "MMLU-PRO": 37.30607269503546,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-12",
    "Submission Date": "2024-10-14",
    "Generation": 1,
    "Base Model": "zelk12/MT1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT2-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT2-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT2-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT2-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT2-Gen1-gemma-2-9B",
    "Model sha": "167abf8eb4ea01fecd42dc32ad68160c51a8685a",
    "Average ‚¨ÜÔ∏è": 32.46022320307114,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.3832097768252645,
    "IFEval Raw": 0.7855778224001206,
    "IFEval": 78.55778224001206,
    "BBH Raw": 0.6100802027920743,
    "BBH": 44.141103157274806,
    "MATH Lvl 5 Raw": 0.10120845921450151,
    "MATH Lvl 5": 10.120845921450151,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.42432291666666666,
    "MUSR": 12.007031250000002,
    "MMLU-PRO Raw": 0.4376662234042553,
    "MMLU-PRO": 37.51846926713948,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-27",
    "Generation": 1,
    "Base Model": "zelk12/MT2-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT2-Gen2-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT2-Gen2-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT2-Gen2-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT2-Gen2-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT2-Gen2-gemma-2-9B",
    "Model sha": "24c487499b5833424ffb9932eed838bb254f61b4",
    "Average ‚¨ÜÔ∏è": 33.4711721257521,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 3,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.037441305392904,
    "IFEval Raw": 0.7889001183526376,
    "IFEval": 78.89001183526376,
    "BBH Raw": 0.6092917531936446,
    "BBH": 44.04450256220759,
    "MATH Lvl 5 Raw": 0.14803625377643503,
    "MATH Lvl 5": 14.803625377643503,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.42702083333333335,
    "MUSR": 12.577604166666669,
    "MMLU-PRO Raw": 0.43882978723404253,
    "MMLU-PRO": 37.64775413711584,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-12",
    "Submission Date": "2024-11-12",
    "Generation": 1,
    "Base Model": "zelk12/MT2-Gen2-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT2-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT2-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT2-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT2-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT2-gemma-2-9B",
    "Model sha": "d20d7169ce0f53d586504c50b4b7dc470bf8a781",
    "Average ‚¨ÜÔ∏è": 33.28249991535748,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1941098711237372,
    "IFEval Raw": 0.7885754243185858,
    "IFEval": 78.85754243185858,
    "BBH Raw": 0.611511004530543,
    "BBH": 44.16748136989228,
    "MATH Lvl 5 Raw": 0.14728096676737162,
    "MATH Lvl 5": 14.728096676737163,
    "GPQA Raw": 0.34731543624161076,
    "GPQA": 12.975391498881436,
    "MUSR Raw": 0.42165625,
    "MUSR": 11.540364583333334,
    "MMLU-PRO Raw": 0.43683510638297873,
    "MMLU-PRO": 37.42612293144209,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-14",
    "Submission Date": "2024-10-15",
    "Generation": 1,
    "Base Model": "zelk12/MT2-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT3-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT3-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT3-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT3-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT3-Gen1-gemma-2-9B",
    "Model sha": "cd78df9e67e2e710d8d305f5a03a92c01b1b425d",
    "Average ‚¨ÜÔ∏è": 31.05484507719724,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1136659017141057,
    "IFEval Raw": 0.7837792612490415,
    "IFEval": 78.37792612490415,
    "BBH Raw": 0.6106760932030332,
    "BBH": 44.119494687402835,
    "MATH Lvl 5 Raw": 0.0324773413897281,
    "MATH Lvl 5": 3.2477341389728096,
    "GPQA Raw": 0.3464765100671141,
    "GPQA": 12.863534675615215,
    "MUSR Raw": 0.41511458333333334,
    "MUSR": 10.755989583333337,
    "MMLU-PRO Raw": 0.43267952127659576,
    "MMLU-PRO": 36.96439125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-24",
    "Submission Date": "2024-10-28",
    "Generation": 1,
    "Base Model": "zelk12/MT3-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT3-Gen2-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT3-Gen2-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT3-Gen2-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT3-Gen2-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT3-Gen2-gemma-2-9B",
    "Model sha": "e4ef057d20751d89934025e9088ba98d89b921b5",
    "Average ‚¨ÜÔ∏è": 30.96362589742966,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 1.9191083985983404,
    "IFEval Raw": 0.7843289139483238,
    "IFEval": 78.43289139483238,
    "BBH Raw": 0.6091473194676166,
    "BBH": 43.94022574925496,
    "MATH Lvl 5 Raw": 0.02039274924471299,
    "MATH Lvl 5": 2.0392749244712993,
    "GPQA Raw": 0.3573825503355705,
    "GPQA": 14.317673378076066,
    "MUSR Raw": 0.41111458333333334,
    "MUSR": 10.022656250000002,
    "MMLU-PRO Raw": 0.43326130319148937,
    "MMLU-PRO": 37.02903368794326,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-11-20",
    "Submission Date": "2024-11-20",
    "Generation": 1,
    "Base Model": "zelk12/MT3-Gen2-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT3-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT3-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT3-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT3-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT3-gemma-2-9B",
    "Model sha": "d501b6ea59896fac3dc0a623501a5493b3573cde",
    "Average ‚¨ÜÔ∏è": 32.35252414501381,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.136652887768583,
    "IFEval Raw": 0.7786085364610345,
    "IFEval": 77.86085364610345,
    "BBH Raw": 0.61307842026088,
    "BBH": 44.24846451595969,
    "MATH Lvl 5 Raw": 0.10498489425981873,
    "MATH Lvl 5": 10.498489425981873,
    "GPQA Raw": 0.3447986577181208,
    "GPQA": 12.639821029082773,
    "MUSR Raw": 0.4242916666666667,
    "MUSR": 11.903125000000001,
    "MMLU-PRO Raw": 0.43267952127659576,
    "MMLU-PRO": 36.96439125295508,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-15",
    "Submission Date": "2024-10-16",
    "Generation": 1,
    "Base Model": "zelk12/MT3-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT4-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT4-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT4-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT4-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT4-Gen1-gemma-2-9B",
    "Model sha": "6ed2c66246c7f354decfd3579acb534dc4b0b48c",
    "Average ‚¨ÜÔ∏è": 33.54499421347736,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.1035608008152042,
    "IFEval Raw": 0.7894996387363307,
    "IFEval": 78.94996387363307,
    "BBH Raw": 0.6093827996028333,
    "BBH": 44.0095244503664,
    "MATH Lvl 5 Raw": 0.15030211480362535,
    "MATH Lvl 5": 15.030211480362535,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.43222916666666666,
    "MUSR": 13.0953125,
    "MMLU-PRO Raw": 0.4389128989361702,
    "MMLU-PRO": 37.65698877068557,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-29",
    "Generation": 1,
    "Base Model": "zelk12/MT4-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT4-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT4-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT4-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT4-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT4-gemma-2-9B",
    "Model sha": "2167ea02baf9145a697a7d828a17c75b86e5e282",
    "Average ‚¨ÜÔ∏è": 33.44734860931925,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.155258571724388,
    "IFEval Raw": 0.7761605872418517,
    "IFEval": 77.61605872418517,
    "BBH Raw": 0.607313601341302,
    "BBH": 43.55382749958519,
    "MATH Lvl 5 Raw": 0.17371601208459217,
    "MATH Lvl 5": 17.371601208459218,
    "GPQA Raw": 0.33808724832214765,
    "GPQA": 11.74496644295302,
    "MUSR Raw": 0.43092708333333335,
    "MUSR": 12.999218750000002,
    "MMLU-PRO Raw": 0.43658577127659576,
    "MMLU-PRO": 37.39841903073286,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-16",
    "Submission Date": "2024-10-20",
    "Generation": 1,
    "Base Model": "zelk12/MT4-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT5-Gen1-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT5-Gen1-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT5-Gen1-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT5-Gen1-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT5-Gen1-gemma-2-9B",
    "Model sha": "0291b776e80f38381788cd8f1fb2c3435ad891b5",
    "Average ‚¨ÜÔ∏è": 31.89763198194814,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 2.01725286807503,
    "IFEval Raw": 0.7831298731809377,
    "IFEval": 78.31298731809378,
    "BBH Raw": 0.6110476837383056,
    "BBH": 44.18333461079421,
    "MATH Lvl 5 Raw": 0.06873111782477341,
    "MATH Lvl 5": 6.873111782477341,
    "GPQA Raw": 0.34731543624161076,
    "GPQA": 12.975391498881436,
    "MUSR Raw": 0.4203854166666667,
    "MUSR": 11.614843750000004,
    "MMLU-PRO Raw": 0.43683510638297873,
    "MMLU-PRO": 37.42612293144209,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-25",
    "Submission Date": "2024-10-31",
    "Generation": 1,
    "Base Model": "zelk12/MT5-Gen1-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_MT5-gemma-2-9B_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/MT5-gemma-2-9B\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/MT5-gemma-2-9B</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__MT5-gemma-2-9B-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/MT5-gemma-2-9B",
    "Model sha": "b627ae7d796b1ae85b59c55e0e043b8d3ae73d83",
    "Average ‚¨ÜÔ∏è": 32.59530496884161,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.269830202109796,
    "IFEval Raw": 0.8047868544351211,
    "IFEval": 80.4786854435121,
    "BBH Raw": 0.6112225549321132,
    "BBH": 44.27125659181852,
    "MATH Lvl 5 Raw": 0.09516616314199396,
    "MATH Lvl 5": 9.516616314199396,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.4203854166666667,
    "MUSR": 11.481510416666671,
    "MMLU-PRO Raw": 0.4366688829787234,
    "MMLU-PRO": 37.4076536643026,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-19",
    "Submission Date": "2024-10-21",
    "Generation": 1,
    "Base Model": "zelk12/MT5-gemma-2-9B (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Ataraxy-9B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Ataraxy-9B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1",
    "Model sha": "b4208ddf6c741884c16c77b9433d9ead8f216354",
    "Average ‚¨ÜÔ∏è": 30.344893469651527,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 2,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.4431913479258918,
    "IFEval Raw": 0.7648949232480928,
    "IFEval": 76.48949232480928,
    "BBH Raw": 0.6074511952177571,
    "BBH": 43.706516090138706,
    "MATH Lvl 5 Raw": 0.013595166163141995,
    "MATH Lvl 5": 1.3595166163141996,
    "GPQA Raw": 0.3498322147651007,
    "GPQA": 13.31096196868009,
    "MUSR Raw": 0.41362499999999996,
    "MUSR": 10.303125000000003,
    "MMLU-PRO Raw": 0.43209773936170215,
    "MMLU-PRO": 36.89974881796691,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-03",
    "Submission Date": "2024-10-03",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25",
    "Model sha": "e652c9e07265526851dad994f4640aa265b9ab56",
    "Average ‚¨ÜÔ∏è": 33.300246308043576,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1949907185536626,
    "IFEval Raw": 0.7706651684197928,
    "IFEval": 77.06651684197928,
    "BBH Raw": 0.6075432245295168,
    "BBH": 43.85035014659934,
    "MATH Lvl 5 Raw": 0.1555891238670695,
    "MATH Lvl 5": 15.55891238670695,
    "GPQA Raw": 0.34312080536912754,
    "GPQA": 12.416107382550338,
    "MUSR Raw": 0.43226041666666665,
    "MUSR": 13.132552083333337,
    "MMLU-PRO Raw": 0.4399933510638298,
    "MMLU-PRO": 37.77703900709219,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-04",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.25 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75",
    "Model sha": "eb0e589291630ba20328db650f74af949d217a97",
    "Average ‚¨ÜÔ∏è": 28.421762158489,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.75145279542679,
    "IFEval Raw": 0.7208063493752133,
    "IFEval": 72.08063493752132,
    "BBH Raw": 0.5995203934792884,
    "BBH": 42.487153128065906,
    "MATH Lvl 5 Raw": 0.0,
    "MATH Lvl 5": 0.0,
    "GPQA Raw": 0.3498322147651007,
    "GPQA": 13.31096196868009,
    "MUSR Raw": 0.3951145833333333,
    "MUSR": 7.755989583333336,
    "MMLU-PRO Raw": 0.4140625,
    "MMLU-PRO": 34.895833333333336,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-04",
    "Submission Date": "2024-10-04",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.1-t0.75 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Ataraxy-9B-v0.2_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Ataraxy-9B-v0.2\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Ataraxy-9B-v0.2</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Ataraxy-9B-v0.2-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.2",
    "Model sha": "76f56b25bf6d8704282f8c77bfda28ca384883bc",
    "Average ‚¨ÜÔ∏è": 30.113979139003664,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.4136754381462,
    "IFEval Raw": 0.759999024809727,
    "IFEval": 75.99990248097271,
    "BBH Raw": 0.6066260664115647,
    "BBH": 43.63358839796041,
    "MATH Lvl 5 Raw": 0.012084592145015106,
    "MATH Lvl 5": 1.2084592145015105,
    "GPQA Raw": 0.34815436241610737,
    "GPQA": 13.087248322147648,
    "MUSR Raw": 0.4109583333333333,
    "MUSR": 9.836458333333338,
    "MMLU-PRO Raw": 0.43226396276595747,
    "MMLU-PRO": 36.91821808510639,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-11",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Ataraxy-9B-v0.2 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1",
    "Model sha": "1e3e623e9f0b386bfd967c629dd39c87daef5bed",
    "Average ‚¨ÜÔ∏è": 31.62637597704408,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 1,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.4617518488086505,
    "IFEval Raw": 0.7615227596111651,
    "IFEval": 76.15227596111652,
    "BBH Raw": 0.6098779556010631,
    "BBH": 43.94125829423596,
    "MATH Lvl 5 Raw": 0.07326283987915408,
    "MATH Lvl 5": 7.326283987915408,
    "GPQA Raw": 0.3414429530201342,
    "GPQA": 12.192393736017896,
    "MUSR Raw": 0.43102083333333335,
    "MUSR": 13.310937499999996,
    "MMLU-PRO Raw": 0.4315159574468085,
    "MMLU-PRO": 36.83510638297872,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Gutenberg-Doppel-9B-v0.1 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-Ifable-9B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-Ifable-9B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-Ifable-9B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-Ifable-9B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-Ifable-9B-v0.1",
    "Model sha": "8af6620b39c9a36239879b6b2bd88f66e9e9d930",
    "Average ‚¨ÜÔ∏è": 32.25442290686063,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 6.54286944477872,
    "IFEval Raw": 0.7943955371746965,
    "IFEval": 79.43955371746965,
    "BBH Raw": 0.6064399292200404,
    "BBH": 43.39057008013784,
    "MATH Lvl 5 Raw": 0.09138972809667675,
    "MATH Lvl 5": 9.138972809667676,
    "GPQA Raw": 0.35151006711409394,
    "GPQA": 13.534675615212524,
    "MUSR Raw": 0.42022916666666665,
    "MUSR": 11.0953125,
    "MMLU-PRO Raw": 0.4323470744680851,
    "MMLU-PRO": 36.927452718676115,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-Ifable-9B-v0.1 (Merge)"
  },
  {
    "eval_name": "zelk12_recoilme-gemma-2-psy10k-mental_healt-9B-v0.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "ü§ù base merges and moerges",
    "T": "ü§ù",
    "Weight type": "Original",
    "Architecture": "Gemma2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zelk12/recoilme-gemma-2-psy10k-mental_healt-9B-v0.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zelk12/recoilme-gemma-2-psy10k-mental_healt-9B-v0.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zelk12__recoilme-gemma-2-psy10k-mental_healt-9B-v0.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zelk12/recoilme-gemma-2-psy10k-mental_healt-9B-v0.1",
    "Model sha": "ced039b03be6f65ac0f713efcee76c6534e65639",
    "Average ‚¨ÜÔ∏è": 32.44806132978754,
    "Hub License": "",
    "Hub ‚ù§Ô∏è": 0,
    "#Params (B)": 10,
    "Available on the hub": false,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 3.1322204385886976,
    "IFEval Raw": 0.744536718130117,
    "IFEval": 74.45367181301171,
    "BBH Raw": 0.597759349920723,
    "BBH": 42.1326829485998,
    "MATH Lvl 5 Raw": 0.18051359516616314,
    "MATH Lvl 5": 18.051359516616312,
    "GPQA Raw": 0.34395973154362414,
    "GPQA": 12.527964205816552,
    "MUSR Raw": 0.42946875,
    "MUSR": 12.18359375,
    "MMLU-PRO Raw": 0.41805186170212766,
    "MMLU-PRO": 35.33909574468085,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-07",
    "Submission Date": "2024-10-07",
    "Generation": 1,
    "Base Model": "zelk12/recoilme-gemma-2-psy10k-mental_healt-9B-v0.1 (Merge)"
  },
  {
    "eval_name": "zetasepic_Qwen2.5-72B-Instruct-abliterated_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "Qwen2ForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zetasepic/Qwen2.5-72B-Instruct-abliterated\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zetasepic/Qwen2.5-72B-Instruct-abliterated</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zetasepic__Qwen2.5-72B-Instruct-abliterated-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zetasepic/Qwen2.5-72B-Instruct-abliterated",
    "Model sha": "af94b3c05c9857dbac73afb1cbce00e4833ec9ef",
    "Average ‚¨ÜÔ∏è": 45.293139342041435,
    "Hub License": "other",
    "Hub ‚ù§Ô∏è": 9,
    "#Params (B)": 72,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": true,
    "Flagged": false,
    "Chat Template": false,
    "CO‚ÇÇ cost (kg)": 18.809181620227683,
    "IFEval Raw": 0.7152610628687439,
    "IFEval": 71.52610628687438,
    "BBH Raw": 0.7152257183282452,
    "BBH": 59.912975835046495,
    "MATH Lvl 5 Raw": 0.46148036253776437,
    "MATH Lvl 5": 46.14803625377644,
    "GPQA Raw": 0.40687919463087246,
    "GPQA": 20.917225950782996,
    "MUSR Raw": 0.4719166666666667,
    "MUSR": 19.122916666666665,
    "MMLU-PRO Raw": 0.5871841755319149,
    "MMLU-PRO": 54.13157505910166,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-10-01",
    "Submission Date": "2024-11-08",
    "Generation": 2,
    "Base Model": "Qwen/Qwen2.5-72B"
  },
  {
    "eval_name": "zhengr_MixTAO-7Bx2-MoE-v8.1_bfloat16",
    "Precision": "bfloat16",
    "Type": "üî∂ fine-tuned on domain-specific datasets",
    "T": "üî∂",
    "Weight type": "Original",
    "Architecture": "MixtralForCausalLM",
    "Model": "<a target=\"_blank\" href=\"https://huggingface.co/zhengr/MixTAO-7Bx2-MoE-v8.1\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">zhengr/MixTAO-7Bx2-MoE-v8.1</a>  <a target=\"_blank\" href=\"https://huggingface.co/datasets/open-llm-leaderboard/zhengr__MixTAO-7Bx2-MoE-v8.1-details\" style=\"color: var(--link-text-color); text-decoration: underline;text-decoration-style: dotted;\">üìë</a>",
    "fullname": "zhengr/MixTAO-7Bx2-MoE-v8.1",
    "Model sha": "828e963abf2db0f5af9ed0d4034e538fc1cf5f40",
    "Average ‚¨ÜÔ∏è": 17.168311352749736,
    "Hub License": "apache-2.0",
    "Hub ‚ù§Ô∏è": 54,
    "#Params (B)": 12,
    "Available on the hub": true,
    "Not_Merged": true,
    "MoE": false,
    "Flagged": false,
    "Chat Template": true,
    "CO‚ÇÇ cost (kg)": 0.9273900878215878,
    "IFEval Raw": 0.4187810564856802,
    "IFEval": 41.878105648568024,
    "BBH Raw": 0.42019437560239653,
    "BBH": 19.17690717348315,
    "MATH Lvl 5 Raw": 0.06646525679758308,
    "MATH Lvl 5": 6.646525679758309,
    "GPQA Raw": 0.2986577181208054,
    "GPQA": 6.487695749440718,
    "MUSR Raw": 0.39762499999999995,
    "MUSR": 8.303124999999996,
    "MMLU-PRO Raw": 0.28465757978723405,
    "MMLU-PRO": 20.517508865248228,
    "Maintainer's Highlight": false,
    "Upload To Hub Date": "2024-02-26",
    "Submission Date": "2024-06-27",
    "Generation": 0,
    "Base Model": "zhengr/MixTAO-7Bx2-MoE-v8.1"
  }
]